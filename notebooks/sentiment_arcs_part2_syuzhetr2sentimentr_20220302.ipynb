{"cells":[{"cell_type":"markdown","metadata":{"id":"AtxyCnlCcAi1"},"source":["# **SentimentArcs (Part 2): SyuzhetR and SentimentR Models**\n","\n","* These are two Diachronic Sentiment Analysis Libraries in R that provide the basis the simplist two model famlies. These two contribute 4 Lexical and 7 Heuristic Models to SentimentArc's ensemble of over 34 models as of March 2022.\n","\n","* SyuzhetR by Matthew Jockers combines several popular lexicons as well as its own combined Syuzhet lexicon to analyse sentiment in text. A number of utilities facilitate the manipulation of text, calculation of sentiment time series and various plotting options. https://cran.r-project.org/web/packages/syuzhet/syuzhet.pdf \n","\n","* SentimentR by Tyler Rinker expands upon SyuzhetR by adding more sentiment lexicons, new emotion lexicons, profanity lexicons, heuristics and visualizations. Seven models from SentimentR form most of the Heuristic Family of models that add both custom and fixed rules to alter sentiments due to factors like negation (not happy), intensifiers (very mad), capitalization (FURIOUS), punctuation (WTF?!?!?!?!), etc.  https://cran.r-project.org/web/packages/sentimentr/sentimentr.pdf\n","\n","* SentimentArcs software consists of a series of Google Colab Jupyter notebooks in Python for ease of use, portability and low/no cost. To run these two R Libraries requires two different approaches to load R within a Python environment and exchange data structures between Python and R. The SentimentArcs Github repo contains R Scripts which will run significantly faster for those familiar with RStudio or R on the command line."]},{"cell_type":"markdown","source":["# **[RESTART RUNTIME] May be Required for these Libaries**"],"metadata":{"id":"uXK4k0k7-UsS"}},{"cell_type":"code","source":["# [RESTART RUNTIME] May be Required (only needed for Plotly)\n","\n","# Designed Security Hole in older version of PyYAML, must upgrade to use plotly\n","\n","!pip install pyyaml==5.4.1"],"metadata":{"id":"om21LaNm-QGC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# [STEP 1] Manual Configuration"],"metadata":{"id":"_Y0sLmhmSisA"}},{"cell_type":"markdown","metadata":{"id":"8vykhYcp47yD"},"source":["## [INPUT] Connect Google gDrive to this Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGSD0nWR47yD"},"outputs":[],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive')\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4tMr1i-47yE","cellView":"form"},"outputs":[],"source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/cdh/sentiment_arcs/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","#@markdown (e.g. /gdrive/MyDrive/research/sentiment_arcs/)\n","\n","\n","\n","# Step #2: Move to Parent directory of Sentiment_Arcs\n","# =======\n","parentdir_sentiment_arcs = '/'.join(Path_to_SentimentArcs.split('/')[:-2])\n","print(f'subdir_parent: {parentdir_sentiment_arcs}')\n","%cd $parentdir_sentiment_arcs\n","\n","\n","# Step #3: If project sentiment_arcs subdir does not exist, \n","#          clone it from github\n","# =======\n","import os\n","\n","if not os.path.isdir('sentiment_arcs'):\n","  # NOTE: This will not work until SentimentArcs becomes an open sourced PUBLIC repo\n","  # !git clone https://github.com/jon-chun/sentiment_arcs.git\n","\n","  # Test on open access github repo\n","  !git clone https://github.com/jon-chun/nabokov_palefire.git\n","\n","\n","# Step #4: Change into sentiment_arcs subdir\n","# =======\n","%cd ./sentiment_arcs\n","# Test on open acess github repo\n","# %cd ./nabokov_palefire\n","\n","# Step #5: Confirm contents of sentiment_arcs subdir\n","# =======\n","!ls "]},{"cell_type":"markdown","metadata":{"id":"8r3-sp6k47yE"},"source":["## [INPUT] Define Directory Tree Structure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWdjF3og47yF"},"outputs":[],"source":["#@markdown **Sentiment Arcs Directory Structure** \\\n","#@markdown \\\n","#@markdown **1. Input Directories:** \\\n","#@markdown (a) Raw textfiles in subdir: ./text_raw/(text_type)/  \\\n","#@markdown (b) Cleaned textfiles in subdir: ./text_clean/(text_type)/ \\\n","#@markdown \\\n","#@markdown **2. Output Directories** \\\n","#@markdown (1) Raw Sentiment time series datafiles and plots in subdir: ./sentiment_raw/(text_type) \\\n","#@markdown (2) Cleaned Sentiment time series datafiles and plots in subdir: ./sentiment_clean/(text_type) \\\n","#@markdown \\\n","#@markdown **Which type of texts are you analyzing?** \\\n","\n","Text_Type = \"novels\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","Corpus = \"new_texts\" #@param [\"reference_corpora\", \"new_texts\"]\n","\n","#@markdown Please check that the required textfiles and datafiles exist in the correct subdirectories before continuing.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eo0WXoFn47yF"},"outputs":[],"source":["# Define Directory CONSTANTS based On Document Type\n","\n","if Corpus == \"new_texts\":\n","  Corpus_Type = \"new\"\n","else:\n","  Corpus_Type = \"ref\"\n","\n","# Project Subdirs\n","DIR_ROOT = Path_to_SentimentArcs\n","SUBDIR_TEXT_RAW = f\"./text_raw/{Text_Type}_text_{Corpus_Type}_raw/\"\n","SUBDIR_TEXT_CLEAN = f\"./text_clean/{Text_Type}_text_{Corpus_Type}_clean/\"\n","SUBDIR_SENTIMENT_RAW = f\"./sentiment_raw/{Text_Type}_sentiment_{Corpus_Type}_raw/\"\n","SUBDIR_SENTIMENT_CLEAN = f\"./sentiment_clean/{Text_Type}_sentiment_{Corpus_Type}_clean/\"\n","SUBDIR_PLOTS = f\"./plots/\"\n","SUBDIR_DATA = f\"./data/\"\n","\n","# Destination filename to save raw sentiments \n","FNAME_SENTIMENT_RAW = f'sentiment_raw_{Text_Type}_{Corpus_Type}_syuzhetr2sentimentr.json'\n","\n","# Verify Directory Structure\n","\n","print('Verify the Directory Structure:\\n')\n","print('-------------------------------\\n')\n","\n","print(f'           [Corpus Type]: {Text_Type}\\n')\n","print(f'              [DIR_ROOT]: {DIR_ROOT}\\n')\n","print(f'       [SUBDIR_TEXT_RAW]: {SUBDIR_TEXT_RAW}\\n')\n","print(f'     [SUBDIR_TEXT_CLEAN]: {SUBDIR_TEXT_CLEAN}\\n')\n","print(f'  [SUBDIR_SENTIMENT_RAW]: {SUBDIR_SENTIMENT_RAW}\\n')\n","print(f'[SUBDIR_SENTIMENT_CLEAN]: {SUBDIR_SENTIMENT_CLEAN}\\n')\n","print(f'          [SUBDIR_PLOTS]: {SUBDIR_PLOTS}\\n')\n","print(f'           [SUBDIR_DATA]: {SUBDIR_DATA}\\n')\n","\n","print('\\n\\nVerify the Raw Sentiment Desitnation Datafile:\\n')\n","print('----------------------------------------------\\n')\n","\n","print(f'   [FNAME_SENTIMENT_RAW]: {FNAME_SENTIMENT_RAW}\\n')"]},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","\n","os.chdir(f'{DIR_ROOT}')\n","!pwd\n","print('\\n')"],"metadata":{"id":"hAaWzoLw-2mn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qcsHFqsa5KIo"},"source":["# **[STEP 2] Automatic Configuration**"]},{"cell_type":"markdown","metadata":{"id":"_R11BEJ_47yA"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KRkk4Cg47yB"},"outputs":[],"source":["# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive"]},{"cell_type":"markdown","metadata":{"id":"QJO7kLz-47yF"},"source":["## Read YAML Configuration File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syHXsIGs47yG"},"outputs":[],"source":["# Read SentimentArcs YAML Config Files for Different Corpora Types(3) and Text Files Details\n","\n","import yaml\n","\n","# Read SentimentArcs YAML Config Files on Models\n","\n","# Model in SentimentArcs Ensemble\n","with open(\"./config/models_ref_info.yaml\", \"r\") as stream:\n","  try:\n","    models_titles_dt = yaml.safe_load(stream)\n","  except yaml.YAMLError as exc:\n","    print(exc)\n","\n","if Text_Type == 'novels':\n","\n","  # Novel Text Files\n","  if Corpus == 'new_texts':\n","    # Corpus of New Novels\n","    with open(\"./config/novels_new_info.yaml\", \"r\") as stream:\n","      try:\n","        corpus_titles_dt = yaml.safe_load(stream)\n","      except yaml.YAMLError as exc:\n","        print(exc)\n","  else:\n","    # Corpus of Reference Novels\n","    with open(\"./config/novels_ref_info.yaml\", \"r\") as stream:\n","      try:\n","        corpus_titles_dt = yaml.safe_load(stream)\n","      except yaml.YAMLError as exc:\n","        print(exc)    \n","\n","elif Text_Type == 'finance':\n","\n","  # Finance Text Files\n","  if Corpus == 'new_texts':\n","    # Corpus of New Finance Texts\n","    with open(\"./config/finance_new_info.yaml\", \"r\") as stream:\n","      try:\n","        corpus_titles_dt = yaml.safe_load(stream)\n","      except yaml.YAMLError as exc:\n","        print(exc)\n","  else:\n","    # Corpus of Reference Finance Texts\n","    with open(\"./config/finance_ref_info.yaml\", \"r\") as stream:\n","      try:\n","        corpus_titles_dt = yaml.safe_load(stream)\n","      except yaml.YAMLError as exc:\n","        print(exc)\n","\n","elif Text_Type == 'social_media':\n","\n","  # Social Media Text Files\n","  if Corpus == 'new_texts':\n","    # Corpus of New Social Media Texts\n","    with open(\"./config/social_new_info.yaml\", \"r\") as stream:\n","      try:\n","        corpus_titles_dt = yaml.safe_load(stream)\n","      except yaml.YAMLError as exc:\n","        print(exc)\n","  else:\n","    # Corpus of Reference Social Media Texts\n","    with open(\"./config/social_ref_info.yaml\", \"r\") as stream:\n","      try:\n","        corpus_titles_dt = yaml.safe_load(stream)\n","      except yaml.YAMLError as exc:\n","        print(exc)\n","\n","else:\n","  \n","  print(f\"ERROR: Illegal Text_Type: {Text_Type}\\n\")\n","\n","print(f'Corpus Titles Dictionary =')\n","corpus_titles_dt.keys()\n","\n","print(f'\\n\\nThe Corpus Titles contains [{len(corpus_titles_dt.keys())} {Text_Type}] textfiles ')\n","print(f'\\nFirst Text in Corpus:')\n","print(corpus_titles_dt[next(iter(corpus_titles_dt))])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIEHc08d47yH"},"outputs":[],"source":["# Verify the Corpora Text Titles\n","\n","import json\n","\n","print(json.dumps(corpus_titles_dt, indent=2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yiZ628yfRIqf"},"outputs":[],"source":["# Verfiy all 34 Sentiment Models in SentimentArcs Ensemble\n","\n","print(json.dumps(models_titles_dt, sort_keys=True, indent=2))"]},{"cell_type":"markdown","metadata":{"id":"wk1CPCtH5KIv"},"source":["## Install Libraries: Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDMlhmDv5KIv"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","pd.set_option('max_colwidth', 100) # -1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t91wMij75KIv"},"outputs":[],"source":["from glob import glob\n","import os\n","import copy\n","# import yaml # Already done above\n","# import json # Already done above"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eieKCE997ayn"},"outputs":[],"source":["# Plotly Visualizations\n","# Note: Security Hole in default, must upgrade above\n","#       !pip install pyyaml==5.4.1\n","\n","import plotly.graph_objects as go\n","import plotly.express as px\n","import plotly"]},{"cell_type":"markdown","metadata":{"id":"k4mwR6VP5KIw"},"source":["## Setup Matplotlib Style\n","\n","* https://matplotlib.org/stable/tutorials/introductory/customizing.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7oB2z765KIw"},"outputs":[],"source":["from cycler import cycler\n","\n","colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']   \n","linestyles = ['-', '--', ':', '-.','-', '--', ':', '-.','-', '--']\n","\n","cycle = plt.cycler(\"color\", colors) + plt.cycler(\"linestyle\", linestyles)\n","\n","# View previous matplotlib configuration\n","# print('\\n Old Matplotlib Configurtion Settings:\\n')\n","# plt.rc.show\n","print('\\n\\n')\n","\n","# Update and view new matplotlib configuration\n","# print('\\n New Matplotlib Configurtion Settings:\\n')\n","myparams = {'axes.prop_cycle': cycle}\n","plt.rcParams.update(myparams)\n","\n","plt.rcParams[\"axes.titlesize\"] = 16\n","plt.rcParams['figure.figsize'] = 20,10\n","plt.rcParams[\"legend.fontsize\"] = 10\n","plt.rcParams[\"xtick.labelsize\"] = 12\n","plt.rcParams[\"ytick.labelsize\"] = 12\n","plt.rcParams[\"axes.labelsize\"] = 12\n","plt.rcParams[\"figure.titlesize\"] = 32\n","\n","# View matplotlib options\n","# plt.rcParams.keys()\n","\n","# Set matplotlib plot figure.figsize\n","new_plt_size = plt.rcParams[\"figure.figsize\"]=(20,10)\n","print(\" New figure size: \",new_plt_size)"]},{"cell_type":"markdown","metadata":{"id":"JAD4vPMO5KIy"},"source":["## Setup Seaborn Style"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TP_uoNlx5KIy"},"outputs":[],"source":["# View previous seaborn configuration\n","print('\\n Old Seaborn Configurtion Settings:\\n')\n","sns.axes_style()\n","print('\\n\\n')\n","\n","# Update and View new seaborn configuration\n","print('\\n New Seaborn Configurtion Settings:\\n')\n","# sns.set_style('white')\n","sns.set_context('paper')\n","sns.set_style('white')\n","sns.set_palette('tab10')\n","\n","# Change defaults\n","# sns.set(style='white', context='talk', palette='tab10')\n","\n","# Review Matplotlib Styles\n","# plt.style.available"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDXyVaEf5KIz"},"outputs":[],"source":["# Seaborn: Set Theme (Scale of Font)\n","\n","sns.set_theme('paper')  # paper, notebook, talk, poster\n","plt.style.use('seaborn-whitegrid')\n","\n","# Seaborn: Set Context\n","# sns.set_context(\"notebook\")\n","\n","# Seaborn: Set Style\n","# sns.set_style('ticks') # darkgrid, whitegrid, dark, white, and ticks\n","\n","# Seaborn: Default Palette (Pastel?)\n","# sns.color_palette()\n","\n","# Seaborn: Set to High-Contrast Palette (more Vision Impaired Friendly)\n","# sns.set_palette('tab10')\n","# sns.color_palette()\n"]},{"cell_type":"markdown","metadata":{"id":"CenqyAnJ7NLA"},"source":["## Define Globals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlbzvFAz7M45"},"outputs":[],"source":["# Main Dictionary holding all Lexicon by Name/Key\n","\n","lexicons_dt = {}\n","\n","\n","# Test WORDS of Sentiment Analysis\n","test_words_ls =[\"Love\",\n","                \"Hate\",\n","                \"bizarre\",\n","                \"strange\",\n","                \"furious\",\n","                \"elated\",\n","                \"curious\",\n","                \"beserk\",\n","                \"gambaro\"]\n","\n","\n","# Test SENTENCES of Sentiment Analysis\n","test_sentences_ls =[\"I hate bad evil worthless Mondays.\",\n","                    \"I love Paris in the springtime\",\n","                    \"It was Wednesday.\",\n","                    \"You are a disgusting pig - I hate you.\",\n","                    \"What a delightfully funny and beautiful good man.\",\n","                    \"That was it\"]"]},{"cell_type":"markdown","metadata":{"id":"FNiPKuaJ5KI2"},"source":["## Python Utility Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IGgOqdpv5KI2"},"outputs":[],"source":["# NOTE: SentimentArcs Main datastructure is a Dictionary(Corpus) of DataFrames(Documents: rows=sentences, cols=sentiment, 1 col per model in ensemble)\n","#       This complex data structure has 2 special I/O utility functions to read/write to permanent disk storage as *.json files\n","\n","# Utility functions to read/write nested Dictionary (key=novel) of DataFrames (Cols = Model Sentiment Series) \n","\n","def write_dict_dfs(adict, out_file='sentiments.json', out_dir=SUBDIR_SENTIMENT_RAW):\n","  '''\n","  Given a Dictionary of DataFrames and optional output filename and output directory\n","  Write as nested json file\n","  '''\n","\n","  # convert dataframes into dictionaries\n","  data_dict = {\n","      key: adict[key].to_dict(orient='records') \n","      for key in adict.keys()\n","  }\n","\n","  # write to disk\n","  out_fullpath = f'{out_dir}{out_file}'\n","  print(f'Saving file to: {out_fullpath}')\n","  with open(out_fullpath, 'w') as fp:\n","    json.dump(\n","      data_dict, \n","      fp, \n","      indent=4, \n","      sort_keys=True\n","    )\n","\n","  return \n","\n","def read_dict_dfs(in_file='sentiments.json', in_dir=SUBDIR_SENTIMENT_RAW):\n","  '''\n","  Given a Dictionary of DataFrames and optional output filename and output directory\n","  Read nested json file into Dictionary of DataFrames\n","  '''\n","\n","  # read from disk\n","  in_fullpath = f'{in_dir}{in_file}'\n","  with open(in_fullpath, 'r') as fp:\n","      data_dict = json.load(fp)\n","\n","  # convert dictionaries into dataframes\n","  all_dt = {\n","      key: pd.DataFrame(data_dict[key]) \n","      for key in data_dict\n","  }\n","\n","  return all_dt"]},{"cell_type":"markdown","metadata":{"id":"uNwIAUc6uDdo"},"source":["## Install Libraries: R"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p05_XBc6uBrK"},"outputs":[],"source":["# Load Jupyter rpy2 Extension  \n","#   enables the %%R magic commands\n","\n","# !pip install rpy2\n","\n","%load_ext rpy2.ipython\n","# %reload_ext rpy2.ipython\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgF1pBDQuVKY"},"outputs":[],"source":["%%time \n","%%capture \n","%%R\n","\n","# Install Syuzhet.R, Sentiment.R and Utility Libraries\n","\n","# NOTE: 1m12s \n","#       1m05s\n","\n","install.packages(c('syuzhet', 'sentimentr', 'tidyverse', 'lexicon'))\n","\n","library(syuzhet)\n","library(sentimentr)\n","library(tidyverse)\n","library(lexicon)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUmxULxnKqQb"},"outputs":[],"source":["# Load Python libraries to exchange data with R Program Space and read R Datafiles\n","\n","import rpy2.robjects as robjects\n","from rpy2.robjects.packages import importr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQTZyrZzH-XZ"},"outputs":[],"source":["%%R\n","\n","# Verify R Kernel Session Info\n","sessionInfo()\n","\n","# Verfiy R Kernel Environment\n","# Sys.getenv"]},{"cell_type":"markdown","metadata":{"id":"f2GvYiOSIt4j"},"source":["# **[STEP 3] Read Clean Corpus Texts and any Existing Raw Sentiment Data**\n","\n","**If Exists(Previous output from this Notebook #2):**\n","<br> Read Clean Corpus Text + SyuzhetR/SentimentR Sentiment Values\n","<br>**Else:**\n","<br>Read output from Notebook #1: Just Clean Corpus Texts"]},{"cell_type":"code","source":["print(f'Trying to find EXISTING Raw Sentiment Datafile:\\n  [{FNAME_SENTIMENT_RAW}]\\n  in Raw Sentiment Subdir: {SUBDIR_SENTIMENT_RAW}\\n')\n","\n","corpus_texts_dt = {}\n","subdir_sentiment_raw_ls = os.listdir(SUBDIR_SENTIMENT_RAW)\n","\n","if FNAME_SENTIMENT_RAW not in subdir_sentiment_raw_ls:\n","  # No Existing Raw Sentiment Data, Just read in Clean Text\n","  print(f'No EXISTING Raw Sentiment Datafile exists, Start from Stratch...\\n')\n","\n","  # Create a List (preprocessed_ls) of all preprocessed text files\n","  try:\n","    preprocessed_ls = glob(f'{SUBDIR_TEXT_CLEAN}*.csv')\n","    preprocessed_ls = [x.split('/')[-1] for x in preprocessed_ls]\n","    preprocessed_ls = [x.split('.')[0] for x in preprocessed_ls]\n","  except IndexError:\n","    raise RuntimeError('No csv file found')\n","\n","  # Read all preprocessed text files into master DataFrame (corpus_dt)\n","  for i,anovel in enumerate(preprocessed_ls):\n","    print(f'Processing #{i}: {anovel}...')\n","    afile_fullpath = f'{SUBDIR_TEXT_CLEAN}{anovel}.csv'\n","    print(f'               {afile_fullpath}')\n","    anovel_df = pd.read_csv(afile_fullpath, index_col=[0])\n","    corpus_texts_dt[anovel] = anovel_df\n","\n","  print('\\n'.join(preprocessed_ls))\n","  print('\\n')\n","  print(f'Found {len(preprocessed_ls)} Preprocessed files in {SUBDIR_TEXT_CLEAN}')\n","else:\n","  # Found Existing Raw Sentiment Data with Clean Text, Read Both\n","  print(f'Found EXISTING  Raw Sentiment Datafile exists, Loading...\\n')\n","  corpus_texts_dt = read_dict_dfs(in_file=FNAME_SENTIMENT_RAW, in_dir=SUBDIR_SENTIMENT_RAW)\n","  print(f'The  Models have Analyzed these Texts in the Corpus:\\n\\n  {corpus_texts_dt.keys()}\\n\\n')    \n","\n","# Verify Corpus DataFrame\n","corpus_titles_ls = list(corpus_texts_dt.keys())\n","# corpus_texts_dt[corpus_titles_ls[0]].head()\n","corpus_texts_dt[corpus_titles_ls[0]].info()"],"metadata":{"id":"msLsEF8KBEbr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GuoJERbI0wEJ"},"source":["# **[STEP 4] SyuzhetR (4 Lexicon Models)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h94o_8qOAINH"},"outputs":[],"source":["# Verify text_clean of first Text\n","\n","corpus_texts_dt[corpus_titles_ls[0]]['text_clean'].to_list()[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDAb6bHqya1-"},"outputs":[],"source":["%%time\n","\n","# Compute Sentiments from all 4 Syuzhet Models applied to all 32 Novels (4 x 32 = 128 runs)\n","\n","# NOTE:  9m45s 23:30 on 20220114 Colab Pro (33 Novels)\n","#       28:32s 21:06 on 20220226 Colab Pro (33 Novels)\n","#        3m20s 19:11 on 20220217 Colab Pro (2 Novels)\n","#        3m05s 19:17 on 20220217 Colab Pro (2 Novels)\n","\n","# base = importr('base')\n","syuzhet = importr('syuzhet')\n","\n","# corpus_syuzhetr_dt = {}\n","\n","# base.rank(0, na_last = True)\n","texts_titles_ls = list(corpus_texts_dt.keys())\n","texts_titles_ls.sort()\n","for i, anovel in enumerate(texts_titles_ls):\n","  print(f'Processing Novel #{i}: {anovel}...')\n","  corpus_texts_dt[anovel]['syuzhetr_syuzhet'] = syuzhet.get_sentiment(corpus_texts_dt[anovel]['text_clean'].to_list(), method='syuzhet')\n","  corpus_texts_dt[anovel]['syuzhetr_bing'] = syuzhet.get_sentiment(corpus_texts_dt[anovel]['text_clean'].to_list(), method='bing')\n","  corpus_texts_dt[anovel]['syuzhetr_afinn'] = syuzhet.get_sentiment(corpus_texts_dt[anovel]['text_clean'].to_list(), method='afinn')\n","  corpus_texts_dt[anovel]['syuzhetr_nrc'] = syuzhet.get_sentiment(corpus_texts_dt[anovel]['text_clean'].to_list(), method='nrc')"]},{"cell_type":"code","source":["# Verify First Text in Corpus has New SyuzhetR Columns with Plausible Values\n","\n","corpus_texts_dt[corpus_titles_ls[0]].head()\n","\n","corpus_texts_dt[corpus_titles_ls[0]].info()"],"metadata":{"id":"U9fjy5dzrgg4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"07ygIQkxHEJi"},"source":["## Checkpoint: Save Sentiment"]},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","!pwd"],"metadata":{"id":"vxoOwu5_HEJj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfqXqTB1HEJk"},"outputs":[],"source":["# Save sentiment values to subdir_sentiments\n","# save_filename = f'sentiment_raw_{Corpus_Type}_{Text_Type}_dnn2transformer.json'\n","write_dict_dfs(corpus_texts_dt, out_file=FNAME_SENTIMENT_RAW, out_dir=SUBDIR_SENTIMENT_RAW)\n","print(f'Saving Text_Type: {Text_Type}')\n","print(f'     Corpus_Type: {Corpus_Type}')\n","print('\\n')\n","\n","# Verify Dictionary was saved correctly by reading back the *.json datafile\n","test_dt = read_dict_dfs(in_file=FNAME_SENTIMENT_RAW, in_dir=SUBDIR_SENTIMENT_RAW)\n","print(f'These Text Titles:')\n","test_dt.keys()\n","print('\\n')\n","\n","corpus_texts_dt[corpus_titles_ls[0]].head()\n","print('\\n')\n","\n","test_dt[corpus_titles_ls[0]].info()\n"]},{"cell_type":"markdown","metadata":{"id":"nbQbIO9iqHUo"},"source":["## Plot SyuzhetR 4 Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wxfVwdKvudRa"},"outputs":[],"source":["#@markdown Select option to save plots:\n","Save_Raw_Plots = True #@param {type:\"boolean\"}\n","\n","Save_Smooth_Plots = True #@param {type:\"boolean\"}\n","Resolution = \"300\" #@param [\"100\", \"300\"]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y5TXvTTAsPur"},"outputs":[],"source":["# Get Col Names for all 4 SyuzhetR Models\n","\n","cols_all_ls = corpus_texts_dt[next(iter(corpus_texts_dt))].columns\n","\n","cols_syuzhetr_ls = [x for x in cols_all_ls if 'syuzhetr_' in x]\n","cols_syuzhetr_ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lkWMeKU2BnK"},"outputs":[],"source":["# Verify 4 SyuzhetR Models with Plots\n","\n","for i, anovel in enumerate(list(corpus_texts_dt.keys())):\n","\n","  print(f'Novel #{i}: {corpus_titles_dt[anovel][0]}')\n","\n","  # Raw Sentiments \n","  fig = corpus_texts_dt[anovel][cols_syuzhetr_ls].plot(title=f'{corpus_titles_dt[anovel][0]}\\n SyuzhetR 4 Models: Raw Sentiments', alpha=0.3)\n","  # plt.show();\n","\n","  if Save_Raw_Plots:\n","    save_filename = f'{SUBDIR_PLOTS}plot_syuzhetr_raw_{anovel}_dpi{Resolution}.png'\n","    print(f'\\n\\nSaving to: {save_filename}')\n","    plt.savefig(save_filename, dpi=int(Resolution))\n","\n","  \n","  # Smoothed Sentiments (SMA 10%)\n","  # novel_sample = 'cdickens_achristmascarol'\n","  win_10per = int(corpus_texts_dt[anovel].shape[0] * 0.1)\n","  corpus_texts_dt[anovel][cols_syuzhetr_ls].rolling(win_10per, center=True, min_periods=0).mean().plot(title=f'{corpus_titles_dt[anovel][0]}\\n SyuzhetR 4 Models: Smoothed Sentiments (SMA 10%)', alpha=0.3)\n","  # plt.show();\n","\n","  if Save_Smooth_Plots:\n","    save_filename = f'{SUBDIR_PLOTS}plot_syuzhetr_smooth10sma_{anovel}_dpi{Resolution}.png'\n","    print(f'\\n\\nSaving to: {save_filename}')\n","    plt.savefig(save_filename, dpi=int(Resolution))\n"]},{"cell_type":"markdown","metadata":{"id":"yl8og94l09WX"},"source":["# **[STEP 5] SentimentR (7 Heuristic Models)**"]},{"cell_type":"code","source":["SUBDIR_"],"metadata":{"id":"eylr2KEJIFXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory and cd into ./utils for R programs\n","\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","%cd ./utils\n","!pwd"],"metadata":{"id":"oLsGmEEvHhKo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjytKCXPjZ_o"},"outputs":[],"source":["%%file get_sentimentr.R\n","\n","library(sentimentr)\n","library(lexicon)\n","\n","get_sentimentr_values <- function(s_v) {\n","  \n","  print('Processing sentimentr_jockersrinker')\n","  sentimentr_jockersrinker <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_jockers_rinker, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_jockers')\n","  sentimentr_jockers <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_jockers, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_huliu')\n","  sentimentr_huliu <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_huliu, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_nrc')\n","  sentimentr_nrc <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_nrc, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_senticnet')\n","  sentimentr_senticnet <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_senticnet, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_sentiword')\n","  sentimentr_sentiword <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_sentiword, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_loughran_mcdonald')\n","  sentimentr_loughran_mcdonald <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_loughran_mcdonald, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_socal_google')\n","  sentimentr_socal_google <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_socal_google, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  anovel_sentimentr_df <- data.frame('text_clean' = s_v,\n","                                'sentimentr_jockersrinker' = sentimentr_jockersrinker$sentiment,\n","                                'sentimentr_jockers' = sentimentr_jockers$sentiment,\n","                                'sentimentr_huliu' = sentimentr_huliu$sentiment,\n","                                'sentimentr_nrc' = sentimentr_nrc$sentiment,\n","                                'sentimentr_senticnet' = sentimentr_senticnet$sentiment,\n","                                'sentimentr_sentiword' = sentimentr_sentiword$sentiment,\n","                                'sentimentr_loughran_mcdonald' = sentimentr_loughran_mcdonald$sentiment,\n","                                'sentimentr_socal_google' = sentimentr_socal_google$sentiment\n","                                )\n","  return(anovel_sentimentr_df)\n","\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGpOb1CNkRLJ"},"outputs":[],"source":["# Setup python robject with external library::function()\n","# https://rpy2.github.io/doc/v3.0.x/html/generated_rst/pandas.html\n","\n","# import rpy2.robjects as robjects\n","\n","# Defining the R script and loading the instance in Python\n","# from rpy2.robjects import pandas2ri \n","r = robjects.r\n","\n","# Loading the function we have defined in R.\n","r['source']('get_sentimentr.R')\n","\n","# Reading and processing data\n","get_sentimentr_function_r = robjects.globalenv['get_sentimentr_values']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bYu1QKMmMBR"},"outputs":[],"source":["# Test\n","\n","# Convert Python List of Strings to a R vector of characters\n","test_ls = corpus_texts_dt[next(iter(corpus_texts_dt))]['text_clean'].to_list()\n","s_v = robjects.StrVector(test_ls)\n","type(s_v)\n","\n","get_sentimentr_function_r(s_v)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydWdrTWfxWdE"},"outputs":[],"source":["corpus_texts_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJOt0G-kyhJu"},"outputs":[],"source":["text_clean_ct = corpus_texts_dt[next(iter(corpus_texts_dt))].text_clean.isna().sum()\n","text_clean_ct\n","# len(text_clean_ls.isnull())"]},{"cell_type":"markdown","metadata":{"id":"xi-unkmAJLJl"},"source":["**[RE-EXECUTE] May have to re-execute following code cell several times**"]},{"cell_type":"code","source":["corpus_texts_dt.keys()"],"metadata":{"id":"_ht4lNv9y-HI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3EW-6zVlGxW"},"outputs":[],"source":["%%time\n","\n","# NOTE: 8m19s 13 Novels \n","#      16m39s 19 Novels\n","#     -----------------\n","#      24m58s 32 Novels\n","#       5m00s  @19:44 on 20220227 Colab Pro (2 Novels)\n","\n","# Call external get_sentimentr::get_sentimentr_values with Python loop over all novels\n","\n","# novels_sentimentr_dt = {}\n","\n","anovel_df = pd.DataFrame()\n","\n","novels_titles_ls = list(corpus_texts_dt.keys())\n","novels_titles_ls.sort()\n","# for i, anovel in enumerate(novels_titles_ls[:19]):\n","for i, anovel in enumerate(novels_titles_ls):  \n","  print(f'\\nProcessing Novel #{i}: {anovel}')\n","  \n","  # Delete contents of anovel_df DataFrame\n","  anovel_df = anovel_df[0:0]\n","\n","  print(f'     {corpus_texts_dt[anovel].shape}')\n","  # Get text_clean as list of strings\n","  text_clean_ls = corpus_texts_dt[anovel]['text_clean'].to_list()\n","\n","  # Convert Python List of Strings to a R vector of characters\n","  # https://rpy2.github.io/doc/v3.0.x/html/generated_rst/pandas.html\n","  s_v = robjects.StrVector(text_clean_ls)\n","  anovel_df_r = get_sentimentr_function_r(s_v)\n","\n","  # Convert rpy2.robjects.vectors.DataFrame to pandas.core.frame.DataFrame\n","  # https://stackoverflow.com/questions/20630121/pandas-how-to-convert-r-dataframe-back-to-pandas \n","  print(f'type(anovel_df_r): {type(anovel_df_r)}')\n","  anovel_df = pd.DataFrame.from_dict({ key : np.asarray(anovel_df_r.rx2(key)) for key in anovel_df_r.names })\n","  print(f'type(anovel_df): {type(anovel_df)}')\n","\n","  # Save Results\n","  # novels_dt[anovel] = anovel_df.copy(deep=True)\n","\n","  # This works for Novels New Corpus Texts\n","  corpus_texts_dt[anovel]['sentimentr_jockersrinker'] = anovel_df['sentimentr_jockersrinker']\n","  corpus_texts_dt[anovel]['sentimentr_jockers'] = anovel_df['sentimentr_jockers']\n","  corpus_texts_dt[anovel]['sentimentr_huliu'] = anovel_df['sentimentr_huliu']\n","  corpus_texts_dt[anovel]['sentimentr_nrc'] = anovel_df['sentimentr_nrc']\n","  corpus_texts_dt[anovel]['sentimentr_senticnet'] = anovel_df['sentimentr_senticnet']\n","  corpus_texts_dt[anovel]['sentimentr_sentiword'] = anovel_df['sentimentr_sentiword']\n","  corpus_texts_dt[anovel]['sentimentr_loughran_mcdonald'] = anovel_df['sentimentr_loughran_mcdonald']\n","  corpus_texts_dt[anovel]['sentimentr_socal_google'] = anovel_df['sentimentr_socal_google'] \n","\n","\n","\"\"\"\n","  # This works for Novels Reference Corpus Texts\n","  corpus_texts_dt[anovel]['sentimentr_jockersrinker'] = anovel_df[anovel]['sentimentr_jockersrinker']\n","  corpus_texts_dt[anovel]['sentimentr_jockers'] = anovel_df[anovel]['sentimentr_jockers']\n","  corpus_texts_dt[anovel]['sentimentr_huliu'] = anovel_df[anovel]['sentimentr_huliu']\n","  corpus_texts_dt[anovel]['sentimentr_nrc'] = anovel_df[anovel]['sentimentr_nrc']\n","  corpus_texts_dt[anovel]['sentimentr_senticnet'] = anovel_df[anovel]['sentimentr_senticnet']\n","  corpus_texts_dt[anovel]['sentimentr_sentiword'] = anovel_df[anovel]['sentimentr_sentiword']\n","  corpus_texts_dt[anovel]['sentimentr_loughran_mcdonald'] = anovel_df[anovel]['sentimentr_loughran_mcdonald']\n","  corpus_texts_dt[anovel]['sentimentr_socal_google'] = anovel_df[anovel]['sentimentr_socal_google'] \n","\"\"\" "]},{"cell_type":"code","source":["anovel_df.head()"],"metadata":{"id":"XZBP1FUb0art"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_dt[corpus_titles_ls[0]].head()"],"metadata":{"id":"QajVHfJv1zqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols_sentimentr_ls = [x for x in corpus_texts_dt[corpus_titles_ls[0]].columns if 'sentimentr_' in x]\n","cols_sentimentr_ls"],"metadata":{"id":"gE2SroN2z1iV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjtPRa6oD0Iz"},"outputs":[],"source":["# Verify DataFrame shape of first Text in Corpus\n","\n","corpus_texts_dt[corpus_titles_ls[0]].shape"]},{"cell_type":"markdown","metadata":{"id":"u_F54QyVFXyy"},"source":["## Checkpoint: Save Sentiments"]},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","!pwd"],"metadata":{"id":"RD2Gs0XrFXyz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGEVR03CFXy0"},"outputs":[],"source":["# Save sentiment values to subdir_sentiments\n","# save_filename = f'sentiment_raw_{Corpus_Type}_{Text_Type}_dnn2transformer.json'\n","write_dict_dfs(corpus_texts_dt, out_file=FNAME_SENTIMENT_RAW, out_dir=SUBDIR_SENTIMENT_RAW)\n","print(f'Saving Text_Type: {Text_Type}')\n","print(f'     Corpus_Type: {Corpus_Type}')\n","print('\\n')\n","\n","# Verify Dictionary was saved correctly by reading back the *.json datafile\n","test_dt = read_dict_dfs(in_file=FNAME_SENTIMENT_RAW, in_dir=SUBDIR_SENTIMENT_RAW)\n","print(f'These Text Titles:')\n","test_dt.keys()\n","print('\\n')\n","\n","corpus_texts_dt[corpus_titles_ls[0]].head()\n","print('\\n')\n","\n","test_dt[corpus_titles_ls[0]].info()\n"]},{"cell_type":"markdown","metadata":{"id":"8g41AtTk2OuD"},"source":["## Plot SentimentR 7 Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crebkDiI2OuE"},"outputs":[],"source":["#@markdown Select option to save plots:\n","Save_Raw_Plots = True #@param {type:\"boolean\"}\n","\n","Save_Smooth_Plots = True #@param {type:\"boolean\"}\n","Resolution = \"300\" #@param [\"100\", \"300\"]\n","\n"]},{"cell_type":"code","source":["# Get Col Names for all SentimentR Models\n","cols_all_ls = corpus_texts_dt[next(iter(corpus_texts_dt))].columns\n","cols_sentimentr_ls = [x for x in cols_all_ls if 'sentimentr_' in x]\n","cols_sentimentr_ls"],"metadata":{"id":"5Y8YMcussjxW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TW2i22s6tCaH"},"outputs":[],"source":["corpus_texts_dt[next(iter(corpus_texts_dt))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"as2x7TaotCaH"},"outputs":[],"source":["print(f'Saving Plots to SUBDIR_PLOTS: {SUBDIR_PLOTS}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aq3isNKvtCaI"},"outputs":[],"source":["# Verify 7 SentimentR Models with Plots\n","\n","for i, anovel in enumerate(list(corpus_texts_dt.keys())):\n","\n","  print(f'Novel #{i}: {corpus_titles_dt[anovel][0]}')\n","\n","  # Raw Sentiments \n","  fig = corpus_texts_dt[anovel][cols_sentimentr_ls].plot(title=f'{corpus_titles_dt[anovel][0]}\\n SentimentR 7 Models: Raw Sentiments', alpha=0.3)\n","  # plt.show();\n","\n","  if Save_Raw_Plots:\n","    save_filename = f'{SUBDIR_PLOTS}plot_sentimentr_raw_{anovel}_dpi{Resolution}.png'\n","    print(f'\\n\\nSaving to: {save_filename}')\n","    plt.savefig(save_filename, dpi=int(Resolution))\n","\n","  \n","  # Smoothed Sentiments (SMA 10%)\n","  # novel_sample = 'cdickens_achristmascarol'\n","  win_10per = int(corpus_texts_dt[anovel].shape[0] * 0.1)\n","  corpus_texts_dt[anovel][cols_sentimentr_ls].rolling(win_10per, center=True, min_periods=0).mean().plot(title=f'{corpus_titles_dt[anovel][0]}\\n SentimentR 7 Models: Smoothed Sentiments (SMA 10%)', alpha=0.3)\n","  # plt.show();\n","\n","  if Save_Smooth_Plots:\n","    save_filename = f'{SUBDIR_PLOTS}plot_sentimentr_smooth10sma_{anovel}_dpi{Resolution}.png'\n","    print(f'\\n\\nSaving to: {save_filename}')\n","    plt.savefig(save_filename, dpi=int(Resolution))\n"]},{"cell_type":"markdown","metadata":{"id":"YC3xCnGF22td"},"source":["# **END OF NOTEBOOK**"]},{"cell_type":"markdown","metadata":{"id":"Z2ADWh-aaImU"},"source":["---"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"sentiment_arcs_part2_syuzhetr2sentimentr_20220302.ipynb","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}