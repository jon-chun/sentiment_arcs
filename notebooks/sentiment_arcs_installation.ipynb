{"cells":[{"cell_type":"markdown","source":["# **SentimentArcs (Installation): Run Once to Install**\n","\n","```\n","Jon Chun\n","12 Jun 2021: Started\n","04 Mar 2022: Last Update\n","```\n","\n","* This is the first Notebook you should execute to install the SentimentArcs framework on youru Google GDrive.\n","\n","* This notebook should only be run once to copy the Github repo and create certain files. After the initial install, there is no need to run this again unless you want to completely delete and reinstall SentimentArcs from your GDrive."],"metadata":{"id":"FWHyq7wiKMvE"}},{"cell_type":"markdown","source":["# **[STEP 1] Configuration and Setup**\n","\n"],"metadata":{"id":"vgvTrI7bevn2"}},{"cell_type":"markdown","source":["## [INPUT] Connect Google gDrive to this Jupyter Notebook"],"metadata":{"id":"qkcsI681TaDM"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"2bfkqjgMiw7T","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b9de5208-7ecd-4033-fd77-bbe56ffbf5c1","executionInfo":{"status":"ok","timestamp":1646377761738,"user_tz":300,"elapsed":21817,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to attach your Google gDrive to this Colab Jupyter Notebook\n","Mounted at /gdrive\n"]}],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive')\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"lpxXvi4Xi9VP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1b73077f-b8a3-45cb-ae5f-828e4045784a","executionInfo":{"status":"ok","timestamp":1646377761739,"user_tz":300,"elapsed":11,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["subdir_parent: /gdrive/MyDrive/cdh\n","/gdrive/MyDrive/cdh\n","fatal: destination path 'nabokov_palefire' already exists and is not an empty directory.\n","/gdrive/MyDrive/cdh/nabokov_palefire\n","/gdrive/MyDrive/cdh/nabokov_palefire\n","Foreword_Text.txt  palefire_clean_parts  Poem.txt  README.md\n","/gdrive/MyDrive/cdh/sentiment_arcs\n","/gdrive/MyDrive/cdh/sentiment_arcs\n"]}],"source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/cdh/sentiment_arcs/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","#@markdown (e.g. /gdrive/MyDrive/research/sentiment_arcs/)\n","\n","\n","\n","# Step #2: Move to Parent directory of Sentiment_Arcs\n","# =======\n","parentdir_sentiment_arcs = '/'.join(Path_to_SentimentArcs.split('/')[:-2])\n","print(f'subdir_parent: {parentdir_sentiment_arcs}')\n","%cd $parentdir_sentiment_arcs\n","\n","\n","# Step #3: If project sentiment_arcs subdir does not exist, \n","#          clone it from github\n","# =======\n","import os\n","\n","if ~os.path.isdir('sentiment_arcs'):\n","  # TODO: When public, uncomment to switch to real code\n","  # !git clone https://github.com/jon-chun/sentiment_arcs.git\n","\n","  # Test on open access github repo\n","  !git clone https://github.com/jon-chun/nabokov_palefire.git\n","\n","\n","# Step #4: Change into sentiment_arcs subdir\n","# =======\n","# %cd ./sentiment_arcs\n","# Test on open acess github repo\n","%cd ./nabokov_palefire\n","\n","# Step #5: Confirm contents of sentiment_arcs subdir\n","# =======\n","!pwd\n","!ls\n","\n","# TODO: Correct when switched to live\n","%cd $Path_to_SentimentArcs\n","!pwd"]},{"cell_type":"markdown","metadata":{"id":"mGoFJmeFkTxk"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"kK8zKENjsyig","executionInfo":{"status":"ok","timestamp":1646377761740,"user_tz":300,"elapsed":6,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}}},"outputs":[],"source":["# Configure Jupyter\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"markdown","source":["# Clone SentimentArcs from Github"],"metadata":{"id":"Xt-xF-5CKw_Q"}},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","\n","os.chdir(parentdir_sentiment_arcs)\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qo9DXt47wFPg","executionInfo":{"status":"ok","timestamp":1646360814923,"user_tz":300,"elapsed":244,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}},"outputId":"2170489d-80ae-4ee9-d047-fc25e44b95b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/cdh/sentiment_arcs\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKP33KlhiLz5"},"outputs":[],"source":["!git clone https://github.com/jon-chun/sentiment_arcs...."]},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","!pwd"],"metadata":{"id":"tGdFy5hoL9Wy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Download Lexicons and Datafiles"],"metadata":{"id":"i1InpRCxLA9X"}},{"cell_type":"markdown","source":["## Public Internet Files (wget)"],"metadata":{"id":"XdRwsKH2LLLD"}},{"cell_type":"code","source":["!wget https://drive.google.com/open?id=1wVN-TYx53pbTEnkHzcpMsm9SADvowUx3"],"metadata":{"id":"HjIiimWYLAxK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Google GDrive Public Files"],"metadata":{"id":"o5fQWZ34LT8H"}},{"cell_type":"code","source":["!gdown --id 1wVN-TYx53pbTEnkHzcpMsm9SADvowUx3"],"metadata":{"id":"CnMn_ecTLs7_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Kaggle API Files"],"metadata":{"id":"Zm1XP_CWLcxx"}},{"cell_type":"code","source":["!mkdir .kaggle"],"metadata":{"id":"e0JypA2aLgLF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Upload kaggle.json account credentials\n","\n","from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"],"metadata":{"id":"e7uw6lTpLk9A"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8dH03_z7LKZw"},"outputs":[],"source":["!mv kaggle.json ~/.kaggle\n","!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhYoObo4LKaM"},"outputs":[],"source":["kaggle datasets download -d yelp-dataset/yelp-dataset"]},{"cell_type":"markdown","source":["# Global Variables and Constants"],"metadata":{"id":"-bjOgmiSK_BQ"}},{"cell_type":"markdown","source":["## get_globals.py"],"metadata":{"id":"_8dbrrMBwUkK"}},{"cell_type":"code","source":["%%writefile ./utils/get_globals.py\n","\n","# Define minimum paragraph and sentence lengths for data cleaning\n","#   any parag/sent less than these mins will be ignored/blanked\n","\n","MIN_PARAG_LEN = 10\n","MIN_SENT_LEN = 3\n","\n","# Stopwords to add and delete from default English stopword list\n","STOPWORDS_ADD_EN = ['a', 'the', 'an']\n","STOPWORDS_DEL_EN = ['jimmy', 'dean']\n","\n","# Main Dictionary holding all Lexicon by Name/Key\n","lexicons_dt = {}\n","\n","# Test WORDS of Sentiment Analysis\n","test_words_ls =[\"Love\",\n","                \"Hate\",\n","                \"bizarre\",\n","                \"strange\",\n","                \"furious\",\n","                \"elated\",\n","                \"curious\",\n","                \"beserk\",\n","                \"gambaro\"]\n","                \n","# Test SENTENCES of Sentiment Analysis\n","test_sentences_ls =[\"I hate bad evil worthless Mondays.\",\n","                    \"I love Paris in the springtime\",\n","                    \"It was Wednesday.\",\n","                    \"You are a disgusting pig - I hate you.\",\n","                    \"What a delightfully funny and beautiful good man.\",\n","                    \"That was it\"]\n","\n","\n","# Abbreviation / Slang\n","# https://www.kaggle.com/nmaguette/up-to-date-list-of-slangs-for-text-preprocessing/notebook\n","\n","SLANG_DT = {\n","    \"$\" : \" dollar \",\n","    \"€\" : \" euro \",\n","    \"4ao\" : \"for adults only\",\n","    \"a.m\" : \"before midday\",\n","    \"a3\" : \"anytime anywhere anyplace\",\n","    \"aamof\" : \"as a matter of fact\",\n","    \"acct\" : \"account\",\n","    \"adih\" : \"another day in hell\",\n","    \"afaic\" : \"as far as i am concerned\",\n","    \"afaict\" : \"as far as i can tell\",\n","    \"afaik\" : \"as far as i know\",\n","    \"afair\" : \"as far as i remember\",\n","    \"afk\" : \"away from keyboard\",\n","    \"app\" : \"application\",\n","    \"approx\" : \"approximately\",\n","    \"apps\" : \"applications\",\n","    \"asap\" : \"as soon as possible\",\n","    \"asl\" : \"age, sex, location\",\n","    \"atk\" : \"at the keyboard\",\n","    \"ave.\" : \"avenue\",\n","    \"aymm\" : \"are you my mother\",\n","    \"ayor\" : \"at your own risk\", \n","    \"b&b\" : \"bed and breakfast\",\n","    \"b+b\" : \"bed and breakfast\",\n","    \"b.c\" : \"before christ\",\n","    \"b2b\" : \"business to business\",\n","    \"b2c\" : \"business to customer\",\n","    \"b4\" : \"before\",\n","    \"b4n\" : \"bye for now\",\n","    \"b@u\" : \"back at you\",\n","    \"bae\" : \"before anyone else\",\n","    \"bak\" : \"back at keyboard\",\n","    \"bbbg\" : \"bye bye be good\",\n","    \"bbc\" : \"british broadcasting corporation\",\n","    \"bbias\" : \"be back in a second\",\n","    \"bbl\" : \"be back later\",\n","    \"bbs\" : \"be back soon\",\n","    \"be4\" : \"before\",\n","    \"bfn\" : \"bye for now\",\n","    \"blvd\" : \"boulevard\",\n","    \"bout\" : \"about\",\n","    \"brb\" : \"be right back\",\n","    \"bros\" : \"brothers\",\n","    \"brt\" : \"be right there\",\n","    \"bsaaw\" : \"big smile and a wink\",\n","    \"btw\" : \"by the way\",\n","    \"bwl\" : \"bursting with laughter\",\n","    \"c/o\" : \"care of\",\n","    \"cet\" : \"central european time\",\n","    \"cf\" : \"compare\",\n","    \"cia\" : \"central intelligence agency\",\n","    \"csl\" : \"can not stop laughing\",\n","    \"cu\" : \"see you\",\n","    \"cul8r\" : \"see you later\",\n","    \"cv\" : \"curriculum vitae\",\n","    \"cwot\" : \"complete waste of time\",\n","    \"cya\" : \"see you\",\n","    \"cyt\" : \"see you tomorrow\",\n","    \"dae\" : \"does anyone else\",\n","    \"dbmib\" : \"do not bother me i am busy\",\n","    \"diy\" : \"do it yourself\",\n","    \"dm\" : \"direct message\",\n","    \"dwh\" : \"during work hours\",\n","    \"e123\" : \"easy as one two three\",\n","    \"eet\" : \"eastern european time\",\n","    \"eg\" : \"example\",\n","    \"embm\" : \"early morning business meeting\",\n","    \"encl\" : \"enclosed\",\n","    \"encl.\" : \"enclosed\",\n","    \"etc\" : \"and so on\",\n","    \"faq\" : \"frequently asked questions\",\n","    \"fawc\" : \"for anyone who cares\",\n","    \"fb\" : \"facebook\",\n","    \"fc\" : \"fingers crossed\",\n","    \"fig\" : \"figure\",\n","    \"fimh\" : \"forever in my heart\", \n","    \"ft.\" : \"feet\",\n","    \"ft\" : \"featuring\",\n","    \"ftl\" : \"for the loss\",\n","    \"ftw\" : \"for the win\",\n","    \"fwiw\" : \"for what it is worth\",\n","    \"fyi\" : \"for your information\",\n","    \"g9\" : \"genius\",\n","    \"gahoy\" : \"get a hold of yourself\",\n","    \"gal\" : \"get a life\",\n","    \"gcse\" : \"general certificate of secondary education\",\n","    \"gfn\" : \"gone for now\",\n","    \"gg\" : \"good game\",\n","    \"gl\" : \"good luck\",\n","    \"glhf\" : \"good luck have fun\",\n","    \"gmt\" : \"greenwich mean time\",\n","    \"gmta\" : \"great minds think alike\",\n","    \"gn\" : \"good night\",\n","    \"g.o.a.t\" : \"greatest of all time\",\n","    \"goat\" : \"greatest of all time\",\n","    \"goi\" : \"get over it\",\n","    \"gps\" : \"global positioning system\",\n","    \"gr8\" : \"great\",\n","    \"gratz\" : \"congratulations\",\n","    \"gyal\" : \"girl\",\n","    \"h&c\" : \"hot and cold\",\n","    \"hp\" : \"horsepower\",\n","    \"hr\" : \"hour\",\n","    \"hrh\" : \"his royal highness\",\n","    \"ht\" : \"height\",\n","    \"ibrb\" : \"i will be right back\",\n","    \"ic\" : \"i see\",\n","    \"icq\" : \"i seek you\",\n","    \"icymi\" : \"in case you missed it\",\n","    \"idc\" : \"i do not care\",\n","    \"idgadf\" : \"i do not give a damn fuck\",\n","    \"idgaf\" : \"i do not give a fuck\",\n","    \"idk\" : \"i do not know\",\n","    \"ie\" : \"that is\",\n","    \"i.e\" : \"that is\",\n","    \"ifyp\" : \"i feel your pain\",\n","    \"IG\" : \"instagram\",\n","    \"iirc\" : \"if i remember correctly\",\n","    \"ilu\" : \"i love you\",\n","    \"ily\" : \"i love you\",\n","    \"imho\" : \"in my humble opinion\",\n","    \"imo\" : \"in my opinion\",\n","    \"imu\" : \"i miss you\",\n","    \"iow\" : \"in other words\",\n","    \"irl\" : \"in real life\",\n","    \"j4f\" : \"just for fun\",\n","    \"jic\" : \"just in case\",\n","    \"jk\" : \"just kidding\",\n","    \"jsyk\" : \"just so you know\",\n","    \"l8r\" : \"later\",\n","    \"lb\" : \"pound\",\n","    \"lbs\" : \"pounds\",\n","    \"ldr\" : \"long distance relationship\",\n","    \"lmao\" : \"laugh my ass off\",\n","    \"lmfao\" : \"laugh my fucking ass off\",\n","    \"lol\" : \"laughing out loud\",\n","    \"ltd\" : \"limited\",\n","    \"ltns\" : \"long time no see\",\n","    \"m8\" : \"mate\",\n","    \"mf\" : \"motherfucker\",\n","    \"mfs\" : \"motherfuckers\",\n","    \"mfw\" : \"my face when\",\n","    \"mofo\" : \"motherfucker\",\n","    \"mph\" : \"miles per hour\",\n","    \"mr\" : \"mister\",\n","    \"mrw\" : \"my reaction when\",\n","    \"ms\" : \"miss\",\n","    \"mte\" : \"my thoughts exactly\",\n","    \"nagi\" : \"not a good idea\",\n","    \"nbc\" : \"national broadcasting company\",\n","    \"nbd\" : \"not big deal\",\n","    \"nfs\" : \"not for sale\",\n","    \"ngl\" : \"not going to lie\",\n","    \"nhs\" : \"national health service\",\n","    \"nrn\" : \"no reply necessary\",\n","    \"nsfl\" : \"not safe for life\",\n","    \"nsfw\" : \"not safe for work\",\n","    \"nth\" : \"nice to have\",\n","    \"nvr\" : \"never\",\n","    \"nyc\" : \"new york city\",\n","    \"oc\" : \"original content\",\n","    \"og\" : \"original\",\n","    \"ohp\" : \"overhead projector\",\n","    \"oic\" : \"oh i see\",\n","    \"omdb\" : \"over my dead body\",\n","    \"omg\" : \"oh my god\",\n","    \"omw\" : \"on my way\",\n","    \"p.a\" : \"per annum\",\n","    \"p.m\" : \"after midday\",\n","    \"pm\" : \"prime minister\",\n","    \"poc\" : \"people of color\",\n","    \"pov\" : \"point of view\",\n","    \"pp\" : \"pages\",\n","    \"ppl\" : \"people\",\n","    \"prw\" : \"parents are watching\",\n","    \"ps\" : \"postscript\",\n","    \"pt\" : \"point\",\n","    \"ptb\" : \"please text back\",\n","    \"pto\" : \"please turn over\",\n","    \"qpsa\" : \"what happens\", #\"que pasa\",\n","    \"ratchet\" : \"rude\",\n","    \"rbtl\" : \"read between the lines\",\n","    \"rlrt\" : \"real life retweet\", \n","    \"rofl\" : \"rolling on the floor laughing\",\n","    \"roflol\" : \"rolling on the floor laughing out loud\",\n","    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n","    \"rt\" : \"retweet\",\n","    \"ruok\" : \"are you ok\",\n","    \"sfw\" : \"safe for work\",\n","    \"sk8\" : \"skate\",\n","    \"smh\" : \"shake my head\",\n","    \"sq\" : \"square\",\n","    \"srsly\" : \"seriously\", \n","    \"ssdd\" : \"same stuff different day\",\n","    \"tbh\" : \"to be honest\",\n","    \"tbs\" : \"tablespooful\",\n","    \"tbsp\" : \"tablespooful\",\n","    \"tfw\" : \"that feeling when\",\n","    \"thks\" : \"thank you\",\n","    \"tho\" : \"though\",\n","    \"thx\" : \"thank you\",\n","    \"tia\" : \"thanks in advance\",\n","    \"til\" : \"today i learned\",\n","    \"tl;dr\" : \"too long i did not read\",\n","    \"tldr\" : \"too long i did not read\",\n","    \"tmb\" : \"tweet me back\",\n","    \"tntl\" : \"trying not to laugh\",\n","    \"ttyl\" : \"talk to you later\",\n","    \"u\" : \"you\",\n","    \"u2\" : \"you too\",\n","    \"u4e\" : \"yours for ever\",\n","    \"utc\" : \"coordinated universal time\",\n","    \"w/\" : \"with\",\n","    \"w/o\" : \"without\",\n","    \"w8\" : \"wait\",\n","    \"wassup\" : \"what is up\",\n","    \"wb\" : \"welcome back\",\n","    \"wtf\" : \"what the fuck\",\n","    \"wtg\" : \"way to go\",\n","    \"wtpa\" : \"where the party at\",\n","    \"wuf\" : \"where are you from\",\n","    \"wuzup\" : \"what is up\",\n","    \"wywh\" : \"wish you were here\",\n","    \"yd\" : \"yard\",\n","    \"ygtr\" : \"you got that right\",\n","    \"ynk\" : \"you never know\",\n","    \"zzz\" : \"sleeping bored and tired\"\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acC2Pj8Wsopy","executionInfo":{"status":"ok","timestamp":1646368485887,"user_tz":300,"elapsed":82,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}},"outputId":"6a949f04-072f-4fb8-fdaa-f24d9fcbac55"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting ./utils/get_globals.py\n"]}]},{"cell_type":"markdown","source":["## Create: get_subdirs.py"],"metadata":{"id":"S-xZDNmrKmMl"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3VEL8XMiLxB","executionInfo":{"status":"ok","timestamp":1646361220330,"user_tz":300,"elapsed":108,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}},"outputId":"8dab578c-51e4-4523-c057-b51fafd9496c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing ./utils/get_subdirs.py\n"]}],"source":["%%writefile ./utils/get_subdirs.py\n","\n","def get_subdirs(Corpus_Genre, Corpus_Type, NotebookModels):\n","  '''\n","  Given a two strings: Corpus, Text_type\n","  Set all global SUB/DIR constants\n","  '''\n","\n","  global FNAME_SENTIMENT_RAW\n","  global DIR_ROOT\n","  global SUBDIR_TEXT_RAW\n","  global SUBDIR_TEXT_CLEAN\n","  global SUBDIR_SENTIMENT_RAW\n","  global SUBDIR_SENTIMENT_CLEAN\n","  global SUBDIR_PLOTS\n","  global SUBDIR_DATA\n","  global SUBDIR_UTILS\n","\n","  if NotebookModels == 'syuzhetr2sentimentr':\n","    FNAME_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_syuzhetr2sentimentr.json'\n","  elif NotebookModels == 'lex2ml':\n","    FNAME_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_lex2ml.json'\n","  elif NotebookModels == 'dnn2transformers':\n","    FNAME_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_dnn2transformers.json'\n","  elif NotebookModels == 'none':\n","    FNAME_SENTIMENT_RAW = f'[NONE]'\n","  else:\n","    print(f'ERROR: Illegal value for NotebookModels: {NotebookModels}')\n","    return\n","\n","  SUBDIR_TEXT_RAW = f\"./text_raw/{Corpus_Genre}_text_{Corpus_Type}_raw/\"\n","  SUBDIR_TEXT_CLEAN = f\"./text_clean/{Corpus_Genre}_text_{Corpus_Type}_clean/\"\n","  SUBDIR_SENTIMENT_RAW = f\"./sentiment_raw/{Corpus_Genre}_sentiment_{Corpus_Type}_raw/\"\n","  SUBDIR_SENTIMENT_CLEAN = f\"./sentiment_clean/{Corpus_Genre}_sentiment_{Corpus_Type}_clean/\"\n","  SUBDIR_PLOTS = f\"./plots/\"\n","  SUBDIR_DATA = f\"./data/\"\n","  SUBDIR_UTILS = f\"./utils/\"\n","\n","  # Verify Directory Structure\n","\n","  print('Verify the Directory Structure:\\n')\n","  print('-------------------------------\\n')\n","\n","  print(f'          [Corpus Genre]: {Corpus_Genre}\\n')\n","  print(f'           [Corpus Type]: {Corpus_Type}\\n\\n')\n","\n","  print(f'   [FNAME_SENTIMENT_RAW]: {FNAME_SENTIMENT_RAW}\\n\\n')\n","\n","  print(f'       [SUBDIR_TEXT_RAW]: {SUBDIR_TEXT_RAW}\\n')\n","  print(f'     [SUBDIR_TEXT_CLEAN]: {SUBDIR_TEXT_CLEAN}\\n')\n","  print(f'  [SUBDIR_SENTIMENT_RAW]: {SUBDIR_SENTIMENT_RAW}\\n')\n","  print(f'[SUBDIR_SENTIMENT_CLEAN]: {SUBDIR_SENTIMENT_CLEAN}\\n')\n","  print(f'          [SUBDIR_PLOTS]: {SUBDIR_PLOTS}\\n')\n","  print(f'           [SUBDIR_DATA]: {SUBDIR_DATA}\\n')\n","  print(f'          [SUBDIR_UTILS]: {SUBDIR_UTILS}\\n')\n","\n","  return"]},{"cell_type":"markdown","source":["## Create: read_yaml.py"],"metadata":{"id":"X1LE8fC3Kuw_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wd9T3PiLidv2"},"outputs":[],"source":["%%writefile ./utils/read_yaml.py\n","\n","def read_yaml(Corpus_Genre, Corpus_Type):\n","  '''\n","  Given a Corpus_Genre (e.g. novels)\n","  Read and return the long-form titles for both Models and Corpus Texts\n","  '''\n","\n","  global models_titles_dt\n","  global corpus_titles_dt\n","\n","  # Read SentimentArcs YAML Config Files on Models\n","  # Model in SentimentArcs Ensemble\n","  with open(\"./config/models_ref_info.yaml\", \"r\") as stream:\n","    try:\n","      models_titles_dt = yaml.safe_load(stream)\n","    except yaml.YAMLError as exc:\n","      print(exc)\n","\n","  if Corpus_Genre == 'novels':\n","\n","    # Novel Text Files\n","    if Corpus_Type == 'new':\n","      # Corpus of New Novels\n","      with open(\"./config/novels_new_info.yaml\", \"r\") as stream:\n","        try:\n","          corpus_titles_dt = yaml.safe_load(stream)\n","        except yaml.YAMLError as exc:\n","          print(exc)\n","    else:\n","      # Corpus of Reference Novels\n","      with open(\"./config/novels_ref_info.yaml\", \"r\") as stream:\n","        try:\n","          corpus_titles_dt = yaml.safe_load(stream)\n","        except yaml.YAMLError as exc:\n","          print(exc)    \n","\n","  elif Corpus_Genre == 'finance':\n","\n","    # Finance Text Files\n","    if Corpus_Type == 'new':\n","      # Corpus of New Finance Texts\n","      with open(\"./config/finance_new_info.yaml\", \"r\") as stream:\n","        try:\n","          corpus_titles_dt = yaml.safe_load(stream)\n","        except yaml.YAMLError as exc:\n","          print(exc)\n","    else:\n","      # Corpus of Reference Finance Texts\n","      with open(\"./config/finance_ref_info.yaml\", \"r\") as stream:\n","        try:\n","          corpus_titles_dt = yaml.safe_load(stream)\n","        except yaml.YAMLError as exc:\n","          print(exc)\n","\n","  elif Corpus_Genre == 'social_media':\n","\n","    # Social Media Text Files\n","    if Corpus_Type == 'new':\n","      # Corpus of New Social Media Texts\n","      with open(\"./config/social_new_info.yaml\", \"r\") as stream:\n","        try:\n","          corpus_titles_dt = yaml.safe_load(stream)\n","        except yaml.YAMLError as exc:\n","          print(exc)\n","    else:\n","      # Corpus of Reference Social Media Texts\n","      with open(\"./config/social_ref_info.yaml\", \"r\") as stream:\n","        try:\n","          corpus_titles_dt = yaml.safe_load(stream)\n","        except yaml.YAMLError as exc:\n","          print(exc)\n","\n","  else:\n","    \n","    print(f\"ERROR: Illegal Corpus_Genre: {Corpus_Type}\\n\")\n","\n","  return"]},{"cell_type":"markdown","source":["## Map Col/Model Names"],"metadata":{"id":"wqNmADUhNRk6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ye5KMMCWhLag"},"outputs":[],"source":["# Mapping to standarize col/model names\n","\n","cols_map_dt = {'syuzhet':'syuzhetr',\n","               'huliu':'bing_sentimentr',\n","               'sentiword':'sentiword_sentimentr',\n","               'senticnet':'senticnet_sentimentr',\n","               'lmcd':'lmcd_sentimentr',\n","               'jockers':'jockers_sentimentr',\n","               'jockers_rinker':'jockersrinker_sentimentr'\n","               }\n","\n","cols_missing_ls = ['nrc_sentimentr']"]},{"cell_type":"markdown","source":["## Configure Matplotlib and Seaborn"],"metadata":{"id":"kZLyWjA7PEpq"}},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_40BWonrPwoo","executionInfo":{"status":"ok","timestamp":1646352339289,"user_tz":300,"elapsed":229,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}},"outputId":"61d38120-b51e-4819-a164-d01259eb281c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["%%writefile ./utils/config_matplotlib.py\n","\n","def config_matplotlib():\n","  '''\n","  Set configurations params for Matplotlib\n","  '''\n","\n","  global plt\n","\n","  from cycler import cycler\n","\n","  colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']   \n","  linestyles = ['-', '--', ':', '-.','-', '--', ':', '-.','-', '--']\n","\n","  cycle = plt.cycler(\"color\", colors) + plt.cycler(\"linestyle\", linestyles)\n","\n","  # View previous matplotlib configuration\n","  # print('\\n Old Matplotlib Configurtion Settings:\\n')\n","  # plt.rc.show\n","  print('\\n\\n')\n","\n","  # Update and view new matplotlib configuration\n","  # print('\\n New Matplotlib Configurtion Settings:\\n')\n","  myparams = {'axes.prop_cycle': cycle}\n","  plt.rcParams.update(myparams)\n","\n","  plt.rcParams[\"axes.titlesize\"] = 16\n","  plt.rcParams['figure.figsize'] = 20,10\n","  plt.rcParams[\"legend.fontsize\"] = 10\n","  plt.rcParams[\"xtick.labelsize\"] = 12\n","  plt.rcParams[\"ytick.labelsize\"] = 12\n","  plt.rcParams[\"axes.labelsize\"] = 12\n","  plt.rcParams[\"figure.titlesize\"] = 32\n","\n","  # View matplotlib options\n","  # plt.rcParams.keys()\n","\n","  # Set matplotlib plot figure.figsize\n","  new_plt_size = plt.rcParams[\"figure.figsize\"]=(20,10)\n","  print(\" New figure size: \",new_plt_size)\n","\n","  plt.style.use('seaborn-whitegrid')\n","  \n","  return"],"metadata":{"id":"Y08C65-nPEaM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile ./utils/config_seaborn.py\n","\n","def config_seaborn():\n","  '''\n","  Set configurations params for Seaborn\n","  '''\n","\n","  global sns\n","  # View previous seaborn configuration\n","  # print('\\n Old Seaborn Configurtion Settings:\\n')\n","  sns.axes_style()\n","  print('\\n\\n')\n","\n","  # Update and View new seaborn configuration\n","  # print('\\n New Seaborn Configurtion Settings:\\n')\n","\n","  # Change defaults\n","  # sns.set(style='white', context='talk', palette='tab10')\n","\n","  sns.set_theme('paper') # paper, notebook, talk, poster\n","  sns.set_context('paper')\n","  sns.set_style('white')    # darkgrid, whitegrid, dark, white, and ticks\n","  sns.set_palette('tab10')  # High-Contrast Palette, Vision Impaired Friendly\n","\n","  return"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MMJRTyR7PEXJ","executionInfo":{"status":"ok","timestamp":1646362672656,"user_tz":300,"elapsed":484,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}},"outputId":"637ed2fc-f0b3-492d-ee37-63ebf671762d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting ./utils/config_seaborn.py\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"SNppQcRAp8lZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create: get_model_famalies.py"],"metadata":{"id":"tlqUi7spncTf"}},{"cell_type":"code","source":["%%writefile ./utils/get_model_families.py\n","\n","def get_model_famalies(models_titles_dt):\n","  '''\n","  Given a Dict of Model Titles\n","  Return a list of lists (one for each family populated with corresponding models)\n","  '''\n","\n","  # Convenience lists for each type of model\n","\n","  # Lexicon Models\n","  models_lexicon_ls = [x[0] for x in models_titles_dt.values() if x[1] == 'lexicon']\n","  print(f'\\nThere are {len(models_lexicon_ls)} Lexicon Models')\n","  for i,amodel in enumerate(models_lexicon_ls):\n","    print(f'  Lexicon Model #{i}: {amodel}')\n","\n","  # Heuristic Models\n","  models_heuristic_ls = [x[0] for x in models_titles_dt.values() if x[1] == 'heuristic']\n","  print(f'\\nThere are {len(models_heuristic_ls)} Heuristic Models')\n","  for i,amodel in enumerate(models_heuristic_ls):\n","    print(f'  Heuristic Model #{i}: {amodel}')\n","\n","  # Traditional ML Models\n","  models_tradml_ls = [x[0] for x in models_titles_dt.values() if x[1] == 'tradml']\n","  print(f'\\nThere are {len(models_tradml_ls)} Traditional ML Models')\n","  for i,amodel in enumerate(models_tradml_ls):\n","    print(f'  Traditional ML Model #{i}: {amodel}')\n","\n","  # DNN Models\n","  models_dnn_ls = [x[0] for x in models_titles_dt.values() if x[1] == 'dnn']\n","  print(f'\\nThere are {len(models_dnn_ls)} DNN Models')\n","  for i,amodel in enumerate(models_dnn_ls):\n","    print(f'  DNN Model #{i}: {amodel}')\n","\n","  # Transformer Models\n","  models_transformer_ls = [x[0] for x in models_titles_dt.values() if x[1] == 'transformer']\n","  print(f'\\nThere are {len(models_transformer_ls)} Transformer Models')\n","  for i,amodel in enumerate(models_transformer_ls):\n","    print(f'  Transformer Model #{i}: {amodel}')\n","\n","  # All Models\n","\n","  models_ensemble_dt = {}\n","  models_ensemble_dt['lexicon'] = models_lexicon_ls\n","  models_ensemble_dt['heuristic'] = models_heuristic_ls\n","  models_ensemble_dt['ml'] = models_tradml_ls\n","  models_ensemble_dt['dnn'] = models_dnn_ls\n","  models_ensemble_dt['transformer'] = models_transformer_ls\n","\n","  print(f'\\nThere are {len(models_ensemble_dt.keys())} Total Models:')\n","  for i,amodel in enumerate(models_ensemble_dt.keys()):\n","    print(f'  Model #{i:>2}: {amodel}')\n","\n","  print(f'\\nThere are {len(models_ensemble_dt.keys())} Total Models (+1 for Ensemble Mean)')\n","\n","  return models_ensemble_dt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":525},"id":"IpBzX3LJPEUa","executionInfo":{"status":"error","timestamp":1646358896780,"user_tz":300,"elapsed":217,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}},"outputId":"272ab3a8-8137-417a-e2d7-98e516fc9a7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing ./utils/get_model_families.py\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-311ad522c022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./utils/get_model_families.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\ndef get_model_famalies(models_titles_dt):\\n  '''\\n  Given a Dict of Model Titles\\n  Return a list of lists (one for each family populated with corresponding models)\\n  '''\\n\\n  # Convenience lists for each type of model\\n\\n  # Lexicon Models\\n  models_lexicon_ls = [x[0] for x in models_titles_dt.values() if x[1] == 'lexicon']\\n  print(f'\\\\nThere are {len(models_lexicon_ls)} Lexicon Models')\\n  for i,amodel in enumerate(models_lexicon_ls):\\n    print(f'  Lexicon Model #{i}: {amodel}')\\n\\n  # Heuristic Models\\n  models_heuristic_ls = [x[0] for x in models_titles_dt.values() if x[1] == 'heuristic']\\n  print(f'\\\\nThere are {len(models_heuristic_ls)} Heuristic Models')\\n  for i,amodel in enumerate(models_heuristic_ls):\\n    print(f'  Heuristic Model #{i}: {amodel}')\\n\\n  # Traditional ML Models\\n  models_tradml_ls = [x[0] for x in models_titles_dt.values() if x[1] == 'tradml']\\n  print(f'\\\\nThere are {len(models_tradml_ls)} Traditional ML Models')\\n  for i,amodel in enumerate(models_tradml_ls):\\n    print(f'  Traditional ML Model #{i}: {amodel}')\\n\\n  # DNN Models\\n  models_dnn_ls = [x[0] for x in models_titles_dt.values() if x[1] == 'dnn']\\n  print(f'\\\\nThere are {len(models_dnn_ls)} DNN Models')\\n  for i,amodel in enumerate(models_dnn_ls):\\n    print(f'  DNN Model #{i}...\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-97>\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './utils/get_model_families.py'"]}]},{"cell_type":"markdown","source":["# Utility Functions"],"metadata":{"id":"Cp4HNjVOnDd3"}},{"cell_type":"markdown","source":["## Files"],"metadata":{"id":"xCh0HyMspboB"}},{"cell_type":"markdown","source":["### file_utils.py"],"metadata":{"id":"DRUSfXvQsPOJ"}},{"cell_type":"code","source":["%%writefile ./utils/file_utils.py\n","\n","def get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False):\n","  '''\n","  Given a required file_type(ftype:['data_clean','data_raw','plot']) and\n","    optional first_note: str inserted after Title and before (optional) SMA/Standardization info\n","            last_note: str insterted after (optional) SMA/Standardization info and before (optional) timedate stamp\n","            plot_ext: change default *.png extension of plot file\n","            no_date: don't add trailing datetime stamp to filename\n","  Generate and return a fullpath (/subdir/filename.ext) to save file to\n","  '''\n","\n","  # String with full path/filename.ext to return\n","  fname = ''\n","\n","  # Get current datetime stamp as a string\n","  if no_date:\n","    date_dt = ''\n","  else:\n","    date_dt = f'_{datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")}'\n","\n","  # Clean optional file notation if passed in\n","  if first_note:\n","    fnote_str = first_note.replace(' ', '_')\n","    fnote_str = '_'.join(fnote_str.split())\n","    fnote_str = '_'.join(fnote_str.split('.'))\n","    fnote_str = '_'.join(fnote_str.split('__'))\n","    fnote_str = fnote_str.lower()\n","\n","  if first_note:\n","    text_title_str = f'{text_title_str}_{first_note}'\n","\n","  # Option (a): Cleaned Model Data (Smoothed then Standardized)\n","  if ftype == 'data_clean':\n","    fprefix = 'sa_clean_'\n","    fname_str = f'{SUBDIR_SENTIMENT_CLEAN}{fprefix}{text_title_str}_{Model_Standardization_Method.lower()}_sma{Window_Percent}'\n","    if last_note:\n","      fname = f'{fname_str}_{last_note}{date_dt}.csv'\n","    else:\n","      fname = f'{fname_str}{date_dt}.csv'\n","\n","  # Option (b): Raw Model Data\n","  elif ftype == 'data_raw':\n","    fprefix = 'sa_raw_'\n","    fname_str = f'{SUBDIR_SENTIMENT_RAW}{fprefix}{text_title_str}'\n","    if last_note:\n","      fname = f'{fname_str}_{last_note}{date_dt}.csv'\n","    else:\n","      fname = f'{fname_str}{date_dt}.csv'\n","\n","  # Option (c): Plot Figure\n","  elif ftype == 'plot':\n","    if fig_no:\n","      fprefix = f'plot_{fig_no}_'\n","    else:\n","      fprefix = 'plot_'\n","    fname_str = f'{SUBDIR_SENTIMENT_PLOTS}{fprefix}{text_title_str}'\n","    if last_note:\n","      fname = f'{fname_str}_{last_note}{date_dt}.{plot_ext}'\n","    else:\n","      fname = f'{fname_str}{date_dt}.{plot_ext}'\n","\n","  # Option (d): Crux Text\n","  elif ftype == 'crux_text':\n","    fprefix = 'crux_'\n","    fname_str = f'{SUBDIR_SENTIMENT_CRUXES}{fprefix}{text_title_str}'\n","    if last_note:\n","      fname = f'{fname_str}_{last_note}{date_dt}.txt'\n","    else:\n","      fname = f'{fname_str}{date_dt}.txt'\n","\n","  else:\n","    print(f'ERROR: In get_fullpath() with illegal arg ftype:[{ftype}]')\n","    return f'ERROR: ftype:[{ftype}]'\n","\n","  return fname\n","\n","# --------------------------------------------------\n","def textfile2df(fullpath_str):\n","  '''\n","  Given a full path to a *.txt file\n","  Return a DataFrame with one Sentence per row\n","  '''\n","\n","  textfile_df = pd.DataFrame()\n","\n","  with open(fullpath_str,'r') as fp:\n","    content_str = fp.read() # .replace('\\n',' ')\n","\n","  sents_ls = text_str2sents(content_str)\n","\n","  textfile_df['text_raw'] = pd.Series(sents_ls)\n","\n","  return textfile_df\n","\n","# --------------------------------------------------\n","# NOTE: SentimentArcs Main datastructure is a Dictionary(Corpus) of DataFrames(Documents: rows=sentences, cols=sentiment, 1 col per model in ensemble)\n","#       This complex data structure has 2 special I/O utility functions to read/write to permanent disk storage as *.json files\n","\n","# Utility functions to read/write nested Dictionary (key=novel) of DataFrames (Cols = Model Sentiment Series) \n","\n","def write_dict_dfs(adict, out_file='sentiments.json', out_dir=SUBDIR_SENTIMENT_RAW):\n","  '''\n","  Given a Dictionary of DataFrames and optional output filename and output directory\n","  Write as nested json file\n","  '''\n","\n","  # convert dataframes into dictionaries\n","  data_dict = {\n","      key: adict[key].to_dict(orient='records') \n","      for key in adict.keys()\n","  }\n","\n","  # write to disk\n","  out_fullpath = f'{out_dir}{out_file}'\n","  print(f'Saving file to: {out_fullpath}')\n","  with open(out_fullpath, 'w') as fp:\n","    json.dump(\n","      data_dict, \n","      fp, \n","      indent=4, \n","      sort_keys=True\n","    )\n","\n","  return \n","\n","def read_dict_dfs(in_file='sentiments.json', in_dir=SUBDIR_SENTIMENT_RAW):\n","  '''\n","  Given a Dictionary of DataFrames and optional output filename and output directory\n","  Read nested json file into Dictionary of DataFrames\n","  '''\n","\n","  # read from disk\n","  in_fullpath = f'{in_dir}{in_file}'\n","  with open(in_fullpath, 'r') as fp:\n","      data_dict = json.load(fp)\n","\n","  # convert dictionaries into dataframes\n","  all_dt = {\n","      key: pd.DataFrame(data_dict[key]) \n","      for key in data_dict\n","  }\n","\n","  return all_dt\n","\n","# --------------------------------------------------\n","\n","\n","# --------------------------------------------------\n","\n","\n","# --------------------------------------------------"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DtgRfAjKp0S_","executionInfo":{"status":"ok","timestamp":1646368596745,"user_tz":300,"elapsed":94,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}},"outputId":"12e6a9df-8366-4a68-8e4d-220b106f92c6"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting ./utils/file_utils.py\n"]}]},{"cell_type":"code","source":["%%writefile ./utils/file_utils.py\n","\n","def textfile2df(fullpath_str):\n","  '''\n","  Given a full path to a *.txt file\n","  Return a DataFrame with one Sentence per row\n","  '''\n","\n","  textfile_df = pd.DataFrame()\n","\n","  with open(fullpath_str,'r') as fp:\n","    content_str = fp.read() # .replace('\\n',' ')\n","\n","  sents_ls = text_str2sents(content_str)\n","\n","  textfile_df['text_raw'] = pd.Series(sents_ls)\n","\n","  return textfile_df"],"metadata":{"id":"kYaKvH9S6I8Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Text Cleaning"],"metadata":{"id":"9UMhay_Ypc51"}},{"cell_type":"markdown","source":["### text_cleaners.py"],"metadata":{"id":"JDxF_0LWsaF9"}},{"cell_type":"code","source":["%%writefile ./utils/text_cleaners.py\n","\n","def text2lemmas(comment, lowercase, remove_stopwords):\n","    if lowercase:\n","        comment = comment.lower()\n","    comment = nlp(comment)\n","    lemmatized = list()\n","    for word in comment:\n","        lemma = word.lemma_.strip()\n","        if lemma:\n","            if not remove_stopwords or (remove_stopwords and lemma not in stopwords_ls):\n","                lemmatized.append(lemma)\n","    return \" \".join(lemmatized)\n","\n","# --------------------------------------------------\n","def text_str2sents(text_str, pysbd_only=False):\n","  '''\n","  Given a long text string (e.g. a novel) and pysbd_only flag\n","  Return a list of every Sentence defined by (a) 2+ newlines as paragraph separators, \n","                                            (b) SpaCy+PySBD Pipeline, and \n","                                            (c) Optionally, NLTK sentence tokenizer\n","  '''\n","\n","  parags_ls = []\n","  sents_ls = []\n","\n","  from pysbd.utils import PySBDFactory\n","  nlp = spacy.blank('en')\n","  nlp.add_pipe(PySBDFactory(nlp))\n","\n","  print(f'BEFORE stripping out headings len: {len(text_str)}')\n","\n","  parags_ls = re.split(r'[\\n]{2,}', text_str)\n","\n","  parags_ls = [x.strip() for x in parags_ls]\n","\n","  # Strip out non-printing characters\n","  parags_ls = [re.sub(f'[^{re.escape(string.printable)}]', '', x) for x in parags_ls]\n","\n","  # Filter out empty lines Paragraphs\n","  parags_ls = [x for x in parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n","\n","  print(f'   Parag count before processing sents: {len(parags_ls)}')\n","  # FIRST PASS at Sentence Tokenization with PySBD\n","\n","  for i, aparag in enumerate(parags_ls):\n","  \n","\n","    aparag_nonl = re.sub('[\\n]{1,}', ' ', aparag)\n","    doc = nlp(aparag_nonl)\n","    aparag_sents_pysbd_ls = list(doc.sents)\n","    print(f'pysbd found {len(aparag_sents_pysbd_ls)} Sentences in Paragraph #{i}')\n","\n","    # Strip ofaparag_sents_pysbd_lsf whitespace from Sentences\n","    aparag_sents_pysbd_ls = [str(x).strip() for x in aparag_sents_pysbd_ls]\n","\n","    # Filter out empty line Sentences\n","    aparag_sents_pysbd_ls = [x for x in aparag_sents_pysbd_ls if (len(x.strip()) > MIN_SENT_LEN)]\n","\n","    print(f'      {len(aparag_sents_pysbd_ls)} Sentences remain after cleaning')\n","\n","    sents_ls += aparag_sents_pysbd_ls\n","\n","  # (OPTIONAL) SECOND PASS as Sentence Tokenization with NLTK\n","  if pysbd_only == True:\n","    # Only do one pass of SpaCy/PySBD Sentence tokenizer\n","    # sents_ls += aparag_sents_pysbd_ls\n","    pass\n","  else:\n","    # Do second NLTK pass at Sentence tokenization if pysbd_only == False\n","    # Do second pass, tokenize again with NLTK to catch any Sentence tokenization missed by PySBD\n","    # corpus_sents_all_nltk_ls = []\n","    # sents_ls = []\n","    # aparag_sents_nltk_ls = []\n","    aparag_sents_pysbd_ls = deepcopy(sents_ls)\n","    sents_ls = []\n","    for asent in aparag_sents_pysbd_ls:\n","      print(f'Processing asent: {asent}')\n","      aparag_sents_nltk_ls = []\n","      aparag_sents_nltk_ls = sent_tokenize(asent)\n","\n","      # Strip off whitespace from Sentences\n","      aparag_sents_nltk_ls = [str(x).strip() for x in aparag_sents_nltk_ls]\n","\n","      # Filter out empty line Sentences\n","      aparag_sents_nltk_ls = [x for x in aparag_sents_nltk_ls if (len(x.strip()) > MIN_SENT_LEN)]\n","\n","      # corpus_sents_all_second_ls += aparag_sents_nltk_ls\n","\n","      sents_ls += aparag_sents_nltk_ls\n","\n","  print(f'About to return sents_ls with len = {len(sents_ls)}')\n","  \n","  return sents_ls\n","\n","# --------------------------------------------------\n","def textfile2df(fullpath_str):\n","  '''\n","  Given a full path to a *.txt file\n","  Return a DataFrame with one Sentence per row\n","  '''\n","\n","  textfile_df = pd.DataFrame()\n","\n","  with open(fullpath_str,'r') as fp:\n","    content_str = fp.read() # .replace('\\n',' ')\n","\n","  sents_ls = text_str2sents(content_str)\n","\n","  textfile_df['text_raw'] = pd.Series(sents_ls)\n","\n","  return textfile_df\n","\n","# --------------------------------------------------\n","def emojis2text(atext):\n","  for emot, text_desc in UNICODE_EMOJI.items():\n","    atext = atext.replace(emot, ' '.join(text_desc.replace(\",\", \"\").split()))\n","\n","  atext = atext.replace('_', ' ').replace(':','')\n","\n","  return atext\n","\n","# --------------------------------------------------\n","def all_emos2text(atext):\n","  '''\n","  Given a text string with embedded emojis and/or emoticons\n","  Return a expanded text string with all emojis/emoticons translated into text\n","  '''\n","\n","  # First, convert emoticons to text\n","  for emot, text_desc in EMOTICONS_EMO.items():\n","    atext = atext.replace(emot, ' ' + ' '.join(text_desc.replace(\",\", \" \").split()))\n","\n","  # Second, convert emojis to text\n","  for emot, text_desc in UNICODE_EMOJI.items():\n","    atext = atext.replace(emot, ' ' + ' '.join(text_desc.replace(\",\", \" \").split()))\n","\n","  atext = re.sub(r':([A-Za-z_]*):',r'\\1',atext)\n","  # atext = re.sub(r'([\\w]+)([_])([\\w]+)',r'\\1 \\3',atext)\n","  atext = re.sub(r'_', ' ', atext)\n","  atext = ' '.join(atext.split())\n","\n","  return atext\n","\n","# --------------------------------------------------\n","def expand_slang(astring):\n","  words_ls = []\n","  words_expanded_ls = []\n","  slang_keys = SLANG_DT.keys()\n","\n","  words_ls = astring.split()\n","  for aword in words_ls:\n","    if aword.lower() in SLANG_DT.keys():\n","      words_expanded_ls.append(SLANG_DT[aword.lower()])\n","    else:\n","      words_expanded_ls.append(aword.lower())\n","\n","  # abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n","\n","  astring_expanded = ' '.join(words_expanded_ls)\n","\n","  return astring_expanded \n","\n","# --------------------------------------------------\n","def clean_text(text_df, text_col, text_type='formal'): \n","  '''\n","  Given a DataFrame with a Text Column of raw text of type (formal, informal, tweet)\n","  Return a Series of clean texts\n","  '''\n","\n","  text_clean_ser = pd.Series()\n","\n","  # Extra processing steps for 'informal' and 'tweet' types of text\n","  if text_type in ['informal', 'tweet']:\n","\n","    # Remove URLs\n","    text_clean_ser = hero.remove_urls(text_df[text_col])\n","\n","    # Emoticons and then Emojis to Text\n","    text_clean_ser = text_clean_ser.apply(lambda x : all_emos2text(x))\n","\n","    # Expand Slang/Abbr\n","    text_clean_ser = text_clean_ser.apply(lambda x : expand_slang(x))\n","\n","  else:\n","\n","    text_clean_ser = text_df[text_col]\n","\n","\n","  # Expand Contractions\n","  text_clean_ser = text_clean_ser.apply(lambda x : contractions.fix(x))\n","\n","  # Clean text: lowercase, remove punctuation/numbers, etc\n","  # text_clean_ser = text_clean_ser.pipe(hero.clean, hero_pre_pipeline)\n","  text_clean_ser = hero.clean(text_clean_ser, pipeline = def_pipeline)\n","\n","  return text_clean_ser\n","\n","# --------------------------------------------------\n","def lemma_pipe(texts):\n","  '''\n","  Given a text string\n","  Return a text string with all tokens lemmatized using SpaCy pipe for speed\n","  Called by clean_text() with SpaCy Lemmatizer\n","  '''\n","  # https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html\n","\n","  lemma_tokens = []\n","  # for doc in nlp.pipe(docs, batch_size=32, n_process=3, disable=[\"tagger\", \"parser\", \"ner\"]):\n","  for doc in nlp.pipe(texts, batch_size=200, n_process=3, disable=[\"tagger\", \"parser\", \"ner\"]):\n","    # lemma_tokens.append([str(tok.lemma_).lower() if tok.lemma_ != '-PRON-' else str(tok.orth_).lower() for tok in doc])\n","    temp_ls = [str(tok.lemma_).lower() if tok.lemma_ != '-PRON-' else str(tok.orth_).lower() for tok in doc]\n","    lemma_tokens.append(' '.join(temp_ls))\n","\n","  return lemma_tokens\n","\n","# --------------------------------------------------\n","\n","# --------------------------------------------------"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yx6VHHrJnlIs","executionInfo":{"status":"ok","timestamp":1646364956019,"user_tz":300,"elapsed":76,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}},"outputId":"e31c5879-d60a-444a-f882-4dd20ba90b6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting ./utils/text_cleaners.py\n"]}]},{"cell_type":"code","source":["  def text_str2sents(text_str, pysbd_only=False):\n","    '''\n","    Given a long text string (e.g. a novel) and pysbd_only flag\n","    Return a list of every Sentence defined by (a) 2+ newlines as paragraph separators, \n","                                              (b) SpaCy+PySBD Pipeline, and \n","                                              (c) Optionally, NLTK sentence tokenizer\n","    '''\n","\n","    parags_ls = []\n","    sents_ls = []\n","\n","    from pysbd.utils import PySBDFactory\n","    nlp = spacy.blank('en')\n","    nlp.add_pipe(PySBDFactory(nlp))\n","\n","    print(f'BEFORE stripping out headings len: {len(text_str)}')\n","\n","    parags_ls = re.split(r'[\\n]{2,}', text_str)\n","\n","    parags_ls = [x.strip() for x in parags_ls]\n","\n","    # Strip out non-printing characters\n","    parags_ls = [re.sub(f'[^{re.escape(string.printable)}]', '', x) for x in parags_ls]\n","\n","    # Filter out empty lines Paragraphs\n","    parags_ls = [x for x in parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n","\n","    print(f'   Parag count before processing sents: {len(parags_ls)}')\n","    # FIRST PASS at Sentence Tokenization with PySBD\n","\n","    for i, aparag in enumerate(parags_ls):\n","    \n","\n","      aparag_nonl = re.sub('[\\n]{1,}', ' ', aparag)\n","      doc = nlp(aparag_nonl)\n","      aparag_sents_pysbd_ls = list(doc.sents)\n","      print(f'pysbd found {len(aparag_sents_pysbd_ls)} Sentences in Paragraph #{i}')\n","\n","      # Strip ofaparag_sents_pysbd_lsf whitespace from Sentences\n","      aparag_sents_pysbd_ls = [str(x).strip() for x in aparag_sents_pysbd_ls]\n","\n","      # Filter out empty line Sentences\n","      aparag_sents_pysbd_ls = [x for x in aparag_sents_pysbd_ls if (len(x.strip()) > MIN_SENT_LEN)]\n","\n","      print(f'      {len(aparag_sents_pysbd_ls)} Sentences remain after cleaning')\n","\n","      sents_ls += aparag_sents_pysbd_ls\n","\n","    # (OPTIONAL) SECOND PASS as Sentence Tokenization with NLTK\n","    if pysbd_only == True:\n","      # Only do one pass of SpaCy/PySBD Sentence tokenizer\n","      # sents_ls += aparag_sents_pysbd_ls\n","      pass\n","    else:\n","      # Do second NLTK pass at Sentence tokenization if pysbd_only == False\n","      # Do second pass, tokenize again with NLTK to catch any Sentence tokenization missed by PySBD\n","      # corpus_sents_all_nltk_ls = []\n","      # sents_ls = []\n","      # aparag_sents_nltk_ls = []\n","      aparag_sents_pysbd_ls = deepcopy(sents_ls)\n","      sents_ls = []\n","      for asent in aparag_sents_pysbd_ls:\n","        print(f'Processing asent: {asent}')\n","        aparag_sents_nltk_ls = []\n","        aparag_sents_nltk_ls = sent_tokenize(asent)\n","\n","        # Strip off whitespace from Sentences\n","        aparag_sents_nltk_ls = [str(x).strip() for x in aparag_sents_nltk_ls]\n","\n","        # Filter out empty line Sentences\n","        aparag_sents_nltk_ls = [x for x in aparag_sents_nltk_ls if (len(x.strip()) > MIN_SENT_LEN)]\n","\n","        # corpus_sents_all_second_ls += aparag_sents_nltk_ls\n","\n","        sents_ls += aparag_sents_nltk_ls\n","\n","    print(f'About to return sents_ls with len = {len(sents_ls)}')\n","    \n","    return sents_ls"],"metadata":{"id":"iv1rJf9n4xRl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sentiment Analysis"],"metadata":{"id":"5pz-GfZSaRox"}},{"cell_type":"markdown","source":["### get_sentiments.py"],"metadata":{"id":"J5QcfjFO1Bky"}},{"cell_type":"code","source":["%%writefile ./utils/get_sentiments.py\n","\n","\n","def get_lexsent_sentiment(asent_str, lexicon_dt):\n","  '''\n","  Given a Sentence in string form and a Lexicon Dictionary\n","  Return the Sentiment of the Sentence = Sum(Sentiment(all words))\n","  '''\n","\n","  sent_sentiment = 0\n","  asent_str = str(asent_str)\n","  word_ls = asent_str.split()\n","  for aword in word_ls:\n","    word_sentiment = lexicon_dt.get(aword)\n","    if word_sentiment != None:\n","      sent_sentiment += float(word_sentiment)\n","\n","  return sent_sentiment\n","\n","# --------------------------------------------------\n","\"\"\"\n","def lexicon_sentiment(lexicon_dt, text_str):\n","  '''\n","  Given a lexicon dict[word]=sentiment and a string\n","  Return a sentiment ('pos'|'neg') and a polarity (-1.0 to 1.0)\n","  '''\n","\n","  word_ls = text_str.split()\n","  text_polarity = 0\n","\n","  for aword in word_ls:\n","    word_sentiment = lexicon_dt.get(aword)\n","    if word_sentiment != None: #lexicon_dt.get(aword) != None:\n","      # print(f'Word: {aword} Polarity: {word_sentiment}')\n","      text_polarity += word_sentiment # lexicon_dt[aword]\n","\n","  if text_polarity > 0.0:\n","    text_sentiment = 'pos'\n","  else:\n","    text_sentiment = 'neg'\n","  \n","  # Return tuple of polarity ('positive'|'negative') and sentiment float value (-1.0 to 1.0)\n","  return text_sentiment, round(text_polarity, 4)\n","\n","# Test\n","test_str = \"I love enjoying the great outdoors!\"\n","test_tp = lexicon_sentiment(lexicon_jockersrinker_dt, test_str)\n","print(f'The Sentence: {test_str}\\n\\n  Sentiment: {test_tp[0]}\\n\\n  Polarity:  {test_tp[1]}')\n","\n","\"\"\";\n","\n","# --------------------------------------------------\n","\"\"\"\n","def pattern_discrete2continous_sentiment(text):\n","  '''\n","  Given a plain text string, give it to\n","    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n","  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n","    that approximates a normal distribution for all values\n","    In order to get more fine grained measure of overall Sentence sentiment\n","    Sentiment values will be Normalized/Standardized so absolute precision is not required\n","  '''\n","  text_sentiment_total = 0.\n","  text_ls = text.split()\n","  text_len = len(text_ls)\n","  for aword in text_ls:\n","    text_sentiment_total += pattern_sa(str(aword))[0]\n","  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+0.01)\n","\n","  return text_sentiment_norm\n","\"\"\";\n","\n","# --------------------------------------------------\n","def sent2vader_comp(asent_str):\n","  '''\n","  Given a Sentence as a text string\n","  Return a Sentiment = sum(VADER sentiments for each word)\n","  '''\n","\n","  words_ls = asent_str.split()\n","  sent_sentiment_fl = 0.0\n","\n","  for j, atest_word in enumerate(words_ls):\n","    sent_sentiment_fl += vader_analyzer.polarity_scores(atest_word.lower())['compound']\n","\n","  return sent_sentiment_fl\n","\n","# --------------------------------------------------\n","def sent2textblob(asent_str):\n","  '''\n","  Given a Sentence as a text string\n","  Return a Sentiment = sum(TextBlob sentiments for each word)\n","  '''\n","\n","  words_ls = asent_str.split()\n","  sent_sentiment_fl = 0.0\n","\n","  for j, atest_word in enumerate(words_ls):\n","    sent_sentiment_fl += TextBlob(atest_word.lower()).sentiment.polarity\n","\n","  return sent_sentiment_fl\n","\n","# --------------------------------------------------\n","def flair_sentiment(asent_str):\n","  '''\n","  Given a text string, get sentiment str using Flair (e.g. 'NEGATIVE (0.9243)') \n","  Return a floating point -1.0 to 1.0\n","  '''\n","  sentence = Sentence(asent_str)\n","  classifier.predict(sentence)\n","\n","  # print(f'   Sentence: {atest_str}')\n","  sentiment_str = str(sentence.labels[0])\n","\n","  polarity_str, polarity_val_str = sentiment_str.split()\n","\n","  pol_str = polarity_str.strip()\n","  if pol_str.strip() == \"POSITIVE\":\n","    sign_val = 1.0\n","  elif pol_str.strip() == \"NEGATIVE\":\n","    sign_val = -1.0\n","  else:\n","    print(f'ERROR: Illegal value for polarity_str: {pol_str}')\n","\n","  pol_val_str = polarity_val_str.strip()\n","  pol_val_str = pol_val_str[1:-1]\n","  pol_fl = sign_val * float(pol_val_str)\n","\n","  return pol_fl\n","\n","# --------------------------------------------------\n","def stanza_discrete2continous_sentiment(text):\n","  '''\n","  Given a plain text string, give it to\n","    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n","  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n","    that approximates a normal distribution for all values\n","    In order to get more fine grained measure of overall Sentence sentiment\n","    Sentiment values will be Normalized/Standardized so absolute precision is not required\n","  '''\n","  text_sentiment_tot = 0.\n","  text_ls = text.split()\n","  text_len = len(text_ls)\n","  for aword in text_ls:\n","    adoc = nlp(aword)\n","    for i, sentence in enumerate(adoc.sentences):\n","      text_sentiment_tot += float(sentence.sentiment)\n","  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.1)\n","\n","  return text_sentiment_norm\n","\n","# --------------------------------------------------\n","def ml_metrics(model,x,y):\n","  # https://www.kaggle.com/aditya6040/7-models-on-imdb-dataset-best-score-88-2/notebook\n","  y_pred = model.predict(x)\n","  acc = accuracy_score(y, y_pred)\n","  f1=f1_score(y, y_pred)\n","  cm=confusion_matrix(y, y_pred)\n","  report=classification_report(y,y_pred)\n","  plt.figure(figsize=(4,4))\n","  sns.heatmap(cm,annot=True,cmap='Blues',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\n","  plt.xlabel(\"Predicted\",fontsize=16)\n","  plt.ylabel(\"Actual\",fontsize=16)\n","  plt.show()\n","  print(\"\\nAccuracy: \",round(acc,2))\n","  print(\"\\nF1 Score: \",round(f1,2))\n","  print(\"\\nConfusion Matrix: \\n\",cm) # Comment out?\n","  print(\"\\nReport:\",report)\n","\n","# --------------------------------------------------\n","def lexicon_metrics(y, y_pred):\n","  acc = accuracy_score(y, y_pred)\n","  f1=f1_score(y, y_pred)\n","  cm=confusion_matrix(y, y_pred)\n","  report=classification_report(y, y_pred)\n","  plt.figure(figsize=(4,4))\n","  sns.heatmap(cm,annot=True,cmap='Blues',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\n","  plt.xlabel(\"Predicted\",fontsize=16)\n","  plt.ylabel(\"Actual\",fontsize=16)\n","  plt.show()\n","  print(\"\\nAccuracy: \",round(acc,2))\n","  print(\"\\nF1 Score: \",round(f1,2))\n","  print(\"\\nConfusion Matrix: \\n\",cm) # Comment out?\n","  print(\"\\nReport:\",report)\n","\n","# --------------------------------------------------\n","def labelscore2fl(labelscore_sentiment_ls, sa_model):\n","  '''\n","  Given the list of dict returned by RoBERTa15lg\n","  Return a floating point value for sentiment\n","  '''\n","  sentiment_fl = -99.99\n","\n","  label_str = labelscore_sentiment_ls[0]['label'].strip().lower()\n","  score_fl = float(labelscore_sentiment_ls[0]['score'])\n","\n","  # For lablels POSTIVE/POS, NEGATIVE/NEG\n","  if label_str in ['positive','pos']:\n","    sentiment_fl = score_fl\n","  elif label_str in ['negative','neg']:\n","    sentiment_fl = -1.0 * (score_fl)\n","  elif label_str in ['neutral','neu']:\n","    sentiment_fl = 0\n","\n","  # For Labels 'n Stars' where n=[1..5]\n","  elif label_str == '1 star':\n","    sentiment_fl = score_fl\n","  elif label_str == '2 stars':\n","    sentiment_fl = 1.0 + score_fl\n","  elif label_str == '3 stars':\n","    sentiment_fl = 2.0 + score_fl\n","  elif label_str == '4 stars':\n","    sentiment_fl = 3.0 + score_fl\n","  elif label_str == '5 stars':\n","    sentiment_fl = 4.0 + score_fl\n","\n","  # Else ERROR on illegal Label value\n","  else:\n","    print(f'ERROR: Illegal value for RoBERTa Label: {label_str}')\n","\n","  return sentiment_fl\n","\n","# --------------------------------------------------\n","def logitstensor2sentiment(hugseqclass_output):\n","  '''\n","  Given a Huggingface SequenceClassifierOutput logits tensor\n","  Return Sentiment and assoc softmax probability values\n","  '''\n","\n","  text_smax_ls_ls = hugseqclass_output.logits.softmax(dim=-1).tolist()\n","  text_smax_ls = text_smax_ls_ls[0]\n","  # print(type(text_smax_ls[0]))\n","  # print(f'  sMAX: {text_smax_ls}')\n","  max_val = max(text_smax_ls)            # Probability based upon logits %\n","  max_indx = text_smax_ls.index(max_val) # Sentiment (starting from 0 up)\n","  val_scale = len(text_smax_ls)\n","  # print(f'   MAX: {max_val} at indx={max_indx}')\n","\n","  return max_indx, val_scale, max_val\n","\n","\n","\n","# --------------------------------------------------"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dZ9vmXFKaRc5","executionInfo":{"status":"ok","timestamp":1646379198779,"user_tz":300,"elapsed":127,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}},"outputId":"77d48ded-22f7-4cff-d720-92ba4bbfe4e7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting ./utils/get_sentiments.py\n"]}]},{"cell_type":"markdown","source":["# R Code"],"metadata":{"id":"aUY_e2lUROba"}},{"cell_type":"code","source":["%%writefile ./utils/get_sentimentr.R\n","\n","library(sentimentr)\n","library(lexicon)\n","\n","get_sentimentr_values <- function(s_v) {\n","  \n","  print('Processing sentimentr_jockersrinker')\n","  sentimentr_jockersrinker <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_jockers_rinker, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_jockers')\n","  sentimentr_jockers <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_jockers, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_huliu')\n","  sentimentr_huliu <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_huliu, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_nrc')\n","  sentimentr_nrc <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_nrc, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_senticnet')\n","  sentimentr_senticnet <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_senticnet, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_sentiword')\n","  sentimentr_sentiword <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_sentiword, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_loughran_mcdonald')\n","  sentimentr_loughran_mcdonald <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_loughran_mcdonald, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  print('Processing sentimentr_socal_google')\n","  sentimentr_socal_google <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_socal_google, \n","                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n","                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n","\n","  anovel_sentimentr_df <- data.frame('text_clean' = s_v,\n","                                'sentimentr_jockersrinker' = sentimentr_jockersrinker$sentiment,\n","                                'sentimentr_jockers' = sentimentr_jockers$sentiment,\n","                                'sentimentr_huliu' = sentimentr_huliu$sentiment,\n","                                'sentimentr_nrc' = sentimentr_nrc$sentiment,\n","                                'sentimentr_senticnet' = sentimentr_senticnet$sentiment,\n","                                'sentimentr_sentiword' = sentimentr_sentiword$sentiment,\n","                                'sentimentr_loughran_mcdonald' = sentimentr_loughran_mcdonald$sentiment,\n","                                'sentimentr_socal_google' = sentimentr_socal_google$sentiment\n","                                )\n","  return(anovel_sentimentr_df)\n","\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDYRJWukRQ9-","executionInfo":{"status":"ok","timestamp":1646369516681,"user_tz":300,"elapsed":312,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"}},"outputId":"5a4622d9-d7fd-419e-937e-1acbce78c801"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting ./utils/get_sentimentr.R\n"]}]}],"metadata":{"colab":{"name":"sentiment_arcs_installation.ipynb","provenance":[],"collapsed_sections":["mGoFJmeFkTxk"],"toc_visible":true,"authorship_tag":"ABX9TyMuOA+wtug2DKD0gurDZps5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}