{"cells":[{"cell_type":"markdown","metadata":{"id":"ibHFmIWoU3Vx"},"source":["# **Compute Sentiments Using Python Lexical, ML, DNN and Transformers**\n","\n","By: Jon Chun\n","12 Jun 2021\n","\n","References:\n","\n","* Coming...\n","\n","TODO:\n","* Demo datafiles\n","* Error detection around Crux points context (out of bounds)\n","* lex_discrete2continous (research binary->gaussian transformation fn)\n","* Text Preprocessing hints/tips/flowchart\n","* Clearly document workflow and partition across notebooks/libraries\n","* Code review and extraction to libraries\n","* Corpus ingestion for any format\n","* XAI (mlm false peak 1717SyuzhetR/1732SentimentR/1797robertalg15 adam watches war argument at dinner) \n","* Centralize and Standardize Model name lists\n","* Normalize model SA Series lengths\n","* Standardize all SA Series with the same method\n","* Seamless report generation/file saving\n","* Get raw text from SentimentR\n","* Filter out non-printable characters\n","* Roll-over Crux-Points (SentNo+Sent/Parag) (plotly)\n","* Label/Roll-over Chapter/Sect No at Boundries\n","* Generate Report PDF/csv\n","* Option to select raw or discrete2continous transformation (Bing)\n","* Annotation functionality + Share/Collaboration of findings/reseearch\n","* clusters, centroids = kmeans1d.cluster(np.array(corpus_sentimentr_df['jockers_rinker']), k)\n","* plotly prefered library to save dynamic images: kaleido\n","* Correlation heatmaps: Justify choice of Spearman, Pearson, or other algo\n","\n","Facts:\n","* SyuzhetR vs SentimentTime Clean/Preprocess\n","* V.Woolf - To The Lighthouse\n","* SyuzhetR Clean: 3511 (SyuzhetR Preprocessed) Sentences (SentimentTime Preprocessed) 3403\n","* SentimentTime Clean: (Raw) 3402  (Clean) 3402\n","\n","\n","Preprocessing of Corpus Textfile\n","* Put headers in ALL CAPS\n","* Put \\n\\n between each CHAPTER/BOOK or SECTION header or Paragraphs\n","* Keep your format/spacing consistent\n","* Try to use utf-8 (not cp1252 (e.g. \\n <- \\n\\r)\n","* No leading blank lines, one trailing blank line at end of textfile\n","* Check for illegal, non-printable or other problematic code (e.g. curly single/double quotes)"]},{"cell_type":"markdown","metadata":{"id":"jmk-6u1fYP9b"},"source":["# **Reference Code**\n","\n","Surveys:\n","* https://github.com/prrao87/fine-grained-sentiment (20210409) Fine-grained SA (7 Models)\n","\n","\n","Other:\n","* https://github.com/annabiancajones/GA_capstone_project/blob/master/part3_mine_refine.ipynb\n","* https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6 CV"]},{"cell_type":"markdown","metadata":{"id":"43oGeYK19Pyq"},"source":["# **Installs Requiring [Restart Runtime]**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fat5hgY9yHTq"},"outputs":[],"source":["# [RESTART RUNTIME] May be Required\n","\n","!pip install flair"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mT-77aUWNOT_"},"outputs":[],"source":["# [RESTART RUNTIME] May be Required\n","\n","# Designed Security Hole in older PyYAML\n","#   must upgrade to use plotly\n","\n","!pip install pyyaml==5.4.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UF6optLIJT2i"},"outputs":[],"source":["# [RESTART RUNTIME] May be Required\n","\n","!pip install texthero"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"00aSE0js9V9z"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5nckvibLOtL"},"outputs":[],"source":["%%time\n","\n","# NOTE: \n","\n","import transformers"]},{"cell_type":"markdown","metadata":{"id":"mB542sPZq6YT"},"source":["# **[STEP 1] Configuration and Setup**"]},{"cell_type":"markdown","metadata":{"id":"_R11BEJ_47yA"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KRkk4Cg47yB"},"outputs":[],"source":["# Ignore warnings\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enIWEYpn47yC"},"outputs":[],"source":["# Configure Jupyter\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive"]},{"cell_type":"markdown","metadata":{"id":"8vykhYcp47yD"},"source":["## [INPUT] Connect Google gDrive to this Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGSD0nWR47yD"},"outputs":[],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive')\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4tMr1i-47yE"},"outputs":[],"source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/cdh/sentiment_arcs/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","#@markdown (e.g. /gdrive/MyDrive/research/sentiment_arcs/)\n","\n","\n","\n","# Step #2: Move to Parent directory of Sentiment_Arcs\n","# =======\n","parentdir_sentiment_arcs = '/'.join(Path_to_SentimentArcs.split('/')[:-2])\n","print(f'subdir_parent: {parentdir_sentiment_arcs}')\n","%cd $parentdir_sentiment_arcs\n","\n","\n","# Step #3: If project sentiment_arcs subdir does not exist, \n","#          clone it from github\n","# =======\n","import os\n","\n","if not os.path.isdir('sentiment_arcs'):\n","  # NOTE: This will not work until SentimentArcs becomes an open sourced PUBLIC repo\n","  # !git clone https://github.com/jon-chun/sentiment_arcs.git\n","\n","  # Test on open access github repo\n","  !git clone https://github.com/jon-chun/nabokov_palefire.git\n","\n","\n","# Step #4: Change into sentiment_arcs subdir\n","# =======\n","%cd ./sentiment_arcs\n","# Test on open acess github repo\n","# %cd ./nabokov_palefire\n","\n","# Step #5: Confirm contents of sentiment_arcs subdir\n","# =======\n","!ls\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMIh7Y9547yE"},"outputs":[],"source":["# [VERIFY]: Ensure that all the manually preprocessed novel are in plain text\n","#   files and file names are formatted correctly\n","\n","# %cd ../sentiment_arcs\n","!pwd\n","!ls ./text_raw"]},{"cell_type":"markdown","metadata":{"id":"8r3-sp6k47yE"},"source":["## Define Directory Tree Structure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWdjF3og47yF"},"outputs":[],"source":["#@markdown **Sentiment Arcs Directory Structure** \\\n","#@markdown \\\n","#@markdown **1. Input Directories:** \\\n","#@markdown (a) Raw textfiles in subdir: ./text_raw/(text_type)/  \\\n","#@markdown (b) Cleaned textfiles in subdir: ./text_clean/(text_type)/ \\\n","#@markdown \\\n","#@markdown **2. Output Directories** \\\n","#@markdown (1) Raw Sentiment time series datafiles and plots in subdir: ./sentiment_raw/(text_type) \\\n","#@markdown (2) Cleaned Sentiment time series datafiles and plots in subdir: ./sentiment_clean/(text_type) \\\n","#@markdown \\\n","#@markdown **Which type of texts are you analyzing?** \\\n","\n","Text_Type = \"novels\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","Corpus = \"new_texts\" #@param [\"reference_corpora\", \"new_texts\"]\n","\n","#@markdown Please check that the required textfiles and datafiles exist in the correct subdirectories before continuing.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eo0WXoFn47yF"},"outputs":[],"source":["# Create Directory CONSTANTS based On Document Type\n","\n","if Corpus == \"new_texts\":\n","  Corpus_Type = \"new\"\n","else:\n","  Corpus_Type = \"ref\"\n","\n","SUBDIR_TEXT_RAW = f\"./text_raw/{Text_Type}_text_{Corpus_Type}_raw/\"\n","SUBDIR_TEXT_CLEAN = f\"./text_clean/{Text_Type}_text_{Corpus_Type}_clean/\"\n","SUBDIR_SENTIMENT_RAW = f\"./sentiment_raw/{Text_Type}_sentiment_{Corpus_Type}_raw/\"\n","SUBDIR_SENTIMENT_CLEAN = f\"./sentiment_clean/{Text_Type}_sentiment_{Corpus_Type}_clean/\"\n","SUBDIR_PLOTS = f\"./plots/{Text_Type}_plots/\"\n","\n","# Verify Directory Structure\n","\n","print('Verify the Directory Structure:\\n')\n","print('-------------------------------\\n')\n","\n","print(f'           [Corpus Type]: {Text_Type}\\n')\n","print(f'       [SUBDIR_TEXT_RAW]: {SUBDIR_TEXT_RAW}\\n')\n","print(f'     [SUBDIR_TEXT_CLEAN]: {SUBDIR_TEXT_CLEAN}\\n')\n","print(f'  [SUBDIR_SENTIMENT_RAW]: {SUBDIR_SENTIMENT_RAW}\\n')\n","print(f'[SUBDIR_SENTIMENT_CLEAN]: {SUBDIR_SENTIMENT_CLEAN}\\n')\n","print(f'          [SUBDIR_PLOTS]: {SUBDIR_PLOTS}\\n')"]},{"cell_type":"markdown","metadata":{"id":"QJO7kLz-47yF"},"source":["## Read YAML Configuration File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64xjTPHk47yF"},"outputs":[],"source":["!pip install pyyaml\n","import yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syHXsIGs47yG"},"outputs":[],"source":["# Read SentimentArcs YAML Config Files for Different Corpora Types(3) and Text Files Details\n","\n","# Read SentimentArcs YAML Config Files on Models\n","\n","# Model in SentimentArcs Ensemble\n","with open(\"./config/models_ref_info.yaml\", \"r\") as stream:\n","  try:\n","    models_titles_dt = yaml.safe_load(stream)\n","  except yaml.YAMLError as exc:\n","    print(exc)\n","\n","if Text_Type == 'novels':\n","\n","  # Novel Text Files\n","  if Corpus == 'new_texts':\n","    # Corpus of New Novels\n","    with open(\"./config/novels_new_info.yaml\", \"r\") as stream:\n","      try:\n","        corpus_titles_dt = yaml.safe_load(stream)\n","      except yaml.YAMLError as exc:\n","        print(exc)\n","  else:\n","    # Corpus of Reference Novels\n","    with open(\"./config/novels_ref_info.yaml\", \"r\") as stream:\n","      try:\n","        corpus_titles_dt = yaml.safe_load(stream)\n","      except yaml.YAMLError as exc:\n","        print(exc)    \n","\n","elif Text_Type == 'finance':\n","\n","  # Finance Text Files\n","  if Corpus == 'new_texts':\n","    # Corpus of New Finance Texts\n","    with open(\"./config/finance_new_info.yaml\", \"r\") as stream:\n","      try:\n","        corpus_titles_dt = yaml.safe_load(stream)\n","      except yaml.YAMLError as exc:\n","        print(exc)\n","  else:\n","    # Corpus of Reference Finance Texts\n","    with open(\"./config/finance_ref_info.yaml\", \"r\") as stream:\n","      try:\n","        corpus_titles_dt = yaml.safe_load(stream)\n","      except yaml.YAMLError as exc:\n","        print(exc)\n","\n","elif Text_Type == 'social_media':\n","\n","  # Social Media Text Files\n","  if Corpus == 'new_texts':\n","    # Corpus of New Social Media Texts\n","    with open(\"./config/social_new_info.yaml\", \"r\") as stream:\n","      try:\n","        corpus_titles_dt = yaml.safe_load(stream)\n","      except yaml.YAMLError as exc:\n","        print(exc)\n","  else:\n","    # Corpus of Reference Social Media Texts\n","    with open(\"./config/social_ref_info.yaml\", \"r\") as stream:\n","      try:\n","        corpus_titles_dt = yaml.safe_load(stream)\n","      except yaml.YAMLError as exc:\n","        print(exc)\n","\n","else:\n","  \n","  print(f\"ERROR: Illegal Text_Type: {Text_Type}\\n\")\n","\n","print(f'Corpus Titles Dictionary =')\n","corpus_titles_dt.keys()\n","\n","print(f'\\n\\nThe Corpus Titles contains [{len(corpus_titles_dt.keys())} {Text_Type}] textfiles ')\n","print(f'\\nFirst Text in Corpus:')\n","print(corpus_titles_dt[next(iter(corpus_titles_dt))])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1BOAbsTYdNF"},"outputs":[],"source":["import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIEHc08d47yH"},"outputs":[],"source":["# Verify the Corpora Text Titles\n","\n","print(json.dumps(corpus_titles_dt, indent=2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yiZ628yfRIqf"},"outputs":[],"source":["# Verfiy all 34 Sentiment Models in SentimentArcs Ensemble\n","\n","print(json.dumps(models_titles_dt, sort_keys=True, indent=2))"]},{"cell_type":"markdown","metadata":{"id":"If55aLQYsk_K"},"source":["## Install Libraries: Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVNT7dGQsmw3"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","pd.set_option('max_colwidth', 100) # -1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"emaLq2QGn0rw"},"outputs":[],"source":["from glob import glob\n","import copy\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eieKCE997ayn"},"outputs":[],"source":["# Plotly Visualizations\n","\n","import plotly.graph_objects as go\n","import plotly.express as px\n","import plotly"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VoJdRfJh7FSz"},"outputs":[],"source":["# Scikit Utilities, Metrics, Pipelines and Models\n","\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB\n"]},{"cell_type":"markdown","metadata":{"id":"EA1yTaY_9Qod"},"source":["## Setup Matplotlib Style\n","\n","* https://matplotlib.org/stable/tutorials/introductory/customizing.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"68l3lkgjmOX0"},"outputs":[],"source":["plt.rcParams.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Y1s24O-S9JJX"},"outputs":[],"source":["from cycler import cycler\n","\n","colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']   \n","linestyles = ['-', '--', ':', '-.','-', '--', ':', '-.','-', '--']\n","\n","cycle = plt.cycler(\"color\", colors) + plt.cycler(\"linestyle\", linestyles)\n","\n","# View previous matplotlib configuration\n","print('\\n Old Matplotlib Configurtion Settings:\\n')\n","# plt.rc.show\n","print('\\n\\n')\n","\n","# Update and view new matplotlib configuration\n","print('\\n New Matplotlib Configurtion Settings:\\n')\n","myparams = {'axes.prop_cycle': cycle}\n","plt.rcParams.update(myparams)\n","\n","plt.rcParams[\"axes.titlesize\"] = 16\n","plt.rcParams['figure.figsize'] = 20,10\n","plt.rcParams[\"legend.fontsize\"] = 10\n","plt.rcParams[\"xtick.labelsize\"] = 12\n","plt.rcParams[\"ytick.labelsize\"] = 12\n","plt.rcParams[\"axes.labelsize\"] = 12\n","plt.rcParams[\"figure.titlesize\"] = 32\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"O9noShbNaHV1"},"outputs":[],"source":["\"\"\"\n","import matplotlib.colors as mcolors\n","\n","mcolors.TABLEAU_COLORS\n","\n","all_named_colors = {}\n","all_named_colors.update(mcolors.TABLEAU_COLORS)\n","\n","print('\\n')\n","all_named_colors.values()\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"M5A41pzM9aJY"},"outputs":[],"source":["# Set matplotlib plot figure.figsize\n","\n","new_plt_size = plt.rcParams[\"figure.figsize\"]=(20,10)\n","\n","print(\" New figure size: \",new_plt_size)"]},{"cell_type":"markdown","metadata":{"id":"7dPPrZwyIIze"},"source":["## Setup Seaborn Style"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hM3oRY-UOmzX"},"outputs":[],"source":["# View previous seaborn configuration\n","print('\\n Old Seaborn Configurtion Settings:\\n')\n","sns.axes_style()\n","print('\\n\\n')\n","\n","# Update and View new seaborn configuration\n","print('\\n New Seaborn Configurtion Settings:\\n')\n","# sns.set_style('white')\n","sns.set_context('paper')\n","sns.set_style('white')\n","sns.set_palette('tab10')\n","\n","# Change defaults\n","# sns.set(style='white', context='talk', palette='tab10')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"22vaCL09Iaja"},"outputs":[],"source":["# Seaborn: Set Theme (Scale of Font)\n","\n","sns.set_theme('paper')  # paper, notebook, talk, poster\n","\n","\n","# Seaborn: Set Context\n","# sns.set_context(\"notebook\")\n","\n","\n","\n","# Seaborn: Set Style\n","\n","# sns.set_style('ticks') # darkgrid, whitegrid, dark, white, and ticks"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"a2upm4ohIWcI"},"outputs":[],"source":["# Seaborn: Default Palette (Pastel?)\n","\n","sns.color_palette()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XpIbYX-DIWcJ"},"outputs":[],"source":["# Seaborn: Set to High-Contrast Palette (more Vision Impaired Friendly)\n","\n","sns.set_palette('tab10')\n","sns.color_palette()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"R1U4pM1ri4Te"},"outputs":[],"source":["plt.style.available"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gWOxcTgbi6Wx"},"outputs":[],"source":["plt.style.use('seaborn-whitegrid')"]},{"cell_type":"markdown","metadata":{"id":"CenqyAnJ7NLA"},"source":["## Define Globals"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QlbzvFAz7M45"},"outputs":[],"source":["# Main Dictionary holding all Lexicon by Name/Key\n","\n","lexicons_dt = {}\n","\n","\n","# Test WORDS of Sentiment Analysis\n","test_words_ls =[\"Love\",\n","                \"Hate\",\n","                \"bizarre\",\n","                \"strange\",\n","                \"furious\",\n","                \"elated\",\n","                \"curious\",\n","                \"beserk\",\n","                \"gambaro\"]\n","\n","\n","# Test SENTENCES of Sentiment Analysis\n","test_sentences_ls =[\"I hate bad evil worthless Mondays.\",\n","                    \"I love Paris in the springtime\",\n","                    \"It was Wednesday.\",\n","                    \"You are a disgusting pig - I hate you.\",\n","                    \"What a delightfully funny and beautiful good man.\",\n","                    \"That was it\"]"]},{"cell_type":"markdown","metadata":{"id":"X229IbToHwa2"},"source":["## Python Utility Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HqZri_3GHvqc"},"outputs":[],"source":["# Utility functions to read/write nested Dictionary (key=novel) of DataFrames (Cols = Model Sentiment Series) \n","\n","def write_dict_dfs(adict, out_file='sentiments.json', out_dir=SUBDIR_SENTIMENT_RAW):\n","  '''\n","  Given a Dictionary of DataFrames and optional output filename and output directory\n","  Write as nested json file\n","  '''\n","\n","  # convert dataframes into dictionaries\n","  data_dict = {\n","      key: adict[key].to_dict(orient='records') \n","      for key in adict.keys()\n","  }\n","\n","  # write to disk\n","  out_fullpath = f'{out_dir}{out_file}'\n","  print(f'Saving file to: {out_fullpath}')\n","  with open(out_fullpath, 'w') as fp:\n","    json.dump(\n","      data_dict, \n","      fp, \n","      indent=4, \n","      sort_keys=True\n","    )\n","\n","  return \n","\n","def read_dict_dfs(in_file='sentiments.json', in_dir=SUBDIR_SENTIMENT_RAW):\n","  '''\n","  Given a Dictionary of DataFrames and optional output filename and output directory\n","  Read nested json file into Dictionary of DataFrames\n","  '''\n","\n","  # read from disk\n","  in_fullpath = f'{in_dir}{in_file}'\n","  with open(in_fullpath, 'r') as fp:\n","      data_dict = json.load(fp)\n","\n","  # convert dictionaries into dataframes\n","  all_dt = {\n","      key: pd.DataFrame(data_dict[key]) \n","      for key in data_dict\n","  }\n","\n","  return all_dt"]},{"cell_type":"markdown","metadata":{"id":"fys3dkJSB656"},"source":["# **[STEP 2] Read all Preprocessed Novels**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uapoBvVxY7e9"},"outputs":[],"source":["# Verify cwd and subdir of Cleaned Corpus Texts\n","\n","print('Current Working Directory:')\n","!pwd\n","\n","print(f'\\nSubdir with all Cleaned Texts of Corpus:\\n  {SUBDIR_TEXT_CLEAN}')\n","\n","print(f'\\n\\nFilenames of Cleaned Texts:\\n')\n","!ls -1 $SUBDIR_TEXT_CLEAN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFSQhwuZY7fG"},"outputs":[],"source":["# Create a List (preprocessed_ls) of all preprocessed text files\n","\n","try:\n","    preprocessed_ls = glob(f'{SUBDIR_TEXT_CLEAN}*.csv')\n","    preprocessed_ls = [x.split('/')[-1] for x in preprocessed_ls]\n","    preprocessed_ls = [x.split('.')[0] for x in preprocessed_ls]\n","except IndexError:\n","    raise RuntimeError('No csv file found')\n","\n","print('\\n'.join(preprocessed_ls))\n","print('\\n')\n","print(f'Found {len(preprocessed_ls)} Preprocessed files in {SUBDIR_TEXT_CLEAN}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8f0-SXaY7fK"},"outputs":[],"source":["# Read all preprocessed text files into master DataFrame (corpus_dt)\n","\n","corpus_texts_dt = {}\n","\n","for i,anovel in enumerate(preprocessed_ls):\n","  print(f'Processing #{i}: {anovel}...')\n","  afile_fullpath = f'{SUBDIR_TEXT_CLEAN}{anovel}.csv'\n","  print(f'               {afile_fullpath}')\n","  anovel_df = pd.read_csv(afile_fullpath, index_col=[0])\n","  corpus_texts_dt[anovel] = anovel_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxEfH9LFY7fQ"},"outputs":[],"source":["# Verify the novels read into master Dictionary of DataFrames\n","\n","corpus_texts_dt.keys()\n","print('\\n')\n","print(f'There were {len(corpus_texts_dt)} preprocessed novels read into the Dict corpus_texts_dt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3wp7RnpY7fT"},"outputs":[],"source":["# Check if there are any Null strings in the text_clean columns\n","\n","for i, anovel in enumerate(list(corpus_texts_dt.keys())):\n","  print(f'\\nNovel #{i}: {anovel}')\n","  nan_ct = corpus_texts_dt[anovel].text_clean.isna().sum()\n","  if nan_ct > 0:\n","    print(f'      {nan_ct} Null strings in the text_clean column')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_1OX4O_Y7fW"},"outputs":[],"source":["# Fill in all the Null value of text_clean with placeholder 'empty_string'\n","\n","for i, anovel in enumerate(list(corpus_texts_dt.keys())):\n","  # print(f'Novel #{i}: {anovel}')\n","  # Fill all text_clean == Null with 'empty_string' so sentimentr::sentiment doesn't break\n","  corpus_texts_dt[anovel][corpus_texts_dt[anovel].text_clean.isna()] = 'empty_string'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JF3eD4qdY7fY"},"outputs":[],"source":["# Verify DataFrame of first Text in Corpus Dictionary\n","\n","corpus_texts_dt[next(iter(corpus_texts_dt))].head()"]},{"cell_type":"markdown","metadata":{"id":"LvpxQEbwy9zS"},"source":["# **[STEP 3] OPTIONAL: Read Previous/Partial Sentiments Computed in this Notebook**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UtvwVQx5y8mP"},"outputs":[],"source":["!ls -altr $SUBDIR_SENTIMENT_RAW"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mUKYpNvIzPfk"},"outputs":[],"source":["# Read (partially finished) Python-based Models computed in this Notebook\n","\n","pysentiment_raw = \"all_novels_new_pymodels.json\"\n","\n","corpus_texts_dt = read_dict_dfs(in_file=pysentiment_raw, in_dir=SUBDIR_SENTIMENT_RAW)\n","corpus_texts_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kb8hFu3QzO8y"},"outputs":[],"source":["corpus_titles_ls = list(corpus_texts_dt.keys())\n","\n","corpus_texts_dt[corpus_titles_ls[0]].head()\n","corpus_texts_dt[corpus_titles_ls[0]].info()"]},{"cell_type":"markdown","metadata":{"id":"GuoJERbI0wEJ"},"source":["# **[STEP 4] Get Sentiments**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubKkbRe2BCKm"},"outputs":[],"source":["# Get global list of Corpus Titles/Keys into Dictionary corpus_texts_dt\n","\n","corpus_titles_ls = list(corpus_texts_dt.keys())\n","corpus_titles_ls"]},{"cell_type":"markdown","metadata":{"id":"Aozb5eJlGa7t"},"source":["## **Model Utilities**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MRV23rqjTvR"},"outputs":[],"source":["def lexicon_sentiment(lexicon_dt, text_str):\n","  '''\n","  Given a lexicon dict[word]=sentiment and a string\n","  Return a sentiment ('pos'|'neg') and a polarity (-1.0 to 1.0)\n","  '''\n","\n","  word_ls = text_str.split()\n","  text_polarity = 0\n","\n","  for aword in word_ls:\n","    word_sentiment = lexicon_dt.get(aword)\n","    if word_sentiment != None: #lexicon_dt.get(aword) != None:\n","      # print(f'Word: {aword} Polarity: {word_sentiment}')\n","      text_polarity += word_sentiment # lexicon_dt[aword]\n","\n","  if text_polarity > 0.0:\n","    text_sentiment = 'pos'\n","  else:\n","    text_sentiment = 'neg'\n","  \n","  # Return tuple of polarity ('positive'|'negative') and sentiment float value (-1.0 to 1.0)\n","  return text_sentiment, round(text_polarity, 4)\n","\n","# Test\n","\"\"\"\n","test_str = \"I love enjoying the great outdoors!\"\n","test_tp = lexicon_sentiment(lexicon_jockersrinker_dt, test_str)\n","print(f'The Sentence: {test_str}\\n\\n  Sentiment: {test_tp[0]}\\n\\n  Polarity:  {test_tp[1]}')\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aSLqFGzPBCHB"},"outputs":[],"source":["# https://www.kaggle.com/aditya6040/7-models-on-imdb-dataset-best-score-88-2/notebook\n","\n","def metrics(model,x,y):\n","    y_pred = model.predict(x)\n","    acc = accuracy_score(y, y_pred)\n","    f1=f1_score(y, y_pred)\n","    cm=confusion_matrix(y, y_pred)\n","    report=classification_report(y,y_pred)\n","    plt.figure(figsize=(4,4))\n","    sns.heatmap(cm,annot=True,cmap='Blues',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\n","    plt.xlabel(\"Predicted\",fontsize=16)\n","    plt.ylabel(\"Actual\",fontsize=16)\n","    plt.show()\n","    print(\"\\nAccuracy: \",round(acc,2))\n","    print(\"\\nF1 Score: \",round(f1,2))\n","#     print(\"\\nConfusion Matrix: \\n\",cm)\n","    print(\"\\nReport:\",report)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATqWwSe0wYpA"},"outputs":[],"source":["def lexicon_metrics(y, y_pred):\n","    acc = accuracy_score(y, y_pred)\n","    f1=f1_score(y, y_pred)\n","    cm=confusion_matrix(y, y_pred)\n","    report=classification_report(y, y_pred)\n","    plt.figure(figsize=(4,4))\n","    sns.heatmap(cm,annot=True,cmap='Blues',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\n","    plt.xlabel(\"Predicted\",fontsize=16)\n","    plt.ylabel(\"Actual\",fontsize=16)\n","    plt.show()\n","    print(\"\\nAccuracy: \",round(acc,2))\n","    print(\"\\nF1 Score: \",round(f1,2))\n","#     print(\"\\nConfusion Matrix: \\n\",cm)\n","    print(\"\\nReport:\",report)"]},{"cell_type":"markdown","metadata":{"id":"Ov1cUBUm6HJ4"},"source":["## **Lexicons**\n","\n","* https://github.com/trinker/lexicon/tree/master/data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnE4x26nblZ_"},"outputs":[],"source":["!pip install pyreadr\n","\n","import pyreadr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQfDHsxGeTmy"},"outputs":[],"source":["def get_sent_sentiment(asent_str, lexicon_dt):\n","  '''\n","  Given a Sentence in string form and a Lexicon Dictionary\n","  Return the Sentiment of the Sentence = Sum(Sentiment(all words))\n","  '''\n","\n","  sent_sentiment = 0\n","  word_ls = asent_str.split()\n","  for aword in word_ls:\n","    word_sentiment = lexicon_dt.get(aword)\n","    if word_sentiment != None:\n","      sent_sentiment += float(word_sentiment)\n","\n","  return sent_sentiment\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A8cIYM2V-zHG"},"outputs":[],"source":["# TODO: UNUSED?\n","\n","# Calculate Pattern Sentiment [0,1,2]\n","\n","def pattern_discrete2continous_sentiment(text):\n","  '''\n","  Given a plain text string, give it to\n","    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n","  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n","    that approximates a normal distribution for all values\n","    In order to get more fine grained measure of overall Sentence sentiment\n","    Sentiment values will be Normalized/Standardized so absolute precision is not required\n","  '''\n","  text_sentiment_total = 0.\n","  text_ls = text.split()\n","  text_len = len(text_ls)\n","  for aword in text_ls:\n","    text_sentiment_total += pattern_sa(str(aword))[0]\n","  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+0.01)\n","\n","  return text_sentiment_norm"]},{"cell_type":"markdown","metadata":{"id":"KZe-VOc16dYh"},"source":["### **Jockers-Rinker**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ww2IOTXetbV"},"outputs":[],"source":["# Get Lexicon from SentimentR github repo\n","\n","lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pysentimentr_jockersrinker'\n","model_type = 'Lexicon'\n","\n","url = f\"https://github.com/trinker/lexicon/blob/master/data/hash_sentiment_{lexicon_name}.rda?raw=true\"\n","dst_path = f\"{lexicon_dense}.rda\"\n","dst_path_again = pyreadr.download_file(url, dst_path)\n","res = pyreadr.read_r(dst_path)\n","# print(f'type(res): {type(res)}\\n')\n","# res\n","\n","# Convert to DataFrame\n","lexicon_df = res[f'hash_sentiment_{lexicon_name}']\n","# lexicon_df.head()\n","lexicon_df.info()\n","print('\\n')\n","\n","# Reshape into Dictionary[word] = sentiment\n","lexicon_df.set_index('x', inplace=True)\n","lexicons_dt[lexicon_name] = lexicon_df.to_dict()['y']\n","\n","# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_word in enumerate(test_words_ls):\n","\n","  if (lexicons_dt[lexicon_name].get(atest_word.lower()) is None):\n","    # print(f'ERROR: {atest_word} not found in lexicon')\n","    word_sentiment_fl = 'ERROR'\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word} [NOT IN LEXICON]\\n')\n","    continue\n","  else:\n","    word_sentiment_fl = lexicons_dt[lexicon_name].get(atest_word.lower())\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_str in enumerate(test_sentences_ls):\n","\n","  str_sentiment_fl = get_sent_sentiment(atest_str, lexicons_dt[lexicon_name])\n","  print(f'[{str_sentiment_fl: ^8}]: {atest_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uriyktG9jIyu"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: get_sent_sentiment(x, lexicons_dt[lexicon_name]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XG3o2yV0jgUc"},"outputs":[],"source":["corpus_texts_dt[corpus_titles_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9bCl3yjz5Oy"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","novel_indx = 1\n","\n","text_title_str = corpus_titles_ls[novel_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"wdP1zLpY6lm_"},"source":["### **HuLiu (aka Bing)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vn7EdnHJkDDT"},"outputs":[],"source":["# Get Lexicon from SentimentR github repo\n","\n","lexicon_name = 'huliu'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pysentimentr_huliu'\n","model_type = 'Lexicon'\n","\n","url = \"https://github.com/trinker/lexicon/blob/master/data/hash_sentiment_huliu.rda?raw=true\"\n","dst_path = f\"{lexicon_dense}.rda\"\n","dst_path_again = pyreadr.download_file(url, dst_path)\n","res = pyreadr.read_r(dst_path)\n","# print(f'type(res): {type(res)}\\n')\n","# res\n","\n","# Convert to DataFrame\n","lexicon_df = res[f'hash_sentiment_{lexicon_name}']\n","# lexicon_df.head()\n","lexicon_df.info()\n","print('\\n')\n","\n","# Reshape into Dictionary[word] = sentiment\n","lexicon_df.set_index('x', inplace=True)\n","lexicons_dt[lexicon_name] = lexicon_df.to_dict()['y']\n","\n","# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_word in enumerate(test_words_ls):\n","\n","  if (lexicons_dt[lexicon_name].get(atest_word.lower()) is None):\n","    # print(f'ERROR: {atest_word} not found in lexicon')\n","    word_sentiment_fl = 'ERROR'\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word} [NOT IN LEXICON]\\n')\n","    continue\n","  else:\n","    word_sentiment_fl = lexicons_dt[lexicon_name].get(atest_word.lower())\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_str in enumerate(test_sentences_ls):\n","\n","  str_sentiment_fl = get_sent_sentiment(atest_str, lexicons_dt[lexicon_name])\n","  print(f'[{str_sentiment_fl: ^8}]: {atest_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgkD8_YRkDDV"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: get_sent_sentiment(x, lexicons_dt[lexicon_name]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h54Q_R4fkDDX"},"outputs":[],"source":["corpus_texts_dt[corpus_titles_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CG2hYqXckDDY"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","novel_indx = 1\n","\n","text_title_str = corpus_titles_ls[novel_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"ZXHUQV-f63j_"},"source":["### **NRC**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mCtNSbmoQ2A"},"outputs":[],"source":["# Get Lexicon from SentimentR github repo\n","\n","lexicon_name = 'nrc'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pysentimentr_nrc'\n","model_type = 'Lexicon'\n","\n","url = \"https://github.com/trinker/lexicon/blob/master/data/hash_sentiment_nrc.rda?raw=true\"\n","dst_path = f\"{lexicon_dense}.rda\"\n","dst_path_again = pyreadr.download_file(url, dst_path)\n","res = pyreadr.read_r(dst_path)\n","# print(f'type(res): {type(res)}\\n')\n","# res\n","\n","# Convert to DataFrame\n","lexicon_df = res[f'hash_sentiment_{lexicon_name}']\n","# lexicon_df.head()\n","lexicon_df.info()\n","print('\\n')\n","\n","# Reshape into Dictionary[word] = sentiment\n","lexicon_df.set_index('x', inplace=True)\n","lexicons_dt[lexicon_name] = lexicon_df.to_dict()['y']\n","\n","# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_word in enumerate(test_words_ls):\n","\n","  if (lexicons_dt[lexicon_name].get(atest_word.lower()) is None):\n","    # print(f'ERROR: {atest_word} not found in lexicon')\n","    word_sentiment_fl = 'ERROR'\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word} [NOT IN LEXICON]\\n')\n","    continue\n","  else:\n","    word_sentiment_fl = lexicons_dt[lexicon_name].get(atest_word.lower())\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_str in enumerate(test_sentences_ls):\n","\n","  str_sentiment_fl = get_sent_sentiment(atest_str, lexicons_dt[lexicon_name])\n","  print(f'[{str_sentiment_fl: ^8}]: {atest_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRVTs8aZoQ2C"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: get_sent_sentiment(x, lexicons_dt[lexicon_name]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyMa3VZyoQ2G"},"outputs":[],"source":["corpus_texts_dt[corpus_titles_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mn_VpRGJoQ2I"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","novel_indx = 1\n","\n","text_title_str = corpus_titles_ls[novel_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"1_TKBGnF6ooO"},"source":["### **SentiWord**\n","\n","* https://www.sentic.net/sentic-patterns.pdf\n","* https://www.quora.com/Sentiment-Analysis-How-does-CLiPS-Pattern-calculate-the-polarity-of-a-sentence-What-is-the-maths-involved-in-it \n","* https://github.com/clips/pattern/wiki/pattern-en#sentiment\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMMKo1ehoiWp"},"outputs":[],"source":["# Get Lexicon from SentimentR github repo\n","\n","lexicon_name = 'sentiword'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pysentimentr_sentiword'\n","model_type = 'Lexicon'\n","\n","url = \"https://github.com/trinker/lexicon/blob/master/data/hash_sentiment_sentiword.rda?raw=true\"\n","dst_path = f\"{lexicon_dense}.rda\"\n","dst_path_again = pyreadr.download_file(url, dst_path)\n","res = pyreadr.read_r(dst_path)\n","# print(f'type(res): {type(res)}\\n')\n","# res\n","\n","# Convert to DataFrame\n","lexicon_df = res[f'hash_sentiment_{lexicon_name}']\n","# lexicon_df.head()\n","lexicon_df.info()\n","print('\\n')\n","\n","# Reshape into Dictionary[word] = sentiment\n","lexicon_df.set_index('x', inplace=True)\n","lexicons_dt[lexicon_name] = lexicon_df.to_dict()['y']\n","\n","# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, aword_str in enumerate(test_words_ls):\n","\n","  if (lexicons_dt[lexicon_name].get(aword_str.lower()) is None):\n","    # print(f'ERROR: {aword_str} not found in lexicon')\n","    word_sentiment_fl = 'ERROR'\n","    print(f'[{word_sentiment_fl: ^8}]: {aword_str} [NOT IN LEXICON]\\n')\n","    continue\n","  else:\n","    word_sentiment_fl = lexicons_dt[lexicon_name].get(aword_str.lower())\n","    print(f'[{word_sentiment_fl: ^8}]: {aword_str}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, asent_str in enumerate(test_sentences_ls):\n","\n","  str_sentiment_fl = get_sent_sentiment(asent_str, lexicons_dt[lexicon_name])\n","  print(f'[{str_sentiment_fl: ^8}]: {asent_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLWd8zF5oiWv"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: get_sent_sentiment(x, lexicons_dt[lexicon_name]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCa0SKpzoiWz"},"outputs":[],"source":["corpus_texts_dt[corpus_titles_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaZle71ToiW0"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","novel_indx = 1\n","\n","text_title_str = corpus_titles_ls[novel_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"igCCPduG6vjZ"},"source":["### **SenticNet**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NtFDg-dozaZ"},"outputs":[],"source":["# Get Lexicon from SentimentR github repo\n","\n","lexicon_name = 'senticnet'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pysentimentr_senticnet'\n","model_type = 'Lexicon'\n","\n","url = \"https://github.com/trinker/lexicon/blob/master/data/hash_sentiment_senticnet.rda?raw=true\"\n","dst_path = f\"{lexicon_dense}.rda\"\n","dst_path_again = pyreadr.download_file(url, dst_path)\n","res = pyreadr.read_r(dst_path)\n","# print(f'type(res): {type(res)}\\n')\n","# res\n","\n","# Convert to DataFrame\n","lexicon_df = res[f'hash_sentiment_{lexicon_name}']\n","# lexicon_df.head()\n","lexicon_df.info()\n","print('\\n')\n","\n","# Reshape into Dictionary[word] = sentiment\n","lexicon_df.set_index('x', inplace=True)\n","lexicons_dt[lexicon_name] = lexicon_df.to_dict()['y']\n","\n","# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_word in enumerate(test_words_ls):\n","\n","  if (lexicons_dt[lexicon_name].get(atest_word.lower()) is None):\n","    # print(f'ERROR: {atest_word} not found in lexicon')\n","    word_sentiment_fl = 'ERROR'\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word} [NOT IN LEXICON]\\n')\n","    continue\n","  else:\n","    word_sentiment_fl = lexicons_dt[lexicon_name].get(atest_word.lower())\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_str in enumerate(test_sentences_ls):\n","\n","  str_sentiment_fl = get_sent_sentiment(atest_str, lexicons_dt[lexicon_name])\n","  print(f'[{str_sentiment_fl: ^8}]: {atest_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8PDMHBzozaf"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: get_sent_sentiment(x, lexicons_dt[lexicon_name]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uuLUHJunozah"},"outputs":[],"source":["corpus_texts_dt[corpus_titles_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_iFjMRiqozai"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","novel_indx = 1\n","\n","text_title_str = corpus_titles_ls[novel_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"D7QotExQ6xBB"},"source":["### **Loughran-McDonald**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IXTS5DCYpHHE"},"outputs":[],"source":["# Get Lexicon from SentimentR github repo\n","\n","lexicon_name = 'loughran_mcdonald'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pysentimentr_lmcd'\n","model_type = 'Lexicon'\n","\n","url = \"https://github.com/trinker/lexicon/blob/master/data/hash_sentiment_loughran_mcdonald.rda?raw=true\"\n","dst_path = f\"{lexicon_dense}.rda\"\n","dst_path_again = pyreadr.download_file(url, dst_path)\n","res = pyreadr.read_r(dst_path)\n","# print(f'type(res): {type(res)}\\n')\n","# res\n","\n","# Convert to DataFrame\n","lexicon_df = res[f'hash_sentiment_{lexicon_name}']\n","# lexicon_df.head()\n","lexicon_df.info()\n","print('\\n')\n","\n","# Reshape into Dictionary[word] = sentiment\n","lexicon_df.set_index('x', inplace=True)\n","lexicons_dt[lexicon_name] = lexicon_df.to_dict()['y']\n","\n","# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_word in enumerate(test_words_ls):\n","\n","  if (lexicons_dt[lexicon_name].get(atest_word.lower()) is None):\n","    # print(f'ERROR: {atest_word} not found in lexicon')\n","    word_sentiment_fl = 'ERROR'\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word} [NOT IN LEXICON]\\n')\n","    continue\n","  else:\n","    word_sentiment_fl = lexicons_dt[lexicon_name].get(atest_word.lower())\n","    print(f'[{word_sentiment_fl: ^8}]: {atest_word}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, atest_str in enumerate(test_sentences_ls):\n","\n","  str_sentiment_fl = get_sent_sentiment(atest_str, lexicons_dt[lexicon_name])\n","  print(f'[{str_sentiment_fl: ^8}]: {atest_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuPqJG8lpHHH"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: get_sent_sentiment(x, lexicons_dt[lexicon_name]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2JVdy4JpHHJ"},"outputs":[],"source":["corpus_texts_dt[corpus_titles_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzslktMQpHHK"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","novel_indx = 1\n","\n","text_title_str = corpus_titles_ls[novel_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[test_title_str].shape[0])\n","plot_title_str = f\"{corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[test_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"4EdTcDNj67cK"},"source":["### **MPQA**\n","\n","* https://mpqa.cs.pitt.edu/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9rae5Dm7AiG"},"outputs":[],"source":["!wget https://mpqa.cs.pitt.edu/corpora/mpqa_corpus/mpqa_corpus_3_0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vY4Wgq6vrZ5"},"outputs":[],"source":["!ls -altr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3drYoVZAvtsw"},"outputs":[],"source":["!head -n 10 mpqa_corpus_3_0"]},{"cell_type":"markdown","metadata":{"id":"5BIT9nhu69u_"},"source":["### **LIWC**\n","\n","* https://github.com/search?q=LIWC\n","* https://github.com/search?q=LIWC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfsQI5n66F8B"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"b9mEYM-p6kL5"},"source":["### **AFINN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kO2VyxeN6f15"},"outputs":[],"source":["!pip install afinn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRF2b7BnwVC9"},"outputs":[],"source":["from afinn import Afinn\n","afinn = Afinn(language='en')\n","\n","lexicon_name = 'AFINN'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'afinn'\n","model_type = 'Lexicon'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJ_4xTVEpo-Z"},"outputs":[],"source":["# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, aword_str in enumerate(test_words_ls):\n","\n","  word_sentiment_fl = afinn.score(aword_str.lower())\n","  print(f'[{word_sentiment_fl: ^8}]: {aword_str}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, asent_str in enumerate(test_sentences_ls):\n","\n","  words_ls = asent_str.split()\n","  sent_sentiment_fl = 0.0\n","\n","  for j, atest_word in enumerate(words_ls):\n","    sent_sentiment_fl += afinn.score(atest_word.lower())\n","\n","  print(f'[{sent_sentiment_fl: ^8}]: {asent_str}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYpUFHOlqt1Z"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: afinn.score(x.lower()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iZ8NMFgwrNPu"},"outputs":[],"source":["corpus_texts_dt[corpus_titles_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bC5I-fYErNPw"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","novel_indx = 1\n","\n","text_title_str = corpus_titles_ls[novel_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"Q7s_OD2J_QHA"},"source":["### **Save Checkpoint**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Durdojkm7zl5"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","#   and destination Subdir for Raw Sentiment Values\n","\n","!pwd\n","print('\\n')\n","\n","print(f'SUBDIR_SENTIMENT_RAW: {SUBDIR_SENTIMENT_RAW}\\n\\n')\n","\n","print('Existing Sentiment Datafiles in Destination Subdir:\\n')\n","\n","!ls $SUBDIR_SENTIMENT_RAW"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EbiBAtrg9lJe"},"outputs":[],"source":["# Verify Saving Corpus\n","\n","print(f'Saving Text_Type: {Text_Type}')\n","print(f'     Corpus_Type: {Corpus_Type}')\n","\n","print(f'\\nThese Text Titles:\\n')\n","corpus_texts_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-Kzyc3FEZ71"},"outputs":[],"source":["# Save sentiment values to subdir_sentiments\n","\n","save_filename = f'all_{Text_Type}_{Corpus_Type}_pymodels.json'\n","\n","write_dict_dfs(corpus_texts_dt, out_file=save_filename, out_dir=SUBDIR_SENTIMENT_RAW)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTjSK5KX-BQm"},"outputs":[],"source":["# Verify Dictionary was saved correctly by reading back the *.json datafile\n","\n","test_dt = read_dict_dfs(in_file=save_filename, in_dir=SUBDIR_SENTIMENT_RAW)\n","test_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTP5p-jOzGXu"},"outputs":[],"source":["test_dt['cmieville_thecityandthecity'].info()"]},{"cell_type":"markdown","metadata":{"id":"kSACJ2N06MNf"},"source":["## **Lexicons + Heuristics**"]},{"cell_type":"markdown","metadata":{"id":"XQXGPyAT7CMk"},"source":["### **VADER**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIt-cGAMPiv0"},"outputs":[],"source":["!pip install vaderSentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCb8sbP16Gz0"},"outputs":[],"source":["from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","vader_analyzer = SentimentIntensityAnalyzer()\n","\n","lexicon_name = 'VADER'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'vader'\n","model_type = 'Heuristic'\n","\n","test_str = \"The food was great!\"\n","\n","vs = vader_analyzer.polarity_scores(test_str)\n","print(\"{:-<65} {}\".format(test_str, str(vs)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"brIvCNIAty6Y"},"outputs":[],"source":["def sent2vader_comp(asent_str):\n","  '''\n","  Given a Sentence as a text string\n","  Return a Sentiment = sum(VADER sentiments for each word)\n","  '''\n","\n","  words_ls = asent_str.split()\n","  sent_sentiment_fl = 0.0\n","\n","  for j, atest_word in enumerate(words_ls):\n","    sent_sentiment_fl += vader_analyzer.polarity_scores(atest_word.lower())['compound']\n","\n","  return sent_sentiment_fl\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mk62GVwHsIR6"},"outputs":[],"source":["# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, aword_str in enumerate(test_words_ls):\n","\n","  # print(f'Looking up VADER sentiment for {aword_str}')\n","  word_sentiment_fl = vader_analyzer.polarity_scores(aword_str.lower())['compound']\n","  print(f'[{word_sentiment_fl: ^8.3f}]: {aword_str}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, asent_str in enumerate(test_sentences_ls):\n","\n","  sent_sentiment_fl = sent2vader_comp(asent_str)\n","  print(f'[{sent_sentiment_fl: ^8.3f}]: {asent_str}\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JyLw15CUtrnI"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: sent2vader_comp(x.lower()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvxCFDrgtrnM"},"outputs":[],"source":["corpus_texts_dt[corpus_titles_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_eKXyCTtrnQ"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","novel_indx = 1\n","\n","text_title_str = corpus_titles_ls[novel_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"rH0_9K_x7L1E"},"source":["## **Embeddings**\n","\n","* https://neptune.ai/blog/document-classification-small-datasets\n","* https://neptune.ai/blog/sentiment-analysis-python-textblob-vs-vader-vs-flair (TB,VADER,Flair)"]},{"cell_type":"markdown","metadata":{"id":"EJsKKRNT3jfI"},"source":["### **(FUTURE) FastText**\n","\n","* https://github.com/facebookresearch/fastText\n","* https://medium.com/@lope.ai/sentiment-analysis-example-using-fasttext-6b1b4d334c53\n","* https://colab.research.google.com/drive/1bb2OWQcDDolESwhkATD0el0RvF33fenZ#scrollTo=X5PWbhOzZ3ze\n","* https://fasttext.cc/docs/en/english-vectors.html (embeddings)\n","* https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb\n","* https://www.analyticsvidhya.com/blog/2017/07/word-representations-text-classification-using-fasttext-nlp-facebook/\n","* https://github.com/jatinmandav/Neural-Networks/tree/master/Sentiment-Analysis (Universal Sentence Encoder 77%, fastText 69%, word2vec 69%)\n","* https://github.com/search?q=fasttext+sentiment\n","\n","Code:\n","* https://github.com/charlesmalafosse/FastText-sentiment-analysis-for-tweets/blob/master/betsentiment_sentiment_analysis_fasttext.py (tweets)\n","* https://gist.github.com/hiteshn97/8f222a2773e11d6921b937abaa21ab75 (fastText,  keras)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ueVJ4OdG3jMV"},"outputs":[],"source":["!wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\n","!unzip v0.9.2.zip\n","%cd fastText-0.9.2\n","!make"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOivPfK_4RAs"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWrTkxMS4aT7"},"outputs":[],"source":["!ls ../"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BJ2LBlT365R"},"outputs":[],"source":["%%time\n","\n","train = open('tweets.train','w')  \n","test = open('tweets.valid','w')  \n","# with open('../sentiment140.1600000.csv', mode='r', encoding = \"ISO-8859-1\") as csv_file:  \n","with open('../sentiment140.csv', mode='r', encoding = \"ISO-8859-1\") as csv_file:  \n","    csv_reader = csv.DictReader(csv_file, fieldnames=['target', 'id', 'date', 'flag', 'user', 'text'])\n","    line = 0\n","    for row in csv_reader:\n","        # Clean the training data\n","        # First we lower case the text\n","        text = row[\"text\"].lower()\n","        # remove links\n","        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',text)\n","        #Remove usernames\n","        text = re.sub('@[^\\s]+','', text)\n","        # replace hashtags by just words\n","        text = re.sub(r'#([^\\s]+)', r'\\1', text)\n","        #correct all multiple white spaces to a single white space\n","        text = re.sub('[\\s]+', ' ', text)\n","        # Additional clean up : removing words less than 3 chars, and remove space at the beginning and teh end\n","        text = re.sub(r'\\W*\\b\\w{1,3}\\b', '', text)\n","        text = text.strip()\n","        line = line + 1\n","        # Split data into train and validation\n","        if line%16 == 0:\n","            print(f'__label__{row[\"target\"]} {text}', file=test)\n","        else:\n","            print(f'__label__{row[\"target\"]} {text}', file=train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oyqYDJNp4t_0"},"outputs":[],"source":["%%time\n","\n","!./fasttext supervised -input tweets.train -output model_tweet\n","# !./fasttext supervised -input tweets.train -output model_tweet -epoch 30 -lr 0.1\n","# !./fasttext supervised -input tweets.train -output model_tweet -dim 300 -label __label__ -pretrainedVecctors wiki.ar.vec # Arabic for Netflix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8W2OKSnx4t4y"},"outputs":[],"source":["%%time\n","\n","!./fasttext test model_tweet.bin tweets.valid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPELBLRrV2_-"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29mKvcVm5iu7"},"outputs":[],"source":["!pip install fasttext"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G78xUCbg4tzL"},"outputs":[],"source":["from fasttext import load_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pwEoQbMu5gIb"},"outputs":[],"source":["classifier = load_model('model_tweet.bin')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJyjaQ925gC3"},"outputs":[],"source":["text_ls = ['Ugghhh... Not happy at all! sorry', 'Happyyyyyyy', 'OH yeah! lets rock.']\n","labels = classifier.predict(text_ls)\n","print(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hz-EMuLT7n8-"},"outputs":[],"source":["with open('test.txt','w') as fp:\n","  fp.write(\"\\n\".join(text_ls))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qEXO_n957Aq-"},"outputs":[],"source":["!cat test.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsnuxOT68MaA"},"outputs":[],"source":["!./fasttext predict model_tweet.bin test.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G43FzjuP6tGs"},"outputs":[],"source":["!./fasttext predict-prob model_tweet.bin test.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zirGXVaF6OH8"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBVa-CKW6SQP"},"outputs":[],"source":["!ls ../"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYhVWbJ_6HKC"},"outputs":[],"source":["!head -n 10 tweets.train\n","\n","!cat tweets.train | wc -l"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eOLyKOgR82Az"},"outputs":[],"source":["# Load Different Embeddings\n","\n","!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQa8KVjB-3fS"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O1dE0ZqO8158"},"outputs":[],"source":["!unzip crawl-300d-2M-subword.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYZCsiQx-nvE"},"outputs":[],"source":["!ls "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKiZtpQe81wZ"},"outputs":[],"source":["import io\n","\n","fname = 'crawl-300d-2M-subword.vec'\n","\n","def load_vectors(fname):\n","    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    n, d = map(int, fin.readline().split())\n","    data = {}\n","    for line in fin:\n","        tokens = line.rstrip().split(' ')\n","        data[tokens[0]] = map(float, tokens[1:])\n","    return data\n","\n","load_vectors(fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vFYH2ctz9Zls"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmQFnTNn6HEN"},"outputs":[],"source":["%cd .."]},{"cell_type":"markdown","metadata":{"id":"GRGZrCnB7NnW"},"source":["### **TextBlob**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmg-Ru7jltY1"},"outputs":[],"source":["from textblob import TextBlob\n","\n","lexicon_name = 'TextBlob'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'textblob'\n","model_type = 'Heuristic'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7IsU2pbvwfxa"},"outputs":[],"source":["def sent2textblob(asent_str):\n","  '''\n","  Given a Sentence as a text string\n","  Return a Sentiment = sum(TextBlob sentiments for each word)\n","  '''\n","\n","  words_ls = asent_str.split()\n","  sent_sentiment_fl = 0.0\n","\n","  for j, atest_word in enumerate(words_ls):\n","    sent_sentiment_fl += TextBlob(atest_word.lower()).sentiment.polarity\n","\n","  return sent_sentiment_fl\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0CI3lnbwIh5"},"outputs":[],"source":["# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, aword_str in enumerate(test_words_ls):\n","\n","  # print(f'Looking up VADER sentiment for {aword_str}')\n","  word_sentiment_fl = TextBlob(aword_str.lower()).sentiment.polarity\n","  print(f'[{word_sentiment_fl: ^8.3f}]: {aword_str}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, asent_str in enumerate(test_sentences_ls):\n","\n","  sent_sentiment_fl = sent2textblob(asent_str)\n","  print(f'[{sent_sentiment_fl: ^8.3f}]: {asent_str}\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDYKKgkZwIh7"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: sent2textblob(x.lower()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJnbDLrVwIiI"},"outputs":[],"source":["corpus_texts_dt[corpus_titles_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ytrRoVcuwIiK"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","novel_indx = 1\n","\n","text_title_str = corpus_titles_ls[novel_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"uRJH_w_ZxaQr"},"source":["### **Save Checkpoint**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9vCT6IxxaQw"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","#   and destination Subdir for Raw Sentiment Values\n","\n","!pwd\n","print('\\n')\n","\n","print(f'SUBDIR_SENTIMENT_RAW: {SUBDIR_SENTIMENT_RAW}\\n\\n')\n","\n","print('Existing Sentiment Datafiles in Destination Subdir:\\n')\n","\n","!ls $SUBDIR_SENTIMENT_RAW"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENgwvfYCxaQ2"},"outputs":[],"source":["# Verify Saving Corpus\n","\n","print(f'Saving Text_Type: {Text_Type}')\n","print(f'     Corpus_Type: {Corpus_Type}')\n","\n","print(f'\\nThese Text Titles:\\n')\n","corpus_texts_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dm7bXVwOxaQ6"},"outputs":[],"source":["# Save sentiment values to subdir_sentiments\n","\n","save_filename = f'all_{Text_Type}_{Corpus_Type}_pymodels.json'\n","\n","write_dict_dfs(corpus_texts_dt, out_file=save_filename, out_dir=SUBDIR_SENTIMENT_RAW)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXZJY-4lxaQ9"},"outputs":[],"source":["# Verify Dictionary was saved correctly by reading back the *.json datafile\n","\n","test_dt = read_dict_dfs(in_file=save_filename, in_dir=SUBDIR_SENTIMENT_RAW)\n","test_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PewdsYwIxaQ_"},"outputs":[],"source":["test_dt['cmieville_thecityandthecity'].info()"]},{"cell_type":"markdown","metadata":{"id":"J-AMkbJv7PDQ"},"source":["### **Flair**\n","\n","* https://colab.research.google.com/drive/1tUr5t0ZJ-I4Ni40dkbjku92HAU5SyR_2?usp=sharing (TextBlob, Flair, VADER with UnivSentEmbd)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6h--uMK6GmL"},"outputs":[],"source":["from flair.models import TextClassifier\n","from flair.data import Sentence\n","\n","classifier = TextClassifier.load('en-sentiment')\n","\n","lexicon_name = 'Flair'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'flair'\n","model_type = 'Embeddings'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V57D2RfvfrXA"},"outputs":[],"source":["sentence = Sentence('The food was great!')\n","classifier.predict(sentence)\n","\n","# print sentence with predicted labels\n","print('Sentence above is: ', sentence.labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhOcH3Hu1ksd"},"outputs":[],"source":["print(sentence.labels[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSTgYe901lR3"},"outputs":[],"source":["def flair_sentiment(asent_str):\n","  '''\n","  Given a text string, get sentiment str using Flair (e.g. 'NEGATIVE (0.9243)') \n","  Return a floating point -1.0 to 1.0\n","  '''\n","  sentence = Sentence(asent_str)\n","  classifier.predict(sentence)\n","\n","  # print(f'   Sentence: {atest_str}')\n","  sentiment_str = str(sentence.labels[0])\n","\n","  polarity_str, polarity_val_str = sentiment_str.split()\n","\n","  pol_str = polarity_str.strip()\n","  if pol_str.strip() == \"POSITIVE\":\n","    sign_val = 1.0\n","  elif pol_str.strip() == \"NEGATIVE\":\n","    sign_val = -1.0\n","  else:\n","    print(f'ERROR: Illegal value for polarity_str: {pol_str}')\n","\n","  pol_val_str = polarity_val_str.strip()\n","  pol_val_str = pol_val_str[1:-1]\n","  pol_fl = sign_val * float(pol_val_str)\n","\n","  return pol_fl\n","\n","# Test\n","flair_sentiment(\"I love Paris in the springtime.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JOm80AHG1lR5"},"outputs":[],"source":["# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, aword_str in enumerate(test_words_ls):\n","\n","  # print(f'Looking up VADER sentiment for {aword_str}')\n","  word_sentiment_fl = flair_sentiment(aword_str.lower())\n","  print(f'[{word_sentiment_fl: ^8.3f}]: {aword_str}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, asent_str in enumerate(test_sentences_ls):\n","\n","  sent_sentiment_fl = flair_sentiment(asent_str)\n","  print(f'[{sent_sentiment_fl: ^8.3f}]: {asent_str}\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dtm-iRjM1lR6"},"outputs":[],"source":["%%time\n","\n","# NOTE:  4m10s @19:38 on 20220228 Colab Pro (2 Novels)\n","\n","# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: flair_sentiment(x.lower()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEn18LsW1lR7"},"outputs":[],"source":["corpus_texts_dt[corpus_titles_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQWLSaP01lR9"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","novel_indx = 1\n","\n","text_title_str = corpus_titles_ls[novel_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"ik_8vPz36AaK"},"source":["### **Save Checkpoint**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5d71yIa6AaK"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","#   and destination Subdir for Raw Sentiment Values\n","\n","!pwd\n","print('\\n')\n","\n","print(f'SUBDIR_SENTIMENT_RAW: {SUBDIR_SENTIMENT_RAW}\\n\\n')\n","\n","print('Existing Sentiment Datafiles in Destination Subdir:\\n')\n","\n","!ls $SUBDIR_SENTIMENT_RAW"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jeo9CyBn6AaL"},"outputs":[],"source":["# Verify Saving Corpus\n","\n","print(f'Saving Text_Type: {Text_Type}')\n","print(f'     Corpus_Type: {Corpus_Type}')\n","\n","print(f'\\nThese Text Titles:\\n')\n","corpus_texts_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c__d_6q86AaL"},"outputs":[],"source":["# Save sentiment values to subdir_sentiments\n","\n","save_filename = f'all_{Text_Type}_{Corpus_Type}_pymodels.json'\n","\n","write_dict_dfs(corpus_texts_dt, out_file=save_filename, out_dir=SUBDIR_SENTIMENT_RAW)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fvr1Qi5a6AaM"},"outputs":[],"source":["# Verify Dictionary was saved correctly by reading back the *.json datafile\n","\n","test_dt = read_dict_dfs(in_file=save_filename, in_dir=SUBDIR_SENTIMENT_RAW)\n","test_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6McesBw6AaM"},"outputs":[],"source":["test_dt['cmieville_thecityandthecity'].info()"]},{"cell_type":"markdown","metadata":{"id":"b1YbJ36B77Jy"},"source":["## **Linguistic Models**"]},{"cell_type":"markdown","metadata":{"id":"D0cBuDeT8ABx"},"source":["### **Pattern**\n","\n","* https://github.com/clips/pattern/blob/master/examples/03-en/07-sentiment.py\n","\n","* https://github.com/clips/pattern/wiki/pattern-en#sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cW0EnGst74Cm"},"outputs":[],"source":["!pip install pattern"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JeqRykx8O8b"},"outputs":[],"source":["from pattern.en import sentiment, polarity, subjectivity, positive\n","\n","lexicon_name = 'Pattern'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'pattern'\n","model_type = 'Linguistic'\n","\n","# Sentiment analysis (or opinion mining) attempts to determine if\n","# a text is objective or subjective, positive or negative.\n","# The sentiment analysis lexicon bundled in Pattern focuses on adjectives.\n","# It contains adjectives that occur frequently in customer reviews,\n","# hand-tagged with values for polarity and subjectivity.\n","\n","# The polarity() function measures positive vs. negative, as a number between -1.0 and +1.0.\n","# The subjectivity() function measures objective vs. subjective, as a number between 0.0 and 1.0.\n","# The sentiment() function returns an averaged (polarity, subjectivity)-tuple for a given string.\n","for word in (\"amazing\", \"horrible\", \"public\"):\n","    print(word, sentiment(word))\n","\n","print(\"\")\n","print(sentiment(\n","    \"The movie attempts to be surreal by incorporating time travel and various time paradoxes,\"\n","    \"but it's presented in such a ridiculous way it's seriously boring.\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5pOXkW-85OW"},"outputs":[],"source":["# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, aword_str in enumerate(test_words_ls):\n","\n","  # print(f'Looking up VADER sentiment for {aword_str}')\n","  word_sentiment_fl = polarity(aword_str.lower())\n","  print(f'[{word_sentiment_fl: ^8.3f}]: {aword_str}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, asent_str in enumerate(test_sentences_ls):\n","\n","  sent_sentiment_fl = polarity(asent_str)\n","  print(f'[{sent_sentiment_fl: ^8.3f}]: {asent_str}\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3icn6UiW-EXS"},"outputs":[],"source":["# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: polarity(x.lower()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NeiLvBYn-EXT"},"outputs":[],"source":["corpus_texts_dt[corpus_titles_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbDHAD6d-EXT"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","novel_indx = 1\n","\n","text_title_str = corpus_titles_ls[novel_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"QEhVfWRu79oI"},"source":["### **Stanza**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DarbPTFB_GXC"},"outputs":[],"source":["!pip install stanza"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjjNS8Th_GXD"},"outputs":[],"source":["%time\n","\n","import stanza\n","\n","stanza.download('en')\n","\n","lexicon_name = 'Stanza'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","lexicon_df = pd.DataFrame()\n","\n","model_name = 'stanza'\n","model_type = 'Linguistic'\n","\n","nlp = stanza.Pipeline('en', processors='tokenize,sentiment')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-Oml3pJ_GXF"},"outputs":[],"source":["# Test stanza directly\n","\n","doc = nlp('Ram is a bad boy')\n","for i, sentence in enumerate(doc.sentences):\n","    print(i, sentence.sentiment)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ay1cB2fl_GXG"},"outputs":[],"source":["# Calculate Stanza Sentiment [0,1,2]\n","\n","def stanza_discrete2continous_sentiment(text):\n","  '''\n","  Given a plain text string, give it to\n","    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n","  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n","    that approximates a normal distribution for all values\n","    In order to get more fine grained measure of overall Sentence sentiment\n","    Sentiment values will be Normalized/Standardized so absolute precision is not required\n","  '''\n","  text_sentiment_tot = 0.\n","  text_ls = text.split()\n","  text_len = len(text_ls)\n","  for aword in text_ls:\n","    adoc = nlp(aword)\n","    for i, sentence in enumerate(adoc.sentences):\n","      text_sentiment_tot += float(sentence.sentiment)\n","  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.1)\n","\n","  return text_sentiment_norm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSLDxgfr_GXH"},"outputs":[],"source":["# Test\n","\n","sent_test='I hate Mondays.'\n","print(stanza_discrete2continous_sentiment(sent_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_vssla4_r2h"},"outputs":[],"source":["# Test Words\n","print(f'Testing {lexicon_name} lexicon for WORDS Sentiment')\n","print('--------------------------------------------------')\n","for i, aword_str in enumerate(test_words_ls):\n","\n","  # print(f'Looking up VADER sentiment for {aword_str}')\n","  word_sentiment_fl = stanza_discrete2continous_sentiment(aword_str.lower())\n","  print(f'[{word_sentiment_fl: ^8.3f}]: {aword_str}\\n')\n","\n","\n","# Test Sentences\n","print(f'\\nTesting {lexicon_name} lexicon for SENTENCES Sentiment')\n","print('--------------------------------------------------')\n","for i, asent_str in enumerate(test_sentences_ls):\n","\n","  sent_sentiment_fl = stanza_discrete2continous_sentiment(asent_str)\n","  print(f'[{sent_sentiment_fl: ^8.3f}]: {asent_str}\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVSxFIf1_r2i"},"outputs":[],"source":["%%time\n","\n","# NOTE: \n","\n","# Compute Sentiments based upon SentimentR Lexicon\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f\"Processing #{i}: {atext}\")\n","\n","  corpus_texts_dt[atext][model_name] = corpus_texts_dt[atext]['text_clean'].apply(lambda x: stanza_discrete2continous_sentiment(x.lower()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AiH0_mVa_r2i"},"outputs":[],"source":["corpus_texts_dt[corpus_titles_ls[0]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vy_iaYJ_r2i"},"outputs":[],"source":["# Plot New Lexicon Values\n","\n","novel_indx = 1\n","\n","text_title_str = corpus_titles_ls[novel_indx]\n","# lexicon_name = 'jockers_rinker'\n","lexicon_dense = ''.join(lexicon_name.split('_'))\n","\n","win_10per = int(0.10*corpus_texts_dt[text_title_str].shape[0])\n","plot_title_str = f\"{corpus_titles_dt[text_title_str][0]}\\nSentiment Analysis\\n{model_type} Model: {lexicon_name}\"\n","corpus_texts_dt[text_title_str][model_name].rolling(win_10per, center=True, min_periods=0).mean().plot(title=plot_title_str)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"aGYiQtBt6ogX"},"source":["### **Save Checkpoint**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WT6AE7-Y6ogZ"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","#   and destination Subdir for Raw Sentiment Values\n","\n","!pwd\n","print('\\n')\n","\n","print(f'SUBDIR_SENTIMENT_RAW: {SUBDIR_SENTIMENT_RAW}\\n\\n')\n","\n","print('Existing Sentiment Datafiles in Destination Subdir:\\n')\n","\n","!ls $SUBDIR_SENTIMENT_RAW"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8BS2JHk6oga"},"outputs":[],"source":["# Verify Saving Corpus\n","\n","print(f'Saving Text_Type: {Text_Type}')\n","print(f'     Corpus_Type: {Corpus_Type}')\n","\n","print(f'\\nThese Text Titles:\\n')\n","corpus_texts_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaF2KEUS6ogb"},"outputs":[],"source":["# Save sentiment values to subdir_sentiments\n","\n","save_filename = f'all_{Text_Type}_{Corpus_Type}_pymodels.json'\n","\n","write_dict_dfs(corpus_texts_dt, out_file=save_filename, out_dir=SUBDIR_SENTIMENT_RAW)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NOj30yXh6ogc"},"outputs":[],"source":["# Verify Dictionary was saved correctly by reading back the *.json datafile\n","\n","test_dt = read_dict_dfs(in_file=save_filename, in_dir=SUBDIR_SENTIMENT_RAW)\n","test_dt.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWPq-p9M6ogd"},"outputs":[],"source":["test_dt['cmieville_thecityandthecity'].info()"]},{"cell_type":"markdown","metadata":{"id":"pWJPszHQFsUY"},"source":["## Statistical ML Models\n"]},{"cell_type":"markdown","metadata":{"id":"uCxhjm8BFxZu"},"source":["### Get Sentiment Training Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMDev_d1Fqtb"},"outputs":[],"source":["# Verify in project root Directory\n","\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEG1gTB_Fw33"},"outputs":[],"source":["!mkdir ~/.kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IV3AjO_pFwzV"},"outputs":[],"source":["from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pk-GVEkhGM17"},"outputs":[],"source":["!mv kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8GzXNBXGMw3"},"outputs":[],"source":["# Get IMDB Dataset\n","\n","%cd ./data\n","!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fuf_6fzGcDR"},"outputs":[],"source":["!unzip imdb-dataset-of-50k-movie-reviews.zip\n","!ls -altr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAcrC6QZGWMQ"},"outputs":[],"source":["imdb50k_df = pd.read_csv(\"IMDB Dataset.csv\")\n","imdb50k_df[\"polarity\"] = imdb50k_df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n","imdb50k_df[\"text_raw\"] = imdb50k_df[\"review\"].astype('string')\n","imdb50k_df.drop(columns=['sentiment', 'review'], inplace=True)\n","\n","# supervised_db = 'imdb50k'\n","\n","imdb50k_df.head()\n","imdb50k_df.info()"]},{"cell_type":"markdown","metadata":{"id":"fqMeeO7_HGpB"},"source":["### Clean Sentiment Training Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEPOwVW2I-ta"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAQbmVzhGWH_"},"outputs":[],"source":["# Remove non-alphanumeric chacters\n","# imdb50k_df['text_lower'] = imdb50k_df['text_raw']\n","\n","\"\"\"\n","pattern = re.compile(r\"[A-Za-z0-9\\-]{3,50}\")\n","imdb50k_df['text_clean'] = imdb50k_df['text_raw'].str.lower().str.strip().str.findall(pattern).str.join(' ')\n","imdb50k_df.head(1)\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44YvUyvMGWEM"},"outputs":[],"source":["import spacy\n","\n","nlp = spacy.blank('en')\n","\n","nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","# explicitly adding component to pipeline\n","# (recommended - makes it more readable to tell what's going on)\n","# nlp.add_pipe(PySBDFactory(nlp))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVlhR2c1HNXO"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPrpL1wyNPva"},"outputs":[],"source":["%%time\n","\n","# NOTE: (no stem) 4m09s\n","#       (w/ stem) 4m24s\n","\n","i = 0\n","\n","for key_novel, atext_df in corpus_texts_dt.items():\n","\n","  print(f'Processing Novel #{i}: {key_novel}...')\n","\n","  atext_df['text_clean'] = clean_text(atext_df, 'text_raw', text_type='formal')\n","\n","  atext_df['text_clean'] = lemma_pipe(atext_df['text_clean'])\n","  atext_df['text_clean'] = atext_df['text_clean'].astype('string')\n","\n","  # TODO: Fill in all blank 'text_clean' rows with filler semaphore\n","  atext_df.text_clean = atext_df.text_clean.fillna('this_blank')\n","\n","  atext_df.head(2)\n","\n","  print(f'  shape: {atext_df.shape}')\n","\n","  i += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXuwwg2_XgZu"},"outputs":[],"source":["# Verify the first Text in Corpus is cleaned\n","\n","corpus_texts_dt[corpus_titles_ls[0]].head(20)\n","corpus_texts_dt[corpus_titles_ls[0]].info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBHhuO0MHNRQ"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wS_5p6nOHNNa"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQqNOF34HNJD"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"-muOiNX18Ew0"},"source":["## **Deep Neural Networks**\n","\n","* https://github.com/thoailinh/Sentiment-Analysis-using-BERT (Viet Comparison)\n","\n","* https://github.com/Feuoy/sentiment-analysis (Chinese Comparison)\n","\n","* https://www.kaggle.com/aditya6040/7-models-on-imdb-dataset-best-score-88-2/notebook#CNN-Model\n","* https://github.com/bentrevett/pytorch-sentiment-analysis\n","\n","* https://github.com/nileshsah/deep-text-classifier/blob/master/inshorts_notebook.ipynb\n","* https://github.com/saurabhrathor/InceptionModel_SentimentAnalysis (fasttext emb CNN+LSTM) BB_twtr SemEval2017\n","* https://github.com/kaliahinartem/twitter_sentiment_analysis\n","* https://github.com/leelaylay/TweetSemEval\n"]},{"cell_type":"markdown","metadata":{"id":"eLVXPTokkcbd"},"source":["### **Common Setup**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dyXFbMUohfV"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","\n","from keras.wrappers.scikit_learn import KerasClassifier\n","\n","from keras.preprocessing import text, sequence\n","from keras import layers, models, optimizers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYbbXlASkeHJ"},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.callbacks import ModelCheckpoint\n","from keras.layers import LSTM,Dropout\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","from keras.layers import LSTM, Conv1D, MaxPooling1D, Dropout\n","from keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbJuiBZakhgX"},"outputs":[],"source":["def plot_history(history):\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    x = range(1, len(acc) + 1)\n","\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(x, acc, 'b', label='Training acc')\n","    plt.plot(x, val_acc, 'r', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(x, loss, 'b', label='Training loss')\n","    plt.plot(x, val_loss, 'r', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eln2U453nXfm"},"outputs":[],"source":["training_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YsJ3uB-zkohV"},"outputs":[],"source":["# Split labeled dataset into training, validation and test sets\n","# e.g. for IMDB 50k reviews: Out of 50k dataset, 36k for training, 4k for Validationa and 10k for testing\n","\n","X_train, X_test, y_train, y_test = train_test_split(training_df['text_raw'], training_df['polarity'],test_size=0.2, random_state=0)\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,test_size=0.1, random_state=0)\n","\n","[x.shape for x in [X_train,X_valid,X_test]]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DN8r_7lNB0C"},"outputs":[],"source":["X_train.shape\n","print('\\n')\n","type(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vEbTnMvSim8"},"outputs":[],"source":["type(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XaDIHBBJSfcG"},"outputs":[],"source":["X_train[:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRda68cYkuyk"},"outputs":[],"source":["%%time\n","\n","# Tokenize text\n","\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(training_df.text_raw)\n","\n","X_train1 = tokenizer.texts_to_sequences(X_train)\n","X_valid1 = tokenizer.texts_to_sequences(X_valid)\n","X_test1 = tokenizer.texts_to_sequences(X_test)\n","\n","vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n","\n","print(X_train[2])\n","print(X_train1[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RPP78tSoNpQy"},"outputs":[],"source":["type(X_train1)\n","print('\\n')\n","X_train1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRTtHS72NRPW"},"outputs":[],"source":["corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"in-_if34TBG3"},"outputs":[],"source":["corpus_sents_df['sent_clean'] = corpus_sents_df['sent_clean'].astype('string')\n","X_corpus_ser = corpus_sents_df['sent_clean']\n","type(X_corpus_ser)\n","X_corpus_ser[:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zj3td2_6Th6b"},"outputs":[],"source":["X_corpus1 = tokenizer.texts_to_sequences(X_corpus_ser)\n","type(X_corpus1)\n","X_corpus_ser[4]\n","X_corpus1[4]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lLmBRBaKPJ5U"},"outputs":[],"source":["\"\"\"\n","X_corpus = np.asarray(tokenizer.texts_to_sequences(X_corpus_ser)) # , dtype=int)\n","X_corpus.shape\n","type(X_corpus)\n","X_corpus\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDvn8SidQrDJ"},"outputs":[],"source":["\"\"\"\n","X_corpus = np.array(tokenizer.texts_to_sequences(corpus_sents_df['sent_clean'])) # , dtype=int)\n","X_corpus.shape\n","type(X_corpus)\n","X_corpus\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1FNX2ulWQX5Y"},"outputs":[],"source":["\"\"\"\n","tokens_ls_ls = tokenizer.texts_to_sequences(corpus_sents_df['sent_clean'])\n","X_corpus_ar = np.array([np.array(lsi) for lsi in tokens_ls_ls])\n","X_corpus_ar\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7TOagh9M7Px"},"outputs":[],"source":["\"\"\"\n","X_corpus = np.asarray(tokenizer.texts_to_sequences(corpus_sents_df['sent_clean'])) # , dtype=int)\n","# X_corpus = X_corpus.astype('int32')\n","# npa = np.asarray(someListOfLists, dtype=np.float32)\n","X_corpus[:3]\n","print('\\n')\n","type(X_corpus)\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRpVqkKXkuuM"},"outputs":[],"source":["print(X_train[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zktxzNIcntgd"},"outputs":[],"source":["seq_lens = [len(s) for s in X_train1]\n","print(\"average length: %0.1f\" % np.mean(seq_lens))\n","print(\"max length: %d\" % max(seq_lens))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VxELvkWwUfsA"},"outputs":[],"source":["seq_lens = [len(s) for s in X_corpus1]\n","print(\"average length: %0.1f\" % np.mean(seq_lens))\n","print(\"max length: %d\" % max(seq_lens))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8y5wpugcntYY"},"outputs":[],"source":["# Set max sequence and pad where necessary\n","\n","maxlen = 150\n","\n","X_train1 = pad_sequences(X_train1, padding='post', maxlen=maxlen)\n","X_valid1 = pad_sequences(X_valid1, padding='post', maxlen=maxlen)\n","X_test1 = pad_sequences(X_test1, padding='post', maxlen=maxlen)\n","\n","print(X_train1[2, :])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LlWQpSPmUotH"},"outputs":[],"source":["X_corpus1 = pad_sequences(X_corpus1, padding='post', maxlen=maxlen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fA-Lhx4IntTY"},"outputs":[],"source":["vocab_size"]},{"cell_type":"markdown","metadata":{"id":"6qTZDoL-8IWk"},"source":["### **Fully Connected Networks (FCN)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2vNqoEKn1aA"},"outputs":[],"source":["# Build the Network\n","\n","embedding_dim = 50\n","callback = EarlyStopping(monitor='val_loss', patience=2)\n","\n","model = Sequential()\n","model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n","model.add(layers.Flatten())\n","model.add(layers.Dense(10, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtYjLvgon1VK"},"outputs":[],"source":["%%time\n","\n","# Train the Network\n","\n","history = model.fit(X_train1, y_train,epochs=10,verbose=True,validation_data=(X_valid1, y_valid),batch_size=1000,callbacks=[callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCwahEGlIPBG"},"outputs":[],"source":["y_test1_pred = model.predict(X_test1)\n","y_test1_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mYJKBS_UIhum"},"outputs":[],"source":["y_test1_pred.size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkO-EI39LUFB"},"outputs":[],"source":["y_test1_pred_bin = np.where(y_test1_pred > 0.5, 1, 0)\n","y_test1_pred_bin = y_test1_pred_bin.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lAdpanp_L2fK"},"outputs":[],"source":["type(y_test1_pred_bin[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SV5KsOSvLPY9"},"outputs":[],"source":["y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3ftUPxgIs9w"},"outputs":[],"source":["y_test_fl = y_test.apply(lambda x: float(x))\n","y_test_fl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFC7sK2eI8D4"},"outputs":[],"source":["y_test_ar = np.array(y_test_fl, dtype=np.float32)\n","type(y_test_ar)\n","print('\\n')\n","y_test_ar.shape\n","print('\\n')\n","y_test_ar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBCVQ5NgJjpO"},"outputs":[],"source":["type(y_test1_pred)\n","print('\\n')\n","y_test1_pred = y_test1_pred.squeeze()\n","y_test1_pred.shape\n","print('\\n')\n","y_test1_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQqWHoKin1Q_"},"outputs":[],"source":["accuracy_score(y_test, y_test1_pred_bin)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_vBXX6y73-x"},"outputs":[],"source":["plot_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlqHceOEpHpN"},"outputs":[],"source":["plt.figure(figsize=(4,4))\n","# sns.heatmap(confusion_matrix(y_test, model.predict(X_test1)),annot=True,cmap='coolwarm',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\n","sns.heatmap(confusion_matrix(y_test, y_test1_pred_bin),annot=True,cmap='coolwarm',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\n","plt.xlabel(\"Predicted\",fontsize=16)\n","plt.ylabel(\"Actual\",fontsize=16)\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aZA8tG_hpHjh"},"outputs":[],"source":["y_corpus1_pred = model.predict(X_corpus1)\n","type(y_corpus1_pred)\n","print('\\n')\n","print(y_corpus1_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BwCcLOhVBAy"},"outputs":[],"source":["fcn_ar = y_corpus1_pred.squeeze()\n","fcn_ar.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AwVgkZAiVPbC"},"outputs":[],"source":["corpus_sents_df['fcn'] = pd.Series(fcn_ar)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEfY-rKyVPK_"},"outputs":[],"source":["# corpus_sents_df['fcn'].apply(lambda x: 6*(x-0.3)).rolling(900, center=True).mean().plot(label='Fully Connected Neural Net')\n","corpus_sents_df['fcn'].apply(lambda x: 8*(x-0.55)).rolling(900, center=True).mean().plot(label='Fully Connected Neural Net')\n","corpus_sents_df['sentimentr_stdscaler'].rolling(900, center=True).mean().plot(label='SentimentR')\n","corpus_sents_df['vader_stdscaler'].rolling(900, center=True).mean().plot(label='VADER')\n","plt.legend(loc='best');\n","plt.title(f'{CORPUS_FULL}\\nFully Connected Neural Net (Default w/IMDB) SMA=10%');"]},{"cell_type":"markdown","metadata":{"id":"K-PeeJ2O8Mku"},"source":["### **RNN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmUYoHoN8IGq"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"r7lppwbU8PnC"},"source":["### **LSTM**"]},{"cell_type":"markdown","metadata":{"id":"_7cIk8Zd5sce"},"source":["**Ref: https://www.kaggle.com/derrelldsouza/imdb-sentiment-analysis-eda-ml-lstm-bert#4.-Predictive-Modelling-using-Machine-Learning**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyfL7-yh5Agm"},"outputs":[],"source":["def plotLearningCurve(history,epochs):\n","  epochRange = range(1,epochs+1)\n","  fig , ax = plt.subplots(1,2,figsize = (10,5))\n","  \n","  ax[0].plot(epochRange,history.history['accuracy'],label = 'Training Accuracy')\n","  ax[0].plot(epochRange,history.history['val_accuracy'],label = 'Validation Accuracy')\n","  ax[0].set_title('Training and Validation accuracy')\n","  ax[0].set_xlabel('Epoch')\n","  ax[0].set_ylabel('Accuracy')\n","  ax[0].legend()\n","  ax[1].plot(epochRange,history.history['loss'],label = 'Training Loss')\n","  ax[1].plot(epochRange,history.history['val_loss'],label = 'Validation Loss')\n","  ax[1].set_title('Training and Validation loss')\n","  ax[1].set_xlabel('Epoch')\n","  ax[1].set_ylabel('Loss')\n","  ax[1].legend()\n","  fig.tight_layout()\n","  plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jsv_NlzP5Eu5"},"outputs":[],"source":["#set up the tokenizer\n","MAX_VOCAB_SIZE = 10000\n","tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE,oov_token=\"<oov>\")\n","tokenizer.fit_on_texts(X_train)\n","word_index = tokenizer.word_index\n","\n","#print(word_index)\n","V = len(word_index)\n","print(\"Vocabulary of the dataset is : \",V)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-InuwbHq5Epy"},"outputs":[],"source":["##create sequences of reviews\n","seq_train = tokenizer.texts_to_sequences(X_train)\n","seq_test =  tokenizer.texts_to_sequences(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwYYamks5Q_A"},"outputs":[],"source":["#choice of maximum length of sequences\n","seq_len_list = [len(i) for i in seq_train + seq_test]\n","\n","#if we take the direct maximum then\n","max_len=max(seq_len_list)\n","print('Maximum length of sequence in the list: {}'.format(max_len))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htoHmhnq5Q5U"},"outputs":[],"source":["# when setting the maximum length of sequence, variability around the average is used.\n","max_seq_len = np.mean(seq_len_list) + 2 * np.std(seq_len_list)\n","max_seq_len = int(max_seq_len)\n","print('Maximum length of the sequence when considering data only two standard deviations from average: {}'.format(max_seq_len))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gN29x-_u5Q0V"},"outputs":[],"source":["perc_covered = np.sum(np.array(seq_len_list) < max_seq_len) / len(seq_len_list)*100\n","print('The above calculated number coveres approximately {} % of data'.format(np.round(perc_covered,2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cr_pKFPM5bd3"},"outputs":[],"source":["#create padded sequences\n","pad_train=pad_sequences(seq_train,truncating = 'post', padding = 'pre',maxlen=max_seq_len)\n","pad_test=pad_sequences(seq_test,truncating = 'post', padding = 'pre',maxlen=max_seq_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ystbZw-n5bYR"},"outputs":[],"source":["#Splitting training set for validation purposes\n","Xtrain,Xval,ytrain,yval=train_test_split(pad_train,y_train, test_size=0.2,random_state=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u940z0na6DBx"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.layers import Dense,Input, Embedding,LSTM,Dropout,Conv1D, MaxPooling1D, GlobalMaxPooling1D,Dropout,Bidirectional,Flatten,BatchNormalization\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import plot_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnnRQv-m5bT2"},"outputs":[],"source":["def lstm_model(Xtrain,Xval,ytrain,yval,V,D,maxlen,epochs):\n","\n","    print(\"----Building the model----\")\n","    i = Input(shape=(maxlen,))\n","    x = Embedding(V + 1, D,input_length = maxlen)(i)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.3)(x)\n","    x = Conv1D(32,5,activation = 'relu')(x)\n","    x = Dropout(0.3)(x)\n","    x = MaxPooling1D(2)(x)\n","    x = Bidirectional(LSTM(128,return_sequences=True))(x)\n","    x = LSTM(64)(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(1, activation='sigmoid')(x)\n","    model = Model(i, x)\n","    model.summary()\n","\n","    #Training the LSTM\n","    print(\"----Training the network----\")\n","    model.compile(optimizer= Adam(0.0005),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","    \n","#     #early_stop = EarlyStopping(monitor='val_accuracy', \n","#                                mode='min', \n","#                                patience = 2 )\n","#     #checkpoints= ModelCheckpoint(filepath='./',\n","#                             monitor=\"val_accuracy\",\n","#                             verbose=0,\n","#                             save_best_only=True\n","#                            )\n","  #  callbacks = [checkpoints,early_stop]\n","    r = model.fit(Xtrain,ytrain, \n","                  validation_data = (Xval,yval), \n","                  epochs = epochs, \n","                  verbose = 2,\n","                  batch_size = 32)\n","                  #callbacks = callbacks\n","    print(\"Train score:\", model.evaluate(Xtrain,ytrain))\n","    print(\"Validation score:\", model.evaluate(Xval,yval))\n","    n_epochs = len(r.history['loss'])\n","    \n","    return r,model,n_epochs "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eez3ILE050f7"},"outputs":[],"source":["%%time\n","\n","# NOTE: 3m51s\n","\n","D = 64 #embedding dims\n","epochs = 5\n","r,model,n_epochs = lstm_model(Xtrain,Xval,ytrain,yval,V,D,max_seq_len,epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdfAE3bu50bG"},"outputs":[],"source":["#Plot accuracy and loss\n","\n","plotLearningCurve(r,n_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ss3yg0lU6aGc"},"outputs":[],"source":["print(\"Evaluate Model Performance on Test set\")\n","result = model.evaluate(pad_test,y_test)\n","print(dict(zip(model.metrics_names, result)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RK3_YwOF6aCS"},"outputs":[],"source":["#Generate predictions for the test dataset\n","ypred = model.predict(pad_test)\n","ypred = ypred>0.5\n","#Get the confusion matrix\n","cf_matrix = confusion_matrix(y_test, ypred)\n","sns.heatmap(cf_matrix,annot = True,fmt ='g', cmap='Blues')\n","plt.xlabel('Predicted label')\n","plt.ylabel('True label')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s41pz7yNYPIO"},"outputs":[],"source":["X_corpus_ser = corpus_sents_df['sent_clean']\n","type(X_corpus_ser)\n","X_corpus_ser[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEanWgXsZllz"},"outputs":[],"source":["type(X_corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mt0PHX5rY3hG"},"outputs":[],"source":["##create sequences of reviews\n","seq_corpus = tokenizer.texts_to_sequences(X_corpus_ser)\n","# seq_test =  tokenizer.texts_to_sequences(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ahgDGIwmZtzj"},"outputs":[],"source":["type(seq_corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-glUVGRY3hI"},"outputs":[],"source":["#choice of maximum length of sequences\n","seq_len_list = [len(i) for i in seq_corpus]\n","\n","#if we take the direct maximum then\n","max_len=max(seq_len_list)\n","print('Maximum length of sequence in the list: {}'.format(max_len))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7fv7Uz3QY3hJ"},"outputs":[],"source":["# when setting the maximum length of sequence, variability around the average is used.\n","max_seq_len = np.mean(seq_len_list) + 2 * np.std(seq_len_list)\n","max_seq_len = int(max_seq_len)\n","print('Maximum length of the sequence when considering data only two standard deviations from average: {}'.format(max_seq_len))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"up_PGoC0Y3hK"},"outputs":[],"source":["perc_covered = np.sum(np.array(seq_len_list) < max_seq_len) / len(seq_len_list)*100\n","print('The above calculated number coveres approximately {} % of data'.format(np.round(perc_covered,2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tdyh1vjzanUu"},"outputs":[],"source":["max_seq_len"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9R3IQhUTYvX6"},"outputs":[],"source":["#create padded sequences\n","pad_corpus=pad_sequences(seq_corpus,truncating = 'post', padding = 'pre',maxlen=584) # max_seq_len)\n","# pad_test=pad_sequences(seq_test,truncating = 'post', padding = 'pre',maxlen=max_seq_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIqghbZhaVHi"},"outputs":[],"source":["#Generate predictions for the corpus dataset\n","y_corpus_pred = model.predict(pad_corpus)\n","y_corpus_pred.shape\n","y_corpus_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-g0ye0WpYvQE"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTxr6TUIa54k"},"outputs":[],"source":["lstm_ar = y_corpus_pred.squeeze()\n","lstm_ar.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ModO5-mBa54n"},"outputs":[],"source":["corpus_sents_df['lstm'] = pd.Series(lstm_ar)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48-PFUTqa54q"},"outputs":[],"source":["corpus_sents_df['fcn'].apply(lambda x: 8*(x-0.55)).rolling(900, center=True).mean().plot(label='Fully Connected Neural Net')\n","corpus_sents_df['lstm'].apply(lambda x: 5*(x-0.47)).rolling(900, center=True).mean().plot(label='LSTM Neural Net')\n","corpus_sents_df['sentimentr_stdscaler'].rolling(900, center=True).mean().plot(label='SentimentR')\n","corpus_sents_df['vader_stdscaler'].rolling(900, center=True).mean().plot(label='VADER')\n","plt.legend(loc='best');\n","plt.title(f'{CORPUS_FULL}\\nLSTM Neural Net (Default w/IMDB) SMA=10%');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NrEG7Z0c62N"},"outputs":[],"source":["corpus_root_filename"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Qps8ecocyet"},"outputs":[],"source":["corpus_sents_df.to_csv(f'sum_4andDNN_{corpus_root_filename}.csv')"]},{"cell_type":"markdown","metadata":{"id":"t-fDw_6w5wwO"},"source":["**Ref: https://www.kaggle.com/aditya6040/7-models-on-imdb-dataset-best-score-88-2/notebook#CNN-Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTSn2QQR8IC-"},"outputs":[],"source":["embedding_vecor_length = 32\n","callback = EarlyStopping(monitor='val_loss', patience=2)\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","print(model.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O52tFKAjpf-D"},"outputs":[],"source":["model.fit(X_train1, y_train, epochs=10, batch_size=256,verbose = 1,validation_data=(X_valid1,y_valid),callbacks=[callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6W0pNrHpf4W"},"outputs":[],"source":["accuracy_score(y_test, model.predict(X_test1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uk7FcX-mpfy2"},"outputs":[],"source":["history.history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vqg9rTOpfsw"},"outputs":[],"source":["plot_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FufBJTkbppAf"},"outputs":[],"source":["plt.figure(figsize=(4,4))\n","sns.heatmap(confusion_matrix(y_test, model.predict(X_test1)),annot=True,cmap='coolwarm',xticklabels=[0,1],fmt='d',annot_kws={\"fontsize\":19})\n","plt.xlabel(\"Predicted\",fontsize=16)\n","plt.ylabel(\"Actual\",fontsize=16)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2m5RBxppo8O"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"O9ZNqxlA8RWK"},"source":["### **CNN**\n","\n","* https://github.com/bentrevett/pytorch-sentiment-analysis (CNN w/GLoVE and IMDB)\n","\n","* https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMIrZkmj8H8f"},"outputs":[],"source":["embedding_vecor_length = 32\n","callback = EarlyStopping(monitor='val_loss', patience=2)\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n","model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(LSTM(100))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","print(model.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F4yqkemapuyH"},"outputs":[],"source":["%%time\n","\n","# NOTE: \n","\n","model.fit(X_train1, y_train, epochs=10, batch_size=256,verbose = 1,validation_data=(X_valid1,y_valid),callbacks=[callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZy5hT67pus-"},"outputs":[],"source":["accuracy_score(y_test, model.predict_classes(X_test1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxjdLVp9punq"},"outputs":[],"source":["plot_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KBSDjJhdRs4"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWCJM742dRir"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"xHfsEmzN6oYf"},"source":["### **BERT**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ua1As3UJpuii"},"outputs":[],"source":["# https://www.kaggle.com/derrelldsouza/imdb-sentiment-analysis-eda-ml-lstm-bert#5.-Predictive-Modelling-using-Deep-Learning\n","\n","#Perform tokenization\n","# automatically download the vocab used during pretraining or fine-tuning a given model,use from_pretrained() method\n","tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8YcSOql2pudG"},"outputs":[],"source":["#pass our texts to the tokenizer\n","\n","Xtrain_enc = tokenizer(Xtrain.tolist(), max_length=max_seq_len, \n","                         truncation=True, padding='max_length', \n","                         add_special_tokens=True, return_tensors='np') #return numpy object\n","Xval_enc = tokenizer(Xval.tolist(), max_length=max_seq_len, \n","                         truncation=True, padding='max_length', \n","                         add_special_tokens=True, return_tensors='np') #return numpy object\n","Xtest_enc = tokenizer(Xtest.tolist(), max_length=max_seq_len, \n","                         truncation=True, padding='max_length', \n","                         add_special_tokens=True, return_tensors='np') #return numpy object"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVviZDtmpuY2"},"outputs":[],"source":["#preparing our datasets\n","\n","train_dataset = tf.data.Dataset.from_tensor_slices((\n","    dict(Xtrain_enc),\n","    ytrain\n","))\n","val_dataset = tf.data.Dataset.from_tensor_slices((\n","    dict(Xval_enc),\n","    yval\n","))\n","test_dataset = tf.data.Dataset.from_tensor_slices((\n","    dict(Xtest_enc),\n","    ytest\n","))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DvOYExK969Bi"},"outputs":[],"source":["def bert_model(train_dataset,val_dataset,transformer,max_len,epochs):\n","    print(\"----Building the model----\")\n","    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n","    attention_mask = Input(shape=(max_len,),dtype=tf.int32,name = 'attention_mask') #attention mask\n","    sequence_output = transformer(input_ids,attention_mask)[0]\n","    cls_token = sequence_output[:, 0, :]\n","    x = Dense(512, activation='relu')(cls_token)\n","    x = Dropout(0.1)(x)\n","    y = Dense(1, activation='sigmoid')(x)\n","    model = Model(inputs=[input_ids,attention_mask], outputs=y)\n","    model.summary()\n","    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n","    r = model.fit(train_dataset.batch(32),batch_size = 32,\n","                  validation_data = val_dataset.batch(32),epochs = epochs)\n","                  #callbacks = callbacks\n","    print(\"Train score:\", model.evaluate(train_dataset.batch(32)))\n","    print(\"Validation score:\", model.evaluate(val_dataset.batch(32)))\n","    n_epochs = len(r.history['loss'])\n","    \n","    return r,model,n_epochs "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjXBoci4688U"},"outputs":[],"source":["transformer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNSErWp57AsC"},"outputs":[],"source":["epochs = 2\n","max_len = max_seq_len\n","r,model,n_epochs = bert_model(train_dataset,val_dataset,transformer,max_len,epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_gh_lqY7Ami"},"outputs":[],"source":["#Plot accuracy and loss\n","plotLearningCurve(r,n_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IY-9_U257G47"},"outputs":[],"source":["print(\"Evaluate Model Performance on Test set\")\n","result = model.evaluate(test_dataset.batch(32))\n","print(dict(zip(model.metrics_names, result)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pIqOGTi7GzN"},"outputs":[],"source":["#Generate predictions for the test dataset\n","ypred = model.predict(test_dataset.batch(32))\n","ypred = ypred>0.5\n","#Get the confusion matrix\n","cf_matrix = confusion_matrix(ytest, ypred)\n","sns.heatmap(cf_matrix,annot = True,fmt ='g', cmap='Blues')\n","plt.xlabel('Predicted label')\n","plt.ylabel('True label')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DKWimdXSq4KB"},"source":["## **AutoKeras**\n","\n","* https://autokeras.com/tutorial/text_classification/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVhrU7nOriiz"},"outputs":[],"source":["# RESTART RUNTIME\n","\n","!pip install autokeras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrtoVghPrB5I"},"outputs":[],"source":["import os\n","\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.datasets import load_files\n","\n","import autokeras as ak"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RUCSp_9Kq3ZS"},"outputs":[],"source":["dataset = tf.keras.utils.get_file(\n","    fname=\"aclImdb.tar.gz\",\n","    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n","    extract=True,\n",")\n","\n","# set path to dataset\n","IMDB_DATADIR = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n","\n","classes = [\"pos\", \"neg\"]\n","train_data = load_files(\n","    os.path.join(IMDB_DATADIR, \"train\"), shuffle=True, categories=classes\n",")\n","test_data = load_files(\n","    os.path.join(IMDB_DATADIR, \"test\"), shuffle=False, categories=classes\n",")\n","\n","x_train = np.array(train_data.data)\n","y_train = np.array(train_data.target)\n","x_test = np.array(test_data.data)\n","y_test = np.array(test_data.target)\n","\n","print(x_train.shape)  # (25000,)\n","print(y_train.shape)  # (25000, 1)\n","print(x_train[0][:50])  # this film was just brilliant casting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2gamB_CrHfk"},"outputs":[],"source":["# Initialize the text classifier.\n","clf = ak.TextClassifier(\n","    overwrite=True, max_trials=1\n",")  # It only tries 1 model as a quick demo.\n","\n","# Feed the text classifier with training data.\n","clf.fit(x_train, y_train, epochs=2)\n","\n","# Predict with the best model.\n","predicted_y = clf.predict(x_test)\n","\n","# Evaluate the best model with testing data.\n","print(clf.evaluate(x_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TwcFG-vrHXo"},"outputs":[],"source":["clf.fit(\n","    x_train,\n","    y_train,\n","    # Split the training data and use the last 15% as validation data.\n","    validation_split=0.15,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p78W87xsrt7O"},"outputs":[],"source":["\"\"\"\n","split = 5000\n","x_val = x_train[split:]\n","y_val = y_train[split:]\n","x_train = x_train[:split]\n","y_train = y_train[:split]\n","\n","clf.fit(\n","    x_train,\n","    y_train,\n","    epochs=2,\n","    # Use your own validation set.\n","    validation_data=(x_val, y_val),\n",")\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X458PVj1rtyJ"},"outputs":[],"source":["input_node = ak.TextInput()\n","output_node = ak.TextBlock(block_type=\"ngram\")(input_node)\n","output_node = ak.ClassificationHead()(output_node)\n","clf = ak.AutoModel(\n","    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",")\n","clf.fit(x_train, y_train, epochs=2)"]},{"cell_type":"markdown","metadata":{"id":"RGe1OgqcoHL5"},"source":["## **Pytorch-Optimize**\n","\n","* https://github.com/jettify/pytorch-optimizer (20210705 2k)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58WpZNZooG45"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"e6KRUi49nuT9"},"source":["## **Keras-Tuner**\n","\n","* https://github.com/keras-team/keras-tuner"]},{"cell_type":"markdown","metadata":{"id":"KZb_79A59c8r"},"source":["# **END**"]},{"cell_type":"markdown","metadata":{"id":"JWRZ7aHOd0R3"},"source":["# **[OLD STARTING POINT]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oiapgHzZdm4x"},"outputs":[],"source":["groups_ls = ['models_baseline_ls',\n","                'models_sentimentr_ls',\n","                'models_syuzhetr_ls',\n","                'models_transformer_ls']\n","\n","# Could add suffix '_sst2' if classifiers trained on SST2 (currently requires 30m on Colab Pro/GPU+RAM)\n","models_supervised_ls = ['linreg_imdb50k',\n","                   'svc_imdb50k',\n","                   'logreg_imdb50k',\n","                   'dforest_imdb50k',\n","                   'multinb_imdb50k']\n","\n","models_baseline_ls = ['sentimentr',\n","                      'syuzhet',\n","                      'bing',\n","                      'sentiword',\n","                      'senticnet',\n","                      'nrc',\n","                      'afinn',\n","                      'vader',\n","                      'textblob',\n","                      'flair',\n","                      'pattern',\n","                      'stanza']\n","\n","models_sentimentr_ls = ['jockers_rinker',\n","                        'jockers',\n","                        'huliu',\n","                        'senticnet',\n","                        'sentiword',\n","                        'nrc',\n","                        'lmcd']\n","\n","models_syuzhetr_ls = ['syuzhet',\n","                      'bing',\n","                      'afinn',\n","                      'nrc']\n","\n","models_transformer_ls = ['roberta15lg', \n","                         'nlptown', \n","                         'yelp', \n","                         'hinglish',\n","                         'imdb2way', \n","                         'huggingface', \n","                         't5imdb50k', \n","                         'robertaxml8lang']\n","\n","# Temporarily redefine from English to French Transformer Models\n","# models_transformer_ls = ['flaubert', 'nlptown', 'robertaxml8lang']"]},{"cell_type":"markdown","metadata":{"id":"MOPa6HH-OjZp"},"source":["**Install Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"drpZJilASHUN"},"outputs":[],"source":["# fast detection of character set encoding for text/files\n","\n","!pip install cchardet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TA7Nw-SA_si1"},"outputs":[],"source":["!pip install pysbd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEjSzsusOOJ-"},"outputs":[],"source":["# common ML code\n","\n","!pip install sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZ0UVdasuTTS"},"outputs":[],"source":["%pip install contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9MbHfUCz6qTQ"},"outputs":[],"source":["!pip install pysbd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENUk4UsK6qTV"},"outputs":[],"source":["!pip install spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4Nis-KA6qTY"},"outputs":[],"source":["import pysbd\n","import spacy\n","from pysbd.utils import PySBDFactory\n","\n","# Conditionally loads english or french PySBD/NLTK Sentence tokenizers \n","#   in parags2sents()\n","\n","# nlp = spacy.blank('en')\n","\n","# nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","# explicitly adding component to pipeline\n","# (recommended - makes it more readable to tell what's going on)\n","# nlp.add_pipe(PySBDFactory(nlp))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxLPTsA_6qTa"},"outputs":[],"source":["\n","\n","# or you can use it implicitly with keyword\n","# pysbd = nlp.create_pipe('pysbd')\n","# nlp.add_pipe(pysbd)\n","\n","# doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n","# print(list(doc.sents))\n","# [My name is Jonas E. Smith., Please turn to p. 55.]"]},{"cell_type":"markdown","metadata":{"id":"cmtqmvu6OlR9"},"source":["**Import Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7bf4lfgwMEz"},"outputs":[],"source":["import os\n","import sys\n","import io\n","import glob\n","import json\n","import contextlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOmyq4h7OOFi"},"outputs":[],"source":["# IMPORT LIBRARIES\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OslLdEsvOuFU"},"outputs":[],"source":["import re\n","import string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YelenXz5BcmE"},"outputs":[],"source":["from itertools import cycle  # For plotly\n","\n","import collections\n","from collections import OrderedDict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Suximbjnw8D"},"outputs":[],"source":["# Import libraries for logging\n","\n","import logging\n","from datetime import datetime\n","import time                     # (TODO: check no dependencies and delete)\n","from time import gmtime, strftime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPZmScjVDYyw"},"outputs":[],"source":["import nltk\n","\n","# Download for sentence tokenization\n","import nltk.data\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')\n","\n","# Download for nltk/VADER sentiment analysis\n","nltk.download('vader_lexicon')\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u34kPKO0_xF_"},"outputs":[],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm') # Load the English Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMl2mfF8Haw8"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler   # To normalize time series\n","from sklearn.preprocessing import StandardScaler # To Standardize time series: center(sub mean) and rescale within 1 SD (only for well-behaved guassian distributions)\n","from sklearn.preprocessing import RobustScaler   # To Standardize time series: center(sub median) and rescale within 25%-75% (1st-3rd) IQR (better for noisy, outliers distributions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nckwluDXwa1c"},"outputs":[],"source":["minmax_scaler = MinMaxScaler()\n","mean_std_scaler = StandardScaler()\n","median_iqr_scaler = RobustScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U589lvKXmFV-"},"outputs":[],"source":["# Zoom interpolates new datapoints between existing datapoints to expand a time series \n","\n","from scipy.ndimage.interpolation import zoom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wcZfSOuBlW7"},"outputs":[],"source":["from scipy import interpolate\n","from scipy.interpolate import CubicSpline\n","from scipy import signal\n","from scipy.signal import argrelextrema"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CY3UyvYjAvDN"},"outputs":[],"source":["from statsmodels.nonparametric.smoothers_lowess import lowess as sm_lowess\n","from statsmodels import robust"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02LJQlYpgQGs"},"outputs":[],"source":["corpus_sects_df = pd.DataFrame()"]},{"cell_type":"markdown","metadata":{"id":"UcSc4jsggSy2"},"source":["**Define Library-Dependent Objects**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjGN2sN3uRpN"},"outputs":[],"source":["import contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AllIwMngDGC3"},"outputs":[],"source":["# Necessary to define before defining Utility Functions using these DataFrames\n","\n","corpus_sents_df = pd.DataFrame()"]},{"cell_type":"markdown","metadata":{"id":"Kwl0MBDyOwtX"},"source":["**Configure Jupyter Notebook**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APCau-T26XQ3"},"outputs":[],"source":["from IPython.display import HTML, display\n","\n","def my_css():\n","   display(HTML(\"\"\"<style>table.dataframe td{white-space: nowrap;}</style>\"\"\"))\n","\n","get_ipython().events.register('pre_run_cell', my_css)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nD1cyqWsfjxA"},"outputs":[],"source":["# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzfybE5kfmE-"},"outputs":[],"source":["# Configure matplotlib and seaborn\n","\n","# Plotting pretty figures and avoid blurry images\n","# %config InlineBackend.figure_format = 'retina'\n","# Larger scale for plots in notebooks\n","# sns.set_context('talk')\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = [16, 8]\n","plt.rcParams['figure.dpi'] = 100\n","plt.rc('figure', facecolor='white')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vIIjSbyeP2fg"},"outputs":[],"source":["# Configure Jupyter\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from ipywidgets import widgets, interactive\n","\n","# Configure Google Colab\n","\n","%load_ext google.colab.data_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GS_El2PiQlyP"},"outputs":[],"source":["# Text wrap\n","\n","from IPython.display import HTML\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vuM_qnOHUil5"},"outputs":[],"source":["from IPython.display import HTML\n","\n","import plotly.graph_objects as go\n","import plotly.express as px\n","import plotly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxQtrH196gl3"},"outputs":[],"source":["from IPython.display import HTML, display\n","\n","def my_css():\n","   display(HTML(\"\"\"<style>table.dataframe td{white-space: nowrap;}</style>\"\"\"))\n","\n","get_ipython().events.register('pre_run_cell', my_css)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIPzbt5Ikldp"},"outputs":[],"source":["# with pd.option_context('display.max_colwidth', None):\n","#   display(corpus_transformer_df['sent_raw'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDR54Pbg5zqz"},"outputs":[],"source":["# with pd.option_context('display.max_colwidth', None):\n","#   display(corpus_sentimentr_df.iloc[:10]['sent_raw'])"]},{"cell_type":"markdown","metadata":{"id":"4dLkfn4KFmDf"},"source":["**Configuration Details Snapshot**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FNPovQBFZky"},"outputs":[],"source":["# Snap Shot of Time, Machine, Data and Library/Version Blueprint\n","# TODO:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBPCBVkuzw_2"},"outputs":[],"source":["!pip list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5HKj6KbIzoyM"},"outputs":[],"source":["# !pip install watermark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qpMRnqjzzjS5"},"outputs":[],"source":["# %load_ext watermark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5uwS7H_zhaC"},"outputs":[],"source":["# %watermark"]},{"cell_type":"markdown","metadata":{"id":"9wiSBHxoOGZz"},"source":["# **Delete/Reset Main DataStructure**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UZXoa5lANBCM"},"outputs":[],"source":["%whos DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N3b35AXqNF7w"},"outputs":[],"source":["# %reset_selective corpus_sents_df corpus_parags_df corpus_sects_df corpus_chaps_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRiqyCInNOnY"},"outputs":[],"source":["# %reset_selective corpus_sentimentr corpus_syuzhetr_df corpus_transformer_df corpus_unified_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YP8j6XqSNbhr"},"outputs":[],"source":["# %reset_selective temp_baseline_df temp_df temp_sentimentr_df temp_syuzhetr_df temp_transformer_df unified_crux_df corr_df"]},{"cell_type":"markdown","metadata":{"id":"Ji9GwSuNZRdi"},"source":["# **Connect to Corpus Text files**"]},{"cell_type":"markdown","metadata":{"id":"6KRfiXXQOZcq"},"source":["## **Option (a): Connect to Google gDrive**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G64etjAUOOSm"},"outputs":[],"source":["# Connect to Google gDrive\n","\n","# Flag to indicate first run through code \n","flag_first_run = True\n","\n","from google.colab import drive, files\n","drive.mount('/gdrive')\n","%cd /gdrive/MyDrive/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0fvFZq-eFaw"},"outputs":[],"source":["# Select the Corpus subdirectory on your Google gDrive\n","\n","# Done\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/imcewan_machineslikeme\" #@param {type:\"string\"}\n","\n","\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/cdickens_greatexpectations\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/ddefoe_robinsoncrusoe\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/emforster_howardsend\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/fdouglass_narrativelifeslave\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/fscottfitzgerald_thegreatgatsby\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/geliot_middlemarch\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/hjames_portraitofalady\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/homer_odyssey\" #@param {type:\"string\"}\n","gdrive_subdir = \"./research/2021/sa_book_code/books_sa/jausten_prideandprejudice\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/jconrad_heartofdarkness\" #@param {type:\"string\"} \n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/jjoyce_portraitoftheartist\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/jkrowling_harrypotter\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/mproust_time\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/staugustine_confessions\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/tmorrison_beloved\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_mrsdalloway\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_thewaves\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_orlando\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vnabokov_palefire\" #@param {type:\"string\"}\n","\n","# Current\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/homer_odyssey\" #@param {type:\"string\"}\n","\n","# To do\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/geliot_middlemarch\" #@param {type:\"string\"}\n","\n","CORPUS_SUBDIR = gdrive_subdir\n","corpus_filename = CORPUS_SUBDIR\n","\n","# Change to working subdirectory\n","if flag_first_run == True:\n","  full_path_str = gdrive_subdir\n","  flag_first_run = False\n","else:\n","  full_path_str = f'/gdrive/MyDrive{gdrive_subdir[1:]}'\n","\n","%cd $full_path_str\n"]},{"cell_type":"markdown","metadata":{"id":"XKV1uMBEO8TR"},"source":["## **Option (b): Upload Corpus Textfile**\n","\n","***Only do this if your Google subdirectory doesn't already contain a plain text file of your Corpus or you wish to overwrite it and use a newer version***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bH6dlB2fO6Ln"},"outputs":[],"source":["# Execute this code cell to upload plain text file of corpus\n","#   Should be *.txt format with paragraphs separated by at least 2 newlines\n","\n","uploaded = files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3LORQ4fRGBW"},"outputs":[],"source":["# Verify file was uploaded\n","\n","# Get uploaded filename\n","corpus_filename = list(uploaded.keys())[0]\n","print(f'Uploaded Corpus filename is: {corpus_filename}')\n","CORPUS_FILENAME = corpus_filename\n","\n","!ls -al $corpus_filename"]},{"cell_type":"markdown","metadata":{"id":"Bsm8awD4AZ8O"},"source":["# **Configuration (Manual)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfilGg6Mkxnd"},"outputs":[],"source":["# Verify subdirectory change\n","\n","!pwd\n","!ls -altr *\n","\n","# TODO: Intelligently automate the filling of form based upon directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HP3WLEv_g5aq"},"outputs":[],"source":["# CORPUS_TITLE = 'Beloved' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Toni Morrison\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"tmorrison_beloved.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/tmorrison_belovedy\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Confessions' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Saint Augustine\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"staugustine_confessions.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/staugustine_confessions\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Great Expectations' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Charles Dickens\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"cdickens_greatexpectations.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/cdickens_greatexpectations\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Heart of Darkness' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Joseph Conrad\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"jconrad_heartofdarkness.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/jconrad_heartofdarkness\" #@param {type:\"string\"}\n","\n","# ORPUS_TITLE = 'Howards End' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"EM Forster\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"emforster_howardsend.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/emforster_howardsend\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Machines Like Me' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Ian McEwan\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"imcewan_machineslikeme.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/imcewan_machineslikeme\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Middlemarch' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"George Eliot\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"geliot_middlemarch_wprelude.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/geliot_middlemarch\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Mrs. Dalloway' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"vwoolf_mrsdalloway.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_mrsdalloway\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Narrative Life of Frederick Douglass' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Frederick Douglass\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"fdouglass_narrative.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/fdouglass_narrativelifeslave\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Orlando' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"vwoolf_orlando.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_orlando\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Palefire - Commentary' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Vladimir Nabokov\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"vnabokov_palefire_commentary.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vnabokov_palefire\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Portrait of a Lady' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Henry James\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"hjames_portraitofalady.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/hjames_portraitofalady\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Portrait of the Artist as a Young Man' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"James Joyce\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"jjoyce_portraitoftheartist.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/jjoyce_portraitoftheartist\" #@param {type:\"string\"}\n","\n","CORPUS_TITLE = 'Pride and Prejudice' #@param {type:\"string\"}\n","CORPUS_AUTHOR = \"Jane Austen\" #@param {type:\"string\"}\n","CORPUS_FILENAME = \"jausten_prideandprejustice.txt\" #@param {type:\"string\"}\n","CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/jausten_prideandprejustice\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Robinson Crusoe' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Daniel Defoe\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"ddefoe_robinsoncrusoe.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/ddefoe_robinsoncrusoe\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Great Gatsby' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"F. Scott Fitzgerald\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"fscottfitzgerald_thegreatgatsby.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/fscottfitzgerald_thegreatgatsby\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Socerers Stone' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"J.K. Rowling\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"hpotter1_sorcerersstone.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/jkrowling_harrypotter\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Waves' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"vwoolf_thewaves.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_thewaves\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'To The Lighthouse' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"vwoolf_tothelighthouse.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Odyssey' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Homer SButler\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"sbutler_odyssey.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/sbutler_odyssey\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Odyssey' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Homer EWilson\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"ewilson_odyssey.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/homer_odyssey\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Guermantes Way - English' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Marcel Proust\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"mproust_3guermantesway_mtreharne_en.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/mproust_time\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Guermantes Way - French' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Marcel Proust\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"mproust_guermantes_fr.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/mproust_time\"  #@param {type:\"string\"}\n","\n","CORPUS_LANGUAGE = \"English\" #@param [\"English\", \"French\"]\n","\n","CHAPTER_HEADINGS = \"CHAPTER\" #@param [\"CHAPTER\", \"BOOK\", \"None\"]\n","CHAPTER_NUMBERING = \"Arabic (1,2,...)\" #@param [\"Arabic (1,2,...)\", \"Roman (I,II,...)\"]\n","SECTION_HEADINGS = \"None\" #@param [\"SECTION (ArabicNo)\", \"SECTION (RomanNo)\", \"----- (Hyphens)\", \"None\"]\n","\n","LEXICONS_SUBDIR = \"./research/2021/sa_book_code/books_sa/lexicons\" #@param {type:\"string\"}\n","\n","CORPUS_FULL = f'{CORPUS_TITLE} by: {CORPUS_AUTHOR}'\n","\n","PLOT_OUTPUT = \"Major\" #@param [\"None\", \"Major\", \"All\"]\n","\n","FILE_OUTPUT = \"Major\" #@param [\"None\", \"Major\", \"All\"]\n","\n","\n","gdrive_subdir = CORPUS_SUBDIR\n","corpus_filename = CORPUS_FILENAME\n","CORPUS_LANGUAGE = CORPUS_LANGUAGE.lower()\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","author_abbr_str = (CORPUS_AUTHOR.split(' ')[0][0]+CORPUS_AUTHOR.split(' ')[1]).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","title_str = re.sub(r'[^A-Za-z0-9]','', title_str).lower()\n","\n","print(f'\\nWorking Corpus Datafile: ------------------------------ \\n\\n    {CORPUS_SUBDIR}')\n","print(f'\\nFull Corpus Title/Author: ------------------------------ \\n\\n    {CORPUS_FULL}')\n","\n","\n","if CHAPTER_HEADINGS == 'CHAPTER':\n","  if CHAPTER_NUMBERING == \"Arabic (1,2,...)\":\n","    # pattern_chap = r'CHAPTER [0123456789]{1,2} ' # [\\.]?[^\\n]*'\n","    pattern_chap = r'CHAPTER [0123456789]{1,2}[.]?[^\\n]*' # [os.return]*'\n","  elif CHAPTER_NUMBERING == \"Roman (I,II,...)\":\n","    pattern_chap = r'CHAPTER[\\s]{1,5}[IVXL]{1,10}[.:]?[\\s]+' # [^\\n]+'\n","    # pattern_chap = r'CHAPTER[\\s]{1,}[IVXL]{1,10}[.:]?[^\\n\\r]*'\n","  else:\n","    print(f'ERROR: Illegal CHAPTER_NUMBERING value = {CHAPTER_NUMBERING}')\n","\n","elif CHAPTER_HEADINGS == 'BOOK':\n","  if CHAPTER_NUMBERING == \"Arabic (1,2,...)\":\n","    pattern_chap = r'BOOK [0123456789]{1,2}[.]?[^\\n]*'\n","  elif CHAPTER_NUMBERING == \"Roman (I,II,...)\":\n","    pattern_chap = r'[\\s]*BOOK[\\s]{1,5}[IVXL]{1,10}[.:]?[\\s]+' # [.:]?[\\s]*[^\\n]*[\\n\\r]+' # ]{0,1}[^\\n]*' # [^\\n]*' # Problems with embedded 'Book'\n","  else:\n","    print(f'ERROR: Illegal CHAPTER_NUMBERING value = {CHAPTER_NUMBERING}')\n","\n","elif CHAPTER_HEADINGS == \"None\":\n","  pattern_chap = r'CHAPTER [0123456789]{1,2}[.]?[^\\n]*'\n","\n","else:\n","  print(f'ERROR: Illegal CHAPTER_HEADINGS value = {CHAPTER_HEADINGS}')\n","\n","# Default Section RegEx Pattern\n","pattern_sect = 'SECTION [0123456789]{1,2}[^\\n]*'\n","\n","if SECTION_HEADINGS == 'SECTION (ArabicNo)':\n","  # pattern_sect = r'SECTION [0-9]{1,2} [^\\n]*'\n","  # TODO: [^\\n] gets parsed into [^\\\\n] causing problems, so simplify\n","  pattern_sect = r'SECTION [0123456789]{1,2}[.:]?[^\\n]*'\n","elif SECTION_HEADINGS == 'SECTION (RomanNo)':\n","  pattern_sect = r'SECTION [IVXL]{1,10}[.:]?[^\\n\\r]+' # } [A-Z \\.-:—;-’\\'\"]*[\\n]*'\n","elif SECTION_HEADINGS == '----- (Hyphens)':\n","  pattern_sect = r'^[- ]{3,}[^\\n]*'\n","elif SECTION_HEADINGS == 'None':\n","  pass\n","else:\n","  print(f'ERROR: Illegal SECTION_HEADING value = {SECTION_HEADINGS}')\n","\n","print(f'\\nCHAPTER Headings: ------------------------------ \\n\\n    {CHAPTER_HEADINGS}')\n","\n","print(f'\\nSECTION Headings: ------------------------------ \\n\\n    {SECTION_HEADINGS}')\n","\n","\n","print(f'\\nCorpus file information: ------------------------------ \\n')\n","!ls -al $CORPUS_FILENAME\n","\n","# Verify contents of Corpus File is Correctly Formatted\n","#   \n","# TODO: ./utils/verify_format.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnWBZCGMT-0G"},"outputs":[],"source":["corpus_filename\n","print('\\n')\n","CORPUS_FULL"]},{"cell_type":"markdown","metadata":{"id":"8owpM75RILKn"},"source":["# **Utility Functions (Auto)**"]},{"cell_type":"markdown","metadata":{"id":"aMLbyx6gIPqj"},"source":["## **File Manipulations**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vBOvvP-BSIiC"},"outputs":[],"source":["# https://dev.to/bowmanjd/character-encodings-and-detection-with-python-chardet-and-cchardet-4hj7\n","\n","import cchardet as chardet\n","\n","from pathlib import Path\n","import sys\n","\n","def get_file_encoding(filename):\n","    \"\"\"Detect encoding and return decoded text, encoding, and confidence level.\"\"\"\n","    filepath = Path(filename)\n","\n","    # We must read as binary (bytes) because we don't yet know encoding\n","    blob = filepath.read_bytes()\n","\n","    detection = chardet.detect(blob)\n","    encoding = detection[\"encoding\"]\n","    confidence = detection[\"confidence\"]\n","    text = blob.decode(encoding)\n","\n","    return text, encoding, confidence"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"09qXAWa0jStn"},"outputs":[],"source":["# re.split(r'SECTION', 'There is one SECTION in this. - can you SECTION string', flags=re.I)\n","\n","re.split(r'SECTION', 'There is one string', flags=re.I)\n"]},{"cell_type":"markdown","metadata":{"id":"2CEiOZzv5lw9"},"source":["## **Text Wrangling**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KQJdrsbpkSSw"},"outputs":[],"source":["import nltk\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Mxh27xfMkXDN"},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer \n","\n","# Init the Wordnet Lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Lemmatize Single Word\n","print(lemmatizer.lemmatize(\"bats\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dIjRdm3rlw85"},"outputs":[],"source":["test_str = \"I love eating bats and driving many cars.\" #  around the darken town at night.\"\n","\n","doc = nlp(test_str)\n","\n","# Extract the lemma for each token and join\n","\" \".join([token.lemma_ for token in doc])\n","#> 'the strip bat be hang on -PRON- foot for good'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"v374fE0tIrea"},"outputs":[],"source":["\"\"\"\n","\n","def stem_str(text_str):\n","  '''\n","  Given a text string\n","  Return the same string with all tokens stemmed where possible\n","  '''\n","\n","  from nltk.stem import WordNetLemmatizer # For stemming the sentence\n","  from nltk.stem import SnowballStemmer # For stemming the sentence\n","\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gZaL4cZbg3td"},"outputs":[],"source":["def lemmatize_str(text_str):\n","  '''\n","  Given a text string\n","  Return the same string with all tokens lemmatized where possible\n","  '''\n","\n","  # NOTE: depends upon Setup above importing minimal SpaCy pipeline defined as [nlp]\n","\n","  # Lemmaitization MAY help performance, but depends on SA Model and Corpus (see below)\n","  # https://opendatagroup.github.io/data%20science/2019/03/21/preprocessing-text.html \n","  # Which library: NLTK, SpaCy, TextBlob, CLiPS Pattern, gensim, Stanford OpenNLP/Stanza, Flair, etc.\n","  # https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n","  # Spelling, Contractions, etc\n","  # https://cnvrg.io/sentiment-analysis-python/\n","\n","  text_lemma_ls = []\n","\n","  token_ls = text_str.split()\n","\n","  for atoken in token_ls:\n","    if len(atoken) > 2:\n","      print(f'lemmatizing: {atoken}')\n","      atoken_lemma = lemmatizer.lemmatize(atoken)\n","      print(f'    lemma: {atoken_lemma}')\n","      text_lemma_ls.append(atoken_lemma)\n","    else:\n","      print(f'skip lemmatizing')\n","      text_lemma_ls.append(atoken)\n","\n","  text_lemma_str = ' '.join(text_lemma_ls)\n","\n","  return text_lemma_str.strip()\n","\n","# Test\n","\n","test_str = \"I love eating bats and driving many cars.\" #  around the darken town at night.\"\n","\n","test_lemma_str = lemmatize_str(test_str)\n","print(f'test_lemma_str: {test_lemma_str}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HAl8P6zSkzBM"},"outputs":[],"source":["lemmatizer.lemmatize('eating')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GGje9LyN5hx4"},"outputs":[],"source":["def del_leadroman(str_raw):\n","  '''\n","  Given a text string\n","  Return the string with any possible leaning roman numerals removed [IVXLCM]\n","  '''\n","\n","\n","  # Rule 1: consecutive 'i' tokens indicate a roman numeral followed by 'I' pronoun\n","  pattern_doublei = re.compile(r'\\b[iI][\\s]+[iI]\\b')\n","  str_clean1 = re.sub(pattern_doublei, \"i\", str_raw)\n","\n","  # Rule 2: any [vxlcm]-only tokens are stray roman numerals\n","  pattern_romanno = re.compile(r'\\b[vxlcm]{1,10}\\b')\n","  str_clean2 = re.sub(pattern_romanno, \"\", str_clean1)\n","\n","  return str_clean2\n","\n","\n","# Test\n","\n","test_str = \"\"\"I I was born in Tuckahoe, near Hillsborough, and about twelve miles from Easton, in Talbot county, Maryland. I have no accurate knowledge of my age, never having seen any authentic record containing it. By far the larger part of the slaves know as little of their ages as horses know of theirs, and it is the wish of most masters within my knowledge to keep their slaves thus ignorant. I do not remember to have ever met a slave who could tell of his birthday. They seldom come nearer to it than planting-time, harvest-time, cherry-time, spring-time, or fall-time. A want of information concerning my own was a source of unhappiness to me even during childhood. The white children could tell their ages. I could not tell why I ought to be deprived of the same privilege. I was not allowed to make any inquiries of my master concerning it. He deemed all such inquiries on the part of a slave improper and impertinent, and evidence of a restless spirit. The nearest estimate I can give makes me now between twenty-seven and twenty-eight years of age. I come to this, from hearing my master say, some time during 1835, I was about seventeen years old.\n","\n","My mother was named Harriet Bailey. She was the daughter of Isaac and Betsey Bailey, both colored, and quite dark. My mother was of a darker complexion than either my grandmother or grandfather.\n","\"\"\"\n","\n","print(f\"\\n\\nRESULT:\\n\\n{del_leadroman(test_str)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0rMQwo0cKHS4"},"outputs":[],"source":["# Defined for clean_text()\n","\n","from unicodedata import normalize\n","\n","# prepare regex for char filtering\n","re_print = re.compile('[^%s]' % re.escape(string.printable))\n","# prepare translation table for removing punctuation\n","table = str.maketrans('', '', string.punctuation)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6MMyldysrOMv"},"outputs":[],"source":["# CORPUS_LANGUAGE = 'french'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T7b75qK_Uzfv"},"outputs":[],"source":["import nltk\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OuhSaXnFU9hY"},"outputs":[],"source":["nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"541lpDXaU_P0"},"outputs":[],"source":["# Create WordNetLemmatizer object\n","wnl = WordNetLemmatizer()\n","  \n","# single word lemmatization examples\n","list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling', \n","         'driving', 'died', 'tried', 'feet']\n","for words in list1:\n","    print(words + \" ---> \" + wnl.lemmatize(words))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DsPbZyHhVu-q"},"outputs":[],"source":["type(nlp)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"802sQm-GWyRK"},"outputs":[],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","  \n","# Create a Doc object\n","doc = nlp(u'the bats saw the cats with best stripes hanging upside down by their feet')\n","  \n","# Create list of tokens from given string\n","tokens = []\n","for token in doc:\n","    tokens.append(token)\n","  \n","print(tokens)\n","#> [the, bats, saw, the, cats, with, best, stripes, hanging, upside, down, by, their, feet]\n","  \n","lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n","  \n","print(lemmatized_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kJ0v_zfHt6Nz"},"outputs":[],"source":["\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wg48zQwecr7g"},"outputs":[],"source":["from nltk.corpus import stopwords\n","stopwords_en = stopwords.words(\"english\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TtPCs5C3ctgL"},"outputs":[],"source":["stopwords_custom = ['bazinga', 'hoohaw', 'pating']\n","stopwords_en.extend(stopwords_custom)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OaFLzGy0Rdlm"},"outputs":[],"source":["def clean_stemlemma_text(text, stem_fl =False, lemma_fl=True, punct_fl=True, stopword_ls=stopwords_en):\n","  '''\n","  Given a text string, flags for stemming or lemmatizing and \n","  Return a clened version of the sting\n","\n","  Preprocess a string.\n","  :parameter\n","      :param text: string - name of column containing text\n","      :param lst_stopwords: list - list of stopwords to remove\n","      :param stem_fl: bool - whether stemming is to be applied\n","      :param lemma_fl: bool - whether lemmitisation is to be applied\n","  :return\n","      cleaned tex\n","  '''\n","  # https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794\n","\n","  ## clean (convert to lowercase then strip)\n","  text = str(text).lower().strip()\n","\n","  ## Punctuation (remove -ing, -ly, ...)\n","  if punct_fl == True:\n","      text = re.sub(r'[^\\w\\s]', '', text)\n","\n","  ## Tokenize (convert from string to list)\n","  text_ls = text.split()    ## remove Stopwords\n","  if stopword_ls is not None:\n","      text_ls = [word for word in text_ls if word not in stopword_ls]\n","  \n","  ## Stemming (remove -ing, -ly, ...)\n","  if stem_fl == True:\n","      ps = nltk.stem.porter.PorterStemmer()\n","      text_ls = [ps.stem(word) for word in text_ls]\n","              \n","  ## Lemmatisation (convert the word into root word)\n","  # https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/ \n","  if lemma_fl == True:\n","      # lem = nltk.stem.wordnet.WordNetLemmatizer()\n","      # text_ls = [lem.lemmatize(word) for word in text_ls]\n","      text_str = ' '.join(text_ls)\n","      doc = nlp(text_str)\n","      text_ls = [token.lemma_ for token in doc]\n","          \n","  ## back to string from list\n","  text = \" \".join(text_ls)\n","  \n","  return text\n","\n","# Test\n","\n","clean_stemlemma_text('I going to have a interesting dinner party this coming Saturday.')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T9FDtpQ6QaEJ"},"outputs":[],"source":["# This function converts to lower-case, removes square bracket, removes numbers/punctuation, end of line hyphens\n","\n","# https://towardsdatascience.com/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0\n","# https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/ \n","\n","def clean_text(text):\n","\n","  # normalize unicode characters\n","  #   library [normalize] imported in Setup above\n","\n","  if CORPUS_LANGUAGE == 'english':\n","    text = normalize('NFD', text).encode('ascii', 'ignore')\n","    text = text.decode('UTF-8')\n","\n","    # remove non-printable chars form each token\n","    # regex pattern [re_print] defined in Setup above\n","    text = re_print.sub('', text)\n","\n","    # to lower\n","    text = text.lower()\n","\n","    # Spelling correction\n","    # from autocorrect import Speller #correcting the spellings\n","    \n","    # Adjust apostrophes and contractions\n","    # from contractions import contractions_dict # to solve contractions\n","    text = contractions.fix(text)  # Expand contrations\n","    # TODO: Problem with The Great Gatsby [I'm] -> [I' ]\n","    text = re.sub(\"\\\\'s\", \" own\", text)  # After expanding normal apostrophes, expand possessive apostrophes \"Mary's car\" -> \"Mary own car\"\n","\n","    # Join end of line words split by continuation hyphens \n","    text = re.sub(\"-\\n\", \" \", text)       \n","    text = re.sub(\"-\\n\\r\", \" \", text)\n","    text = re.sub(\"-\\r\", \" \", text)\n","    text = re.sub(\"\\[.*?\\]\", \" \", text)\n","\n","    text = re.sub(\"-\", \" \", text)  # Special care for hypenated words well-known: choose option (a)\n","                                    # (a) 'well known', (b) 'wellknown' (c) 'well known' and 'wellknown' cf: https://datascience.stackexchange.com/questions/81072/how-to-process-the-hyphenated-english-words-for-any-nlp-problem\n","\n","    text = re.sub(\"/\", \" \", text)  # sociability/conversation/interesting -> sociability conversation interesting                             \n","\n","    # Split string into tokens\n","    line = text.split()\n","\n","    # remove punctuation from each token\n","    line = [word.translate(table) for word in line]\n","    # OLD string way: text = re.sub(\"[%s]\" % re.escape(string.punctuation), \" \", text)\n","\n","    # remove tokens with numbers in them\n","    line = [word for word in line if word.isalpha()]    \n","    # OLD stirng way: text = re.sub(\"\\w*\\d\\w*\", \" \", text)\n","\n","    # collapse/replace any whitespace(s) with a single hard space\n","    # OLD string way: text = re.sub(\"[\\n]\", \" \", text)  # Replace newline with space\n","    # reassemble tokens into single string to return\n","    text_cleaned = ' '.join(line)\n","\n","  elif CORPUS_LANGUAGE == 'french':\n","    # FRENCH: Minimal processing to preserve accents for Transformers\n","    # text = normalize('NFD', text).encode('ascii', 'ignore')\n","    # text = text.decode('UTF-8')\n","\n","    # remove non-printable chars form each token\n","    # regex pattern [re_print] defined in Setup above\n","    # text = re_print.sub('', text)\n","\n","    # to lower\n","    text = text.lower()\n","\n","    # Spelling correction\n","    # from autocorrect import Speller #correcting the spellings\n","    \n","    # Adjust apostrophes and contractions\n","    # from contractions import contractions_dict # to solve contractions\n","    # text = contractions.fix(text)  # Expand contrations\n","    # TODO: Problem with The Great Gatsby [I'm] -> [I' ]\n","    # text = re.sub(\"\\\\'s\", \" own\", text)  # After expanding normal apostrophes, expand possessive apostrophes \"Mary's car\" -> \"Mary own car\"\n","\n","    # Join end of line words split by continuation hyphens \n","    text = re.sub(\"-\\n\", \" \", text)       \n","    text = re.sub(\"-\\n\\r\", \" \", text)\n","    text = re.sub(\"-\\r\", \" \", text)\n","    text = re.sub(\"\\[.*?\\]\", \" \", text)\n","\n","    text = re.sub(\"-\", \" \", text)  # Special care for hypenated words well-known: choose option (a)\n","                                    # (a) 'well known', (b) 'wellknown' (c) 'well known' and 'wellknown' cf: https://datascience.stackexchange.com/questions/81072/how-to-process-the-hyphenated-english-words-for-any-nlp-problem\n","\n","    text = re.sub(\"/\", \" \", text)  # sociability/conversation/interesting -> sociability conversation interesting                             \n","\n","    # Split string into tokens\n","    line = text.split()\n","\n","    # remove punctuation from each token\n","    # line = [word.translate(table) for word in line]\n","    # OLD string way: text = re.sub(\"[%s]\" % re.escape(string.punctuation), \" \", text)\n","\n","    # remove tokens with numbers in them\n","    line = [word for word in line if word.isalpha()]    \n","    # OLD stirng way: text = re.sub(\"\\w*\\d\\w*\", \" \", text)\n","\n","    # collapse/replace any whitespace(s) with a single hard space\n","    # OLD string way: text = re.sub(\"[\\n]\", \" \", text)  # Replace newline with space\n","    # reassemble tokens into single string to return\n","    text_cleaned = ' '.join(line)\n","\n","  else:\n","    print(f'ERROR: CORPUS_LANG must be [english|french] but was set to: {CORPUS_LANG}')\n","\n","  return text_cleaned\n","\n","# Test\n","\n","print(clean_text(\"Le pépiement matinal des oiseaux semblait insipide à Françoise. I'm going to eat at Sloppy Joes's Place tonight.\"))\n","\"\"\"\n","\n","# clean a list of lines\n","def clean_lines(lines):\n","\tcleaned = list()\n","\t# prepare regex for char filtering\n","\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor line in lines:\n","\t\t# normalize unicode characters\n","\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n","\t\tline = line.decode('UTF-8')\n","\t\t# tokenize on white space\n","\t\tline = line.split()\n","\t\t# convert to lower case\n","\t\tline = [word.lower() for word in line]\n","\t\t# remove punctuation from each token\n","\t\tline = [word.translate(table) for word in line]\n","\t\t# remove non-printable chars form each token\n","\t\tline = [re_print.sub('', w) for w in line]\n","\t\t# remove tokens with numbers in them\n","\t\tline = [word for word in line if word.isalpha()]\n","\t\t# store as string\n","\t\tcleaned.append(' '.join(line))\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yZFUOC43UL2E"},"outputs":[],"source":["import string\n","string.punctuation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MQjBZrzCUHo8"},"outputs":[],"source":["test_str = 'what the @#$*(! do you mean!??'\n","print(test_str.strip(string.punctuation))\n","\n","res = re.sub(r'[^\\w\\s]', '', test_str)\n","print(f\"res = {' '.join(res.split())}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KxqqtrotUAPI"},"outputs":[],"source":["\"\"\"\n","\n","import string\n","p = re.compile(\"[\" + re.escape(string.punctuation) + \"]\")\n","print(p.sub(\"\", \"\\\"hello world!\\\", he's told me.\"))\n","\n","import string\n","string.punctuation\n","'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n","'!Hello.'.strip(string.punctuation)\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4ZSXxliIZOJS"},"outputs":[],"source":["test_str = 'CHAPTER 1. -  HELLO\\n\\n  '\n","# test_str = 'The rain in Spain\\n\\n  '\n","\n","test_str.isupper()\n","\n","test_str.islower()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"u7m6cUZVXuBw"},"outputs":[],"source":["test_str = 'CHAPTER 1. HELLOa'\n","\n","res_str = re.sub(r'[^\\s\\w]','', test_str)\n","print(f'res_str: {res_str}')\n","print(True == re.match(r'[A-Z]', res_str))\n","\n","if (re.match(r'[^A-Z]', re.sub(r'[^\\s\\w]','', test_str))):\n","  print('True')\n","else:\n","  print('False')\n","\n","if (re.match(r'[^A-Z]', re.sub(r'[^\\s\\w]','', test_str))):\n","  print('True')\n","else:\n","  print('False')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T2ozSRonDhhV"},"outputs":[],"source":["def del_sectchap_headers(text_raw_ls, text_type='section'):\n","  '''\n","  Given a list of either Chapter or Section texts (with possible embedded SECTION or CHAPTER lines)\n","  Return 2 lists free of all SECTION and CHAPTER lines, with the _clean_ version passed through clean_text(text_raw)\n","  '''\n","\n","  print(f'Entered clean_sectchap() with text_type: {text_type} and text_raw_ls len: {len(text_raw_ls)}')\n","  text_filtered_ls = []\n","  text_headers_ls = []\n","\n","  if text_type == 'chapter':\n","    text_min_len = MIN_CHAP_LEN\n","  elif text_type == 'section':\n","    text_min_len = MIN_SECT_LEN\n","  else:\n","    print(f\"ERROR: In clean_sectchap() with text_type={text_type}, must be ['section'|'chapter']\")\n","    return [-99], [-99], [-99], [-99], [-99], '-99' # Return with ERROR condition\n","\n","\n","  # Strip off whitespace\n","  text_raw_ls = [x.strip() for x in text_raw_ls]\n","\n","  # Filter out chapters that are empty or shorter than MIN_PARAG_LEN\n","  text_raw_ls = [x for x in text_raw_ls if not (len(x.strip()) <= text_min_len)]\n","\n","  # Filter out SECTION/CHAPTER lines (could be embedded or leading)\n","  corpus_segs_filtered_ls = []\n","\n","  if text_type == 'chapter':\n","    # Remove possible SECTION lines within Chapter text segments\n","    print(f'Removing any SECTION headers from Chapters')\n","\n","    for i, achap_raw in enumerate(text_raw_ls):  #  (corpus_chaps_raw_ls):\n","      print(f'  In Chapter #{i}: {achap_raw[:50]}\\n')\n","      achap_nosectheads_ls = []\n","      if bool(re.match(rf\"{pattern_sect}\", achap_raw)):\n","        print(f'    Before filtering SECTION line, len: {len(achap_raw)}')\n","        achap_nosectheads_ls = re.split(rf'{pattern_sect}', achap_raw, flags=re.I) # , flags=re.I)\n","        achap_nosectheads_ls = [x.strip() for x in achap_nosectheads_ls]\n","        achap_nosectheads_ls = [x for x in achap_nosectheads_ls if len(x) > text_min_len]\n","        achap_sects_noheads_str = '\\n\\n'.join([x.strip() for x in achap_nosectheads_ls])\n","        achap_nosectheads_ls = [x.strip() for x in achap_nosectheads_ls]\n","        print(f'    After filtering SECTION line, len: {len(achap_sects_noheads_str)}')\n","        print(f'                                  {len(achap_nosectheads_ls)} SECTION headers in Chapter #{i}')\n","      else:\n","        print(f'    No filtering needed')\n","        achap_sects_noheads_str = achap_raw.strip()\n","      text_filtered_ls.append(achap_sects_noheads_str)\n","\n","    print(f'  Chapter #{i} filtered of any SECTION lines')\n","\n","  elif text_type == 'section':\n","    # Remove possible CHAPTER lines within Section text segments\n","    print(f'Removing any CHAPTER headers from Sections')\n","\n","    for i, asect_raw in enumerate(text_raw_ls):  #  (corpus_sects_raw_ls):\n","      print(f'  In Section #{i}: {asect_raw[:50]}\\n')\n","      asect_nochapheads_ls = []\n","      if bool(re.match(rf\"{pattern_chap}\", asect_raw)):\n","        print(f'    Before filtering CHAPTER line, len: {len(asect_raw)}')\n","        asect_nochapheads_ls = re.split(rf'{pattern_chap}', asect_raw, flags=re.I) # , flags=re.I)\n","        asect_nochapheads_ls = [x for x in asect_nochapheads_ls if not re.escape(x).isupper()]  # TODO: Don't Assume CHAPTER X. TITLE IN ALL CAPS\n","        asect_nochapheads_ls = [x.strip() for x in asect_nochapheads_ls]\n","        asect_nochapheads_ls = [x for x in asect_nochapheads_ls if len(x) > text_min_len]\n","        asect_chaps_noheads_str = '\\n\\n'.join([x.strip() for x in asect_nochapheads_ls])\n","        asect_nochapheads_ls = [x.strip() for x in asect_nochapheads_ls]\n","        print(f'    After filtering CHAPTER line, len: {len(asect_chaps_noheads_str)}')\n","        print(f'                                       {len(asect_nochapheads_ls)} CHAPTER headers in Section #{i}')\n","      else:\n","        print(f'    No filtering needed')\n","        asect_chaps_noheads_str = asect_raw.strip()\n","      text_filtered_ls.append(asect_chaps_noheads_str)\n","\n","    print(f'  Section #{i} filtered of any CHAPTER lines')\n","\n","  # Collapse multiple whitespaces down to one\n","  text_filtered_ls = [re.sub(r'[ ]{2,}',' ', x) for x in text_filtered_ls] # ' '.join(x.split()).strip() for x in corpus_segs_raw_ls]\n","\n","  # Filter out chapters that are empty or shorter than MIN_PARAG_LEN\n","  text_filtered_ls = [x for x in text_filtered_ls if not (len(x.strip()) <= text_min_len)]\n","\n","  # Call clean_text on text sgements (Chapters/Segments)\n","  # text_raw_ls = [clean_text(x) for x in text_filtered_ls]\n","\n","  print(f'  Returning from clean_sectchap() with text_filtered_ls: {len(text_filtered_ls)}')\n","  \n","  return text_filtered_ls, text_raw_ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"o4Fsqx1iXTPY"},"outputs":[],"source":["def delete_multiple_element(list_object, indices):\n","  '''\n","  Given a list of objects and a list of indicies into that list \n","  Remove all the objects in the list and return the resulting shortened list\n","    being careful to remove all objects at the original index positions \n","\n","  Ref: https://thispointer.com/python-remove-elements-from-list-by-index/\n","  '''\n","\n","  indices = sorted(indices, reverse=True)\n","  for idx in indices:\n","    if idx < len(list_object):\n","      list_object.pop(idx)\n","\n","  return list_object\n","\n","# Test\n","list_of_num = [51, 52, 53, 54, 55, 56, 57, 58, 59]\n","list_of_indices = [4, 2, 6]\n","\n","# Remove elements from list_of_num at index 4,2 and 6\n","delete_multiple_element(list_of_num, list_of_indices)\n","print(list_of_num)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hS_zBObiIVoM"},"outputs":[],"source":["test_ls = ['one','two','three','four','five']\n","\n","[i for i, word in enumerate(test_ls) if len(word) == 4 ]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nzv7H9KXbzRa"},"outputs":[],"source":["def corpus2chapsect(corpus_source, corpus_type='file'):\n","  '''\n","  Given a corpus_source and a corpus_type=['file'|'string'] that tells how to to extract the corpus from corpus_source\n","    (if 'file' type, assume already %cd into correct subdir)\n","  Return a lists of Chapters and Section texts, raw or minimally processed \n","  '''\n","\n","  # corpus_chaps_raw_ls, corpus_chaps_clean_ls, corpus_sects_raw_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str = corpus2chapsect(corpus_filename)\n","\n","  # Return variables\n","  corpus_chaps_raw_ls = []      #\n","  corpus_chaps_filtered_ls = [] # List of raw/filtered/clean Chapter text segments extracted from Corpus\n","  corpus_chaps_clean_ls = []    #      length = 1 if no Chapter structure\n","  corpus_sects_raw_ls = []      # \n","  corpus_sects_filtered_ls = [] # List of raw/filtered/clean Section text segments extracted from Chapters\n","  corpus_sects_clean_ls = []    #      length = 1 if no Chapter or Section structure\n","                                #      length = Chapter segments length if no Section structure\n","  \n","  sect_chapno_ls = []           # List of Chapter numbers sequenced by unique Section number in Corpus\n","  corpus_raw_str = ''           # String with Corpus as raw string\n","  \n","  # This extra layer corpus source (file/string) allows the corpus text to have an\n","  #   additional layer optionally preprocessed after reading from file\n","  #   which could be useful for special text types (e.g. non-Latin encoding, tweets, etc)\n","  if corpus_type == 'file':\n","    # with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n","    #   corpus_raw_str = infp.read()\n","\n","    encoding_type='cp1252'\n","    encoding_type='utf-8'\n","    encoding_type=''\n","\n","    # with open(corpus_source, \"r\", encoding=CORPUS_ENCODING) as infp:\n","    with open(corpus_source, \"r\", errors='ignore') as infp:\n","      corpus_raw_str = infp.read()\n","  else:\n","    corpus_raw_str = corpus_source\n","\n","\n","\n","  # Strip out non-printing characters\n","  # corpus_clean_str = re.sub(f'[^{re.escape(string.printable)}]', ' ', corpus_raw_str)\n","  # corpus_clean_str = corpus_raw_str\n","\n","  # normalize unicode characters\n","  #   library [normalize] imported in Setup above\n","  if CORPUS_LANGUAGE == 'english':\n","    corpus_raw_str = normalize('NFD', corpus_raw_str).encode('ascii', 'ignore')\n","    corpus_raw_str = corpus_raw_str.decode('UTF-8')\n","  elif CORPUS_LANGUAGE == 'french':\n","    pass\n","  else:\n","    print(f'ERROR: CORPUS_LANGUAGE must be [english|french] but was set to: {CORPUS_LANGUAGE}')\n","\n","  # remove non-printable chars form each token\n","  #   regex pattern [re_print] defined in Setup above\n","  if CORPUS_LANGUAGE == 'english':\n","    corpus_clean_str = re_print.sub('', corpus_raw_str)\n","  elif CORPUS_LANGUAGE == 'french':\n","    corpus_clean_str = corpus_raw_str\n","  else:\n","    print(f'ERROR: CORPUS_LANGUAGE must be [english|french] but was set to: {CORPUS_LANGUAGE}')\n","\n","\n","  print(f'BEFORE regex extraction of CHAPTER/SECTION: {len(corpus_clean_str)}')\n","\n","  # Check if a Chapter structure is found in this corpus\n","  print(f'Using RegEx pattern_chap: {pattern_chap}')\n","  corpus_chaps_raw_ls = re.split(rf'{pattern_chap}', corpus_clean_str) # , flags=re.I)\n","  corpus_chap_flag = len(corpus_chaps_raw_ls) > 1\n","  # print(f'Do Chapters exist in Corpus? {corpus_chap_flag} (Chapter count = {len(corpus_chaps_raw_ls)})')\n","\n","  # Check if a Section structure is found in this corpus\n","  print(f'Using RegEx pattern_sect: {pattern_sect}')\n","  corpus_sects_raw_ls = re.split(rf'{pattern_sect}', corpus_clean_str, flags=re.I) # , flags=re.I)\n","  corpus_sect_flag = len(corpus_sects_raw_ls) > 1\n","  # print(f'Do Sections exist in Corpus? {corpus_sect_flag} (Section count = {len(corpus_sects_raw_ls)})')\n","\n","  print(f'The Corpus at {corpus_filename}:')\n","  print(f'    Has Chapters? {corpus_chap_flag} (Count: {len(corpus_chaps_raw_ls)})')\n","  print(f'    Has Sections? {corpus_sect_flag} (Count: {len(corpus_sects_raw_ls)})')\n","\n","\n","\n","  # TEST A: Does Chapter structure exist in Corpus?\n","\n","  if corpus_chap_flag == True:\n","    # If a Chapter structure is found, filter out SECTION headers from all Chapters  corpus_chaps_raw_ls\n","    corpus_chaps_filtered_ls, corpus_chaps_clean_ls = del_sectchap_headers(corpus_chaps_raw_ls, text_type='chapter')\n","    print(f'TEST A (False): {len(corpus_chaps_filtered_ls)} Chapters in Corpus')\n","\n","    # Create list of Chapter Numbers\n","    corpus_chapno_ls = list(range(len(corpus_chaps_filtered_ls)))\n","\n","    # TEST B: Does Section structure exist in Corpus with Chapter struture? (TEST A: Chapter structure exists == True)\n","\n","    if corpus_sect_flag == True:    \n","      # TODO: Verify with manual seeting SECTION_HEADINGS != \"None\":\n","      # TEST B: (True) Yes Section/Yes Chapter\n","      #         Process Sections\n","\n","      # NOTE: Could call del_sectchap_headers to segment original Corpus into Segments, but if Chapter structure exists\n","      #       it is safer to directly segement already parsed Chapters due to edge cases in various Corpora\n","      # corpus_sects_raw_ls, corpus_sects_clean_ls = del_sectchap_headers(corpus_sects_raw_ls, text_type='section')\n","\n","      print(f'  TEST A(True)/TEST B(True): {len(corpus_sects_raw_ls)} Sections in Corpus') # Chapters = True, Sections = True')  del_sectchap_headers\n","      corpus_sectno = 0\n","      corpus_sects_dt = {}       # Dictionary of key=SectionNo, value=SectionText\n","      achap_sectchapno_ls = []  # list of ChapterNo corresponding to counting sequence of SectionNo     \n","      achap_sects_raw_ls = []    # List of raw Sections extracted from current Chapter\n","      # achap_sects_clean_ls = []  # List of clean Sections extracted from current Chapter\n","      for achap_no, achap_filtered_str in enumerate(corpus_chaps_filtered_ls):\n","        # Split current filtered Chapter into multiple Sections\n","        print(f'Calling del_sectchap_headers() with text_type=section and len of Chaps: {len(achap_filtered_str)}')\n","        # Split current Chapter at Section lines\n","        achap_sects_raw_ls = re.split(rf'{pattern_sect}', achap_filtered_str, flags=re.I) # , flags=re.I)\n","        print(f'Calling del_sectchap_headers() with {len(achap_sects_raw_ls)} SECTIONS in Chapter #{achap_no}')\n","        achap_sects_filtered_ls, achap_sects_clean_ls = del_sectchap_headers(achap_sects_raw_ls, text_type='section')\n","        print(f'  Found {len(achap_sects_filtered_ls)} Sections in Chapter #{achap_no}')\n","\n","        # For each Section extracted from the current Chapter, store details\n","        achap_sects_ct = len(achap_sects_filtered_ls)\n","        for achap_asect_no, achap_asect_filtered_str in enumerate(achap_sects_raw_ls):\n","          print(f'Processed: Chapter #{achap_no}, Section #{achap_asect_no}')\n","          corpus_sects_dt[corpus_sectno] = achap_sects_filtered_ls[achap_asect_no]     # Store Section text in Dict indexed by Corpus Section No\n","          corpus_sectno += 1\n","          print(f'Section #{corpus_sectno} is in Chapter #{achap_no}')\n","          sect_chapno_ls.append(achap_no)                              # Store Chapter No corresponding to sequence of Sections in List\n","\n","        corpus_sects_filtered_ls += achap_sects_filtered_ls\n","        corpus_sects_clean_ls += achap_sects_clean_ls\n","        # sect_chapno_ls += achap_sectchapno_ls\n","\n","        # # use index to identify objects to remove from parallel data structures\n","\n","    else:\n","      # corpus_sect_flat == False:\n","      # TEST B: (False) No Section/Yes Chapter stucture\n","\n","      # Create pseudo-Sections by copying Chapters to Sections\n","      # Note, Sections much be at least as fine-grained as Chapters\n","      #       cannot have multiple Chapters with one/no Sections \n","      #       because Sections are used to mark Corpus boundaries in Plots\n","\n","      print(f'  TEST A(True)/TEST B(False): {len(corpus_sects_raw_ls)} Sections in Corpus') # Chapters = True, Sections = True')\n","      # print('Chapters = True, Sections = False')\n","      corpus_sects_filtered_ls = [x for x in corpus_chaps_filtered_ls]\n","      corpus_sects_clean_ls = [x for x in corpus_chaps_clean_ls]\n","      # Create list of Chapter Numbers for each Section\n","      sect_chapno_ls = [x for x in corpus_chapno_ls]\n","\n","\n","  else:\n","    # TEST A: (False) No Chapter structure in Corpus\n","    print(f'TEST A (False): {len(corpus_chaps_raw_ls)} Chapters in Corpus')\n","\n","    # TEST B: Does Section structure exists within Corpus without Chapter structure? (TEST A: Chapter structure exists == False)\n","    if corpus_sect_flag == True:    \n","      # TODO: Verify with manual seeting SECTION_HEADINGS != \"None\":\n","\n","      # TEST B: (True) Yes Section/No Chapter\n","      #         Create Sections with multiple entires, but dummy Chapter with just one row\n","      # If a chapter structure is found, process it\n","      print(f'  TEST A(False)/TEST B(True): {len(corpus_sects_raw_ls)} Sections in Corpus') # Chapters = True, Sections = True')\n","      # print('Chapters = False, Sections = True')\n","      corpus_sects_filtered_ls, corpus_sects_clean_ls = del_sectchap_headers(corpus_sects_raw_ls, text_type='section')\n","      # Create list of Chapter numbers for each Section (trivial since no Chapter structure)\n","      sect_chapno_ls = list(range(len(corpus_sects_filtered_ls)))\n","      # Pad out empty Chapter structure\n","      corpus_chaps_raw_ls = [corpus_raw_str]\n","      corpus_chaps_filtered_ls = [corpus_raw_str]\n","      corpus_chaps_clean_ls = [clean_text(corpus_raw_str)]\n","\n","    else:\n","      # TEST B: (False) No Section/No Chapter\n","      #         Create same dummy Section/Chapter DataFrame with just one row\n","      print(f'  TEST A(False)/TEST B(False): {len(corpus_sects_raw_ls)} Sections in Corpus') # Chapters = True, Sections = True')  achp_no\n","      # print('Chapters = False, Sections = False')\n","      corpus_chaps_raw_ls = [corpus_raw_str]\n","      corpus_chaps_filtered_ls = [corpus_raw_str]\n","      corpus_chaps_clean_ls = [clean_text(corpus_raw_str)]\n","      corpus_sects_raw_ls = [corpus_raw_str]\n","      corpus_sects_filtered_ls = [corpus_raw_str]\n","      corpus_sects_clean_ls = [clean_text(corpus_raw_str)]\n","      sect_chapno_ls = [1]\n","      \n","\n","  print(f'corpus_chaps_raw_ls: {len(corpus_chaps_raw_ls)}')\n","  print(f'corpus_chaps_clean_ls: {len(corpus_chaps_clean_ls)}')\n","  print(f'corpus_sects_raw_ls: {len(corpus_sects_raw_ls)}')\n","  print(f'corpus_sects_clean_ls: {len(corpus_sects_clean_ls)}')\n","  print(f'sect_chapno_ls: {len(sect_chapno_ls)}')\n","\n","\n","  # Last step is to remove strings that are too short/null, \n","  #   removal has to be synchronized with removal across parallel lists containing related data\n","\n","  if corpus_chap_flag == True:\n","    #  get indicies of too short Chapters to delete\n","    chaps_del_idx = [i for i, achap in enumerate(corpus_chaps_raw_ls) if len(achap) < MIN_CHAP_LEN]\n","    print(f'  Deleting {len(chaps_del_idx)} shorth/null Chapters')\n","    # use index to identify objects to remove from parallel data structure\n","    corpus_chaps_raw_ls = delete_multiple_element(corpus_chaps_raw_ls, chaps_del_idx)\n","    print(f'    returned with {len(corpus_chaps_raw_ls)} Chapters left')\n","    # corpus_chaps_clean_ls = delete_multiple_element(corpus_chaps_clean_ls, chaps_del_idx)\n","    # delete_multiple_element(chapno_ls, chaps_del_idx) \n","\n","  if corpus_sect_flag == True:\n","    # get indicies of too short Sections to be removed\n","    sects_del_idx = [i for i, asect in enumerate(corpus_sects_raw_ls) if len(asect) < MIN_SECT_LEN]  \n","    print(f'  Deleting {len(sects_del_idx)} shorth/null Sections')\n","    # use index to identify objects to remove from parallel data structures\n","    corpus_sects_raw_ls = delete_multiple_element(corpus_sects_raw_ls, sects_del_idx)\n","    corpus_sects_clean_ls = delete_multiple_element(corpus_sects_clean_ls, sects_del_idx)\n","    sect_chapno_ls = delete_multiple_element(sect_chapno_ls, sects_del_idx)\n","\n","  # TODO: Redundance check for null/empty elements after last cleaning pass\n","  corpus_sects_clean_ls = [clean_text(x) for x in corpus_sects_clean_ls]\n","  corpus_chaps_clean_ls = [clean_text(x) for x in corpus_chaps_clean_ls]\n","\n","  print(f'corpus_chaps_raw_ls: {len(corpus_chaps_raw_ls)}')\n","  print(f'corpus_chaps_clean_ls: {len(corpus_chaps_clean_ls)}')\n","  print(f'corpus_sects_raw_ls: {len(corpus_sects_raw_ls)}')\n","  print(f'corpus_sects_clean_ls: {len(corpus_sects_clean_ls)}')\n","  print(f'sect_chapno_ls: {len(sect_chapno_ls)}')\n","\n","\n","  \"\"\"\n","\n","  # parag_sents_ls = list(doc.sents)\n","\n","  \n","  # Naive method will not work, does not maintain order and synchronization across parellel data structures\n","  corpus_chaps_raw_ls = [x for x in corpus_chaps_raw_ls if len(x) > MIN_CHAP_LEN]    \n","  corpus_chaps_clean_ls = [x for x in corpus_chaps_clean_ls if len(x) > MIN_CHAP_LEN]  \n","  chaps_del_idx = [i for i, achap in enumerate(corpus_chaps_raw_ls) if len(achap) < MIN_CHAP_LEN]\n","  #  get indicies of too short Sections to be removed\n","  corpus_sects_raw_ls = [x for x in corpus_sects_raw_ls if len(x) > MIN_CHAP_LEN]    \n","  corpus_sects_clean_ls = [x for x in corpus_sects_clean_ls if len(x) > MIN_CHAP_LEN] \n","  sects_del_idx = [i for i, asect in enumerate(corpus_sects_raw_ls if len(asect) < MIN_SECT_LEN]\n","  \"\"\";\n","\n","  return corpus_chaps_raw_ls, corpus_chaps_clean_ls, corpus_sects_raw_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AceDrCdU7vLB"},"outputs":[],"source":["\"\"\"\n","\n","corpus_chaps_raw_ls, corpus_chaps_clean_ls, corpus_sects_raw_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str = corpus2chapsect(corpus_filename)\n","\n","# Create list of Chapter Numbers\n","corpus_chapno_ls = list(range(len(corpus_chaps_raw_ls)))\n","\n","# print('\\n')\n","# print(f'Chapter #{achap_no} Ch Length: {len(achap_str)}')\n","print(f'\\n             Chapters:  {len(corpus_chaps_raw_ls)}')\n","print('\\n')\n","print(f'        First Chapter:\\n    {corpus_chaps_raw_ls[0][:500]}\\n')\n","print(f'       Second Chapter:\\n    {corpus_chaps_raw_ls[1][:500]}')\n","print('\\n')\n","print(f'  Second-Last Chapter:\\n    {corpus_chaps_raw_ls[-2][:500]}\\n')\n","print(f'         Last Chapter:\\n    {corpus_chaps_raw_ls[-1][:500]}')\n","print('\\n')\n","\n","# print('\\n')\n","# print(f'Chapter #{achap_no} Ch Length: {len(achap_str)}')\n","print(f'\\n             Sections:  {len(corpus_sects_raw_ls)}')\n","print('\\n')\n","print(f'        First Section:\\n    {corpus_sects_raw_ls[0][:500]}\\n')\n","print(f'       Second Section:\\n    {corpus_sects_raw_ls[1][:500]}')\n","print('\\n')\n","print(f'  Second-Last Section:\\n    {corpus_sects_raw_ls[-2][:500]}\\n')\n","print(f'         Last Section:\\n    {corpus_sects_raw_ls[-1][:500]}')\n","print('\\n')\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"o9ObfIZA_i27"},"outputs":[],"source":["test_raw_str = \"\"\"\n","CHAPTER 0.\n","\n","Many friends have helped me in writing this book. \n","\n","CHAPTER 1.\n","\n","Some are dead and so illustrious that I scarcely dare name them, yet no one can read or write without being perpetually in the debt of Defoe, Sir Thomas Browne, Sterne, Sir Walter Scott, Lord Macaulay, Emily Bronte, De Quincey, and Walter Pater,--to name the first that come to mind. \n","\n","CHAPTER 2.\n","\n","Others are alive, and though perhaps as illustrious in their own way, are less formidable for that very reason. I am specially indebted to Mr C.P. Sanger\n","\"\"\"\n","\n","len(test_raw_str)\n","print('\\n')\n","test_raw_str = re.sub(f'[^{re.escape(string.printable)}]', ' ', test_raw_str)\n","\n","test_chaps_raw_ls, test_chaps_clean_ls, test_sects_raw_ls, test_sects_clean_ls, test_sect_chapno_ls, test_raw_str = corpus2chapsect(test_raw_str, corpus_type='string')\n","\n","\"\"\"\n","pattern = r'CHAPTER [0123456789]{1,2}[.]+[\\w]*[os.return]*'\n","test_raw_ls = re.split(rf'{pattern}', test_raw_str, flags=re.I) # , flags=re.I)\n","print(f'{len(test_raw_ls)} Chapters found in this corpus.\\n')\n","print(f'Chapter 0 len is {len(test_raw_ls[0])}')\n","\"\"\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_JPk7404_iyz"},"outputs":[],"source":["# test_raw_ls[1].strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0_nBxtq6-J8l"},"outputs":[],"source":["def filter_nonprintable(text):\n","    import itertools\n","    # Use characters of control category\n","    nonprintable = itertools.chain(range(0x00,0x20),range(0x7f,0xa0))\n","    # Use translate to remove all non-printable characters\n","    return text.translate({character:None for character in nonprintable})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PB3OrR5g8VYQ"},"outputs":[],"source":["# build a table mapping all non-printable characters to None\n","\n","NOPRINT_TRANS_TABLE = {\n","    i: None for i in range(0, sys.maxunicode + 1) if not chr(i).isprintable()\n","}\n","\n","def make_printable(s):\n","    \"\"\"Replace non-printable characters in a string.\"\"\"\n","\n","    # the translate method on str removes characters\n","    # that map to None from the string\n","    return s.translate(NOPRINT_TRANS_TABLE)\n","\n","\n","assert make_printable('Café') == 'Café'\n","assert make_printable('\\x00\\x11Hello') == 'Hello'\n","assert make_printable('') == ''\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"woDWvRZHjU2E"},"outputs":[],"source":["\"\"\"\n","def corpus2chunks(corpus_filename, sent_tok='pysbd'):\n","  '''\n","  Given a corpus filename (assuming already %cd into correct subdir) \n","    and a sentence tokeniziation method in ['pysbd'(default)|'both'|'nltk']\n","  Return 6 lists and a string of the raw corpus:\n","    4 Paragraph length lists -----\n","    parag_raw_ls : list of raw text for each paragraph\n","    parag_clean_ls : list of clean text for each sentence\n","\n","    parag_sentno_start_ls : list of the Sentence Number at the start of every Paragraph\n","    parag_sentno_end_ls : list of the Sentence Number at the end of every Paragraph\n","\n","    2 Sentence length lists -----\n","    sent_raw_ls : list of raw text for each sentence\n","    sent_clean_ls : list of clean text for each sentence\n","  '''\n","\n","  # Load PySBD if necessary\n","  if (sent_tok == 'pysbd') | (sent_tok == 'both'):\n","    from pysbd.utils import PySBDFactory\n","    nlp = spacy.blank('en')\n","    # explicitly adding component to pipeline\n","    # (recommended - makes it more readable to tell what's going on)\n","    nlp.add_pipe(PySBDFactory(nlp))\n","    # pysbd = nlp.create_pipe('pysbd')\n","    # nlp.add_pipe(pysbd)\n","    # doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n","    # print(list(doc.sents))\n","\n","  # Read file into raw text string\n","  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n","    corpus_raw_str = infp.read()\n","\n","  # Split into Raw Paragraphs\n","  print(f'BEFORE stripping out headings len: {len(corpus_raw_str)}')\n","  corpus_parags_raw_ls = re.split(r'[\\n]{2,}', corpus_raw_str)\n","  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_ls)}')\n","\n","\n","  # Copy/Clean Paragraphs into new list\n","  # Filter out numbers(often footnotes) from Paragraphs\n","  corpus_parags_ls = [re.sub(r'[0-9]','',x) for x in corpus_parags_raw_ls]\n","\n","  # Filter out empty lines Paragraphs\n","  corpus_parags_ls = [x for x in corpus_parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n","\n","  # Strip out non-printing characters\n","  corpus_parags_ls = [re.sub(f'[^{re.escape(string.printable)}]', '', x) for x in corpus_parags_ls]\n","\n","  print(f'   Parag count before processing sents: {len(corpus_parags_ls)}')\n","  # FIRST PASS at Sentence Tokenization with PySBD\n","  corpus_sents_all_ls = []\n","  for i, aparag in enumerate(corpus_parags_ls):\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AMA-040nIypX"},"outputs":[],"source":["def corpus2lines(corpus_filename, pysbd_only=False):\n","  '''\n","  Given a corpus_filename (assuming already %cd into correct subdir)\n","  Return a list of every line defined by puncutation/NLTK.sent_tokenize or newlines [\\n]{2,}\n","  '''\n","\n","  from pysbd.utils import PySBDFactory\n","  nlp = spacy.blank('en')\n","  # explicitly adding component to pipeline\n","  # (recommended - makes it more readable to tell what's going on)\n","  nlp.add_pipe(PySBDFactory(nlp))\n","  # pysbd = nlp.create_pipe('pysbd')\n","  # nlp.add_pipe(pysbd)\n","  # doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n","  # print(list(doc.sents))\n","\n","  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n","    corpus_raw_str = infp.read()\n","\n","  print(f'BEFORE stripping out headings len: {len(corpus_raw_str)}')\n","\n","  corpus_parags_ls = re.split(r'[\\n]{2,}', corpus_raw_str)\n","  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_ls)}')\n","\n","  # Strip off whitespace from Paragraphs\n","  corpus_parags_ls = [x.strip() for x in corpus_parags_ls]\n","\n","  # Filter out numbers(often footnotes) from Paragraphs\n","  corpus_parags_ls = [re.sub(r'[0-9]','',x) for x in corpus_parags_ls]\n","\n","  # Filter out empty lines Paragraphs\n","  corpus_parags_ls = [x for x in corpus_parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n","\n","  # Strip out non-printing characters\n","  corpus_parags_ls = [re.sub(f'[^{re.escape(string.printable)}]', '', x) for x in corpus_parags_ls]\n","\n","  print(f'   Parag count before processing sents: {len(corpus_parags_ls)}')\n","  # FIRST PASS at Sentence Tokenization with PySBD\n","  corpus_sents_all_ls = []\n","  for i, aparag in enumerate(corpus_parags_ls):\n","\n","    # Generally PySBD outperforms NLTK and SpaCy, \n","    #   but for Samuel Butler's 1900 translation of Homer's Odyssey\n","    #   it failed in many cases so we combine/stack PySBD with NLTK\n","    #   (exception to rule: NLTK > SpaCy for SentTokenization circa 2020\n","    #    https://www.kaggle.com/questions-and-answers/130344)\n","\n","    # NLTK Sentence Tokenization\n","    # 3605 lines with 'To the Lighthouse' by V.Woolf\n","    # aparag_sents_ls = (sent_tokenize(aparag))\n","    \n","    # SpaCy Sentence Tokenization\n","    # TODO: Speed up my specializaing pipe\n","    # 3968 lines for 'To the Lighthouse' by V.Woolf\n","    # doc = nlp(aparag)   \n","    # aparag_sents_ls = [sent for sent in doc.sents]\n","    # aparag_sents_ls = [x for x in doc]\n","\n","    # FIRST, tokenize with PySBD\n","    # PySBD Sentence Tokenization\n","    # 3457 lines for 'To the Lighthouse' by V.Woolf\n","    # using pysbd and SpaCy\n","    # or you can use it implicitly with keyword\n","    \n","\n","    aparag_nonl = re.sub('[\\n]{1,}', ' ', aparag)\n","    doc = nlp(aparag_nonl)\n","    aparag_sents_first_ls = list(doc.sents)\n","    print(f'pysbd found {len(aparag_sents_first_ls)} Sentences in Paragraph #{i}')\n","\n","    # Strip off whitespace from Sentences\n","    aparag_sents_first_ls = [str(x).strip() for x in aparag_sents_first_ls]\n","\n","    # Filter out empty line Sentences\n","    aparag_sents_first_ls = [x for x in aparag_sents_first_ls if (len(x.strip()) > MIN_SENT_LEN)]\n","\n","    print(f'      {len(aparag_sents_first_ls)} Sentences remain after cleaning')\n","\n","    corpus_sents_all_ls += aparag_sents_first_ls\n","\n","  # (OPTIONAL) SECOND PASS as Sentence Tokenization with NLTK\n","  if pysbd_only == True:\n","    # Only do one pass at Sentence tokenization with PySBD above\n","    corpus_sents_all_ls = aparag_sents_first_ls\n","  else:\n","    # Do second pass, tokenize again with NLTK to catch any Sentence tokenization missed by PySBD\n","    corpus_sents_all_second_ls = []\n","    aparag_sents_second_ls = []\n","    for asent_first in corpus_sents_all_ls:\n","      aparag_sents_second_ls = sent_tokenize(asent_first)\n","\n","      # Strip off whitespace from Sentences\n","      aparag_sents_second_ls = [str(x).strip() for x in aparag_sents_second_ls]\n","\n","      # Filter out empty line Sentences\n","      aparag_sents_second_ls = [x for x in aparag_sents_second_ls if (len(x.strip()) > MIN_SENT_LEN)]\n","\n","      corpus_sents_all_second_ls += aparag_sents_second_ls\n","\n","    corpus_sents_all_ls = corpus_sents_all_second_ls\n","\n","  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n","  # parag_before_punctstrip_ct = len(corpus_parags_ls)\n","  # corpus_parags_ls = [x for x in corpus_parags_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n","  # print(f'Punctuation only Paragraph Count: {len(corpus_parags_ls) - parag_before_punctstrip_ct}')\n","\n","  # Filter out the Section separator '-----' lines\n","  # corpus_parags_ls = [x for x in corpus_parags_ls if not (x.strip().startswith('----- '))]\n","\n","  # Filter out the Section separator 'SECTION ' lines\n","  # for i,temp_str in enumerate(corpus_parags_ls):\n","  #   if temp_str.startswith('SECTION '):\n","  #     print(f'Parag #{i}: {temp_str}')\n","  # corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('SECTION '))]\n","\n","  # Filter out any possible embedded 'SECTION ' lines\n","  # for i,temp_str in enumerate(corpus_parags_ls):\n","  #   if 'SECTION' in temp_str:   # .contains('SECTION '):\n","  #     print(f'Parag #{i}: {temp_str}')\n","  # corpus_parags_ls = del_substrs_list(corpus_parags_ls, pattern_sect) # [re.sub(rf'{pattern_sect}', '', x) for x in corpus_parags_ls]\n","\n","  # Filter out the Chapter separator 'CHAPTER ' lines\n","  # for i,temp_str in enumerate(corpus_parags_ls):\n","  #   if temp_str.startswith('CHAPTER '):\n","  #     print(f'Parag #{i}: {temp_str}')\n","  # corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('CHAPTER '))]\n","\n","  print(f'About to return corpus_sents_all_ls with len = {len(corpus_sents_all_ls)}')\n","  return corpus_sents_all_ls, corpus_raw_str\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Z5nrHLut8AEl"},"outputs":[],"source":["def corpus2sects(corpus_filename):\n","  '''\n","  Given a corpus_filename (assuming already %cd into correct subdir)\n","  Return a 3 lists: First, the list of Section text strings\n","                    Second, a list of tuples that match (Sentence No, Segment No)\n","                    Third, a list of Sentences that not found in any Section \n","  '''\n","\n","  corpus_sects_ls = []\n","\n","\n","  # encoding = CORPUS_ENCODING,  'windows-1252', 'utf-8', 'cp1252', 'iso-8859-1'\n","  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n","    corpus_raw_str = infp.read()\n","\n","  # pattern_sect = 'SECTION [\\d]{1,2}[.]?[^\\n]*'\n","  # pattern_sect = 'SECTION [0123456789]{1,2}[^\\n]*'\n","  # corpus_sects_ls = re.split(r'SECTION [\\d]{1,2}[.]* [A-Z \\.-:—;-’\\'\"]*[\\n]*', corpus_raw_str flags=re.I) # , flags=re.I)\n","  corpus_sects_ls = re.split(rf'{pattern_sect}', corpus_raw_str, flags=re.I) # , flags=re.I)\n","  # corpus_sects_ls = re.split(rf'{pattern_sect}', corpus_raw_str, flags=re.I) # , flags=re.I)\n","  print(f'len(corpus_raw_str: {len(corpus_raw_str)}')\n","  print(f'len(corpus_sects_ls): {len(corpus_sects_ls)}')\n","  print(f'    First section: Length={len(corpus_sects_ls[0])}\\n    {corpus_sects_ls[0][:500]}')\n","  print(f'    Second section: {corpus_sects_ls[1][:500]}')\n","  print('\\n')\n","  print(f'    Second-Last section: {corpus_sects_ls[-2][:500]}')\n","  print(f'    Last section: {corpus_sects_ls[-1][:500]}')\n","\n","\n","\n","  # Strip off whitespace \n","  corpus_sects_ls = [x.strip() for x in corpus_sects_ls]\n","\n","  # Filter out empty lines\n","  corpus_sects_ls = [x for x in corpus_sects_ls if not (len(x.strip()) <= MIN_SECT_LEN)]\n","\n","  # Filter out the Section separator '-----' lines\n","  corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('----- '))]\n","\n","  # Filter out the Section separator 'SECTION ' lines\n","  corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('SECTION '))]\n","\n","  # Filter out any possible embedded 'CHAPTER ' lines\n","  # TODO: Zeroing out corpus_sects_ls\n","  # corpus_sects_ls = del_substrs_list(corpus_sects_ls, pattern_chap) # corpus_sects_ls = [re.sub(rf'{pattern_sect}', '', x) for x in corpus_sects_ls]\n","\n","  # Filter out the Chapter separator 'CHAPTER ' lines\n","  # Keep for now, messy but enables proper SECTION assignments to appropraite CHAPTERs\n","  # corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('CHAPTER '))]\n","\n","\n","  print(f'About to process {len(corpus_sects_ls)} Sections')\n","  # Filter out Sentences in Section that don't have a corresponding Sentence in corpus_sents_df \n","  # Old Strategy: Filter out lines containing embedded SECTION or CHAPTER RegEx patterns \n","\n","  sect_sentences_match_ls = []\n","  sect_sentences_unmatch_ls = []\n","  sect_sentences_unmatch_ct = 0\n","  corpus_sentence_current = 0\n","\n","\n","  for asect_no, asection in enumerate(corpus_sects_ls):\n","    # sect_sentences_ls = []\n","\n","    # NLTK Sentence Tokenization\n","    # 3605 lines with 'To the Lighthouse' by V.Woolf\n","    # asect_sents_ls = (sent_tokenize(asect))\n","    \n","    # SpaCy Sentence Tokenization\n","    # TODO: Speed up my specializaing pipe\n","    # 3968 lines for 'To the Lighthouse' by V.Woolf\n","    # doc = nlp(asect)   \n","    # asect_sents_ls = [sent for sent in doc.sents]\n","    # asect_sents_ls = [x for x in doc]\n","\n","    # PySBD Sentence Tokenization\n","    # 3457 lines for 'To the Lighthouse' by V.Woolf\n","    # using pysbd and SpaCy\n","    from pysbd.utils import PySBDFactory\n","    nlp = spacy.blank('en')\n","    # explicitly adding component to pipeline\n","    # (recommended - makes it more readable to tell what's going on)\n","    nlp.add_pipe(PySBDFactory(nlp))\n","    # or you can use it implicitly with keyword\n","    # pysbd = nlp.create_pipe('pysbd')\n","    # nlp.add_pipe(pysbd)\n","    # doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n","    # print(list(doc.sents))\n","    doc = nlp(asection)\n","    asect_sents_ls = list(doc.sents)\n","\n","    # Create a normalized/no puncutation list of Corpus sentences for clean test comparisions filtered\n","    corpus_sents_nopunct_ls = [re.sub(r'[^A-Za-z0-9]', ' ',x) for x in corpus_sents_ls]\n","    # corpus_sents_nopunct_ls = [x for x in corpus_sents_nopunct_ls if x.isalnum()]\n","    corpus_sents_nopunct_strip_ls = [x.strip() for x in corpus_sents_nopunct_ls]\n","\n","    for j, asection_sentence_raw in enumerate(asect_sents_ls):\n","      asection_sentence_str = str(asection_sentence_raw)\n","      asection_sentence_nopunct_str = re.sub(r'[^A-Za-z0-9]', ' ', asection_sentence_str)\n","      asection_sentence_nopunct_strip_str = asection_sentence_nopunct_str.strip()\n","      # This 'in' test is not sufficient, need to strip out punctuation/normalize\n","      if asection_sentence_nopunct_strip_str in corpus_sents_nopunct_strip_ls:\n","        sect_sentences_match_ls.append((asect_no, asection_sentence_str))\n","        print(f'  Matched Segment Sent')\n","      else:\n","        sect_sentences_unmatch_ct += 1\n","        print(f'  UNMATCHED Corpus Sentence #[{corpus_sentence_current}]\\n           Segment Sentence #{j}: [{asection_sentence_str}]\\n            [{asection_sentence_nopunct_strip_str}]')\n","        sect_sentences_unmatch_ls.append(asection_sentence_str)\n","\n","      corpus_sentence_current += 1\n","\n","    # section_str = ' '.join(sect_sentences_ls)\n","    # sect_sentences_match_ls.append(section_str)\n","\n","    # if re.search(rf'{pattern_chap}', asect):\n","    #   print(f'In Section #{i} removing embedded CHAPTER:\\n\\n    {asect}')\n","    #   asect = re.sub(rf'{pattern_chap}', ' ', asect)\n","\n","  return corpus_sects_ls, sect_sentences_match_ls, sect_sentences_unmatch_ls\n","\n","\n","# return corpus_sects_ls, corpus_raw_str"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"d8z8ZY5n8BRs"},"outputs":[],"source":["def corpus2parags(corpus_filename):\n","  '''\n","  Given a corpus_filename (assuming already %cd into correct subdir)\n","  Return a list of min preprocessed raw paragraphs (corpus_parags_ls)\n","  '''\n","\n","  # parag_sents_ls = list(doc.sents)\n","  \n","  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n","    corpus_raw_str = infp.read()\n","\n","  corpus_parags_ls = re.split(r'[\\n]{2,}', corpus_raw_str)\n","  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_ls)}')\n","\n","  # Strip off whitespace\n","  corpus_parags_ls = [x.strip() for x in corpus_parags_ls]\n","\n","  # Filter out numbers(often footnotes) from Paragraphs\n","  corpus_parags_ls = [re.sub(r'[0-9]',' ',x) for x in corpus_parags_ls]\n","\n","  # Filter out the Section separator '-----' lines\n","  # Redundant, filed by punctuation only filter above\n","  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.strip().startswith('----- '))]\n","\n","  # Filter out the Chapter/Section header lines\n","  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('CHAPTER '))]\n","  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('SECTION '))]\n","  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('BOOK '))]\n","  corpus_parags_ls = [x for x in corpus_parags_ls if not (re.match(r\"^[ ]*[0-9]{1,3}[\\.]?[ ]*$\", x))]\n","  corpus_parags_ls = [x for x in corpus_parags_ls if not (re.match(r\"^[ ]*[IVXLC]{1,10}[\\.]?[ ]*$\", x))]\n","\n","  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n","  parag_before_punctstrip_ct = len(corpus_parags_ls)\n","  corpus_parags_ls = [x for x in corpus_parags_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n","  print(f'Punctuation only Paragraph Count: {len(corpus_parags_ls) - parag_before_punctstrip_ct}')\n","\n","  # Filter out empty lines Paragraphs\n","  corpus_parags_ls = [x for x in corpus_parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n","\n","  # Made a deepcopy of the original raw paragraphs after simple cleaning while continuing to clean the original\n","  corpus_parags_raw_ls = [x for x in corpus_parags_ls]\n","\n","  # Strip out non-printing characters\n","  corpus_parags_ls = [re.sub(f'[^{re.escape(string.printable)}]', ' ', x) for x in corpus_parags_ls]\n","\n","  # Condense multiple whitespaces down into one\n","  corpus_parags_ls = [' '.join(x.split()).strip() for x in corpus_parags_ls]\n","\n","  # Verify no Chapter/Section header lines remain\n","  for i,temp_str in enumerate(corpus_parags_ls):\n","    if temp_str.startswith('CHAPTER '):\n","      print(f'Parag #{i}: {temp_str}')\n","\n","  return corpus_parags_ls, corpus_parags_raw_ls, corpus_raw_str\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GxTYt6xsD5Iq"},"outputs":[],"source":["def sect2parags(sect_str):\n","  '''\n","  Given a Section as a text string\n","  Return a list of raw Paragraphs and a raw Section text string\n","  '''\n","\n","  if CORPUS_LANGUAGE == 'english':\n","    sect_clean_str = re.sub(f'[^{re.escape(string.printable)}]', ' ', sect_str)\n","  elif CORPUS_LANGUAGE == 'french':\n","    sect_clean_str = sect_str \n","  else:\n","    print(f'ERROR: CORPUS_LANGUAGE must be [english|french] but was set to {CORPUS_LANGUAGE}')\n","\n","  sect_parags_raw_ls = re.split(r'[\\n]{2,}', sect_clean_str)\n","  # print(f'Section Paragraph Raw Count: {len(sect_parags_raw_ls)}')\n","\n","  # parag_sents_ls = list(doc.sents)\n","  \n","  # Strip off whitespace\n","  sect_parags_raw_ls = [x.strip() for x in sect_parags_raw_ls]\n","\n","  # Filter out numbers(often footnotes) from Paragraphs\n","  sect_parags_raw_ls = [re.sub(r'[0-9]',' ',x) for x in sect_parags_raw_ls]\n","\n","  # Filter out the Section separator '-----' lines\n","  # Redundant, filed by punctuation only filter above\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.strip().startswith('----- '))]\n","\n","  # Filter out the Chapter/Section header lines\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('CHAPTER '))]\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('SECTION '))]\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('BOOK '))]\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[0-9]{1,3}[\\.]?[ ]*$\", x))]\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[IVXLC]{1,10}[\\.]?[ ]*$\", x))]\n","\n","  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n","  parag_before_punctstrip_ct = len(sect_parags_raw_ls)\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n","  # print(f'Punctuation only Paragraph Count: {len(sect_parags_raw_ls) - parag_before_punctstrip_ct}')\n","\n","  # Filter out Paragraphs that are empty or shorter than MIN_PARAG_LEN\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n","\n","  # Made a clean copy of the original raw paragraphs after simple cleaning while continuing to clean the original\n","  sect_parags_clean_ls = [clean_text(x) for x in sect_parags_raw_ls]\n","\n","  # Verify no Chapter/Section header lines remain\n","  for i,temp_str in enumerate(sect_parags_raw_ls):\n","    if temp_str.startswith('CHAPTER '):\n","      print(f'ERROR: CHAPTERS not filtered\\n    Parag #{i}: {temp_str}')\n","\n","  return sect_parags_raw_ls, sect_clean_str\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"on9yiXYxcnJQ"},"outputs":[],"source":["test_str = \"\"\"\n","CHAPTER 2.\n","\n","The biographer is now faced with a difficulty which it is better perhaps to confess than to gloss over. Up to this point in telling the story of Orlando's life, documents, both private and historical, have made it possible to fulfil the first duty of a biographer, which is to plod, without looking to right or left, in the indelible footprints of truth; unenticed by flowers; regardless of shade; on and on methodically till we fall plump into the grave and write finis on the tombstone above our heads. But now we come to an episode which lies right across our path, so that there is no ignoring it. Yet it is dark, mysterious, and undocumented; so that there is no explaining it. Volumes might be written in interpretation of it; whole religious systems founded upon the signification of it. Our simple duty is to state the facts as far as they are known, and so let the reader make of them what he may.\n","\n","In the summer of that disastrous winter which saw the frost, the flood, the deaths of many thousands, and the complete downfall of Orlando's hopes--for he was exiled from Court; in deep disgrace with the most powerful nobles of his time; the Irish house of Desmond was justly enraged; the King had already trouble enough with the Irish not to relish this further addition--in that summer Orlando retired to his great house in the country and there lived in complete solitude. One June morning--it was Saturday the 18th--he failed to rise at his usual hour, and when his groom went to call him he was found fast asleep. Nor could he be awakened. He lay as if in a trance, without perceptible breathing; and though dogs were set to bark under his window; cymbals, drums, bones beaten perpetually in his room; a gorse bush put under his pillow; and mustard plasters applied to his feet, still he did not wake, take food, or show any sign of life for seven whole days. On the seventh day he woke at his usual time (a quarter before eight, precisely) and turned the whole posse of caterwauling wives and village soothsayers out of his room, which was natural enough; but what was strange was that he showed no consciousness of any such trance, but dressed himself and sent for his horse as if he had woken from a single night's slumber. Yet some change, it was suspected, must have taken place in the chambers of his brain, for though he was perfectly rational and seemed graver and more sedate in his ways than before, he appeared to have an imperfect recollection of his past life. He would listen when people spoke of the great frost or the skating or the carnival, but he never gave any sign, except by passing his hand across his brow as if to wipe away some cloud, of having witnessed them himself. When the events of the past six months were discussed, he seemed not so much distressed as puzzled, as if he were troubled by confused memories of some time long gone or were trying to recall stories told him by another. It was observed that if Russia was mentioned or Princesses or ships, he would fall into a gloom of an uneasy kind and get up and look out of the window or call one of the dogs to him, or take a knife and carve a piece of cedar wood. But the doctors were hardly wiser then than they are now, and after prescribing rest and exercise, starvation and nourishment, society and solitude, that he should lie in bed all day and ride forty miles between lunch and dinner, together with the usual sedatives and irritants, diversified, as the fancy took them, with possets of newt's slobber on rising, and draughts of peacock's gall on going to bed, they left him to himself, and gave it as their opinion that he had been asleep for a week.\n","\n","But if sleep it was, of what nature, we can scarcely refrain from asking, are such sleeps as these? Are they remedial measures--trances in which the most galling memories, events that seem likely to cripple life for ever, are brushed with a dark wing which rubs their harshness off and gilds them, even the ugliest and basest, with a lustre, an incandescence? Has the finger of death to be laid on the tumult of life from time to time lest it rend us asunder? Are we so made that we have to take death in small doses daily or we could not go on with the business of living? And then what strange powers are these that penetrate our most secret ways and change our most treasured possessions without our willing it? Had Orlando, worn out by the extremity of his suffering, died for a week, and then come to life again? And if so, of what nature is death and of what nature life? Having waited well over half an hour for an answer to these questions, and none coming, let us get on with the story.\n","\n","Now Orlando gave himself up to a life of extreme solitude. His disgrace at Court and the violence of his grief were partly the reason of it, but as he made no effort to defend himself and seldom invited anyone to visit him (though he had many friends who would willingly have done so) it appeared as if to be alone in the great house of his fathers suited his temper. Solitude was his choice. How he spent his time, nobody quite knew. The servants, of whom he kept a full retinue, though much of their business was to dust empty rooms and to smooth the coverlets of beds that were never slept in, watched, in the dark of the evening, as they sat over their cakes and ale, a light passing along the galleries, through the banqueting-halls, up the staircase, into the bedrooms, and knew that their master was perambulating the house alone. None dared follow him, for the house was haunted by a great variety of ghosts, and the extent of it made it easy to lose one's way and either fall down some hidden staircase or open a door which, should the wind blow it to, would shut upon one for ever--accidents of no uncommon occurrence, as the frequent discovery of the skeletons of men and animals in attitudes of great agony made evident. Then the light would be lost altogether, and Mrs Grimsditch, the housekeeper, would say to Mr Dupper, the chaplain, how she hoped his Lordship had not met with some bad accident. Mr Dupper would opine that his Lordship was on his knees, no doubt, among the tombs of his ancestors in the Chapel, which was in the Billiard Table Court, half a mile away on the south side. For he had sins on his conscience, Mr Dupper was afraid; upon which Mrs Grimsditch would retort, rather sharply, that so had most of us; and Mrs Stewkley and Mrs Field and old Nurse Carpenter would all raise their voices in his Lordship's praise; and the grooms and the stewards would swear that it was a thousand pities to see so fine a nobleman moping about the house when he might be hunting the fox or chasing the deer; and even the little laundry maids and scullery maids, the Judys and the Faiths, who were handing round the tankards and cakes, would pipe up their testimony to his Lordship's gallantry; for never was there a kinder gentleman, or one more free with those little pieces of silver which serve to buy a knot of ribbon or put a posy in one's hair; until even the Blackamoor whom they called Grace Robinson by way of making a Christian woman of her, understood what they were at, and agreed that his Lordship was a handsome, pleasant, darling gentleman in the only way she could, that is to say by showing all her teeth at once in a broad grin. In short, all his serving men and women held him in high respect, and cursed the foreign Princess (but they called her by a coarser name than that) who had brought him to this pass.\n","\n","But though it was probably cowardice, or love of hot ale, that led Mr Dupper to imagine his Lordship safe among the tombs so that he need not go in search of him, it may well have been that Mr Dupper was right. Orlando now took a strange delight in thoughts of death and decay, and, after pacing the long galleries and ballrooms with a taper in his hand, looking at picture after picture as if he sought the likeness of somebody whom he could not find, would mount into the family pew and sit for hours watching the banners stir and the moonlight waver with a bat or death's head moth to keep him company. Even this was not enough for him, but he must descend into the crypt where his ancestors lay, coffin piled upon coffin, for ten generations together. The place was so seldom visited that the rats made free with the lead work, and now a thigh bone would catch at his cloak as he passed, or he would crack the skull of some old Sir Malise as it rolled beneath his foot. It was a ghastly sepulchre; dug deep beneath the foundations of the house as if the first Lord of the family, who had come from France with the Conqueror, had wished to testify how all pomp is built upon corruption; how the skeleton lies beneath the flesh: how we that dance and sing above must lie below; how the crimson velvet turns to dust; how the ring (here Orlando, stooping his lantern, would pick up a gold circle lacking a stone, that had rolled into a corner) loses its ruby and the eye which was so lustrous shines no more. 'Nothing remains of all these Princes', Orlando would say, indulging in some pardonable exaggeration of their rank, 'except one digit,' and he would take a skeleton hand in his and bend the joints this way and that. 'Whose hand was it?' he went on to ask. 'The right or the left? The hand of man or woman, of age or youth? Had it urged the war horse, or plied the needle? Had it plucked the rose, or grasped cold steel? Had it--' but here either his invention failed him or, what is more likely, provided him with so many instances of what a hand can do that he shrank, as his wont was, from the cardinal labour of composition, which is excision, and he put it with the other bones, thinking how there was a writer called Thomas Browne, a Doctor of Norwich, whose writing upon such subjects took his fancy amazingly.\n","\n","So, taking his lantern and seeing that the bones were in order, for though romantic, he was singularly methodical and detested nothing so much as a ball of string on the floor, let alone the skull of an ancestor, he returned to that curious, moody pacing down the galleries, looking for something among the pictures, which was interrupted at length by a veritable spasm of sobbing, at the sight of a Dutch snow scene by an unknown artist. Then it seemed to him that life was not worth living any more. Forgetting the bones of his ancestors and how life is founded on a grave, he stood there shaken with sobs, all for the desire of a woman in Russian trousers, with slanting eyes, a pouting mouth and pearls about her neck. She had gone. She had left him. He was never to see her again. And so he sobbed. And so he found his way back to his own rooms; and Mrs Grimsditch, seeing the light in the window, put the tankard from her lips and said Praise be to God, his Lordship was safe in his room again; for she had been thinking all this while that he was foully murdered.\n","\n","Orlando now drew his chair up to the table; opened the works of Sir Thomas Browne and proceeded to investigate the delicate articulation of one of the doctor's longest and most marvellously contorted cogitations.\n","\n","For though these are not matters on which a biographer can profitably enlarge it is plain enough to those who have done a reader's part in making up from bare hints dropped here and there the whole boundary and circumference of a living person; can hear in what we only whisper a living voice; can see, often when we say nothing about it, exactly what he looked like; know without a word to guide them precisely what he thought--and it is for readers such as these that we write--it is plain then to such a reader that Orlando was strangely compounded of many humours--of melancholy, of indolence, of passion, of love of solitude, to say nothing of all those contortions and subtleties of temper which were indicated on the first page, when he slashed at a dead nigger's head; cut it down; hung it chivalrously out of his reach again and then betook himself to the windowseat with a book. The taste for books was an early one. As a child he was sometimes found at midnight by a page still reading. They took his taper away, and he bred glow-worms to serve his purpose. They took the glow-worms away, and he almost burnt the house down with a tinder. To put it in a nutshell, leaving the novelist to smooth out the crumpled silk and all its implications, he was a nobleman afflicted with a love of literature. Many people of his time, still more of his rank, escaped the infection and were thus free to run or ride or make love at their own sweet will. But some were early infected by a germ said to be bred of the pollen of the asphodel and to be blown out of Greece and Italy, which was of so deadly a nature that it would shake the hand as it was raised to strike, and cloud the eye as it sought its prey, and make the tongue stammer as it declared its love. It was the fatal nature of this disease to substitute a phantom for reality, so that Orlando, to whom fortune had given every gift--plate, linen, houses, men-servants, carpets, beds in profusion--had only to open a book for the whole vast accumulation to turn to mist. The nine acres of stone which were his house vanished; one hundred and fifty indoor servants disappeared; his eighty riding horses became invisible; it would take too long to count the carpets, sofas, trappings, china, plate, cruets, chafing dishes and other movables often of beaten gold, which evaporated like so much sea mist under the miasma. So it was, and Orlando would sit by himself, reading, a naked man.\n","\n","The disease gained rapidly upon him now in his solitude. He would read often six hours into the night; and when they came to him for orders about the slaughtering of cattle or the harvesting of wheat, he would push away his folio and look as if he did not understand what was said to him. This was bad enough and wrung the hearts of Hall, the falconer, of Giles, the groom, of Mrs Grimsditch, the housekeeper, of Mr Dupper, the chaplain. A fine gentleman like that, they said, had no need of books. Let him leave books, they said, to the palsied or the dying. But worse was to come. For once the disease of reading has laid upon the system it weakens it so that it falls an easy prey to that other scourge which dwells in the inkpot and festers in the quill. The wretch takes to writing. And while this is bad enough in a poor man, whose only property is a chair and a table set beneath a leaky roof--for he has not much to lose, after all--the plight of a rich man, who has houses and cattle, maidservants, asses and linen, and yet writes books, is pitiable in the extreme. The flavour of it all goes out of him; he is riddled by hot irons; gnawed by vermin. He would give every penny he has (such is the malignity of the germ) to write one little book and become famous; yet all the gold in Peru will not buy him the treasure of a well-turned line. So he falls into consumption and sickness, blows his brains out, turns his face to the wall. It matters not in what attitude they find him. He has passed through the gates of Death and known the flames of Hell.\n","\n","\n","\"\"\"\n","\n","sect_parags_raw_ls, sect_clean_str =sect2parags(test_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QndUpmJac5xa"},"outputs":[],"source":["len(sect_parags_raw_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LWw618b3vQmJ"},"outputs":[],"source":["from pysbd.utils import PySBDFactory\n","nlp = spacy.blank('en')\n","# explicitly adding component to pipeline\n","# (recommended - makes it more readable to tell what's going on)\n","nlp.add_pipe(PySBDFactory(nlp))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3b38jIZ-7cyQ"},"outputs":[],"source":["# parag_sents_raw_ls, parag_clean_str = parag2sents(aparag_raw)\n","'''\n","test_str = \"\"\"\n","As they neared the shore each bar rose, heaped itself, broke and swept a thin veil of white water across the sand. The wave paused, and then drew out again, sighing like a sleeper whose breath comes and goes unconsciously. Gradually the dark bar on the horizon became clear as if the sediment in an old wine-bottle had sunk and left the glass green. Behind it, too, the sky cleared as if the white sediment there had sunk, or as if the arm of a woman couched beneath the horizon had raised a lamp and flat bars of white, green and yellow spread across the sky like the blades of a fan. Then she raised her lamp higher and the air seemed to become fibrous and to tear away from the green surface flickering and flaming in red and yellow fibres like the smoky fire that roars from a bonfire. Gradually the fibres of the burning bonfire were fused into one haze, one incandescence which lifted the weight of the woollen grey sky on top of it and turned it to a million atoms of soft blue. The surface of the sea slowly became transparent and lay rippling and sparkling until the dark stripes were almost rubbed out. Slowly the arm that held the lamp raised it higher and then higher until a broad flame became visible; an arc of fire burnt on the rim of the horizon, and all round it the sea blazed gold.\n","\n","\"\"\"\n","\n","aparag_sents_raw_ls, aparag_clean_str = parag2sents(test_str)\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"g6dIS33zw5vB"},"outputs":[],"source":["# Configure the appropriate Sentence tokenizers according to CORPUS_LANGUAGE\n","\n","if CORPUS_LANGUAGE == 'english':\n","  nlp = spacy.blank('en')\n","  sent_tokenize = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n","elif CORPUS_LANGUAGE == 'french':\n","  # default NLTK/french deletes necessary accented characters, disable until fix found\n","  sent_tok = 'pysbd'\n","  nlp = spacy.blank('fr')\n","  sent_tokenize = nltk.data.load('tokenizers/punkt/PY3/french.pickle')\n","else:\n","  print(f'ERROR: CORPUS_LANGUAGE must be [english|french] but was set to: {CORPUS_LANGUAGE}')\n","\n","# explicitly adding component to pipeline\n","# (recommended - makes it more readable to tell what's going on)\n","nlp.add_pipe(PySBDFactory(nlp))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cAvPMXPS2ELh"},"outputs":[],"source":["# Test Sentence tokinzer in English or French\n","\n","test_en_str = \"My name is Jonas E. Smith. Please turn to p. 55.\"\n","\n","test_fr_str = \"Le pépiement matinal des oiseaux semblait insipide à Françoise. Chaque parole des «bonnes» la faisait sursauter; incommodée par tous leurs pas, elle s'interrogeait sur eux; 'est que nous avions déménagé. Certes les domestiques ne remuaient pas moins, dans le «sixième» de notre ancienne demeure; mais elle les connaissait; elle avait fait de leurs allées et venues des choses amicales. Maintenant elle portait au silence même une attention douloureuse.\"\n","\n","# doc = nlp(test_fr_str)\n","# print(list(doc.sents))\n","# [My name is Jonas E. Smith., Please turn to p. 55.]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9e0OnfrGJ_GB"},"outputs":[],"source":["def parag2sents(parag_str, sent_tok='pysbd'):\n","  '''\n","  Given a Paragraph as a long string and a Sentence tokenizer engine ['pysbd'(default)|'both'|'nltk']\n","  Return a list of every Sentence within the given Paragraph\n","  '''\n","\n","  # Load PySBD if necessary\n","  \"\"\"\n","  if (sent_tok == 'pysbd') | (sent_tok == 'both'):\n","    from pysbd.utils import PySBDFactory\n","    nlp = spacy.blank('en')\n","    # explicitly adding component to pipeline\n","    # (recommended - makes it more readable to tell what's going on)\n","    nlp.add_pipe(PySBDFactory(nlp))\n","    # pysbd = nlp.create_pipe('pysbd')\n","    # nlp.add_pipe(pysbd)\n","    # doc = nlp('My name is Jonas E. Smith. Please turn to p. 55.')\n","    # print(list(doc.sents))\n","  \"\"\";\n","\n","  # Minimally clean Paragraph string of non-printing characters\n","  if CORPUS_LANGUAGE == 'english':\n","    parag_clean_str = re.sub(f'[^{re.escape(string.printable)}]', ' ', parag_str).strip()\n","  elif CORPUS_LANGUAGE == 'french':\n","    parag_clean_str = parag_str.strip()\n","  else:\n","    print(f'ERROR: CORPUS_LANGUAGE must be [english|french] but was set to: {CORPUS_LANGUAGE}')\n","\n","  # Replace embedded \\n with spaces\n","  parag_clean_str = re.sub('[\\n]{1,}', ' ', parag_clean_str)\n","\n","  # Replaces mulitple whitespaces with one space\n","  parag_clean_str = ' '.join(parag_clean_str.split())\n","\n","  # parag_sents_ls = list(doc.sents)\n","\n","\n","  # TOKENIZE with 1 of 3 ways\n"," \n","  # TODO: Try simple/fast RegEx Tokenizer in lieu of NTLK to complement PyBSD\n","\n","  # ONE: Tokenize with PyBSD and NLTK \n","  if (sent_tok == 'both'):\n","    doc = nlp(parag_clean_str)\n","    parag_sents_first_ls = list(doc.sents)\n","    for asent_temp in parag_sents_first_ls:\n","      asent_tokenized_temp = sent_tokenize(asent_temp)\n","      parag_sents_ls.append(asent_tokenized_temp)\n","    # print(f'  BOTH: {len(parag_sents_ls)} Sentences found in Paragraph')\n","\n","  # TWO: Tokenize with PyBSD\n","  elif (sent_tok == 'pysbd'):\n","    doc = nlp(parag_clean_str)\n","    parag_sents_ls = list(doc.sents)\n","    # print(f' PySBD: {len(parag_sents_ls)} Sentences found in Paragraph')\n","\n","  # THREE: Tokenize with NLTK\n","  elif (sent_tok == 'nltk'):\n","    parag_sents_ls = sent_tokenize(parag_clean_str)\n","    # print(f' NLTK: {len(parag_sents_ls)} Sentences found in Paragraph')\n","\n","  # ERROR\n","  else:\n","    print(f'ERROR: sent_tok={sent_tok} but must be [pysbd(default)|both|nltk]')\n","\n","\n","  # CLEAN Sentences\n","\n","  # Strip off whitespace from Sentences\n","  parag_sents_ls = [str(x).strip() for x in parag_sents_ls]\n","\n","  # Copy/Clean Sentences into new list\n","  # Filter out numbers(often footnotes) from Sentences\n","  parag_sents_ls = [re.sub(r'[0-9]',' ',x) for x in parag_sents_ls]\n","\n","  # Filter out the Chapter/Section header lines\n","  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]*CHAPTER[\\s]*$\", x))]\n","  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]*SECTION[\\s]*$\", x))]\n","  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]BOOK[\\s]*$\", x))]\n","  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]*[0-9]{1,3}[\\.]?[\\s]*$\", x))]\n","  parag_sents_ls = [x for x in parag_sents_ls if not (re.match(r\"^[\\s]*[IVXLC]{1,10}[\\.]?[\\s]*$\", x))]\n","\n","  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n","  parag_before_punctstrip_ct = len(parag_sents_ls)\n","  parag_sents_ls = [x for x in parag_sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n","  # print(f'Punctuation only Paragraph Count: {len(parag_sents_ls) - parag_before_punctstrip_ct}')\n","\n","  # Condense multiple consecutive whitespaces down to one whitespace\n","  parag_sents_ls = [' '.join(x.split()) for x in parag_sents_ls]\n","\n","\n","  # Filter Sentences that are empty or shorter than MIN_SENT_LEN\n","  parag_sents_ls = [x for x in parag_sents_ls if (len(x.strip()) >= MIN_SENT_LEN)]\n","\n","  return parag_sents_ls, parag_clean_str\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RX1gjU7S8esY"},"outputs":[],"source":["\"\"\"\n","def parag2sents(parag_str, sent_tok='py'):\n","  '''\n","  Given a Paragraph as a string,\n","  Return a list of raw Sentences and a raw text Paragraph string\n","  '''\n","\n","  parag_clean_str = re.sub(f'[^{re.escape(string.printable)}]', '', parag_str)\n","  parag_parags_raw_ls = re.split(r'[\\n]{2,}', parag_clean_str)\n","  print(f'Section Paragraph Raw Count: {len(parag_parags_raw_ls)}')\n","\n","  # Strip off whitespace\n","  parag_parags_raw_ls = [x.strip() for x in parag_parags_raw_ls]\n","\n","  # Filter out numbers(often footnotes) from Paragraphs\n","  sect_parags_raw_ls = [re.sub(r'[0-9]','',x) for x in sect_parags_raw_ls]\n","\n","  # Filter out the Section separator '-----' lines\n","  # Redundant, filed by punctuation only filter above\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.strip().startswith('----- '))]\n","\n","  # Filter out the Chapter/Section header lines\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('CHAPTER '))]\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('SECTION '))]\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (x.startswith('BOOK '))]\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[0-9]{1,3}[\\.]?[ ]*$\", x))]\n","  sect_parags_raw_ls = [x for x in sect_parags_raw_ls if not (re.match(r\"^[ ]*[IVXLC]{1,10}[\\.]?[ ]*$\", x))]\n","\n","\n","  parag_no = 0\n","  # sent_base = 0\n","  corpus_sents_ls = []\n","  for parag_no,aparag in enumerate(corpus_parags_ls):\n","    sents_ls = sent_tokenize(aparag)\n","    # Delete (whitespace only) sentences\n","    sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n","    # Delete (punctuation only) sentences\n","    sents_ls = [x for x in sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_SENT_LEN]\n","    # Delete numbers (int or float) sentences\n","    sents_ls = [x for x in sents_ls if not (x.strip().isnumeric())]\n","\n","    # Filter out leading SECTION separator 'SECTION ' lines\n","    for i,temp_str in enumerate(sents_ls):\n","      if temp_str.startswith('SECTION '):\n","        print(f'Sentence #{i}: {temp_str}')\n","    sents_ls = [x for x in sents_ls if not (x.startswith('SECTION '))]\n","\n","    # Filter out leading Chapter separator 'CHAPTER ' lines\n","    for i,temp_str in enumerate(sents_ls):\n","      if temp_str.startswith('CHAPTER '):\n","        print(f'Sentence #{i}: {temp_str}')\n","    sents_ls = [x for x in sents_ls if not (x.startswith('CHAPTER '))]\n","    \n","    # Filter out lines containing embedded SECTION or CHAPTER RegEx patterns \n","    for i,temp_str in enumerate(sents_ls):\n","      # TODO: More specific, robust filtering mechnism \n","      if (re.search(rf'{pattern_sect}', temp_str)):\n","        pass\n","      if (re.search(rf'{pattern_chap}', temp_str)):\n","        pass\n","      else:\n","        corpus_sents_ls.append([sent_no, parag_no, temp_str])\n","        sent_no += 1\n","\n","    # print(f'Returning with corpus_sents_ls length = {len(corpus_sents_ls)}')\n","  \n","  return corpus_sents_ls\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LRopU4e3IQ2R"},"outputs":[],"source":["\"\"\"\n","\n","def parag2sents(corpus_parags_ls):\n","  '''\n","  Given a list of paragraphs,\n","  Return a list of lists of Sentences [sent_no, parag_no, asent(text)]\n","  '''\n","\n","  sent_no = 0\n","  # sent_base = 0\n","  corpus_sents_ls = []\n","  for parag_no,aparag in enumerate(corpus_parags_ls):\n","    sents_ls = sent_tokenize(aparag)\n","    # Delete (whitespace only) sentences\n","    sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n","    # Delete (punctuation only) sentences\n","    sents_ls = [x for x in sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_SENT_LEN]\n","    # Delete numbers (int or float) sentences\n","    sents_ls = [x for x in sents_ls if not (x.strip().isnumeric())]\n","\n","    # Filter out leading SECTION separator 'SECTION ' lines\n","    for i,temp_str in enumerate(sents_ls):\n","      if temp_str.startswith('SECTION '):\n","        print(f'Sentence #{i}: {temp_str}')\n","    sents_ls = [x for x in sents_ls if not (x.startswith('SECTION '))]\n","\n","    # Filter out leading Chapter separator 'CHAPTER ' lines\n","    for i,temp_str in enumerate(sents_ls):\n","      if temp_str.startswith('CHAPTER '):\n","        print(f'Sentence #{i}: {temp_str}')\n","    sents_ls = [x for x in sents_ls if not (x.startswith('CHAPTER '))]\n","    \n","    # Filter out lines containing embedded SECTION or CHAPTER RegEx patterns \n","    for i,temp_str in enumerate(sents_ls):\n","      # TODO: More specific, robust filtering mechnism \n","      if (re.search(rf'{pattern_sect}', temp_str)):\n","        pass\n","      if (re.search(rf'{pattern_chap}', temp_str)):\n","        pass\n","      else:\n","        corpus_sents_ls.append([sent_no, parag_no, temp_str])\n","        sent_no += 1\n","\n","    # print(f'Returning with corpus_sents_ls length = {len(corpus_sents_ls)}')\n","  \n","  return corpus_sents_ls\n","\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-adBsfM38eo0"},"outputs":[],"source":["# corpus_sents_ls = parag2sents(corpus_parags_ls)\n","# len(corpus_sents_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MaRD7RL9IOeg"},"outputs":[],"source":["# Generate full path and timestamp for new filepath/filename\n","\n","def gen_pathfiletime(file_str, subdir_str=''):\n","\n","  # Geenreate compressed author and title substrings\n","  author_raw_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","  title_raw_str = ''.join(CORPUS_TITLE.split()).lower()\n","\n","  # Generate current/unique datetime string\n","  datetime_str = str(datetime.now().strftime('%Y%m%d%H%M%S'))\n","\n","  # Built fullpath+filename string\n","  file_base, file_ext = file_str.split('.')\n","\n","  author_str = re.sub('[^A-Za-z0-9]+', '', author_raw_str)\n","  title_str = re.sub('[^A-Za-z0-9]+', '', title_raw_str)\n","\n","  full_filepath_str = f'{subdir_str}{file_base}_{author_str}_{title_str}_{datetime_str}.{file_ext}'\n","\n","  # print(f'Returning from gen_savepath() with full_filepath={full_filepath}')\n","\n","  return full_filepath_str\n","\n","# Test\n","# pathfilename_str = gen_pathfiletime('hist_paraglen.png')\n","# print(pathfilename_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cAxQW9IolZjr"},"outputs":[],"source":["# corpus_sents_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1lYUmVuvl2OK"},"outputs":[],"source":["def save_dataframes(df_ls=['baseline','sentimentr','syuzhetr','transformer','mlsuperivsed','combined','subset','everything']):\n","  '''\n","  Given a list of DataFrame groups to save\n","  Save all DataFrames associated with that group\n","  '''\n","\n","  # Save Preprocessed Corpus Sentences DataFrame\n","\n","  # author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","  # title_str = ''.join(CORPUS_TITLE.split()).lower()\n","  title_str = ''.join(CORPUS_FILENAME.split('.')[0]).lower()\n","  datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","  if ('baseline' in df_ls) | ('everything' in df_ls):\n","    # Sentences Raw\n","    corpus_sents_filename = f'corpus_text_sents_raw_{title_str}.csv'\n","    print(f'Saving Raw Sentences to file: {corpus_sents_filename}')\n","    corpus_sents_df['sent_raw'].to_csv(corpus_sents_filename)\n","\n","    # Sentences Clean\n","    corpus_sents_filename = f'corpus_text_sents_clean_{title_str}.csv'\n","    print(f'Saving Clean Sentences to file: {corpus_sents_filename}')\n","    corpus_sents_df['sent_clean'].to_csv(corpus_sents_filename)\n","\n","    # Sentence DataFrame\n","    corpus_sents_filename = f'corpus_sents_baseline_{title_str}.csv'\n","    print(f'Saving Sentences DataFrame to file: {corpus_sents_filename}')\n","    corpus_sents_df.to_csv(corpus_sents_filename)\n","\n","    # Paragraph DataFrame\n","    corpus_parags_filename = f'corpus_parags_baseline_{title_str}.csv'\n","    print(f'Saving Paragraph DataFrame to file: {corpus_parags_filename}')\n","    corpus_parags_df.to_csv(corpus_parags_filename)\n","\n","    # if SECTION_HEADINGS != 'None':  # Even if no Sections, save dummy placeholder   \n","    #                                   filled with Chapter data\n","    # Section DataFrame\n","    corpus_sects_filename = f'corpus_sects_baseline_{title_str}.csv'\n","    print(f'Saving Section DataFrame to file: {corpus_sects_filename}')\n","    corpus_sects_df.to_csv(corpus_sects_filename)\n","\n","    # Chapter DataFrame\n","    corpus_chaps_filename = f'corpus_chaps_baseline_{title_str}.csv'\n","    print(f'Saving Chapter DataFrame to file: {corpus_chaps_filename}')\n","    corpus_chaps_df.to_csv(corpus_chaps_filename)\n","\n","  if ('syuzhetr' in df_ls) | ('everything' in df_ls):\n","    # SyuzhetR DataFrame\n","    corpus_sents_filename = f'sum_sentiments_sents_syuzhetr_{title_str}.csv'\n","    print(f'Saving Sentences DataFrame to file: {corpus_sents_filename}')\n","    corpus_syuzhetr_df.to_csv(corpus_sents_filename)\n","\n","  if ('sentimentr' in df_ls) | ('everything' in df_ls):\n","    # SyuzhetR DataFrame\n","    corpus_sents_filename = f'sum_sentiments_sents_sentimentr_{title_str}.csv'\n","    print(f'Saving Sentences DataFrame to file: {corpus_sents_filename}')\n","    corpus_sentimentr_df.to_csv(corpus_sents_filename)\n","\n","  if ('transformer' in df_ls) | ('everything' in df_ls):\n","    # SyuzhetR DataFrame\n","    corpus_sents_filename = f'sum_sentiments_sents_transformer_{title_str}.csv'\n","    print(f'Saving Sentences DataFrame to file: {corpus_sents_filename}')\n","    corpus_transformer_df.to_csv(corpus_sents_filename)\n","\n","\n","  # if ('mlsupervised' in df_ls) | ('everything' in df_ls):\n","  #   # Supervised statistical ML DataFrame\n","  #   corpus_sents_filename = f'sum_sentiments_sents_mlsupervised_{title_str}.csv'\n","  #   print(f'Saving Sentences DataFrame to file: {corpus_sents_filename}')\n","  #   corpus_mlsup_df.to_csv(corpus_sents_filename)\n","\n","\n","  if ('combined' in df_ls) | ('everything' in df_ls):\n","    # Save StandardizedScaled SMA Sentences of ALL Models from the Unified DataFrame\n","    corpus_sents_filename = f'sum_sentiments_all31_sents_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n","    print(f'Saving Sentence Unified Models to file: {corpus_sents_filename}')\n","    corpus_unified_df.to_csv(corpus_sents_filename)\n","\n","    # Save Crux Points in Subset of Unified DataFrame (StdScaler/SMA 10% Sentences)\n","    # corpus_sents_filename = f'crux_table_unified_subset_{author_abbr_str}_{title_str}.csv' # _{datetime_now}.csv'\n","    # print(f'Saving Cruxes found in Subset of Unified Models to file: {corpus_sents_filename}')\n","    # unified_crux_df.to_csv(corpus_sents_filename)\n","\n","  return\n","\n","# Test\n","# save_dataframes()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FuHkcWnHQfLj"},"outputs":[],"source":["type(groups_ls[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AQtqkm0iO0rI"},"outputs":[],"source":["# flat_list = [amodel for sublist in groups_ls for amodel in sublist]\n","for sublist in groups_ls:\n","  for amodel in globals()[sublist]:\n","    print(amodel)\n","# flat_list = [''.join(amodel) for sublist in groups_ls for str(amodel) in sublist]\n","# flat_list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"v7H42rn8OqSV"},"outputs":[],"source":["groups_ls = ['models_baseline_ls',\n","                'models_sentimentr_ls',\n","                'models_syuzhetr_ls',\n","                'models_transformer_ls']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9hJbmeyoOm5Z"},"outputs":[],"source":["corpus_sentimentr_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kOeF2B6JOgUf"},"outputs":[],"source":["corpus_unified_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_D84wkIlCgVn"},"outputs":[],"source":["# Used in function any_alphachar()\n","\n","pattern_alpha = re.compile(r'[A-Za-z]')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FRrwu2x3_TgB"},"outputs":[],"source":["def any_alphachar(raw_str):\n","  '''\n","  Given a string\n","  Return a boolean True if there is any alpha char [A-Za-z] in it, else False\n","  '''\n","  \n","  result_obj = re.search(pattern_alpha, raw_str)\n","\n","  if str(result_obj) == 'None':\n","    return False\n","  else:\n","    return True\n","\n","# Test\n","  \n","test_str = '$1.00 ---'\n","\n","if (any_alphachar(test_str)):\n","  print(f'alphachar TRUE: {test_str}')\n","else:\n","  print(f'alphachar FALSE: {test_str}')\n"]},{"cell_type":"markdown","metadata":{"id":"XUvKJEybUIeP"},"source":["## **Sentiment**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DOsfpOS4uHGX"},"outputs":[],"source":["def list2stdscaler(tseries_ls):\n","  '''\n","  Given a list of floating point number\n","  Return a pd.Series that has been Standardized Scaled\n","  '''\n","\n","  scaler = StandardScaler()\n","\n","  tseries_np = np.array(tseries_ls)\n","  \n","  tseries_np = tseries_np.reshape((len(tseries_np), 1))\n","\n","  scaler = scaler.fit(tseries_np)\n","  print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, np.sqrt(scaler.var_)))\n","  tseries_xform_np = scaler.transform(tseries_np)\n","\n","  return pd.Series(tseries_xform_np.flatten())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8z0jQoBlfPir"},"outputs":[],"source":["def standardize_tsls(ts_df, col_ls):\n","  '''\n","  Given a DataFrame and list of Columns in that DataFrame\n","  Create 4 new Standardized Columns for each given Columns\n","  '''\n","\n","  # Create 4 new column names for each column provided\n","  for amodel in col_ls:\n","    # col_meanstd = f'{amodel}_meanstd'\n","    col_medianiqr = f'{amodel}_medianiqr'\n","    col_stdscaler = f'{amodel}_stdscaler'\n","    col_lnorm_meanstd = f'{amodel}_lnorm_meanstd'\n","    col_lnorm_medianiqr = f'{amodel}_lnorm_medianiqr'\n","\n","    # Standardize each column provided using Standard Scaler and  MedianIQRScaling\n","    ts_df[col_stdscaler]  = list2stdscaler(ts_df[amodel])\n","    ts_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(ts_df[amodel]).reshape(-1, 1))\n","    # Normalize the Sentence Sentiment by dividing by Chapter Length\n","    text_len_ls = list(ts_df['token_len'])\n","    text_sentiment_ls = list(ts_df[amodel])\n","    textsentiment_norm_ls = [text_sentiment_ls[i]/text_len_ls[i] for i in range(len(text_len_ls))]\n","    # RobustStandardize Sentence sentiment values\n","    ts_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(text_sentiment_norm_ls)).reshape(-1, 1))\n","    ts_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(text_sentiment_norm_ls)).reshape(-1, 1))\n","\n","  return\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yPELi99z_sWx"},"outputs":[],"source":["# corpus_sents_df.iloc[0]['sent_clean']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sLASKwzZF-Lh"},"outputs":[],"source":["def get_sentiments(model_base, sentiment_fn, sentiment_type='lexicon', text_prep='clean'):\n","  '''\n","  Given a model_base name and sentiment evaluation function\n","  Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","  '''\n","\n","  # Calculate Sentiment Polarities\n","\n","  if sentiment_type == 'lexicon':\n","    if text_prep == 'clean':\n","      print(f'Processing Lexicon Sentiments/Sentences...')\n","      corpus_sents_df[model_base] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))  # TODO: Verify raw/clean choice here\n","      print(f'Processing Lexicon Sentiments/Paragraphs...')\n","      corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Lexicon Sentiments/Sections...')\n","      corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Lexicon Sentiments/Chapters...')\n","      corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n","    elif text_prep == 'raw':\n","      print(f'Processing Lexicon Sentiments/Sentences...')\n","      corpus_sents_df[model_base] = corpus_sents_df['sent_raw'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Lexicon Sentiments/Paragraphs...')\n","      corpus_parags_df[model_base] = corpus_parags_df['parag_raw'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Lexicon Sentiments/Sections...')\n","      corpus_sects_df[model_base] = corpus_sects_df['sect_raw'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Lexicon Sentiments/Chapters...')\n","      corpus_chaps_df[model_base] = corpus_chaps_df['chap_raw'].apply(lambda text: sentiment_fn(str(text)))\n","    else:\n","      print(f'ERROR: text_prep must be [clean|raq] but was set to {text_prep}')\n","      return -99\n","  \n","  elif sentiment_type == 'compound':\n","    # VADER\n","\n","    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean\n","    if text_prep == 'clean':\n","      print(f'Processing Compound Sentiments/Sentences...')\n","      corpus_sents_df['scores'] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Compound Sentiments/Paragraphs...')\n","      corpus_parags_df['scores'] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Compound Sentiments/Sections...')\n","      corpus_sects_df['scores'] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Compound Sentiments/Chapters...')\n","      corpus_chaps_df['scores'] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n","    elif text_prep == 'raw':\n","      print(f'Processing Compound Sentiments/Sentences...')\n","      corpus_sents_df['scores'] = corpus_sents_df['sent_raw'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Compound Sentiments/Paragraphs...')\n","      corpus_parags_df['scores'] = corpus_parags_df['parag_raw'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Compound Sentiments/Sections...')\n","      corpus_sects_df['scores'] = corpus_sects_df['sect_raw'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Compound Sentiments/Chapters...')\n","      corpus_chaps_df['scores'] = corpus_chaps_df['chap_raw'].apply(lambda text: sentiment_fn(str(text)))\n","    else:\n","      print(f'ERROR: text_prep must be [clean|raq] but was set to {text_prep}')\n","      return -99\n","\n","    # Extract Compound Sentiment\n","    corpus_sents_df[model_base]  = corpus_sents_df['scores'].apply(lambda score_dict: score_dict['compound'])\n","    corpus_parags_df[model_base]  = corpus_parags_df['scores'].apply(lambda score_dict: score_dict['compound'])\n","    corpus_sects_df[model_base]  = corpus_sects_df['scores'].apply(lambda score_dict: score_dict['compound'])\n","    corpus_chaps_df[model_base]  = corpus_chaps_df['scores'].apply(lambda score_dict: score_dict['compound'])\n","\n","  elif sentiment_type == 'function':\n","    # TextBlob\n","\n","    if text_prep == 'clean':\n","      # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean parag_clean\n","      print(f'Processing Function Sentiments/Sentences...')\n","      corpus_sents_df[model_base] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Function Sentiments/Paragraphs...')\n","      corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Function Sentiments/Sections...')\n","      corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Function Sentiments/Chapters...')\n","      corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n","    elif text_prep == 'raw':\n","      # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean parag_clean\n","      print(f'Processing Function Sentiments/Sentences...')\n","      corpus_sents_df[model_base] = corpus_sents_df['sent_raw'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Function Sentiments/Paragraphs...')\n","      corpus_parags_df[model_base] = corpus_parags_df['parag_raw'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Function Sentiments/Sections...')\n","      corpus_sects_df[model_base] = corpus_sects_df['sect_raw'].apply(lambda text: sentiment_fn(str(text)))\n","      print(f'Processing Function Sentiments/Chapters...')\n","      corpus_chaps_df[model_base] = corpus_chaps_df['chap_raw'].apply(lambda text: sentiment_fn(str(text)))\n","    else:\n","      print(f'ERROR: text_prep must be [clean|raq] but was set to {text_prep}')\n","      return -99\n","\n","  else:\n","    print(f'ERROR: sentiment_type={sentiment_type} but must be one of (lexicon, compound, function)')\n","    return\n","\n","  # Create new column names\n","  # col_meanstd = f'{model_base}_meanstd'\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_stdscaler = f'{model_base}_stdscaler'\n","  # col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_stdscaler = f'{model_base}_lnorm_stdscaler'\n","\n","\n","  print('Standardizing Chapters')\n","  # Get Chapter Robust Standardization with Standard Scaler and  MedianIQRScaling\n","  corpus_chaps_df[col_stdscaler]  = list2stdscaler(corpus_chaps_df[model_base])\n","  corpus_chaps_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n","  # Normalize the Chapter Sentiment by dividing by Chapter Length\n","  chaps_len_ls = list(corpus_chaps_df['token_len'])\n","  chaps_sentiment_ls = list(corpus_chaps_df[model_base])\n","  chaps_sentiment_norm_ls = [chaps_sentiment_ls[i]/chaps_len_ls[i] for i in range(len(chaps_len_ls))]\n","  # RobustStandardize Chapter sentiment values\n","  # corpus_chaps_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n","  corpus_chaps_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n","  corpus_chaps_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n","\n","  print('Standardizing Sections')\n","  # Get Section Robust Standardization with Standard Scaler and  MedianIQRScaling\n","  # corpus_sects_df[col_stdscaler]  = mean_std_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n","  corpus_sects_df[col_stdscaler]  = list2stdscaler(corpus_sects_df[model_base])\n","  corpus_sects_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n","  # Normalize the Section Sentiment by dividing by Section Length\n","  sects_len_ls = list(corpus_sects_df['token_len'])\n","  sects_sentiment_ls = list(corpus_sects_df[model_base])\n","  sects_sentiment_norm_ls = [sects_sentiment_ls[i]/sects_len_ls[i] for i in range(len(sects_len_ls))]\n","  # RobustStandardize Section sentiment values\n","  # corpus_sects_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n","  corpus_chaps_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n","  corpus_sects_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n","\n","  print('Standardizing Paragraphs')\n","  # Get Paragraph Robust Standardization with Standard Scaler and  MedianIQRScaling\n","  corpus_parags_df[col_stdscaler]  = list2stdscaler(corpus_parags_df[model_base])\n","  corpus_parags_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_parags_df[model_base]).reshape(-1, 1))\n","  # Normalize the Paragraph Sentiment by dividing by Chapter Length\n","  parags_len_ls = list(corpus_parags_df['token_len'])\n","  parags_sentiment_ls = list(corpus_parags_df[model_base])\n","  parags_sentiment_norm_ls = [parags_sentiment_ls[i]/parags_len_ls[i] for i in range(len(parags_len_ls))]\n","  # RobustStandardize Paragraph sentiment values\n","  # corpus_parags_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n","  corpus_parags_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(corpus_parags_df[model_base]).reshape(-1, 1))\n","  corpus_parags_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n","\n","  print('Standardizing Sentences')\n","  # Get Sentence Robust Standardization with Standard Scaler and  MedianIQRScaling\n","  corpus_sents_df[col_stdscaler]  = list2stdscaler(corpus_sents_df[model_base])\n","  corpus_sents_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sents_df[model_base]).reshape(-1, 1))\n","  # Normalize the Sentence Sentiment by dividing by Chapter Length\n","  sents_len_ls = list(corpus_sents_df['token_len'])\n","  sents_sentiment_ls = list(corpus_sents_df[model_base])\n","  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/sents_len_ls[i] for i in range(len(sents_len_ls))]\n","  # RobustStandardize Sentence sentiment values\n","  # corpus_sents_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n","  # corpus_sents_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n","  corpus_sents_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(corpus_sents_df[model_base]).reshape(-1, 1))\n","  corpus_sents_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n","\n","  return"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PJNepWBIkVjm"},"outputs":[],"source":["# Read in lexicon at given path into Dict[word]=polarity\n","\n","def get_lexicon(lexicon_name, lexicon_format=2):\n","    \"\"\"\n","    Read sentiment lexicon.csv file at lexicon_path\n","    into appropriate Dict[word]=polarity\n","\n","    1. lexicon_dt[word] = <polarity value>\n","\n","    Args:\n","        sa_lib (str, optional): [description]. Defaults to 'syuzhet'.\n","    \"\"\"\n","    \n","    # global lexicon_df\n","\n","    lexicon_df = pd.DataFrame()\n","    \n","    # print(os.getcwd())\n","\n","    \n","    try:\n","      lexicon_df = pd.read_csv(lexicon_name)\n","      lexicon_df.info()\n","      # lexicon_df = lexicon_tmp_df.copy()\n","      # print(lexicon_df.head())\n","      return lexicon_df\n","    except:\n","      print(f'ERROR: Cannot read lexicon.csv at {lexicon_name}')\n","      return -1\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jWdzLF0jwI-u"},"outputs":[],"source":["# Sentence to Sentiment Polarity according to passed in Lexicon Dictionary\n","\n","def text2sentiment(text_str, lexicon_dt):\n","  '''\n","  Given a text_str and lexicon_dt, calculate \n","  the sentimety polarity.\n","  '''\n","\n","  # Remove all not alphanumeric and whitespace characters\n","  text_str = re.sub(r'[^\\w\\s]', '', text_str) \n","\n","  text_str = text_str.strip().lower()\n","  if (len(text_str) < 1):\n","      print(f\"ERROR: text2sentiment() given empty/null/invalid string: {text_str}\")\n","\n","  text_ls = text_str.split()\n","  # print(f'text_ls: {text_ls}')\n","\n","  # Accumulated Total Sentiment Polarity for entire Sentence\n","  text_sa_tot = 0.0\n","\n","  for aword in text_ls:\n","      # print(f'getting sa for word: {aword}')\n","      try:\n","          word_sa_fl = float(lexicon_dt[aword])\n","          text_sa_tot += word_sa_fl\n","          # print(f\">>{aword} has a sentiment value of {word_sa_fl}\")\n","      except TypeError: # KeyError:\n","          # aword is not in lexicon so it adds 0 to the sentence sa sum\n","          # print(f\"TypeError: cannot convert {lexicon_dt[aword]} to float\")\n","          continue\n","      except KeyError:\n","          # print(f\"KeyError: missing key {aword} in defaultdict syuzhet_dt\")\n","          continue\n","      except:\n","          e = sys.exc_info()[0]\n","          # print(f\"ERROR {e}: sent2lex_sa() cannot catch aword indexing into syuzhet_dt error\")\n","  \n","  # print(f\"Leaving sent2lex_sa() with sentence sa value = {str(text_sa_tot)}\")\n","  \n","  return text_sa_tot\n","\n","\n","# Test\n","\n","# sent2sentiment('I hate and despise and abhor and dislike and am disgusted by Mondays.', lexicon_jockersrinker_dt)\n","# sent2sentiment('hate Mondays.', lexicon_jockersrinker_dt)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"q6AEBZzvEz4a"},"outputs":[],"source":["def plot_smas(section_view=True, model_name='vader', text_unit='sentence', wins_ls=[20], alpha=0.5, subtitle_str='', y_height=0, save2file=False):\n","  '''\n","  Given a model, text_unit\n","  Plot a SMA using default values and wrapping the function get_smas()\n","  '''\n","\n","  if (section_view == True) and not any(x == text_unit for x in ['sentence', 'paragraph']):\n","    print(f'ERROR: You can only plot SMA within a Section with Sentence or Paragraph text units')\n","    return -99\n","\n","  if text_unit == 'sentence':\n","    if section_view == False:\n","      ts_df = corpus_sents_df\n","    else:\n","      ts_df = section_sents_df\n","    wins_ls = [5,10,20]\n","  elif text_unit == 'paragraph':\n","    if section_view == False:\n","      ts_df = corpus_parags_df\n","    else:\n","      ts_df = section_parags_df\n","    wins_ls = [5,10,20]\n","  elif text_unit == 'section':\n","    ts_df = corpus_sects_df\n","    wins_ls=[20]\n","  else:\n","    print(f'ERROR: {text_unit} must be sentence, paragraph or section')\n","\n","  sectno_loc = ts_df[model_name].min()\n","\n","  if section_view ==False:\n","    # At Section boundries draw blue vertical lines \n","    section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n","    for i, sent_no in enumerate(section_boundries_ls):\n","      plt.text(sent_no, y_height, f'Sec#{i}', alpha=0.2, rotation=90)\n","      plt.axvline(sent_no, color='blue', alpha=0.1)\n","      # 'BigNews1', xy=(sent_no, 0.5), xytext=(-10, 25), textcoords='offset points',                   rotation=90, va='bottom', ha='center', annotation_clip=True)\n","\n","      # plt.text(sent_no, -.5, 'goodbye',rotation=90, zorder=0)\n","\n","    # At Chapter boundaries draw red vertical lines\n","    chapter_boundries_ls = list(corpus_chaps_df['sent_no_start'])\n","    for i, sent_no in enumerate(chapter_boundries_ls):\n","      plt.axvline(sent_no, color='navy', alpha=0.1)\n","      # plt.text(sent_no, .5, 'hello', rotation=90, zorder=0)\n","\n","  get_smas(ts_df, model_name=model_name, text_unit=text_unit, wins_ls=wins_ls, alpha=alpha, subtitle_str=subtitle_str, save2file=save2file)\n","\n","  if (save2file == True):\n","    # Save graph to file.\n","    plot_filename = f'plot_sma_sents_{model_name}.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rcAMyjyn8ugj"},"outputs":[],"source":["# SMA 5% Sentiment of Sentence Sentiment\n","\n","def get_smas(ts_df, model_name, text_unit='sentence', wins_ls=[5,10], alpha=0.5, scale_factor=1., subtitle_str='', mean_adj=0., do_plot=True, save2file=False):\n","  '''\n","  Given a model_name and time series DataFrame and list of win_rolls in percentages\n","  Return the rolling means of the time series using the window sizes in win_rolls\n","  '''\n","\n","  temp_roll_df = pd.DataFrame() # TODO: save sma rolling values into temp_df and return this value\n","\n","  win_1per = int(ts_df.shape[0]*0.01)\n","  if text_unit ==  'sentence':\n","    # win_1per = win_s1per\n","    x_idx = 'sent_no'\n","    fname_abbr = 'sents'\n","  elif text_unit == 'paragraph':\n","    # win_1per = win_p1per\n","    x_idx = 'parag_no'\n","    fname_abbr = 'parags'\n","  elif text_unit == 'section':\n","    win_1per = 1\n","    wins_ls = [int(0.1 * corpus_sects_df.shape[0])]  # Edge case to deal with very few Section data points\n","    x_idx = 'sect_no'\n","    fname_abbr = 'sects'\n","  else:\n","    print(f'ERROR: text_unit={text_unit} but must be either sentence, paragraph or section')\n","  \n","  for i, awin_size in enumerate(wins_ls):\n","    if len(str(awin_size)) == 1:\n","      awin_str = '0' + str(awin_size)\n","    else:\n","      awin_str = str(awin_size) \n","    col_roll_str = f'{model_name}_mean_roll{awin_str}'\n","    win_size = awin_size*win_1per\n","    ts_df[col_roll_str] = ts_df[model_name].rolling(window=win_size, center=True).mean()\n","  \n","    if do_plot == True:\n","      alabel = f'{model_name} (win={awin_size})'\n","      ts_df['y_scaled'] = ts_df[col_roll_str]*scale_factor + mean_adj \n","      sns.lineplot(data=ts_df, x=x_idx, y='y_scaled', legend='brief', label=alabel, alpha=alpha)\n","      \n","  plt.title(f'{CORPUS_FULL} (Model: {model_name}: {subtitle_str}) \\nSMA Smoothed {text_unit} Sentiment Plot (windows={wins_ls})')\n","  # plt.legend(loc='best')\n","\n","  if save2file == True:\n","    # Save graph to file.\n","    plot_filename = f'plot_{fname_abbr}_sa_mean_050100sma.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return temp_roll_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ULzfHeDK8udN"},"outputs":[],"source":["def get_lexstats(ts_df, model_name, text_unit='sentence'):\n","  '''\n","  Given a model name\n","  calculate, store and return time series stats\n","  '''\n","  \n","  global corpus_lexicons_stats_dt\n","\n","  temp_dt = {}\n","  \n","  if text_unit == 'sentence':\n","    stat_idx = f'{model_name}_sents'\n","  elif text_unit == 'paragraph':\n","    stat_idx = f'{model_name}_parags'\n","  elif text_unit == 'section':\n","    stat_idx = f'{model_name}_sects'\n","  elif text_unit == 'chapter':\n","    stat_idx = f'{model_name}_chaps'\n","  else:\n","    print(f'ERROR: {text_unit} must either be sentence, paragraph, or section')\n","\n","  sentiment_min = ts_df[model_name].min()\n","  sentiment_max = ts_df[model_name].max()\n","\n","  temp_dt = {'sentiment_min' : sentiment_min,\n","             'sentiment_max' : sentiment_max}\n","\n","  corpus_lexicons_stats_dt[stat_idx] = temp_dt\n","                                     \n","  return \n","\n","# Test\n","# get_lexstats('afinn')\n","# corpus_lexicons_stats_dt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ltdJn-7ePNM9"},"outputs":[],"source":["def lex_discrete2continous_sentiment(text, lexicon):\n","  '''\n","  Given a plain text string, give it to\n","    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n","  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n","    that approximates a normal distribution for all values\n","    In order to get more fine grained measure of overall Sentence sentiment\n","    Sentiment values will be Normalized/Standardized so absolute precision is not required\n","  '''\n","  text_sentiment_tot = 0.\n","  text_ls = text.split()\n","  text_len = len(text_ls)\n","  for aword in text_ls:\n","    word_sentiment = text2sentiment(str(aword), lexicon)\n","    text_sentiment_tot += word_sentiment\n","  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.01)\n","\n","  return text_sentiment_norm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-6xMI98l8uaH"},"outputs":[],"source":["\"\"\"\n","def clip_outliers(floats_ser):\n","  '''\n","  Given a pd.Series of float values\n","  Return a list with outliers removed, values limited within 3 median absolute deviations from median\n","  '''\n","  # https://www.statsmodels.org/stable/generated/statsmodels.robust.scale.mad.html#statsmodels.robust.scale.mad\n","\n","  # Old mean/std, less robust\n","  # ser_std = floats_ser.std()\n","  # ser_median = floats_ser.mean() # TODO: more robust: asym/outliers -> median/IQR or median/median abs deviation\n","\n","  floats_np = np.array(floats_ser)\n","  ser_median = floats_ser.median()\n","  ser_mad = robust.mad(floats_np)\n","  print(f'ser_median = {ser_median}')\n","  print(f'ser_mad = {ser_mad}')\n","\n","  if ser_mad == 0:\n","    # for TS with small ranges (e.g. -1.0 to +1.0) Median Abs Deviation = 0\n","    #   so pass back the original time series\n","    floats_clip_ls = list(floats_ser)\n","\n","  else:\n","    ser_oldmax = floats_ser.max()\n","    ser_oldmin = floats_ser.min()\n","    print(f'ser_max = {ser_oldmax}')\n","    print(f'ser_min = {ser_oldmin}')\n","\n","    ser_upperlim = ser_median + 2.5*ser_mad\n","    ser_lowerlim = ser_median - 2.5*ser_mad\n","    print(f'ser_upperlim = {ser_upperlim}')\n","    print(f'ser_lowerlim = {ser_lowerlim}')\n","\n","    # Clip outliers to max or min values\n","    floats_clip_ls = np.clip(floats_np, ser_lowerlim, ser_upperlim)\n","    # print(f'max floast_ls {floats_ls.max()}')\n","\n","    # def map2range(value, low, high, new_low, new_high):\n","    #   '''map a value from one range to another'''\n","    #   return value * 1.0 / (high - low + 1) * (new_high - new_low + 1)\n","\n","    # Map all float values to range [-1.0 to 1.0]\n","    # floats_clip_sig_ls = [map2range(i, ser_oldmin, ser_oldmax, ser_upperlim, ser_lowerlim) for i in floats_clip_ls]\n","\n","    # listmax_fl = float(max(floats_ls))\n","    # floats_ls = [i/listmax_fl for i in floats_ls]\n","    #floats_ls = [1/(1+math.exp(-i)) for i in floats_ls]\n","\n","  return floats_clip_ls  # floats_clip_sig_ls\n","\"\"\";\n","\n","# Test\n","# Will not work on first run as corpus_sents_df is not defined yet\n","'''\n","data = np.array([1, 4, 4, 7, 12, 13, 16, 19, 22, 24])\n","test_ls = clip_outliers(corpus_sents_df['vader'])\n","print(f'new min is {min(test_ls)}')\n","print(f'new max is {max(test_ls)}')\n","''';"]},{"cell_type":"markdown","metadata":{"id":"yXwKR4gA8Ouk"},"source":["## **Pandas**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"E8Hf8nU98uXI"},"outputs":[],"source":["\"\"\"\n","def rename_cols(ts_df, col_old_ls, suffix_str='_raw'):\n","  '''\n","  Given a DataFrame, list of columns in DataFrame and a suffix,\n","  Return a Dictionary mapping old col names to new col name (orig+suffix)\n","  '''\n","\n","  col_new_ls = []\n","  for acol in col_old_ls:\n","    acol_new = acol + suffix_str\n","    col_new_ls.append(acol_new)\n","\n","  # Create dict for col mapping: keys=old col names, value=new col names\n","  col_rename_dt = dict(zip(col_old_ls, col_new_ls))\n","\n","  # ts_df.rename(columns=col_rename_dt, errors=\"raise\")\n","\n","  return col_rename_dt\n","\n","# test_ls = [col for col in corpus_sents_df.columns if not(renaming_fun(col) is None)]\n","# print(f'test_ls: {test_ls}')\n","\n","# Test\n","# col_rename_dt = rename_cols(corpus_sents_df, sentiment_only_cols_ls)\n","# col_rename_dt\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"3YJJcvDVnUuT"},"source":["## **Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pY-1_xWAGz0q"},"outputs":[],"source":["def process_timeseries(ts_df, col_models_ls, col_mod):\n","  '''\n","  Given a DataFrame, a list of columns to process and a modification to perform on these columns\n","\n","  Return a new DataFrame with the following new columns inserted from each original Model column:\n","\n","    1. Length-Normed (col_mod='lnorm'])\n","       a. {base_model}_lnorm\n","\n","    2. Standardized (col_mod='std-[mean,median,minmix]')\n","       a. {base_model}_stdscaler\n","       b. {base_model}_medianiqr \n","       a. {base_model_lnorm}_stdscaler\n","       b. {base_model_lnorm}_medianiqr \n","\n","    3. Length-Normed Standardized (col_mod='roll[dd]') # where dd = 01 to 20 indicating rolling window width as % of corpus length\n","       a. {base_model}_roll\n","       b. {base_model}_roll \n","       a. {base_model_lnorm}_roll\n","       b. {base_model_lnorm}_roll \n","       a. {base_model_lnorm_stdscaler}_roll\n","       b. {base_model_lnorm_medianiqr}_roll\n"," \n","  '''\n","\n","  temp_df = pd.DataFrame()  # ts_df.filter(['sent_no','sent_raw'], axis=1)\n","  # print(f'temp_df is {temp_df}')\n","\n","  # Prerequisite: token_len column must exists\n","  if 'token_len' not in ts_df.columns:\n","    ts_df['token_len'] = ts_df['sent_raw'].apply(lambda x: len(x.strip().split()))\n","    ts_df['char_len'] = ts_df['sent_raw'].apply(lambda x: len(x))\n","\n","\n","  # OPOTION 1: Apply (token) Length-Normalization to specified time series\n","  if col_mod == 'lnorm':\n","\n","    # Apply lnorm operation on all specificed columns\n","    for anold_model_no, anold_model in enumerate(col_models_ls):\n","\n","      # lnorm (Length normalization of sentiment values using token_len) operation can only be applied to base model time series\n","      # rule: disallowed substrings in anold_model name if applying lnorm (anew_mod)\n","      rule_badsubstr_lnorm = ['lnorm', 'stdscaler', 'medianiqr', 'roll']\n","      if any(map(anold_model.__contains__, rule_badsubstr_lnorm)):\n","        print(f'ERROR: Length-Normalization (lnorm) operation cannot be applied to {anold_model}\\n    any columns containing {rule_badsubstr_lnorm}')\n","        return -99  # Return ERROR condition\n","      else:\n","        # Normalize specified sentiment time series by dividing by text length\n","\n","        # TODO: replace 'sent_raw'/'sent_clean' with 'text_raw'/'text_clean' for Sentence, Paragraph, Section and Chapter DataFrames\n","\n","        print(f'Applying function=[{col_mod}] on time series [{anold_model}]\\n') \n","\n","        text_len_ls = list(ts_df['token_len'])\n","        text_sentiment_ls = list(ts_df[anold_model])\n","        text_sentiment_norm_ls = [text_sentiment_ls[i]/text_len_ls[i] for i in range(len(text_len_ls))]\n","        col_mod_name = f'{anold_model}_{col_mod}'\n","        # ts_df = ts_df.assign(acol_mod_name = pd.Series(text_sentiment_norm_ls).values)\n","        temp_df[col_mod_name] = pd.Series(text_sentiment_norm_ls)\n","\n","\n","  # OPTION 2: Apply (Robust) Standardization to specified time series\n","  if col_mod.startswith('std'):\n","\n","    # Apply lnorm operation on all specificed columns\n","    for anold_model_no, anold_model in enumerate(col_models_ls):\n","\n","      # std (Standardization) operation can only be applied to base model time series\n","      # rule: disallowed substrings in anold_model name if applying std to existing rolling averages or already standardized time series\n","      rule_badsubstr_std = ['minmax', 'stdscaler', 'medianiqr', 'roll']\n","      if any(map(anold_model.__contains__, rule_badsubstr_std)):\n","        print(f'ERROR: Standardization (std) operation cannot be applied to {anold_model}\\n    any columns containing {rule_badsubstr_std}')\n","        return -99  # Return ERROR condition\n","      else:\n","        # Standardize specificied sentiment time series by dividing by applying (Robust)Standization also configured in code below\n","\n","        # TODO: replace 'sent_raw'/'sent_clean' with 'text_raw'/'text_clean' for Sentence, Paragraph, Section and Chapter DataFrames\n","\n","        print(f'Applying function=[{col_mod}] on time series [{anold_model}]\\n')\n","\n","        # Must set any invalid (E.g. NaN cells to 0)\n","      \n","\n","        if col_mod == 'std-minmax':\n","\n","          col_mod_minmax = f'{anold_model}_minmax'\n","          temp_minmax_np = minmax_scaler.fit_transform(np.array(ts_df[anold_model]).reshape(-1, 1))\n","          temp_df[col_mod_minmax] = pd.Series(temp_minmax_np.ravel())\n","\n","        elif col_mod == 'std-stdscaler':\n","\n","          col_mod_minmax = f'{anold_model}_stdscaler'\n","          temp_minmax_np = mean_std_scaler.fit_transform(np.array(ts_df[anold_model]).reshape(-1, 1))\n","          temp_df[col_mod_minmax] = pd.Series(temp_minmax_np.ravel())\n","\n","        elif col_mod == 'std-medianiqr':\n","\n","          col_mod_minmax = f'{anold_model}_medianiqr'\n","          temp_minmax_np = median_iqr_scaler.fit_transform(np.array(ts_df[anold_model]).reshape(-1, 1))\n","          temp_df[col_mod_minmax] = pd.Series(temp_minmax_np.ravel())\n","\n","        else:\n","          print(f\"ERROR: modification fn (col_mod): {col_mod}, but must be one of ['std-minmax','std-stdscaler','std-medianiqr']\\n\")\n","\n","\n","  # OPTION 3: Apply Simple Rolling Mean to specified time series\n","  if col_mod.startswith('roll'):\n","\n","    roll_per = 0\n","\n","    # Check for roll argument against several rules and return detailed error message if fail to pass any combination of rules\n","    roll_err_ls = []\n","    roll_arg_len = len(col_mod)\n","    if (roll_arg_len != 6):\n","      roll_err_ls.append(f'ERROR: argument roll[dd]={col_mod} is too long with {roll_arg_len} characters (must be 6)')\n","    roll_arg_2last = col_mod[-2:]\n","\n","    if (roll_arg_2last.isdigit() == False):\n","      roll_err_ls.append(f'ERROR: argument roll[dd]={col_mod} last 2 chars {roll_arg_2last} must be both be digits [0-9]')\n","    else:\n","      # print(f'BEFORE: roll_arg_2last={roll_arg_2last}')\n","      roll_per = int(roll_arg_2last)\n","      # print(f'AFTER: roll_per={roll_per}')\n","      if (roll_per > 20):\n","        roll_err_ls.append(f'ERROR: argument roll[dd]={col_mod} last 2 chars {roll_arg_2last} must be integers between 01 and 20')\n","\n","    if len(roll_err_ls) > 0:\n","      print(f'ERROR in process_timeseries() due to invalid argument roll[dd] = {col_mod}\\n\\n')\n","      for anerror_str in roll_err_ls:\n","        # print(f'    {anerror_str}')\n","        return -99  # Return ERROR condition\n","\n","    # Apply lnorm operation on all specificed columns\n","    for anold_model_no, anold_model in enumerate(col_models_ls):\n","\n","      # roll (Simple Rolling Average with specificed window size as 2-digit percentage of corpus length (01-20%)\n","      # rule: disallowed substrings in anold_model name applying roll operation more than once to existing time series\n","      rule_badsubstr_roll = ['roll']\n","      if any(map(anold_model.__contains__, rule_badsubstr_roll)):\n","        print(f'ERROR: Rolling Mean (roll) operation cannot be applied to {anold_model}\\n    any columns containing {rule_badsubstr_roll}')\n","        return -99 # Return ERROR condition\n","\n","      else:\n","          # Compute Rolling Mean with the given window size (extracted above as an int in roll_per) for the specificied sentiment time series\n","\n","          # TODO: replace 'sent_raw'/'sent_clean' with 'text_raw'/'text_clean' for Sentence, Paragraph, Section and Chapter DataFrames\n","\n","          print(f'Applying function=[{col_mod}] on time series [{anold_model}]\\n')\n","\n","          col_mod_roll = f'{anold_model}_{col_mod}'\n","          # print(f'  for col_mod: {col_mod} and roll_per: {roll_per}')\n","          roll_win = int(ts_df.shape[0]*roll_per/100)\n","          # print(f'    and roll_win: {roll_win}')\n","          temp_df[col_mod_roll] = ts_df[anold_model].rolling(roll_win, center=True).mean()\n","\n","  return temp_df  # Return SUCCESS condition\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jIo6-zGKnZps"},"outputs":[],"source":["\"\"\"\n","def norm2negpos1(data_ser):\n","  '''\n","  Given a series of floating number\n","  Return a a list of same values normed between -1.0 and +1.0\n","  '''\n","  # data_np = np.matrix(data_ser)\n","\n","  scaler=MinMaxScaler(feature_range=(-1.0, 1.0))\n","  temp_ser = scaler.fit_transform(np.matrix(data_ser))\n","  \n","  return temp_ser\n","\"\"\";\n","\n","# Test\n","'''\n","temp_np = norm2negpos1(corpus_all_df[['xlnet_sst5']])\n","print(type(temp_np))\n","temp_np.shape\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ylt_kLuEFDrj"},"outputs":[],"source":["# This must be defined AFTER the corpus_sects_df DataFrame is created in the Preprocessing Step below\n","# Raw Plot of Section Sentiments (Adjusted for (x-axis) mid-Section Sentence No and (y-axis) Sentiment weighted by Section length )\n","# corpus_sects_df = pd.DataFrame()  # Create empty early as required by some utility functions\n","\n","def plot_crux_sections(model_names_ls, semantic_type='section', subtitle_str='', label_token_ct=0, title_xpos = 0.8, title_ypos=0.2, sec_y_height=0, save2file=False):\n","  '''\n","  Given a Sections DataFrame, model_name and semantic type,\n","  Return a Plot of the Cruxes\n","  '''\n","\n","  crux_points_dt = {}\n","  model_stand_names_ls = []\n","  section_boundries_ls = []\n","\n","\n","  # print(f'Using model_names: {model_names_ls}')\n","\n","  # sns.lineplot(data=ts_df, x='sent_no_mid', y=amodel_stand, markers=['o'], alpha=0.5, label=amodel_stand); # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment (Bing Lexicon)')\n","\n","\n","  # At Section boundries draw blue vertical lines \n","  section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n","  for i, sent_no in enumerate(section_boundries_ls):\n","    plt.text(sent_no, sec_y_height, f'Sec#{i}', alpha=0.2, rotation=90)\n","    plt.axvline(sent_no, color='blue', alpha=0.1);\n","\n","  # At Chapter boundaries draw red vertical lines\n","  chapter_boundries_ls = list(corpus_chaps_df['sent_no_start'])\n","  for i, sent_no in enumerate(chapter_boundries_ls):\n","    plt.axvline(sent_no, color='navy', alpha=0.1);\n","\n","  # Error check and assign DataFrame associated with each semantic_type\n","  if semantic_type == 'section':\n","    # Get midpoints of each Section\n","    ts_df=corpus_sects_df\n","    midpoints_ls = list(corpus_sects_df['sent_no_mid'])\n","  elif semantic_type == 'chapter':\n","    # Get midpoints of each Chapter\n","    ts_df=corpus_chaps_df\n","    midpoints_ls = list(corpus_chaps_df['sent_no_mid'])\n","  else:\n","    print(f\"ERROR: semantic_type={semantic_type} must be either 'section' or 'chapter'\")\n","    return -1\n","\n","  # How many sentiment time series are we plotting?\n","  if len(model_names_ls) == 1:\n","    \n","    # Plotting only one model\n","    model_name_full = str(model_names_ls[0])\n","    model_name_root = model_name_full.split('_')[0]\n","    print(f'model_name_full: {model_name_full} and model_name_root: {model_name_root}')\n","    if model_name_root in MODELS_LS:\n","      # Plot\n","      print(f'about to sns.lineplot model: ') # {ts_df}')\n","      g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n","      # g._legend.remove()\n","      # print(f'model_name_full={model_name_full}')\n","      # plt.plot(ts_df.sent_no_mid, ts_df[model_name_full], markers=\"o\", alpha=0.5, label=model_name_full)\n","    else:\n","      print(f'ERROR: model_names_ls[0]={model_name_root} is invalid,\\n    must be one of {MODELS_LS}')\n","      return -1\n","\n","    # If plotting only one model, add labels\n","    midpoints_sentiment_ls = list(ts_df[model_name_full])\n","    sect_ct = 0\n","    for x,y in zip(midpoints_ls, midpoints_sentiment_ls): \n","      label_token_int = int(label_token_ct)\n","      if label_token_int < 0:\n","        label = ''\n","      elif label_token_int == 0:\n","        # if arg label_token_ct == 0, just print sent_no\n","        label = f\"#{x}({sect_ct})\"\n","      else:\n","        # if arg label_token_ct > 0, print the first label_token_ct words of sentence at crux point\n","        label = f\"#{x}({sect_ct}) {' '.join(corpus_sents_df.iloc[x-1]['sent_raw'].split()[:label_token_int])}\"; # \\nPolarity: {y:.2f}'\n","\n","      # Save Crux point in crux_points_dt Dictionary if plotting Cruxes for a single/specific Model\n","      crux_full_str = ' '.join(corpus_sents_df.iloc[x]['sent_raw'].split())\n","      crux_points_dt[x] = [y, crux_full_str]\n","\n","      plt.annotate(label,\n","                   (x,y),\n","                   textcoords='offset points',\n","                   xytext=(0,10),\n","                   ha='center',\n","                   rotation=90)\n","      sect_ct += 1\n","\n","    plt.title(f'{CORPUS_FULL} \\n Plot {semantic_type.capitalize()} Sentiment ({model_name_full.capitalize()})\\n{subtitle_str}', x=title_xpos, y=title_ypos);\n","    # Plot\n","    plt.plot(midpoints_ls, midpoints_sentiment_ls, marker=\"o\", ms=6) # , markevery=[0,1])\n","\n","  else:\n","    # If plotting multiple models\n","    model_names_str = 'Multiple Models'\n","    for i, model_name_full in enumerate(model_names_ls):\n","      # Error check and assign correct model names\n","      model_name_root = model_name_full.split('_')[0]\n","      if model_name_root in MODELS_LS:\n","        # Plot\n","        g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n","        # g._legend.remove()\n","        # plt.plot(ts_df.sent_no_mid, ts_df[model_name_full], marker=\"o\", alpha=0.5, label=model_name_full)\n","      else:\n","        print(f'ERROR: model_names_ls[]={model_name_root} is invalid,\\n    must be one of {MODELS_LS}')\n","        return -1\n","\n","      # Plot\n","      g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n","      # g._legend.remove()\n","\n","    plt.title(f'{CORPUS_FULL} \\n Plot {semantic_type.capitalize()} Sentiment (Standardized Models)\\n{subtitle_str}', x=title_xpos, y=title_ypos)\n","\n","  # plt.legend(loc='best');\n","\n","  if (save2file == True):\n","    # Save graph to file.\n","    models_names_ls = [x[:2] for x in model_names_ls]\n","    models_names_str = ''.join(models_names_ls)\n","    plot_filename = f'plot_cruxes_{semantic_type}_{models_names_str}_{models_names_str}.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return crux_points_dt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mUNMIlJKHyz3"},"outputs":[],"source":["def plot_histogram(model_name='vader', text_unit='sentence', save2file=False):\n","  '''\n","  Given a model, text_unit\n","  Plot a Histogram using the default DataFrame\n","  '''\n","\n","  if text_unit == 'sentence':\n","    ts_df = corpus_sents_df\n","\n","  elif text_unit == 'paragraph':\n","    ts_df = corpus_parags_df\n","\n","  elif text_unit == 'section':\n","    ts_df = corpus_sects_df\n","\n","  elif text_unit == 'chapter':\n","    ts_df = corpus_chaps_df\n","\n","  else:\n","    print(f'ERROR: {text_unit} must be sentence, paragraph or section')\n","\n","  sns.histplot(ts_df[model_name], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram {text_unit.capitalize()} Sentiment (Model {model_name.capitalize()})')\n","  # get_smas(ts_df, model_name=model_name, text_unit=text_unit, win_ls=wins_def_ls)\n","\n","  if (save2file == True):\n","    # Save graph to file.\n","    plot_filename = f'plot_hist_{text_unit}_{model_name}.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NGTkfsSWFCeO"},"outputs":[],"source":["# Raw Plot of Section Sentiments (Not scaled by mid-Section Sentence No to match Sentence/Paragraph x-axes)\n","\n","def plot_raw_sections(ts_df='corpus_sents_df', model_name='vader', semantic_type='sentence', save2file=False):\n","  '''\n","  Given a DataFrame, model_name column, semantic_type \n","  Plot the raw sentiment types\n","  Options to save2file\n","  ''' \n","  \n","  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.lineplot(data=ts_df, x='sect_no', y=model_name, alpha=0.5).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n","\n","  if save2file == True:\n","    # Save graph to file.\n","    plot_filename = f'plot_nostand_sects_{model_name}.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return\n","\n","# Test\n","# plot_raw_sections(ts_df=corpus_sects_df, model_name='pattern', semantic_type='section', save2file=False);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FmrtifYoIjOT"},"outputs":[],"source":["# Raw Plot of Section Sentiments (Not scaled by mid-Section Sentence No to match Sentence/Paragraph x-axes)\n","\n","def plot_raw_sentiments(model_name='vader', semantic_type='sentence', save2file=False):\n","  '''\n","  Given a DataFrame, model_name column, semantic_type \n","  Plot the raw sentiment types\n","  Options to save2file\n","  ''' \n","  \n","  if semantic_type == 'sentence':\n","    ts_df = corpus_sents_df\n","    x_units = 'sent_no'\n","  elif semantic_type == 'paragraph':\n","    ts_df = corpus_parags_df\n","    x_units = 'parag_no'\n","  elif (semantic_type == 'section') | (semantic_type == 'section_stand'):\n","    ts_df = corpus_sects_df\n","    x_units = 'sect_no'\n","  elif (semantic_type == 'chapter') | (semantic_type == 'chapter_stand'):\n","    ts_df = corpus_chaps_df\n","    x_units = 'chap_no'\n","    \n","  else:\n","    print(f'ERROR: {semantic_type} must be sentence, paragraph or section')\n","\n","\n","  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.lineplot(data=ts_df, x=x_units, y=model_name, alpha=0.5, label=model_name).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n","  \n","  plt.legend(loc='best')\n","\n","  if save2file == True:\n","    # Save graph to file.\n","    plot_filename = f'plot_raw_sentiments_{semantic_type}_{model_name}.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return\n","\n","# Test\n","# plot_raw_sections(ts_df=corpus_sects_df, model_name='pattern', semantic_type='section', save2file=False);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3FR3G1QH-CAW"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","def get_lowess_ts(data, f=2./3., pts=None, itn=3, order=1):\n","    \"\"\"Fits a nonparametric regression curve to a scatterplot.\n","    Parameters\n","    ----------\n","    data : pandas.Series\n","        Data points in the scatterplot. The\n","        function returns the estimated (smooth) values of y.\n","    **Optionals**\n","    f : float\n","        The fraction of the data set to use for smoothing. A\n","        larger value for f will result in a smoother curve.\n","    pts : int\n","        The explicit number of data points to be used for\n","        smoothing instead of f.\n","    itn : int\n","        The number of robustifying iterations. The function will run\n","        faster with a smaller number of iterations.\n","    order : int\n","        The order of the polynomial used for fitting. Defaults to 1\n","        (straight line). Values < 1 are made 1. Larger values should be\n","        chosen based on shape of data (# of peaks and valleys + 1)\n","    Returns\n","    -------\n","    pandas.Series containing the smoothed data.\n","    \"\"\"\n","    # Authors: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n","    #            original\n","    #          Dan Neuman <https://github.com/dneuman>\n","    #            converted to Pandas series and extended to polynomials\n","    # License: BSD (3-clause)\n","\n","    x = np.array(data.index, dtype=float)\n","    # condition x-values to be between 0 and 1 to reduce errors in linalg\n","    x = x - x.min()\n","    x = x / x.max()\n","    y = data.values\n","    n = len(data)\n","    if pts is None:\n","        f = np.min([f, 1.0])\n","        r = int(np.ceil(f * n))\n","    else:  # allow use of number of points to determine smoothing\n","        r = int(np.min([pts, n]))\n","    r = min([r, n-1])\n","    order = max([1, order])\n","    # Create matrix of 1, x, x**2, x**3, etc, by row\n","    xm = np.array([x**j for j in range(order+1)])\n","    # Create weight matrix, one column per data point\n","    h = [np.sort(np.abs(x - x[i]))[r] for i in range(n)]\n","    w = np.clip(np.abs((x[:, None] - x[None, :]) / h), 0.0, 1.0)\n","    w = (1 - w ** 3) ** 3\n","    # Set up output\n","    yEst = np.zeros(n)\n","    delta = np.ones(n)  # Additional weights for iterations\n","    for iteration in range(itn):\n","        for i in range(n):\n","            weights = delta * w[:, i]\n","            xw = np.array([weights * x**j for j in range(order+1)])\n","            b = xw.dot(y)\n","            a = xw.dot(xm.T)\n","            beta = np.linalg.solve(a, b)\n","            yEst[i] = sum([beta[j] * x[i]**j for j in range(order+1)])\n","        # Set up weights to reduce effect of outlier points on next iteration\n","        residuals = y - yEst\n","        s = np.median(np.abs(residuals))\n","        delta = np.clip(residuals / (6.0 * s), -1, 1)\n","        delta = (1 - delta ** 2) ** 2\n","\n","    return pd.Series(yEst, index=data.index, name='Trend')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"l26W_YY2_whD"},"outputs":[],"source":["\"\"\"\n","\n","print(*corpus_sents_df.columns, sep='\\n')\n","\n","temp_ls = [f'{x}_stdscaler' for x in models_baseline_ls]\n","\n","sns.set(style=\"whitegrid\")\n","\n","for amodel in temp_ls:\n","  corpus_sents_df['temp_ts'] = Lowess(corpus_sents_df[amodel], f=0.1)\n","  g = sns.lineplot(\n","      # data=fmri.query(\"region == 'frontal'\"),\n","      data=corpus_sents_df,\n","      x=\"sent_no\", y='temp_ts', # hue=\"event\", units=\"subject\",\n","      estimator=None, lw=1,\n","      legend=False\n","  )\n","plt.title(f'{CORPUS_FULL}\\n LOWESS')\n","plt.legend(loc='best');\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EeSIwArU-Khu"},"outputs":[],"source":["# _ = get_lowess(corpus_sents_df, models_ls=['vader_stdscaler','bing_stdscaler','textblob_stdscaler','flair_stdscaler','stanza_stdscaler'], text_unit='sentence', afrac=1./20);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yCP2BR-H_XWv"},"outputs":[],"source":["# _ = get_lowess(corpus_sents_df, models_ls=['vader_lnorm_stdscaler','bing_lnorm_stdscaler','textblob_lnorm_stdscaler','flair_lnorm_stdscaler','stanza_lnorm_stdscaler'], text_unit='sentence', afrac=1./20);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iB8ibstaeiVd"},"outputs":[],"source":["# TODO: must plot in order to save, cannot save without first plotting\n","\n","def get_lowess(ts_df='corpus_parags_df', models_ls=MODELS_LS, text_unit='paragraph', plot_subtitle='', alabel='', afrac=1./10, ait=5, alpha=0.5, do_plot=True, save2file=False):\n","  '''\n","  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n","  Return a DataFrame with LOWESS values\n","  If 'plot=True', also output plot\n","  '''\n","\n","  # global corpus_all_df\n","\n","  lowess_df = pd.DataFrame()\n","\n","  # Step 1: Calculate LOWESS smoothed values\n","  for i,acol in enumerate(models_ls):\n","    sm_x, sm_y = sm_lowess(endog=ts_df[acol].values, exog=ts_df.index.values, frac=afrac, it=ait, return_sorted = True).T\n","    col_new = f'{acol}_lowess'\n","    lowess_df[col_new] = pd.Series(sm_y)\n","    # Optionally plot LOWESS for all models\n","    if do_plot:\n","      if alabel == '':\n","        alabel == acol\n","      plt.plot(sm_x, sm_y, label=acol, alpha=alpha, linewidth=2)\n","\n","  lowess_df['median'] = lowess_df.median(axis=1) # sm_y # corpus_all_df[df_cols_ls].median(axis=1)\n","  \n","  # Step 2: Optionally plot LOWESS for median\n","  if do_plot:\n","    # sm_x, sm_y = sm_lowess(endog=lowess_df.median, exog=lowess_df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n","    # plt.plot(sm_x, sm_y, label='median', alpha=0.9, linewidth=2, color='black')\n","    \n","    frac_str = str(round(100*afrac))\n","    plt.title(f'{CORPUS_FULL} \\n {plot_subtitle} {text_unit} Standardized Sentiment Smoothed with LOWESS (frac={frac_str})')\n","    plt.legend() # (title='Sentiment Model', loc='best')\n","\n","  # Step 3: Optionally save to file\n","  if save2file:\n","    # Save Plot to file.\n","    plot_filename = f'plot_{text_unit}_lowess_{plot_subtitle.split()[0].lower()}_{author_abbr_str}_{title_str}.png'\n","    # plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plot_filename, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","\n","  return lowess_df\n","\n","\n","# Test\n","'''\n","new_lowess_col = f'{sa_model}_lowess'\n","my_frac = 1./10\n","my_frac_per = round(100*my_frac)\n","new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n","corpus_all_df[new_lowess_col] = plot_lowess(corpus_all_df, [sa_model], afrac=my_frac)\n","corpus_all_df.head()\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cANCC2iz6nwo"},"outputs":[],"source":["def get_sent2dets(sent_no):\n","  '''\n","  Given a Sentence Number\n","  Return the corresponding Paragraph, Section and Chapter Numbers that contain it\n","  '''\n","\n","  # Get Paragraph No containing given Sentence No\n","  sent_parag_no = int(corpus_sents_df[corpus_sents_df['sent_no']==sent_no]['parag_no'])\n","\n","  # Get Section No containing given Sentence No.\n","  corpus_sects_ls = list(corpus_sects_df['sect_no'])\n","  for asect_no in corpus_sects_ls:\n","    if (int(corpus_sects_df[corpus_sects_df['sect_no'] == asect_no]['sent_no_start']) > sent_no):\n","      break\n","    sent_sect_no = asect_no\n","    # print(f'asect={asect_no}')\n","\n","  # Get Chapter No containing given Sentence No.\n","  corpus_chaps_ls = list(corpus_chaps_df['chap_no'])\n","  for achap_no in corpus_chaps_ls:\n","    if (int(corpus_chaps_df[corpus_chaps_df['chap_no'] == achap_no]['sent_no_start']) > sent_no):\n","      break\n","    sent_chap_no = achap_no\n","    # print(f'achap={achap_no}')\n","\n","\n","  return sent_parag_no, sent_sect_no, sent_chap_no\n","\n","# Test\n","# sent_parag_no, sent_sect_no, sent_chap_no = get_sent2dets(1408)\n","# print(f'sent_parag_no={sent_parag_no}\\nsent_sect_no={sent_sect_no}\\nsent_chap_no={sent_chap_no}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"w6liffwhYtSw"},"outputs":[],"source":["# get_sentnocontext_report(the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sijR4OknJive"},"outputs":[],"source":["def get_sentnocontext(timeser_df, sent_no=1, n_sideparags=1, sent_highlight=True):\n","  '''\n","  Given a sentence number in the Corpus\n","  Return the containing paragraph and n-paragraphs on either side\n","  (e.g. if n=2, return 2+1+2=5 paragraphs)\n","  '''\n","\n","  # print(f'just entered get_sentnocontext withsent_no: {sent_no} and  timeser_df:\\n\\n    {timeser_df}')\n","  timeser_indx = timeser_df['sent_no'] == sent_no\n","  parag_target_no = int(timeser_df[timeser_df['sent_no'] == sent_no]['parag_no'])\n","  # print(f'parag_target_no = {parag_target_no} and type: {type(parag_target_no)}')\n","\n","  if n_sideparags == 0:\n","    parags_context_ls = list(corpus_parags_df[corpus_parags_df['parag_no'] == parag_target_no]['parag_raw'])\n","\n","  else:\n","    parag_start = parag_target_no - n_sideparags\n","    parag_end = parag_target_no + n_sideparags + 1\n","    parags_context_ls = list(corpus_parags_df.iloc[parag_start:parag_end]['parag_raw'])\n","\n","\n","  if sent_highlight == True:\n","    parag_match_str = str(parags_context_ls[n_sideparags])\n","    # print(f'parag_match_str:\\n  {parag_match_str}') parag_no\n","    sent_idx = sent_no\n","    sent_str = (timeser_df[timeser_df['sent_no']==sent_idx]['sent_raw'].values)[0]\n","    sent_str_up = sent_str.upper()\n","    # print(f'sent_str:\\n  {sent_str}')\n","    # parags_context_ls[n_sideparags] \n","    parags_context_ls[n_sideparags] = parag_match_str.replace(sent_str, sent_str_up)\n","\n","  return parags_context_ls\n","\n","# Te\n","# context_highlighted = get_sentnoparags(sent_no=1051, n_sideparags=1)\n","# print(context_highlighted)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zM_I8uDfJztH"},"outputs":[],"source":["corpus_sents_df = pd.DataFrame()\n","\n","def get_sentnocontext_report(ts_df = corpus_sents_df, the_sent_no=7, the_n_sideparags=1, the_sent_highlight=True):\n","  '''\n","  Wrapper function around  get_sentnocontext()   Paragraph(s) Context\n","  Prints a nicely formatted context report\n","  '''\n","\n","  context_noparags = the_n_sideparags*2+1\n","  # tsdf\n","  # print('-------------------------------------------------------------')\n","  print(f'The {context_noparags} Paragraph(s) Context around the Sentence #{the_sent_no} Crux Point:')\n","  # print(f'ts_df = {ts_df}')\n","  print('-------------------------------------------------------------')\n","  print(f\"\\nCrux Sentence #{the_sent_no} Raw Text: -------------------------------\\n\\n    {str(ts_df[ts_df['sent_no'] == the_sent_no]['sent_raw'].values[0])}\\n\") # iloc[the_sent_no]['sent_raw']}\")\n","\n","  sent_parag_no, sent_sect_no, sent_chap_no = get_sent2dets(the_sent_no)\n","  print(f\"\\nCrux Sentence #{the_sent_no} is Contained in: ---------------------------\\n\\n    Paragraph #{sent_parag_no}\\n      Section #{sent_sect_no}\\n      Chapter #{sent_chap_no}\\n\")\n","\n","  print(f\"\\n{context_noparags} Paragraph(s) Context: ------------------------------\")\n","  # print('calling get_sentnocontext')\n","  # context_parags_ls = get_sentnocontext(timeser_df = ts_df, sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n","  context_parags_ls = get_sentnocontext(timeser_df = corpus_sents_df, sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n","  context_len = len(context_parags_ls)\n","  context_mid = context_len//2\n","  for i, aparag in enumerate(context_parags_ls):\n","    if i==context_mid:\n","      # print(f'\\n>>> Paragraph #{i}: <<< Crux Point Sentence CAPITALIZED within this Paragraph\\n\\n    {aparag}') \n","      print(f'\\n<*> {aparag}')\n","    else:\n","      # print(f'\\n    Paragraph #{i}:\\n\\n    {aparag}')\n","      print(f'\\n    {aparag}')\n","\n","  return\n","\n","# Test\n","# get_sentnocontext_report(sent_no=1051, n_sideparags=1, sent_highlight=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Y04GiohGypNX"},"outputs":[],"source":["def get_section_timeseries(sect_no):\n","  '''\n","  Given a Section No in the current Corpus\n","  Return the start,mid and ending Sent No for this Section as well as the Sentiment Time Series between the start/end Sentence for this Section\n","  '''\n","  \n","  section_count = corpus_sects_df.shape[0]\n","\n","  # Compute the start, mid and end Sentence numbers for the selected Section\n","  if Select_Section_No >= section_count:\n","    print(f'ERROR: You picked Section #{Select_Section_No}.\\n  Section for this Corpus must be between 0 and {section_count-1}')\n","    return -1\n","\n","  else:\n","\n","    # Get the starting and middle Sentence No of this Section\n","    sect_sent_start = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No]['sent_no_start'].values)\n","    # sect_sent_mid = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No]['sent_no_mid'].values)\n","\n","    # Calculate last Sentence No of this Section\n","    if Select_Section_No == (section_count-1):   \n","      print(f'You selected the last Section of this Corpus')\n","      sect_sent_end = corpus_sents_df.shape[0] - 1\n","    else:\n","      sect_sent_end = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No+1]['sent_no_start'].values) # - 1\n","      \n","    print(f'Section #{sect_no}:----------')\n","    print(f'\\nsect_sent_start: {sect_sent_start}')\n","    # print(f'sect_sent_mid: {sect_sent_mid}')\n","    print(f'sect_sent_end: {sect_sent_end}')\n","\n","\n","  # Comput the start, and end Paragraph numbers for the selected Section\n","  sect_parag_start = int(corpus_sents_df[corpus_sents_df['sent_no'] == sect_sent_start]['parag_no'].values)\n","  sect_parag_end = int(corpus_sents_df[corpus_sents_df['sent_no'] == sect_sent_end]['parag_no'].values)\n","\n","  print(f'\\nsect_parag_start: {sect_parag_start}')\n","  print(f'sect_parag_end: {sect_parag_end}')\n","\n","\n","  # Extract and Return both a Sentence and Paragraph DataFrame for this Section \n","\n","  section_sents_df = corpus_sents_df.iloc[sect_sent_start:sect_sent_end]\n","\n","  section_parags_df = corpus_parags_df.iloc[sect_parag_start:sect_parag_end]\n","\n","\n","  return section_sents_df, section_parags_df\n","\n","# Test\n","\n","# section_sents_df, section_parags_df = get_section_timeseries(Select_Section_No)\n","\n","# section_sents_df.head()\n","\n","# print(f'\\nsection_sents_df.shape: {section_sents_df.shape}')\n","# print(f'section_parags_df.shape: {section_parags_df.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VgFMgdQ3X33F"},"outputs":[],"source":["def get_lowess_cruxes(ts_df, col_series, text_type='sentence', win_lowess=5, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n","  '''\n","  Given a DataFrame and a Time Series Column within it and a LOWESS window\n","  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n","  '''\n","\n","  crux_ls = []\n","\n","  series_len = ts_df.shape[0]\n","\n","  sent_no_min = ts_df.sent_no.min()\n","  sent_no_max = ts_df.sent_no.max()\n","  # print(f'sent_no_min {sent_no_min}')\n","\n","  sm_x = ts_df.index.values\n","  sm_y = ts_df[col_series].values\n","\n","  half_win = int((win_lowess/100)*series_len)\n","\n","  # Find peaks(max).\n","  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n","  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n","  # peak_indexes = peak_indexes + sent_no_min\n","  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n","  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n","  # peak_indexes_np = peak_indexes_np + sent_no_min\n","  peak_indexes = peak_indexes[0]\n","\n","  peak_x_ls = list(peak_indexes)\n","  peak_y_ls = list(sm_y[peak_indexes])\n","\n","  # Find valleys(min).\n","  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n","  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n","  valley_indexes = valley_indexes[0]\n","  \n","  valley_x_ls = list(valley_indexes)\n","  valley_y_ls = list(sm_y[valley_indexes])\n","\n","  # Save all peaks/valleys as list of (x,y) coordinate tuples\n","  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n","  x_all_ls = peak_x_ls + valley_x_ls\n","  # readjust starting Sentence No to start with first sentence in segement window\n","  x_all_ls = [x+sent_no_min for x in x_all_ls]\n","  y_all_ls = peak_y_ls + valley_y_ls\n","  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n","\n","  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n","\n","\n","  if do_plot == True:\n","    # Plot main graph.\n","    (fig, ax) = plt.subplots()\n","    ax.plot(sm_x, sm_y)\n","\n","    if text_type == 'sentence':\n","      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n","      for i, aparag in enumerate(paragraph_boundries_ls):\n","        if i%5 == 0:\n","          # Plot every 5th paragraph\n","          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n","          plt.text(sent_no, sec_y_height, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n","          plt.axvline(sent_no, color='blue', alpha=0.1)\n","    elif text_type == 'paragraph':\n","      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n","      for i, aparag_no in enumerate(paragraph_boundries_ls):\n","        if i%5 == 0:\n","          # Plot every 5th paragraph\n","          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n","          plt.text(aparag_no, sec_y_height, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n","          plt.axvline(aparag_no, color='blue', alpha=0.1)    \n","    else:\n","      print(f\"ERROR: text_type is {text_type} but must be either 'sentence' or 'paragarph'\")\n","\n","    win_half = 0 # 2500\n","\n","    # Plot peaks.\n","    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n","\n","    # readjust starting Sentence No to start with first sentence in segement window\n","    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n","    ax.scatter(peak_x_ls, peak_y_ls)\n","    for i, txt in enumerate(list(peak_x_ls)):\n","        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n","\n","    # Plot valleys.\n","    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n","    # readjust starting Sentence No to start with first sentence in segement window\n","    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n","    ax.scatter(valley_x_ls, valley_y_ls)\n","    for i, txt in enumerate(list(valley_x_ls)):\n","        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n","\n","    # for i, txt in enumerate(list(valley_x_ls)):\n","    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n","    # plt.plot(x, y, 'bo')\n","    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n","    # adjust_text(texts)\n","\n","    # Confidence Interval (Min/Max Range)\n","    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n","\n","    plt.title(f'{CORPUS_FULL} Raw Sentence Crux Detection in Section #{Select_Section_No}\\nLOWESS Smoothed {subtitle_str} and SciPy find_peaks')\n","    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n","\n","    # locs, labels = xticks()  # Get the current locations and labels.\n","    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n","\n","    plt.ylabel(f'Sentiment Value')\n","    plt.legend(loc='best');\n","  \n","  if save2file == True:\n","    # Save graph to file.\n","    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n","    plt.legend(loc='best')\n","    plt.savefig('argrelextrema.png')\n","\n","  return crux_coord_ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yv376c5_bfrg"},"outputs":[],"source":["def crux_sortsents(crux_ls, corpus_df=corpus_sents_df, atop_n=3, get_peaks=True, sort_key='sent_no'):\n","  '''\n","  Given a list of tuples (sent_no, sentiment value), atop_n cruxes to retrieve and bool flag get_peaks\n","  Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n","  '''\n","  # print(f'Entered crux_sortsents with crux_ls={crux_ls}\\natop_n={atop_n}')\n","\n","  crux_old_ls = []\n","  crux_new_ls = []\n","\n","  # TODO: Error check for null/invalid corpus_df/crus_ls sent_no\n","\n","  if sort_key == 'sent_no':\n","    crux_old_ls = sorted(crux_ls, key=lambda tup: (tup[0]))\n","  else:\n","    crux_old_ls = sorted(crux_ls, key=lambda tup: (tup[1]), reverse=get_peaks)\n","\n","  \"\"\"\n","  if get_peaks == True:\n","    crux_old_ls = [x for x in crux_old_ls if x[1] > 0]\n","  else:\n","    crux_old_ls = [x for x in crux_old_ls if x[1] < 0]\n","  \"\"\";\n","\n","  # Return only the n_top cruxes if more cruxes than n_top else return all cruxes\n","  if (sort_key != 'sent_no') & (len(crux_old_ls) >= atop_n):\n","    # trim crux list if user asked for less than total number\n","    crux_old_ls = crux_old_ls[:atop_n]\n","\n","  # Retrieve the Sentence raw text for each Crux and add as Tuple(sent_no, sentiment_val, raw_text) to return List\n","  for asent_no, asentiment_val in crux_old_ls:\n","    # print(f'  Retrieving Sentence #{asent_no} with Sentiment Value {asentiment_val} from DataFrame {corpus_df}')\n","    asent_int = int(asent_no)\n","    # print(f\"                      asent_int is type: {type(asent_int)} and Sentence Text:\\n\\n     {corpus_df.iloc[asent_int]['sent_raw']}\")\n","\n","    asent_raw = str(corpus_df[corpus_df['sent_no'] == asent_int]['sent_raw'].values[0])\n","    crux_new_ls.append((int(asent_no), float(f'{asentiment_val:.3f}'), str(asent_raw),)) # Append a Tuple to return List\n","\n","  return crux_new_ls\n","\n","# Test\n","# crux_n_top_ls = crux_sortsents(section_crux_ls, atop_n=3, get_peaks=True)"]},{"cell_type":"markdown","metadata":{"id":"qjb60CVnz5SF"},"source":["## **crux_sortsents_report**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"v_7wlJjq2QUo"},"outputs":[],"source":["%whos DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wFDjK1o8gnQ0"},"outputs":[],"source":["def crux_sortsents_report(crux_ls, ts_df=corpus_sents_df, library_type='baseline', top_n=3, get_peaks=True, sort_by='sent_no', n_sideparags=1, sentence_highlight=True):\n","  '''\n","  Wrapper function to produce report based upon 'crux_sortsents() described as:\n","    Given a list of tuples (sent_no, sentiment value), top_n cruxes to retrieve and bool flag get_peaks\n","    Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n","\n","    # get_sentnocontext_report\n","  '''\n","\n","  if get_peaks == True:\n","    crux_label = 'Peak'\n","  else:\n","    crux_label = 'Valley'\n","\n","  # Filter and keep only the desired crux type in List crux_subset_ls\n","  crux_subset_ls = []\n","  for acrux_tup in crux_ls:\n","    crux_type, crux_x_coord, crux_y_coord = acrux_tup\n","    if crux_type.lower() == crux_label.lower():\n","      crux_subset_ls.append((crux_x_coord,crux_y_coord)) # Append a Tuple to List\n","\n","  flag_2few_cruxes = False\n","\n","  # Check to see if asked for more Cruxes than were found \n","  top_n_found = len(crux_subset_ls)\n","  if top_n_found < top_n:\n","    flag_2few_cruxes = True\n","    print(f'\\n\\nWARNING: You asked for {top_n} {crux_label}s\\n         but there only {top_n_found} were found above.\\n')\n","    print(f'             Displaying as many {crux_label}s as possible,')\n","    print(f'             to retrieve more, go back to the previous code cells and re-run with wider Crux Window.\\n\\n')\n","\n","\n","  # Get Sentence no and raw text for appropriate Crux subset\n","  # print(f'Calling crux_n_top_ls with crux_subset_ls={crux_subset_ls}\\ntop_n={top_n}\\nget_peaks={get_peaks}')\n","  crux_n_top_ls = crux_sortsents(corpus_df = ts_df, crux_ls=crux_subset_ls, atop_n=top_n, get_peaks=get_peaks, sort_key=sort_by)\n","  # print(f'Returning crux_n_top_ls = {crux_n_top_ls}')\n","\n","  # Print appropriate header Select_Section_No sent_no\n","  print('------------------------------')\n","  # print(f'library_type: {library_type}')\n","  if library_type in ['baseline','sentimentr','syuzhetr','transformer','unified']:\n","    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n","      print(f'Library: {library_type.capitalize()} ALL Top {top_n} {crux_label}s Found\\n')\n","    else:\n","      print(f'Library #{library_type.capitalize()} ONLY Top {top_n_found} {crux_label}s Found\\n')\n","  else:\n","    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n","      print(f'Section #{Select_Section_No} ALL Top {top_n} {crux_label}s Found\\n')\n","    else:\n","      print(f'Section #{Select_Section_No} ONLY Top {top_n_found} {crux_label}s Found\\n')\n","\n","  # Print summary of subset Cruxes\n","  for i,crux_sent_tup in enumerate(crux_n_top_ls):\n","    # crux_type, crux_x_coord, crux_y_coord = crux_sent_tup\n","    crux_x_coord, crux_y_coord, crux_sent_raw = crux_sent_tup\n","    print(f'   {crux_label} #{i} at Sentence #{crux_x_coord} with Sentiment Value {crux_y_coord}')\n","  # print('------------------------------\\n')\n","  # print('Sent_No  Sentiment   Sentence (Raw Text)\\n')\n","  \n","  # Print details of each Crux in subset\n","  for sent_no, sent_pol, sent_raw in crux_n_top_ls: \n","    sent_no = int(sent_no)\n","    print('\\n\\n-------------------------------------------------------------')\n","    print(f'Sentence #{sent_no}   Sentiment: {sent_pol:.3f}\\n') #     {sent_raw}\\n')\n","    # print('------------------------------')\n","    get_sentnocontext_report(ts_df=ts_df, the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n","    # get_sentnocontext(sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sJw2WDlwHH5y"},"outputs":[],"source":["# For the selected Section, create an expanded Paragraph DataFrame to match the number of Sentences in the Section\n","\n","def expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df', model_name='vader_lnorm_medianiqr'):\n","  '''\n","  Given a Corpus Paragraph DataFrame and a longer Sentence DataFrame that cover the same Section of a Corpus\n","  Return an expanded version of the Paragraph DataFrame of equal length to the Sentence DataFrame so they can be plotted/compared along the same x-axis\n","  '''\n","\n","  parag_sentiment_expanded_ls = []\n","  parags_midpoint_ls = []\n","  sent_sum = 0\n","  parag_start = section_parags_df.parag_no.min()\n","  print(f'parag_start: {parag_start}')\n","  parag_end = section_parags_df.parag_no.max() + 1 # shape[0] + 3\n","  print(f'parag_end: {parag_end}')\n","  parags_range_ls = list(range(parag_start, parag_end, 1))\n","  print(f'parags_range_ls: {parags_range_ls}')\n","  for i, aparag_no in enumerate(parags_range_ls):\n","    aparag_sentiment_fl = float(corpus_parags_df[corpus_parags_df['parag_no']==aparag_no][model_name])\n","    sent_ct = len(corpus_sents_df[corpus_sents_df.parag_no == aparag_no])\n","    parag_midpoint_int = int(sent_ct//2 + sent_sum)\n","    parags_midpoint_ls.append(parag_midpoint_int)\n","    for asent in range(sent_ct):\n","      parag_sentiment_expanded_ls.append(aparag_sentiment_fl)\n","    sent_sum += sent_ct\n","    print(f'#{i}: Paragraph #{aparag_no} has {sent_ct} Sentences and Avg Sentiment: {aparag_sentiment_fl:.3f}')\n","\n","  print(f'\\nSentence Total: {sent_sum} vs Original section_sents_df: {section_sents_df.shape[0]}')\n","  print(f'  Paragraph Sentiment length: {len(parag_sentiment_expanded_ls)}')\n","\n","  # section_sents_parags_df = section_sents_df.copy()\n","  \n","  # section_sents_parags_df.head(1);\n","\n","  # corpus_sents_df['']\n","\n","  return parag_sentiment_expanded_ls, parags_midpoint_ls\n","\n","# Test\n","# section_sents_df['vader_lnorm_medianiqr_parag'] = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mXRq54NQwI7D"},"outputs":[],"source":["def get_crux_points(ts_df, col_series, text_type='sentence', win_per=5, sec_y_labels=True, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n","  '''\n","  Given a DataFrame and a Time Series Column within it and a LOWESS window\n","  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n","  '''\n","\n","  # print('entered get_crux_points') \n","  crux_ls = []\n","\n","  series_len = ts_df.shape[0]\n","  # print(f'series_len = {series_len}')\n","\n","  sent_no_min = ts_df.sent_no.min()\n","  sent_no_max = ts_df.sent_no.max()\n","  # print(f'sent_no_min {sent_no_min}')\n","\n","  sm_x = ts_df.index.values\n","  sm_y = ts_df[col_series].values.flatten()\n","\n","  half_win = int((win_per/100)*series_len)\n","  # print(f'half_win = {half_win}')\n","  # print(f'sm_y type = {type(sm_y)}')\n","\n","  # Find peaks(max).\n","  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n","  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n","  # peak_indexes = peak_indexes + sent_no_min\n","  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n","  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n","  # peak_indexes_np = peak_indexes_np + sent_no_min\n","  # print(f'peak_indexes type = {type(peak_indexes)}') # sent_no_start sent\n","  peak_indexes = peak_indexes[0]\n","\n","  peak_x_ls = list(peak_indexes)\n","  peak_x_adj_ls = [x+sent_no_min for x in peak_x_ls]\n","\n","  peak_y_ls = list(sm_y[peak_indexes])\n","\n","  peak_label_ls = ['peak'] * len(peak_y_ls)\n","  peak_coord_ls = tuple(zip(peak_label_ls, peak_x_adj_ls, peak_y_ls))\n","\n","  # peak_y_all_ls = peak_y_ls + valley_y_ls\n","  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n","\n","  # Find valleys(min).\n","  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n","  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n","  valley_indexes = valley_indexes[0]\n","  \n","  valley_x_ls = list(valley_indexes)\n","  valley_x_adj_ls = [x+sent_no_min for x in valley_x_ls]\n","\n","  valley_y_ls = list(sm_y[valley_indexes])\n","\n","  valley_label_ls = ['valley'] * len(valley_y_ls)\n","  valley_coord_ls = tuple(zip(valley_label_ls, valley_x_adj_ls, valley_y_ls))\n","\n","  # Combine Peaks and Valley Coordinates into List of Tuples(label, x_coord, y_coord)\n","  crux_coord_ls = peak_coord_ls + valley_coord_ls\n","\n","  # Save all peaks/valleys as list of (x,y) coordinate tuples\n","  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n","  #  x_all_ls = peak_x_ls + valley_x_ls\n","  # readjust starting Sentence No to start with first sentence in segement window\n","  #  x_all_ls = [x+sent_no_min for x in x_all_ls]\n","  #  y_all_ls = peak_y_ls + valley_y_ls\n","  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n","\n","  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n","\n","\n","  if do_plot == True:\n","    # Plot main graph.\n","    (fig, ax) = plt.subplots()\n","    ax.plot(sm_x, sm_y)\n","\n","    if sec_y_labels == True:\n","      section_sent_no_boundries_ls = list(corpus_sects_df['sent_no_start'])\n","      section_no_ls = list(corpus_sects_df['sect_no'])\n","      for i, asect_no in enumerate(section_sent_no_boundries_ls):\n","        # Plot vertical lines for section boundries\n","        plt.text(asect_no, sec_y_height, f'Section #{section_no_ls[i]}', alpha=0.2, rotation=90)\n","        plt.axvline(asect_no, color='blue', alpha=0.1)    \n","\n","\n","    win_half = 0 # 2500\n","\n","    # Plot peaks.\n","    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n","\n","    # readjust starting Sentence No to start with first sentence in segement window\n","    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n","    ax.scatter(peak_x_ls, peak_y_ls)\n","    for i, txt in enumerate(list(peak_x_ls)):\n","        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n","\n","    # Plot valleys.\n","    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n","    # readjust starting Sentence No to start with first sentence in segement window\n","    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n","    ax.scatter(valley_x_ls, valley_y_ls)\n","    for i, txt in enumerate(list(valley_x_ls)):\n","        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, annotation_clip=True) # xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n","\n","    # for i, txt in enumerate(list(valley_x_ls)):\n","    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n","    # plt.plot(x, y, 'bo')\n","    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n","    # adjust_text(texts)\n","\n","    # Confidence Interval (Min/Max Range)\n","    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n","\n","    plt.title(f'{CORPUS_FULL} SMA Smoothed Sentence Sentiment Arcs Crux Detection\\n{subtitle_str} Models: {col_series}')\n","    plt.xlabel(f'Sentence No') # within selected Section #{Select_Section_No}')\n","\n","    # locs, labels = xticks()  # Get the current locations and labels.\n","    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n","\n","    plt.ylabel(f'Sentiment Value')\n","    plt.legend(loc='best');\n","  \n","  if save2file == True:\n","    # Save graph to file.\n","    plt.title(f'{BOOK_TITLE_FULL} \\n SMA Smoothed Sentence Sentiment Arcs Crux Points')\n","    # plt.legend(loc='best')\n","    plt.savefig(f\"{CORPUS_FILENAME.split('.')[0]}_find_peaks.png\")\n","\n","  return crux_coord_ls;"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xNdJQGtghS_X"},"outputs":[],"source":["def get_standardscaler(series_name, values_ser):\n","  '''\n","  Given a Series of values\n","  Return a list of StandardSclar transformations on that input Series\n","  '''\n","\n","  scaler = StandardScaler()  \n","\n","  # Convert to np.array\n","  values_np = np.array(values_ser)\n","  \n","  values_flat_np = values_np.reshape((len(values_np), 1))\n","\n","  scaler = scaler.fit(values_flat_np)\n","  print(f'Model: {series_name}\\n       Mean: {scaler.mean_}, StandardDeviation: {np.sqrt(scaler.var_)}') # % (scaler.mean_, np.sqrt(scaler.var_)))\n","  values_flat_xform_np = scaler.transform(values_flat_np)\n","\n","  return values_flat_xform_np.flatten().tolist()\n","\n","# Test\n","# stdscaler_series_ls = get_standardscaler('vader_lnorm_medianiqr_roll100', corpus_sents_df['vader_lnorm_medianiqr_roll100'])\n","# corpus_sents_df['vader_roll100_stdscaler'] = pd.Series(stdscaler_series_ls)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ABW4oQ4xJT4R"},"outputs":[],"source":["def plot_models(models_subset_ls, models_type='baseline', text_unit='sent_no', win_per=10):\n","  '''\n","  Given a DataFrame and list of correponding Models in that DataFrame\n","  Plot the Sentiment Arcs for the stdscaler with the rolling window \n","  '''\n","\n","  Mean_All_Arc = True\n","\n","  if models_type == 'baseline':\n","    models_ls = models_baseline_ls\n","  elif models_type == 'sentimentr':\n","    models_ls = models_sentimentr_ls\n","  elif models_type == 'syuzhetr':\n","    models_ls = models_syuzhetr_ls\n","  elif models_type == 'transformer':\n","    models_ls = models_transformer_ls\n","  else:\n","    print(f'ERROR: model_type={model_type} must be one of: baseline, sentimentr, syuzhetr or transformer')\n","\n","\n","  if (models_type == 'baseline') | (models_type == 'transformers'):\n","    # Have Corpus data in 4 DataFrames: corpus_[sents|parags|sects|chaps]_df\n","    if text_unit == 'sent_no':\n","      ts_df = corpus_sents_df\n","      text_raw = 'sent_raw'\n","      text_type = 'Sentence'\n","      win_roll = int(corpus_sents_df.shape[0]* win_per/100)\n","    elif text_unit == 'parag_no':\n","      ts_df = corpus_parags_df\n","      text_raw = 'parag_raw'\n","      text_type = 'Paragraph'\n","      win_roll = int(corpus_parags_df.shape[0]* win_per/100)\n","    elif text_unit == 'sect_no':\n","      ts_df = corpus_sects_df\n","      text_raw = 'sect_raw'\n","      text_type = 'Section'\n","      win_roll = int(corpus_sects_df.shape[0]* win_per/100)\n","    elif text_unit == 'chap_no':\n","      ts_df = corpus_chaps_df\n","      text_raw = 'chap_raw'\n","      text_type = 'Chapter'\n","      win_roll = int(corpus_chaps_df.shape[0]* win_per/100)\n","    else:\n","      print(f'ERROR: text_unit={text_unit} must be one of: sent_no, parag_no, sect_no or chap_no')\n","\n","  elif (models_type == 'sentimentr') | (models_type == 'syuzhetr'):\n","\n","    # Only have Corpus Sentence data in one DataFrame: corpus_sentimentr_df or corpus_syuzhetr_df\n","    text_raw = 'sent_raw'\n","    text_type = 'Sentence'\n","\n","    if models_type == 'sentimentr':\n","      ts_df = corpus_sentimentr_df\n","      win_roll = int(corpus_sentimentr_df.shape[0]*win_per/100)\n","    else:\n","      ts_df = corpus_syuzhetr_df\n","      win_roll = int(corpus_syuzhetr_df.shape[0]*win_per/100)\n","\n","  else:\n","    print(f'ERROR: model_type = {model_type} but must be one of 4: [baseline|transformer|sentimentr|syuzhet]')\n","\n","\n","  # Get Rolling Window String\n","  if len(str(win_per)) == 1:\n","    roll_str = 'roll' + '0' + str(win_per)\n","  else:\n","    roll_str = 'roll' + str(win_per%100)\n","\n","\n","  # Translate base model name into _stdscaler_rollxxx derivative\n","  all_stdscaler_roll_ls = []\n","  subset_stdscaler_roll_ls = []\n","  for amodel in models_ls:\n","    # Create a Rolling SMA Series from the stdscaler version of model sentiment values\n","    amodel_stdscaler = f'{amodel}_stdscaler'\n","    col_stdscaler_rollwin = f'{amodel}_stdscaler_{roll_str}'\n","    ts_df[col_stdscaler_rollwin] = ts_df[amodel_stdscaler].rolling(win_roll, center=True).mean()\n","    all_stdscaler_roll_ls.append(col_stdscaler_rollwin)\n","    if amodel in models_subset_ls:\n","      subset_stdscaler_roll_ls.append(col_stdscaler_rollwin)\n","    # col_name_roll = f'{amodel}_lnorm_medianiqr_{roll_str}'\n","\n","  # Compute the Mean of All\n","  mean_all_col = 'mean_stdscaler_' + roll_str\n","  col_stdscaler_roll_ls = [f'{x}_stdscaler_{roll_str}' for x in models_ls] #  if ('mean' not in x)]\n","  # print(f'Computing Mean based upon:\\n    {col_stdscaler_roll_ls}')\n","  ts_df[mean_all_col] = ts_df[all_stdscaler_roll_ls].mean(axis=1)\n","\n","\n","  palette = cycle(px.colors.qualitative.Safe)\n","  # palette = cycle(px.colors.sequential.PuBu)\n","\n","  my_layout = go.Layout(\n","      autosize=False,\n","      width=1600,\n","      height=800,\n","      margin=go.layout.Margin(\n","          l=10,\n","          r=50,\n","          b=100,\n","          t=100,\n","          pad = 1\n","      )\n","  )\n","\n","\n","  fig = go.Figure(layout=my_layout)\n","\n","  # add traces\n","  for i,amodel_stdscaler_roll in enumerate(subset_stdscaler_roll_ls):\n","    # print(f'adding trace: {amodel_stdscaler_roll}')\n","    fig.add_traces(go.Line(x = ts_df[text_unit],\n","                          y = ts_df[amodel_stdscaler_roll],\n","                          text = ts_df[text_raw],\n","                          name = amodel_stdscaler_roll,\n","                          hovertemplate = \"Model: <b>\"+amodel_stdscaler_roll+\"</b><br>\"+text_type+\" #<b>%{x}</b><br>Polarity <b>%{y}</b><br>Text: <b><i>%{text}</i></b>\", \n","                          marker_color=next(palette)))\n","  \"\"\"\n","  if Mean_Subset_Arc == True:\n","    mean_subset_col = 'mean_subset_'+roll_str\n","    corpus_sents_df[mean_subset_col] = corpus_sents_df[model_baseline_subset_ls].mean(axis=1)\n","    fig.add_traces(go.Line(x=corpus_sents_df['sent_no'],\n","                          y = corpus_sents_df[mean_subset_col],\n","                          line=dict(\n","                                # color='#000000',\n","                                width=5\n","                                ),\n","                          text = 'NA', # corpus_sents_df['sent_raw'],\n","                          name = 'Mean of Selected Models',\n","                          hovertemplate = \"Model <b>%{mean_subset_col}</b><br>Sentence #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n","                          marker_color=next(palette)))\n","\n","  \"\"\";\n","\n","  if Mean_All_Arc == True:\n","    # mean_all_col = 'mean_all_stdscaler_'+roll_str\n","    # ts_df[mean_all_col] = ts_df[col_stdscaler_rollwin_ls].mean(axis=1)\n","    fig.add_traces(go.Line(x=ts_df[text_unit],\n","                          y = ts_df[mean_all_col],\n","                          line=dict(\n","                                color='#000000',\n","                                width=5,\n","                                dash='dot',\n","                                ),\n","                          text = 'NA', # ts_df['sent_raw'],\n","                          name = 'Mean of All Models',\n","                          hovertemplate = \"Model <b>%{mean_all_col}</b><br>Paragraph #<b>%{x}</b><br>Polarity <b>%{y}</b>\", \n","                          marker_color=next(palette)))\n","\n","\n","  fig.update_layout(\n","      title=f\"{CORPUS_FULL}\\n{text_type} {models_type.capitalize()} Models<b><i> \" + roll_str.upper() + \"</i></b>\",\n","      xaxis_title=text_type + \" Number\",\n","      yaxis_title=\"StdScaler Sentiment Value\",\n","      hoverlabel=dict(\n","          bgcolor=\"white\",\n","          font_size=16,\n","          font_family=\"Rockwell\"\n","      ),\n","      font=dict(\n","          family=\"Courier New, monospace\",\n","          size=18,\n","          color=\"RebeccaPurple\"\n","      )\n","  )\n","\n","  fig.show();\n","\n","# Test\n","\n","# plot_models(models_subset_ls = ['vader','stanza'], models_type='baseline', text_unit='sent_no', win_per=10)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Q4ehet9JjOsK"},"outputs":[],"source":["def standardize_ts_ls(ts_df, col_ls):\n","  '''\n","  Given a DataFrame and list of Columns in that DataFrame\n","  Create 4 new Standardized Columns for each given Columns\n","  '''\n","\n","  # Create 4 new column names for each column provided\n","  for amodel in col_ls:\n","    # col_meanstd = f'{amodel}_meanstd'\n","    col_medianiqr = f'{amodel}_medianiqr'\n","    col_stdscaler = f'{amodel}_stdscaler'\n","    col_lnorm_stdscaler = f'{amodel}_lnorm_stdscaler'\n","    col_lnorm_medianiqr = f'{amodel}_lnorm_medianiqr'\n","\n","    # Standardize each column provided using Standard Scaler and  MedianIQRScaling\n","    ts_df[col_stdscaler]  = list2stdscaler(ts_df[amodel])\n","    ts_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(ts_df[amodel]).reshape(-1, 1))\n","    # Normalize the Sentence Sentiment by dividing by Chapter Length\n","    text_len_ls = list(ts_df['token_len'])\n","    text_sentiment_ls = list(ts_df[amodel])\n","    text_sentiment_norm_ls = [text_sentiment_ls[i]/(text_len_ls[i]+0.01) for i in range(len(text_len_ls))]\n","    # RobustStandardize Sentence sentiment values\n","    ts_df[col_lnorm_stdscaler]  = mean_std_scaler.fit_transform(np.array(pd.Series(text_sentiment_norm_ls)).reshape(-1, 1))\n","    ts_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(text_sentiment_norm_ls)).reshape(-1, 1))\n","\n","  return\n"]},{"cell_type":"markdown","metadata":{"id":"daEzvNu6huM-"},"source":["# **Either (a) Load Precomputed DataFrames or (b) Create Corpus DataFrames**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnWJCkqDhA5L"},"outputs":[],"source":["\"\"\"\n","\n","len(sections_ls)\n","min(sections_ls, key=len) \n","\n","# TODO: Spell check and correct common OCR errors\n","\n","# SymSpellPy\n","# JamSpell\n","# OCR - https://github.com/Alvant/MIL-OCR\n","\n","# !pip install -U symspellpy\n","\n","# Did not need these\n","# dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n","# bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n","\n","\n","import pkg_resources\n","from symspellpy import SymSpell, Verbosity\n","\n","sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n","dictionary_path = pkg_resources.resource_filename(\n","    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n","# term_index is the column of the term and count_index is the\n","# column of the term frequency\n","sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n","\n","# lookup suggestions for single-word input strings\n","input_term = \"memebers\"  # misspelling of \"members\"\n","input_term = \"summermorning\"\n","# max edit distance per lookup\n","# (max_edit_distance_lookup <= max_dictionary_edit_distance)\n","suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST,\n","                               max_edit_distance=2)\n","# display suggestion term, term frequency, and edit distance\n","for suggestion in suggestions:\n","    print(suggestion)\n","\n","\n","\n","import pkg_resources\n","from symspellpy.symspellpy import SymSpell\n","\n","# Set max_dictionary_edit_distance to avoid spelling correction\n","sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n","dictionary_path = pkg_resources.resource_filename(\n","    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n","# term_index is the column of the term and count_index is the\n","# column of the term frequency\n","sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n","\n","# a sentence without any spaces\n","input_term = \"thequickbrownfoxjumpsoverthelazydog\"\n","input_term = \"summermorning\"\n","result = sym_spell.word_segmentation(input_term)\n","print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,\n","                          result.log_prob_sum))\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"wI5Tg1bFiERD"},"source":["### **(a) Load Corpus DataFrames**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zg4NGPlCGffp"},"outputs":[],"source":["!ls -altr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mL_i7AkGGjHB"},"outputs":[],"source":["title_str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxCBdYNTiDrC"},"outputs":[],"source":["# Read Preprocessed Corpus Sentences DataFrame\n","\n","# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","# title_str = ''.join(CORPUS_TITLE.split()).lower()\n","title_str = ''.join(CORPUS_FILENAME.split('.')[0]).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Sentence DataFrame\n","corpus_sents_filename = f'corpus_sents_baseline_{title_str}.csv'\n","print(f'Reading from file: {corpus_sents_filename}')\n","corpus_sents_df = pd.read_csv(corpus_sents_filename)\n","\n","# Paragraph DataFrame\n","corpus_parags_filename = f'corpus_parags_baseline_{title_str}.csv'\n","print(f'Reading from file: {corpus_parags_filename}')\n","corpus_parags_df = pd.read_csv(corpus_parags_filename)\n","\n","# Section DataFrame\n","corpus_sects_filename = f'corpus_sects_baseline_{title_str}.csv'\n","print(f'Reading from file: {corpus_sects_filename}')\n","corpus_sects_df = pd.read_csv(corpus_sects_filename)\n","\n","# Chapter DataFrame\n","corpus_chaps_filename = f'corpus_chaps_baseline_{title_str}.csv'\n","print(f'Reading from file: {corpus_chaps_filename}')\n","corpus_chaps_df = pd.read_csv(corpus_chaps_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqsUHiF9K_5L"},"outputs":[],"source":["# Verify Sentences\n","\n","corpus_sents_df.head(2)\n","corpus_sents_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3fD5CpfKJnw"},"outputs":[],"source":["# corpus_sents_df = corpus_sents_df.loc[:, ~corpus_sents_df.columns.str.contains('^Unnamed')]\n","\n","corpus_sents_df.drop(columns=['Unnamed: 0'], inplace=True)\n","corpus_sents_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sl3hwS0wLEeR"},"outputs":[],"source":["# Verify Paragraphs\n","\n","corpus_parags_df.head(2)\n","corpus_parags_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzP_CrO3J0gd"},"outputs":[],"source":["# corpus_parags_df = corpus_parags_df.loc[:, ~corpus_parags_df.columns.str.contains('^Unnamed')]\n","\n","corpus_parags_df.drop(columns=['Unnamed: 0'], inplace=True)\n","corpus_parags_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZR54xIPJvm-"},"outputs":[],"source":["# Verify Sections\n","\n","# corpus_sects_df.head(2)\n","corpus_sects_df.info(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EmlXbKnbL1N7"},"outputs":[],"source":["corpus_sects_df.rename(columns={'Unnamed: 0':'sect_no','chap_raw':'sect_raw','chap_clean':'sect_clean'}, inplace=True)\n","# corpus_sects_df = corpus_sects_df.loc[:, ~corpus_sects_df.columns.str.contains('^Unnamed')]\n","# corpus_sects_df.drop(columns=['Unnamed: 0'], inplace=True)\n","corpus_sects_df = corpus_sects_df.loc[:, ~corpus_sects_df.columns.duplicated()]\n","# corpus_sects_df.drop(columns=['Unnamed: 0'], inplace=True)\n","\n","corpus_sects_df.info () # [: corpus_sects_df.columns.like('_no')]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lb2DYmbqMEF-"},"outputs":[],"source":["# Verfiy Chapters\n","\n","corpus_chaps_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmfsseAz2F8T"},"outputs":[],"source":["# corpus_chaps_df = corpus_chaps_df.loc[:, ~corpus_chaps_df.columns.str.contains('^Unnamed')]\n","\n","corpus_chaps_df.drop(columns=['Unnamed: 0'], inplace=True)\n","corpus_chaps_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9X5gfDoi_d0"},"outputs":[],"source":["# Verify all 4 semantic unit DataFrame shapes\n","\n","print(f'corpus_sents_df.shape: {corpus_sents_df.shape}')\n","print(f'corpus_parags_df.shape: {corpus_parags_df.shape}')\n","print(f'corpus_sects_df.shape: {corpus_sects_df.shape}')\n","print(f'corpus_chaps_df.shape: {corpus_chaps_df.shape}')\n","\n","\"\"\"\n","SButler Odyssey\n","\n","corpus_sents_df.shape: (2445, 8)\n","corpus_parags_df.shape: (1051, 8)\n","corpus_sects_df.shape: (24, 8)\n","corpus_chaps_df.shape: (24, 8)\n","\"\"\";\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZizvsnBrhAvh"},"outputs":[],"source":["corpus_sents_df.iloc[1618]"]},{"cell_type":"markdown","metadata":{"id":"jQYjye-v9XMY"},"source":["### **(b) Create Corpus DataFrames**"]},{"cell_type":"markdown","metadata":{"id":"E6qZH_qAVPQD"},"source":["#### **Try to Automatically Detected File/Text Encoding**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vtWVf1cJUeSW"},"outputs":[],"source":["!pwd\n","!ls -altr *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JW3HwuvBv1un"},"outputs":[],"source":["# files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k4TH13cFUErD"},"outputs":[],"source":["# Try to automatically discover Corpus text Encoding scheme (default to 'utf-8', but often 'iso-8859-1', 'windows-1252', 'cp1252', or 'ascii')\n","\n","CORPUS_ENCODING = 'utf-8' # Python3 default encoding\n","\n","corpus_str, corpus_encode, encoding_confidence = get_file_encoding(CORPUS_FILENAME)\n","CORPUS_ENCODING = str(corpus_encode).lower()\n","\n","if encoding_confidence > 0.8:\n","  print(f'Setting file/text encoding to {CORPUS_ENCODING}\\n')\n","  print(f\"    {encoding_confidence*100:.2f}% confidence Encoding = '{CORPUS_ENCODING}' for '{CORPUS_FILENAME}'\")\n","else:\n","  print(f\"WARNING: Less than 80% confidence estimating Encoding scheme for '{CORPUS_FILENAME}'\\n\")\n","  print(f\"         Only {encoding_confidence*100:.2f}% confidence Encoding = '{CORPUS_ENCODING}'\")\n","  print(f\"         Manually verify corpus file '{CORPUS_FILENAME}' encoding, set as GLOBAL_CONSTATANT and rerun\")"]},{"cell_type":"markdown","metadata":{"id":"XMkihxoY6T6U"},"source":["#### **Create Chapter and Section DataFrames**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaAZSbN8t537"},"outputs":[],"source":["corpus_filename"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ydj8ITnTE1D"},"outputs":[],"source":["!head -n 10 $corpus_filename"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qULnmeVG0Le"},"outputs":[],"source":["corpus_chaps_filtered_ls, corpus_chaps_clean_ls, corpus_sects_filtered_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str = corpus2chapsect(corpus_filename, corpus_type='file')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rz0uqmK8G1lj"},"outputs":[],"source":["print(f'Corpus Filtered:\\n    {corpus_chaps_filtered_ls[0][:500]}')\n","print('\\n')\n","print(f'Corpus Cleaned:\\n    {corpus_chaps_clean_ls[0][:500]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUQ5-o3HbrMq"},"outputs":[],"source":["# Parse out raw/clean Chapters/Sections from Corpus text file\n","\n","corpus_chaps_filtered_ls, corpus_chaps_clean_ls, corpus_sects_filtered_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str = corpus2chapsect(corpus_filename, corpus_type='file')\n","\n","# Remove problematic leading ROMAN numerials if they exist\n","corpus_chaps_filtered_ls = [del_leadroman(x) for x in corpus_chaps_filtered_ls]\n","corpus_chaps_clean_ls =  [del_leadroman(x) for x in corpus_chaps_clean_ls]\n","corpus_sects_filtered_ls = [del_leadroman(x) for x in corpus_sects_filtered_ls]\n","corpus_sects_clean_ls = [del_leadroman(x) for x in corpus_sects_clean_ls]\n","\n","corpus_raw_str = del_leadroman(corpus_raw_str)\n","\n","len(corpus_chaps_filtered_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhTAWxFUrg6V"},"outputs":[],"source":["corpus_chaps_filtered_ls[0][:500]\n","print('\\n')\n","corpus_chaps_clean_ls[0][:500]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ilQv-883wl6_"},"outputs":[],"source":["# Test\n","\n","# test_ls = [x for x in corpus_chaps_filtered_ls if 'I have no' in x]\n","# test_ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jcx-zqsXSKSE"},"outputs":[],"source":["# Verify Chapter/Section count and sample\n","\n","if type(corpus_chaps_filtered_ls[0]) is not str:\n","\n","  print(f'\\nERROR: Could not parse Corpus file into Chapters and Sections correctly\\n       Edit Corpus file and re-run')\n","\n","else:\n","\n","  print(f'\\n{len(corpus_chaps_filtered_ls)} Chapters found in this Corpus')\n","\n","  print(f'\\n{len(corpus_sects_filtered_ls)} Sections found in this Corpus')\n","\n","\n","  print(f'\\n\\n----------{len(corpus_chaps_filtered_ls)} CHAPTERS ----------')\n","  chap_sample_no = 0\n","  print(f'First 500 character sample from Chapter #{chap_sample_no}\\n\\n     {corpus_chaps_filtered_ls[chap_sample_no][:500]}')\n","\n","  print(f'\\n\\n----------{len(corpus_sects_filtered_ls)} SECTIONS ----------')\n","  sect_sample_no = 0\n","  print(f'First 500 character sample from Section #{sect_sample_no}\\n\\n    {corpus_sects_filtered_ls[sect_sample_no][:500]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"roEQ9qkWbLew"},"outputs":[],"source":["!lsb_release -a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CyY6gwR4Goy6"},"outputs":[],"source":["# Verify shortest Chapter\n","\n","min(corpus_chaps_filtered_ls, key=len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPgOUT6-Urs2"},"outputs":[],"source":["len(corpus_chaps_filtered_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uhKXKE-vTzSD"},"outputs":[],"source":["# BUGFIX: Second Pass CHAPTER split on Chapter text segments\n","#         Fix Chapters thats re.split() couldn't split apart with manual second pass\n","#\n","#  re.split does not work for 4 of 55 Chapters\n","#     on Henry James, Portrait of a Lady, Cannot re.split(rf'{pattern_chap}',..)\n","#     CHAPTER [V|X|XVIII|L]\n","#  Attempts: cut-paste identical working CHAPTER text, encoding/ignore, RegEx minimization, etc\n","# \n","#  Colab Pro: No GPU/TPU, High-RAM\n","#     Python: 3.7.11\n","#    Unbuntu: 18.04.5 LTS (bionic) (!lsb_release -a)\n","#\n","# WORKAROUND: Have to use <string>.split('CHAPTER ') method and clean up variable parts after CHAPTER (e.g. V, X, XVIII, L)\n","\n","\n","corpus_chaps_exp_ls = []\n","for i, achap in enumerate(corpus_chaps_filtered_ls):\n","  print(f'Chapter #{i}: Length: {len(achap)}\\n\\n')\n","  if len(achap) > 50:\n","    print(f'     {achap[:50]}\\n\\n')\n","  if 'CHAPTER' in achap:\n","    print(f'\\n\\n\\n============================\\n')\n","    print(f'      FOUND: Chapter with remaining/embedded CHAPTER Heading\\n')\n","\n","    temp_chap_ls = achap.split('CHAPTER ')\n","    print(f'      string.split(CHAPTER)')\n","\n","    # temp_chap_ls = re.split(r'^CHAPTER ', achap) #  [IVXL]{1,10}[\\s]*', achap)\n","    # print(f'    RegEx = {pattern_chap}')\n","\n","    # temp_chap_ls = re.split(rf'{pattern_chap}', achap)\n","    # print(f'    RegEx = [^CHAPTER [IVXL]{1,10}[\\s]*]')\n","\n","    print(f'      Split into {len(temp_chap_ls)}')\n","    print('\\n=============================\\n\\n\\n')\n","    # temp_chap_ls = [re.sub(r'[IVLX]{1,10}[\\s]{1,}','',x) for x in temp_chap_ls]\n","    # temp_chap_ls = [re.sub(r'CHAPTER ','',x) for x in temp_chap_ls]\n","    temp_chap_ls = [re.sub(rf'{pattern_chap}','',x) for x in temp_chap_ls]\n","    temp_chap_ls = [x.strip() for x in temp_chap_ls]\n","    temp_chap_ls = [del_leadroman(x).strip() for x in temp_chap_ls] \n","    corpus_chaps_exp_ls.extend(temp_chap_ls)\n","  else:\n","    corpus_chaps_exp_ls.append(achap)\n","\n","  corpus_chaps_exp_ls = [x for x in corpus_chaps_exp_ls if len(x) > MIN_CHAP_LEN]\n","\n","print(f'         Old Chapter List had {len(corpus_chaps_filtered_ls)} Chapters')\n","print(f'New Expanded Chapter List has {len(corpus_chaps_exp_ls)} Chapters')\n","\n","# Copy expanded Chapter structure to base/reference list\n","corpus_chaps_filtered_ls = corpus_chaps_exp_ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iw_-X2MftagF"},"outputs":[],"source":["pattern_chap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PEtHYm_Ytksg"},"outputs":[],"source":["corpus_chaps_exp_ls[0][:50]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKn3hT_axHdA"},"outputs":[],"source":["# Test\n","\n","# test_ls = [x for x in corpus_chaps_filtered_ls if 'I have no' in x]\n","# test_ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aG9tnKz_dEVI"},"outputs":[],"source":["corpus_chaps_exp_ls[0][:500]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4pwfl-0axB1X"},"outputs":[],"source":["\"\"\"\n","\n","# Parse out raw/clean Chapters/Sections from Corpus text file\n","\n","corpus_chaps_filtered_ls, corpus_chaps_clean_ls, corpus_sects_filtered_ls, corpus_sects_clean_ls, sect_chapno_ls, corpus_raw_str = corpus2chapsect(corpus_filename)\n","\n","# Verify Paragraph count and sample\n","\n","if type(corpus_chaps_raw_ls[0]) is not str:\n","\n","  print(f'\\nERROR: Could not parse Corpus file into Chapters and Sections correctly\\n       Edit Corpus file and re-run')\n","\n","else:\n","\n","  print(f'\\n{len(corpus_chaps_raw_ls)} Chapters found in this Corpus')\n","\n","  print(f'\\n{len(corpus_sects_raw_ls)} Sections found in this Corpus')\n","\n","\n","  print(f'\\n\\n----------{len(corpus_chaps_filtered_ls)} CHAPTERS ----------')\n","  chap_sample_no = 0\n","  print(f'First 500 character sample from Chapter #{chap_sample_no}\\n\\n     {corpus_chaps_filtered_ls[chap_sample_no][:500]}')\n","\n","  print(f'\\n\\n----------{len(corpus_sects_filtered_ls)} SECTIONS ----------')\n","  sect_sample_no = 0\n","  print(f'First 500 character sample from Section #{sect_sample_no}\\n\\n    {corpus_sects_filtered_ls[sect_sample_no][:500]}')\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"19iueVUIg4Sh"},"outputs":[],"source":["# Check for extraneous CHAPTER or SECTION headers in either Chapter or Section lists\n","\n","print(f'\\n\\nChecking {len(corpus_chaps_filtered_ls)} Chapters for extraneous CHAPTER and SECTION headers')\n","\n","chaps_wchapheads_ls = [x for x in corpus_chaps_filtered_ls if 'CHAPTER ' in x]\n","print(f'{len(chaps_wchapheads_ls)} Sections may still have a CHAPTER heading')\n","\n","chaps_wsectheads_ls = [x for x in corpus_chaps_filtered_ls if 'SECTION ' in x]\n","print(f'{len(chaps_wsectheads_ls)} Sections may still have a SECTION heading')\n","\n","\n","print(f'\\n\\n Checking {len(corpus_sects_filtered_ls)} Sections for extraneous CHAPTER and SECTION headers')\n","\n","sects_wchapheads_ls = [x for x in corpus_sects_filtered_ls if 'CHAPTER ' in x]\n","print(f'{len(sects_wchapheads_ls)} Sections may still have a CHAPTER heading')\n","\n","sects_wsectheads_ls = [x for x in corpus_sects_filtered_ls if 'SECTION ' in x]\n","print(f'{len(sects_wsectheads_ls)} Sections may still have a SECTION heading')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-p7xiakRvHgq"},"outputs":[],"source":["# Verify Chapters and manually delete extraneous CHAPTER or SECTION Headers\n","\n","# TODO: Need to check for all 4 combinations of Chapter/Section existing/missing in Corpus\n","\n","corpus_chap_ct = len(corpus_chaps_filtered_ls)\n","corpus_chaps_clean_ls = []\n","\n","# corpus_chaps_filtered_ls[0]\n","\n","if corpus_chap_ct > 1:\n","  \n","  # Remove the extraneous Chapter by Hand if necessary\n","  corpus_chaps_filtered_ls = [x for x in corpus_chaps_filtered_ls if len(x.strip()) > MIN_CHAP_LEN]\n","\n","  # Clean the text\n","  corpus_chaps_clean_ls = [clean_text(x) for x in corpus_chaps_filtered_ls]\n","\n","else:\n","\n","  print(f'WARNING: Corpus contains no CHAPTER structure')\n","\n","  if len(corpus_sects_filtered_ls) > 1:\n","    # If Sections exists, copy Section structure to Chapter structure\n","    print('         Copying SECTION structure to CHAPTER\\n\\n')\n","    corpus_chaps_filtered_ls = corpus_sects_filtered_ls\n","    corpus_chaps_clean_ls = corpus_sects_clean_ls\n","    chapno_ls = list(range(len(corpus_chaps_filtered_ls)))\n","\n","  else:\n","    # No Chapter nor Section structure, just plain text in Corpus without any headings\n","    print('         No CHAPTER nor SECTION structure in this Corpus\\n\\n')\n","    chapno_ls = [1]\n","\n","\n","if len(corpus_chaps_filtered_ls) > 1:\n","\n","  # Verify\n","  print(f'First Section of {len(corpus_chaps_filtered_ls)}:\\n\\n  Beginning: {corpus_chaps_filtered_ls[0][:50]}\\n\\n  Ending: {corpus_chaps_filtered_ls[0][-50:]}\\n\\n')\n","\n","  print('----------')\n","\n","  print(f'{corpus_chap_ct} Chapters found in this Corpus:\\n\\n')\n","\n","  print(f'First Section of {len(corpus_chaps_filtered_ls)}:\\n\\n  Beginning: {corpus_chaps_filtered_ls[0][:50]}\\n\\n  Ending: {corpus_chaps_filtered_ls[0][-50:]}\\n\\n')\n","\n","  print(f'Second Section of {len(corpus_chaps_filtered_ls)}:\\n\\n  Beginning: {corpus_chaps_filtered_ls[1][:50]}\\n\\n  Ending: {corpus_chaps_filtered_ls[1][-50:]}\\n\\n')\n","\n","  print(f'Second Last Section of {len(corpus_chaps_filtered_ls)}:\\n\\n  Beginning: {corpus_chaps_filtered_ls[-2][:50]}\\n\\n  Ending: {corpus_chaps_filtered_ls[-2][-50:]}\\n\\n')\n","\n","  print(f'Last Section of {len(corpus_chaps_filtered_ls)}:\\n\\n  Beginning: {corpus_chaps_filtered_ls[-1][:50]}\\n\\n  Ending: {corpus_chaps_filtered_ls[-1][-50:]}\\n\\n')\n","\n","  print('----------')\n","\n","else:\n","\n","  print('\\n\\nCorpus has no CHAPTER nor SECTION headers:')\n","  print('  handle as one block to text above Paragraph/Sentence structural level')\n","\n","\n","\"\"\"\n","import re\n","\n","mylist = [\"dog\", \"cat\", \"wildcat\", \"thundercat\", \"cow\", \"hooo\"]\n","r = re.compile(\"CHAPTER [IVXL]{1,10}\")\n","newlist = list(filter(r.match, mylist)) # Read Note below\n","# print(newlist)\n","\"\"\";\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LScTOSIVRE95"},"outputs":[],"source":["# Verify Sections and manually delete extraneous CHAPTER or SECTION Headers\n","\n","corpus_sect_ct = len(corpus_sects_filtered_ls)\n","corpus_sects_clean_ls = []\n","\n","if corpus_sect_ct > 1:\n","\n","  # Remove the extraneous Section by Hand if necessary\n","  corpus_sects_filtered_ls = [x for x in corpus_sects_filtered_ls if not x.isupper()]\n","\n","  # Clean the text\n","  corpus_sects_clean_ls = [clean_text(x) for x in corpus_sects_filtered_ls]\n","\n","else:\n","\n","  print(f'WARNING: Corpus contains no SECTION structure')\n","\n","  if len(corpus_chaps_filtered_ls) > 1:\n","    print('         Copying CHAPTER structure to SECTION\\n\\n')\n","    # If Chapters exists, copy Chapter structure to Section structure\n","    corpus_sects_filtered_ls = corpus_chaps_filtered_ls\n","    corpus_sects_clean_ls = corpus_chaps_clean_ls\n","    sect_chapno_ls = list(range(len(corpus_sects_filtered_ls)))\n","\n","  else:\n","    # No Chapter nor Section structure, just plain text in Corpus without any headings\n","    print('         No CHAPTER nor SECTION structure in this Corpus\\n\\n')\n","    sect_chapno_ls = [1]\n","\n","\n","if len(corpus_sects_filtered_ls) > 1:\n","\n","  # Verify\n","  print(f'First Section of {len(corpus_sects_filtered_ls)}:\\n\\n  Beginning: {corpus_sects_filtered_ls[0][:50]}\\n\\n  Ending: {corpus_sects_filtered_ls[0][-50:]}\\n\\n')\n","\n","  print('----------')  \n","\n","  print(f'{corpus_sect_ct} Chapters found in this Corpus:\\n\\n')\n","\n","  print(f'First Section of {len(corpus_sects_filtered_ls)}:\\n\\n  Beginning: {corpus_sects_filtered_ls[0][:50]}\\n\\n  Ending: {corpus_sects_filtered_ls[0][-50:]}\\n\\n')\n","\n","  print(f'Second Section of {len(corpus_sects_filtered_ls)}:\\n\\n  Beginning: {corpus_sects_filtered_ls[1][:50]}\\n\\n  Ending: {corpus_sects_filtered_ls[1][-50:]}\\n\\n')\n","\n","  print(f'Second Last Section of {len(corpus_sects_filtered_ls)}:\\n\\n  Beginning: {corpus_sects_filtered_ls[-2][:50]}\\n\\n  Ending: {corpus_sects_filtered_ls[-2][-50:]}\\n\\n')\n","\n","  print(f'Last Section of {len(corpus_sects_filtered_ls)}:\\n\\n  Beginning: {corpus_sects_filtered_ls[-1][:50]}\\n\\n  Ending: {corpus_sects_filtered_ls[-1][-50:]}\\n\\n')\n","\n","  print('----------')\n","\n","else:\n","\n","  print('\\n\\nCorpus has no CHAPTER nor SECTION headers:')\n","  print('  handle as one block to text above Paragraph/Sentence structural level')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJgZpAJtxYJO"},"outputs":[],"source":["# Test\n","\n","test_ls = [x for x in corpus_chaps_filtered_ls if 'I have no' in x]\n","len(test_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ovxAu0w1cRou"},"outputs":[],"source":["# TODO: Temp fix\n","\n","corpus_chaps_clean_ls = [clean_text(x) for x in corpus_chaps_filtered_ls]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohieoJ_ehb2e"},"outputs":[],"source":["# Verify that Chapter DataFrame inputs are all same length\n","\n","print(f'chap_no: {list(range(len(corpus_chaps_filtered_ls)))}')\n","print(f'chap_raw: {len(corpus_chaps_filtered_ls)}')\n","print(f'chap_clean: {len(corpus_chaps_clean_ls)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dvljymc6cDee"},"outputs":[],"source":["# corpus_chaps_filtered_ls[4][:50]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Coq6ZS6aMkvd"},"outputs":[],"source":["# Create Chapter DataFrame\n","\n","chap_no_ls = list(range(len(corpus_chaps_filtered_ls)))\n","corpus_chaps_df = pd.DataFrame({'chap_no':chap_no_ls, 'chap_raw':corpus_chaps_filtered_ls, 'chap_clean':corpus_chaps_clean_ls})\n","corpus_chaps_df['chap_raw'] = corpus_chaps_df['chap_raw'].astype('string')\n","corpus_chaps_df['chap_clean'] = corpus_chaps_df['chap_clean'].astype('string')\n","# corpus_chaps_df.head(1)\n","corpus_chaps_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EdKAEsWt-y_w"},"outputs":[],"source":["# Calculate length statistics for Chapters\n","\n","corpus_chaps_df['char_len'] = corpus_chaps_df['chap_raw'].apply(lambda x : len(x))\n","corpus_chaps_df['token_len'] = corpus_chaps_df['chap_raw'].apply(lambda x : len(x.split()))\n","# corpus_chaps_df.head()\n","corpus_chaps_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTDdHkUa45rB"},"outputs":[],"source":["# Correct if lengths don't match due to Section != Chapter structures\n","\n","if corpus_chaps_df.shape[0] < corpus_sects_df.shape[0]:\n","  sect_chapno_ls = [0]*corpus_sects_df.shape[0]\n","\n","len(sect_chapno_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKmMapiBuzKN"},"outputs":[],"source":["corpus_raw_str[:500]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYG8knXZtpHm"},"outputs":[],"source":["# TODO: Temp fix\n","\n","\"\"\"\n","corpus_tokens_ls = corpus_raw_str.split()\n","sect_chapno_ls = []\n","chap_ptr = 0\n","sect_ptr = 0\n","len(corpus_tokens_ls)\n","for i, atoken in enumerate(corpus_tokens_ls):\n","  if atoken == 'CHAPTER':\n","    chap_ptr += 1\n","  if atoken == 'SECTION':\n","    sect_ptr += 1\n","    sect_chapno_ls.append(chap_ptr)\n","\n","print(f'Section ChapNo Length: {len(sect_chapno_ls)}\\n    {sect_chapno_ls}')\n","\n","# If OK\n","\n","# sect_chapno_ls = sect_chapno_ls\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xK7mAwMVirjE"},"outputs":[],"source":["len(sect_chapno_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJRG7cXI5T10"},"outputs":[],"source":["\"\"\"\n","\n","len(sect_chapno_ls)\n","print('\\n')\n","len(sect_no_ls)\n","print('\\n')\n","len(corpus_sects_filtered_ls)\n","print('\\n')\n","\n","# TODO: Fix earlier\n","corpus_sects_clean_ls = [clean_text(x) for x in corpus_sects_filtered_ls]\n","\n","len(corpus_sects_clean_ls)\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvUvQ1WMjEqd"},"outputs":[],"source":["# TODO: To fix\n","\n","sect_no_ls = sect_no_ls = chap_no_ls\n","\n","sect_chapno_ls = list(range(len(sect_no_ls)))\n","\n","# corpus_sects_clean_ls = [clean_text(x) for x in corpus_sects_filtered_ls]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBCM9oqUTmU3"},"outputs":[],"source":["# corpus_sects_clean_ls = [clean_text(x) for x in corpus_sects_filtered_ls]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNYDPkO7izxE"},"outputs":[],"source":["# Verify that Section DataFrame inputs are all same length\n","\n","print(f'sect_no: {len(sect_no_ls)}')\n","print(f'sect_chapno_ls: {len(sect_chapno_ls)}')\n","print(f'sect_raw: {len(corpus_sects_filtered_ls)}')\n","print(f'sect_clean: {len(corpus_sects_clean_ls)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiBMyYDD88z5"},"outputs":[],"source":["# Create Section DataFrame\n","\n","sect_no_ls = list(range(len(corpus_sects_filtered_ls)))\n","corpus_sects_df = pd.DataFrame({'sect_no':sect_no_ls, 'chap_no':sect_chapno_ls, 'sect_raw':corpus_sects_filtered_ls, 'sect_clean':corpus_sects_clean_ls})\n","corpus_sects_df['sect_raw'] = corpus_sects_df['sect_raw'].astype('string')\n","corpus_sects_df['sect_clean'] = corpus_sects_df['sect_clean'].astype('string')\n","# corpus_sects_df.head(1)\n","corpus_sects_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SnBFWwX62RZ7"},"outputs":[],"source":["len(sect_no_ls)\n","print('\\n')\n","len(sect_chapno_ls)\n","print('\\n')\n","len(corpus_sects_filtered_ls)\n","print('\\n')\n","len(corpus_sects_clean_ls)\n","print('\\n')\n","\n","# TODO: Make more robust solution\n","sect_chapno_len = len(sect_no_ls)\n","print(f'sect_chapno_ls: {sect_chapno_len}')\n","if len(sect_chapno_ls) == 0:\n","  sect_chapno_ls = [0]*len(sect_no_ls)\n","\n","print('\\n')\n","len(sect_no_ls)\n","print('\\n')\n","len(sect_chapno_ls)\n","print('\\n')\n","len(corpus_sects_filtered_ls)\n","print('\\n')\n","len(corpus_sects_clean_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9jpNBIGmyBT"},"outputs":[],"source":["# Calculate length statistics for Chapters\n","\n","corpus_sects_df['char_len'] = corpus_sects_df['sect_raw'].apply(lambda x : len(x))\n","corpus_sects_df['token_len'] = corpus_sects_df['sect_raw'].apply(lambda x : len(x.split()))\n","# corpus_sects_df.head()\n","corpus_sects_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mpNdgPL_4rbQ"},"outputs":[],"source":["corpus_sects_df.iloc[0]"]},{"cell_type":"markdown","metadata":{"id":"sFj2CQPD6l01"},"source":["#### **Create Paragraph and Sentence DataFrames**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"td-w3TyEmSn1"},"outputs":[],"source":["corpus_chaps_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ui_fYtR4G34A"},"outputs":[],"source":["# The spacy/nlp/pipe need to be recreated each time a large Corpus is parsed below\n","\n","# import pysbd\n","# import spacy\n","# from pysbd.utils import PySBDFactory\n","\n","\"\"\"\n","if CORPUS_LANGUAGE == 'english':\n","  nlp = spacy.blank('en')\n","elif CORPUS_LANGUAGE == 'french':\n","  nlp = spacy.blank('fr')\n","else:\n","  print(f'ERROR: CORPUS_LANGUAGE must be [english|french] but was set to: {CORPUS_LANGUAGE}')\n","\n","# explicitly adding component to pipeline\n","# (recommended - makes it more readable to tell what's going on)\n","nlp.add_pipe(PySBDFactory(nlp))\n","\"\"\";\n","\n","# Test\n","\n","test_en_str = \"My name is Jonas E. Smith. Please turn to p. 55.\"\n","\n","test_fr_str = \"Le pépiement matinal des oiseaux semblait insipide à Françoise. Chaque parole des «bonnes» la faisait sursauter; incommodée par tous leurs pas, elle s'interrogeait sur eux; 'est que nous avions déménagé. Certes les domestiques ne remuaient pas moins, dans le «sixième» de notre ancienne demeure; mais elle les connaissait; elle avait fait de leurs allées et venues des choses amicales. Maintenant elle portait au silence même une attention douloureuse.\"\n","\n","# doc = nlp(test_fr_str)\n","# print(list(doc.sents))\n","# [My name is Jonas E. Smith., Please turn to p. 55.]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P34vN8P-9TiV"},"outputs":[],"source":["corpus_sects_df.iloc[0]['sect_raw'].split('\\n')[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"weu1ZRsC6XCP"},"outputs":[],"source":["corpus_sectno = -1\n","\n","corpus_paragno = -1\n","corpus_parags_ls = []\n","\n","corpus_sentno = -1\n","corpus_sents_ls = []\n","\n","sects_filtered_ls = list(corpus_sects_df.sect_raw)\n","\n","# For every Section in the Corpus\n","for asectno, asect_raw in enumerate(sects_filtered_ls):\n","  # print(f'Section #{asectno}')\n","  corpus_sectno += 1\n","\n","  # Split into a list of Paragraphs\n","  sect_parags_filtered_ls, sect_clean_str = sect2parags(asect_raw)\n","  # For every Paragraph in Section \n","  for aparagno, aparag_raw in enumerate(sect_parags_filtered_ls):\n","    print(f'  Paragraph #{aparagno}')\n","    print(f'Split Section #{asectno} in {len(sect_parags_filtered_ls)} Paragraphs')\n","    corpus_paragno += 1\n","\n","    # Split into list of Sentences\n","    print(f'\\nPassing Paragraph:\\n\\n    {aparag_raw} to parag2sents')\n","    parag_sents_filtered_ls, parag_clean_str = parag2sents(aparag_raw)\n","    print(f'Split Paragraph into {len(parag_sents_filtered_ls)} Sentences')\n","    aparag_clean = clean_text(aparag_raw)\n","    if any_alphachar(aparag_clean):\n","      # Only add Paragraphs that have at least one alpha char after clean_text() (e.g. skip Paragraph = '$1.00!!! ---' )\n","      corpus_parags_ls.append((corpus_paragno, corpus_sectno, aparag_raw, aparag_clean))\n","\n","    # For every Sentence in Paragraph\n","    for asentno, asent_raw in enumerate(parag_sents_filtered_ls):\n","      print(f'Section #{asectno}, Paragraph #{aparagno}, Section Sentence #{asentno} Corpus Sentence #{corpus_sentno}')\n","      corpus_sentno += 1\n","      asent_clean = clean_text(asent_raw)\n","      if any_alphachar(asent_clean):\n","        # Only add Sentences that have at least one alpha char after clean_text() (e.g. skip Sentence = '$1.00!!! ---' )\n","        corpus_sents_ls.append((corpus_sentno, corpus_paragno, corpus_sectno, asent_raw, asent_clean))\n","\n","    # if asectno > 2:\n","    #   break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5eu0IKxORTS"},"outputs":[],"source":["# Test (The Great Gatsby) ensure iloc[2844] raw_sent '$2.11' removed\n","\n","# corpus_sents_df.shape\n","# corpus_sents_df.iloc[2843]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKFvr35VvfFC"},"outputs":[],"source":["# Verfiy Section counts\n","\n","for i, asect in enumerate(sects_filtered_ls):\n","  print(f'Length of Section #{i}: {len(asect)}\\n')\n","\n","\"\"\"\n","Before FRENCH modifications:\n","\n","Length of Section #0: 409601\n","\n","Length of Section #1: 455968\n","\n","Length of Section #2: 485479\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zc5-8GDOd8rF"},"outputs":[],"source":["# Verify the first 500 chars of first few Sections\n","\n","char_ct = 500\n","\n","for i, asect in enumerate(sects_filtered_ls):\n","  print(f'Start of Section #{i} ------------------------------\\n\\n    {asect[:char_ct]}\\n')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhXKvuQPevmM"},"outputs":[],"source":["# Verify first raw Section text\n","\n","sects_filtered_ls[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4721-er6W96"},"outputs":[],"source":["# Verify Sentence and Paragraph Counts\n","\n","sent_ct = len(corpus_sents_ls)\n","print(f'{sent_ct} Sentences were found in the Corpus')\n","\n","parag_ct = len(corpus_parags_ls)\n","print(f'{parag_ct} Paragraphs were found in the Corpus')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0gltS-r0QPv"},"outputs":[],"source":["print(corpus_parags_ls[0][:50])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zm9fDtVJziE3"},"outputs":[],"source":["# Create Paragraph DataFrame\n","\"\"\"\n","parag_no_ls = list(range(len(corpus_parags_ls)))\n","corpus_parags_df = pd.DataFrame({'sect_no':sect_no_ls, 'chap_no':sect_chapno_ls, 'sect_raw':corpus_sects_raw_ls, 'sect_clean':corpus_sects_clean_ls})\n","corpus_parags_df['sect_raw'] = corpus_parags_df['sect_raw'].astype('string')\n","corpus_parags_df['sect_clean'] = corpus_parags_df['sect_clean'].astype('string')\n","corpus_parags_df.info()\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJ7Xdmyt0Asb"},"outputs":[],"source":["# Create Paragraph DataFrame\n","\n","corpus_parags_df = pd.DataFrame(corpus_parags_ls,columns=['parag_no','sect_no','parag_raw','parag_clean'])\n","corpus_parags_df['parag_raw'] = corpus_parags_df['parag_raw'].astype('string')\n","corpus_parags_df['parag_clean'] = corpus_parags_df['parag_clean'].astype('string')\n","corpus_parags_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Arm9ZlvSl-42"},"outputs":[],"source":["# Calculate length statistics for Paragraphs\n","\n","corpus_parags_df['char_len'] = corpus_parags_df['parag_raw'].apply(lambda x : len(x))\n","corpus_parags_df['token_len'] = corpus_parags_df['parag_raw'].apply(lambda x : len(x.split()))\n","corpus_parags_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqN4aFeT3RMq"},"outputs":[],"source":["# Verify Paragraph DataFrame start and end\n","\n","corpus_parags_df.head(5)\n","corpus_parags_df.tail(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-T27_raCzWk6"},"outputs":[],"source":["# Create Sentences DataFrame\n","\n","corpus_sents_df = pd.DataFrame(corpus_sents_ls,columns=['sent_no', 'parag_no','sect_no','sent_raw','sent_clean'])\n","corpus_sents_df['sent_raw'] = corpus_sents_df['sent_raw'].astype('string')\n","corpus_sents_df['sent_clean'] = corpus_sents_df['sent_clean'].astype('string')\n","corpus_sents_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glceM5TylaA8"},"outputs":[],"source":["# Compute length statistics for Sentences\n","\n","corpus_sents_df['char_len'] = corpus_sents_df['sent_raw'].apply(lambda x : len(x))\n","corpus_sents_df['token_len'] = corpus_sents_df['sent_raw'].apply(lambda x : len(x.split()))\n","corpus_sents_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4WWrd1J2uz_"},"outputs":[],"source":["# Verify Sentence DataFrame start and end\n","\n","corpus_sents_df.head(20)\n","corpus_sents_df.tail(30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSzNKPacx9Nd"},"outputs":[],"source":["# Test\n","\n","search_str = 'I have no'\n","corpus_sents_df[corpus_sents_df['sent_raw'].str.find(search_str) != -1]"]},{"cell_type":"markdown","metadata":{"id":"kgJKnHHk4f3F"},"source":["#### **For each Section, insert [start|mid|end] Sentence numbers**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tnwy-rFW4swr"},"outputs":[],"source":["# Calculate the start, mid and end Sentence No for each Section\n","\n","sect_sent_no_start_ls = np.array(corpus_sents_df.groupby('sect_no')['sent_no'].min())\n","sect_sent_no_end_ls = np.array(corpus_sents_df.groupby('sect_no')['sent_no'].max())\n","\n","def my_mid(anum, bnum):\n","  mid_no = (anum - bnum)//2 + bnum\n","\n","  return mid_no\n","\n","print('\\nSection start sentence no: -----')\n","print(sect_sent_no_start_ls)\n","\n","sect_sent_no_mid_ls = list(map(my_mid, sect_sent_no_end_ls, sect_sent_no_start_ls))\n","print('\\nSection mid-sentence no: -----')\n","print(sect_sent_no_mid_ls)\n","\n","print('\\nSection end sentence no: -----')\n","print(sect_sent_no_end_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0MNNcN_d48xa"},"outputs":[],"source":["# Insert 3 new columns on start,mid and end Sentence No for each Section\n","\n","corpus_sects_df.insert(2, 'sent_no_start', sect_sent_no_start_ls)\n","corpus_sects_df.insert(3, 'sent_no_mid', sect_sent_no_mid_ls)\n","corpus_sects_df.insert(4, 'sent_no_end', sect_sent_no_end_ls)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuxMQpOY2uwF"},"outputs":[],"source":["# Verfiy Section \n","\n","corpus_sects_df.filter(like='_no')\n","corpus_sects_df.info()"]},{"cell_type":"markdown","metadata":{"id":"Z48WKV1W6KdA"},"source":["#### **For Each Chapter, Insert [start|mid|end] Sentence numbers**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77oGMj4v6pZZ"},"outputs":[],"source":["corpus_chaps_filtered_ls[0][:50]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGNlqV5G7Tk-"},"outputs":[],"source":["corpus_chaps_clean_ls[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70ntj-6X8PbU"},"outputs":[],"source":["corpus_chaps_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWFbBuEGQ4fH"},"outputs":[],"source":["search_str = 'goose'\n","\n","corpus_sents_df[corpus_sents_df['sent_clean'].str.contains(search_str)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWvddQwkCjKY"},"outputs":[],"source":["# corpus_sents_df.iloc[1056]['sent_clean']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5DylkEa8cCwG"},"outputs":[],"source":["corpus_chaps_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vrZVqCRVOwF"},"outputs":[],"source":["achap_sentstart_clean_ls = []\n","\n","for indx, achap_tup in corpus_chaps_df.iterrows():\n","  achap_no, achap_raw, achap_clean, achap_charlen, achap_tokenlen = achap_tup\n","  achap_sentstart_clean_ls.append(achap_clean[:100])\n","\n","print(achap_sentstart_clean_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2HcU3WLaHpV"},"outputs":[],"source":["!pip install fuzzywuzzy[speedup]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SU_11lWzaI-I"},"outputs":[],"source":["from fuzzywuzzy import fuzz\n","from fuzzywuzzy import process"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sbxpUygQaL3g"},"outputs":[],"source":[" fuzz.ratio(\"this is a test\", \"this is a test!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6B0n3hdbLyp"},"outputs":[],"source":["corpus_sects_df.info()\n","\n","type(corpus_sects_df['sect_clean'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCarKELJ-S2x"},"outputs":[],"source":["# Optional\n","\n","# corpus_chaps_df.drop(columns=['sect_no'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BDKC_kucCYih"},"outputs":[],"source":["corpus_chaps_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ytgSzv5DCjM"},"outputs":[],"source":["corpus_parags_df.iloc[:5]['parag_clean'][:50]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_oXrE-d0C3Ik"},"outputs":[],"source":["corpus_sects_df.iloc[:5]['sect_clean'][:50]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gaoY4ACmDdEN"},"outputs":[],"source":["corpus_sents_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Qn8CySv9byQ"},"outputs":[],"source":["corpus_chaps_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSqva3DLwu4H"},"outputs":[],"source":["# Clean Chapters DataFrame\n","\n","corpus_chaps_df['chap_clean'] = corpus_chaps_df['chap_clean'].apply(lambda x: re.sub(r'SECTION [IVXL]{1,10}[.]','',x))\n","corpus_chaps_df['chap_clean'] = corpus_chaps_df['chap_clean'].apply(lambda x: clean_text(x).strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDOUcy8fyGBM"},"outputs":[],"source":["corpus_chaps_df.iloc[0]['chap_clean'][:500]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtGurlnd6-Qt"},"outputs":[],"source":["# Calculate the start, mid and end Sentence No for each Section\n","\n","min_charlen = 80  # How many characters to match at the start of start/end Sentences\n","\n","# If there is no Chapter structure\n","if corpus_chaps_df.shape[0] == 1:\n","  \n","  chaps_sentend = corpus_sents_df.shape[0] - 1\n","  chaps_sentmid = chaps_sentend//2\n","\n","  chaps_sentstart_ls = [0]\n","  chaps_sentmid_ls = [chaps_sentmid]\n","  chaps_sentend_ls = [chaps_sentend]\n","\n","else:\n","\n","  # First, find which Sections align with each Chapter and save Section's sent_no_start as the same for Chapter\n","  chaps_sentstart_ls = []\n","  sects_sentstart_ls = list(corpus_sects_df['sect_clean'].apply(lambda x: x[:min_charlen].strip()))\n","  for achap_indx, achap_tup in corpus_chaps_df.iterrows():\n","    # TODO: Insert deal with 5 or 6 returned items in tuple depending on Corpus Chapter/Section structure\n","    # 6 args returned\n","    # achap_no, achap_sentno_start, achap_raw, achap_clean, achap_charlen, achap_tokenlen = achap_tup\n","    # 5 args returned\n","    achap_no, achap_raw, achap_clean, achap_charlen, achap_tokenlen = achap_tup\n","    achap_sentstart_str = achap_clean[:min_charlen].strip()\n","    achap_sentstart_str = ' '.join(achap_sentstart_str.split())\n","\n","    sent_startno_found = False\n","    # Loop over all the Sections to see if the starting text matches the Chapter starting text\n","    for asect_indx, asect_sentstart_str in enumerate(sects_sentstart_ls):\n","      asect_sentstart_str = ' '.join(asect_sentstart_str.split())\n","      achap_sentstart_len = len(achap_sentstart_str)\n","      asect_sentstart_len = len(asect_sentstart_str)\n","      if (achap_sentstart_len > asect_sentstart_len):\n","        len_diff = achap_sentstart_len - asect_sentstart_len\n","        achap_sentstart_str = achap_sentstart_str[:-len_diff]\n","      elif (achap_sentstart_len < asect_sentstart_len):\n","        len_diff = asect_sentstart_len - achap_sentstart_len\n","        asect_sentstart_str = asect_sentstart_str[:-len_diff]\n","      else:\n","        # Both Chapter and Section starting Sentence strings are equal length\n","        pass\n","\n","      print(f'Fuzzy Compare:\\n\\n    Chapter Start: {achap_sentstart_str}\\n    Section Start: {asect_sentstart_str}')\n","      if fuzz.ratio(achap_sentstart_str, asect_sentstart_str) > 97:\n","        sent_startno_ls = list(corpus_sects_df[corpus_sects_df['sect_no'] == asect_indx]['sent_no_start'])\n","        # print(f'MATCH!!! type(sent_startno): {sent_startno_ls[0]}')\n","        chaps_sentstart_ls.append(sent_startno_ls[0])\n","        sent_startno_found = True\n","        break\n","    # If no match found, enter ERROR code -1 in the Sentence start no for the current Chapter\n","    if sent_startno_found == False:\n","      chaps_sentstart_ls.append((achap_indx, -1))\n","\n","  print(f'\\n\\nchaps_sentstart_ls: {chaps_sentstart_ls}')\n","\n","\n","\n","  # Second, get the end Sentence for Each Chapter by rotating Sentence Start No left and pushing on the last Sentence No\n","  chaps_sentend_ls = []\n","  chaps_sentend_ls = [x-1 for x in chaps_sentstart_ls]\n","  corpus_sentlast = corpus_sents_df.shape[0] - 1\n","\n","  chaps_sentend_ls.pop(0)\n","  chaps_sentend_ls.append(corpus_sentlast)\n","\n","  print(f'\\n\\nchaps_sentend_ls: {chaps_sentend_ls}')\n","\n","\n","  # Third, calculate the Sentence No in the middle of each Chapter\n","\n","  chaps_sentmid_ls = [(((chaps_sentend_ls[i] - chaps_sentstart_ls[i])//2)+chaps_sentstart_ls[i]) for i in range(len(chaps_sentend_ls))]\n","\n","  print(f'\\n\\nchaps_sentmid_ls: {chaps_sentmid_ls}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpOzOahxhgvU"},"outputs":[],"source":["# Verify boundry cases\n","\n","corpus_sents_df.iloc[268]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iN_GXkBekH0P"},"outputs":[],"source":["# corpus_chaps_df.drop(columns=['sent_no_start','sent_no_mid','sent_no_end'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrfK9MVAEuYp"},"outputs":[],"source":["corpus_chaps_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZTduIsl6KdB"},"outputs":[],"source":["# Insert 3 new columns on start,mid and end Sentence No for each Chapter\n","\n","corpus_chaps_df.insert(1, 'sent_no_start', chaps_sentstart_ls)\n","corpus_chaps_df.insert(2, 'sent_no_mid', chaps_sentmid_ls)\n","corpus_chaps_df.insert(3, 'sent_no_end', chaps_sentend_ls)\n","corpus_chaps_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTW3Gxmf6KdC"},"outputs":[],"source":["# Verfiy Chapter \n","\n","corpus_chaps_df.filter(like='_no')\n","corpus_chaps_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDSyIfqDQCMI"},"outputs":[],"source":["# Verify\n","\n","print(f'Corpus Sentence DataFrame shape: {corpus_sents_df.shape}')\n","print(f'Corpus Paragraph DataFrame shape: {corpus_parags_df.shape}')\n","print(f'Corpus Section DataFrame shape: {corpus_sects_df.shape}')\n","print(f'Corpus Chapter DataFrame shape: {corpus_chaps_df.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjfAWFJ1xl8d"},"outputs":[],"source":["corpus_sects_df.iloc[0]"]},{"cell_type":"markdown","metadata":{"id":"iEtOOm2UQd1o"},"source":["#### **Save Corpus DataFrames**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"39_ehy97zB1Z"},"outputs":[],"source":["# Test\n","\n","search_str = 'I have no'\n","corpus_sents_df[corpus_sents_df['sent_raw'].str.find(search_str) != -1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Hf92HlV9--1"},"outputs":[],"source":["%whos DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGkvSy4HQdCe"},"outputs":[],"source":["# Save Corpus DataFrames\n","\n","save_dataframes(df_ls=['baseline'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDEqASdRUfVF"},"outputs":[],"source":["corpus_sents_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"zK27hN0KkvBI"},"source":["### **END HERE**"]},{"cell_type":"markdown","metadata":{"id":"PVCkjat0vffd"},"source":["#### **Get All (non-null) Raw Lines**\n","\n","* NOTE: Corpus textfile needs to be Preprocessed before running this\n","- Remove all Curve Parenthesis that span multiple sentences or paragraphs\n","- Remove all Square Parenthesis\n","- Filter out non-printing characters if they exist (or use proper encoding)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgqqpwagWPS7"},"outputs":[],"source":["len(corpus_lines_ls)\n","print('\\n')\n","print(corpus_lines_ls[11])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBl_hrhPZo29"},"outputs":[],"source":["min(corpus_lines_ls, key=lambda word: len(word))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lthUz3kpZ8_K"},"outputs":[],"source":["corpus_lines_ls.sort(key=lambda s: len(s))\n","corpus_lines_ls[:100]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"an7SzlAuvXA0"},"outputs":[],"source":["# NOTE: ~3-15 minutes (one pass with PySBD)\n","#             minutes (two passes with PySBD+NLTK)\n","\n","# Time consuming so only set pysbd_only=True (second NLTK sentence tokenizer pass) \n","#   if necessary (e.g. Samuel Butler's 1900 trans. of Homer's Odyssey)\n","#                 PySBD: 1075 lines, PySBD+NLTK: 3905 lines, NLTK: 3109 lines\n","#                        Why? a 3 Sentence Paragraph enclosed in double quotes is counted as one Sentence by PySBD\n","#           w/spec char strip: PySBD+NLTK: 3926\n","#           w/o digits/footnotes: PySBD+NLTK: 3925\n","\n","corpus_lines_ls, lines_raw_str = corpus2lines(corpus_filename, pysbd_only=False)\n","\n","# Verify\n","print(f'\\n\\nRead raw corpus lines: character count: {len(lines_raw_str)}')\n","print(f'                        raw line count:  {len(corpus_lines_ls)}\\n\\n')\n","\n","line_ct = 10\n","print(f'First {line_ct} raw lines: --------------------\\n')\n","for i,aline in enumerate(corpus_lines_ls[:line_ct]):\n","  print(f'Line #{i}:\\n    {aline}')\n","print(f'\\n\\nLast {line_ct} raw lines: -------------------\\n')\n","for i,aline in enumerate(corpus_lines_ls[-line_ct:]):\n","  print(f'Line #{i}: {aline}')\n","\n","\"\"\"\n","\n","BEFORE stripping out headings len: 610949 (605835 w/2nd pass)\n","Corpus Paragraph Raw Count: 1075\n","   Parag count before processing sents: 1075\n","About to return corpus_sents_raw_ls with len = 2469\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"_b6ORN6A08Dv"},"source":["#### **Create Sentence DataFrame: [corpus_sents_df]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BeGt1ot0_dK"},"outputs":[],"source":["# Filter out all the structural/metatag and blank/punctuation only lines\n","#    and save all semantically meaningful Sentences in corpus_sents_ls\n","\n","corpus_sents_ls = []\n","\n","for i, aline in enumerate(corpus_lines_ls):\n","\n","    # print(f'Examing line #{i}: {aline}')\n","\n","    aline_clean = aline.strip()\n","    # Skip/delete whitespace only sentences\n","    if len(aline_clean) == 0:\n","      continue\n","    \n","    # Skip/delete any sentences starting with CHAPTER RegEx Pattern\n","    if aline_clean.startswith('CHAPTER '):\n","      continue\n","    \n","    # Skip/delete any sentences starting with SECTION RegEx Pattern\n","    if aline_clean.startswith('SECTION '):\n","      continue\n","\n","    # Skip/delete any sentences starting with SECTION RegEx Pattern\n","    if aline_clean.startswith('BOOK '):\n","      continue\n","\n","    # Skip/delete any sentence no alpha/numeric charcters (e.g. only punctuation)\n","    if (re.match('^[^a-zA-Z]+$', aline_clean)):\n","      print(f'No alnum line #{i}: {aline}')\n","      continue\n","\n","    # If passed through all previous filters, save as genuine Sentence\n","    corpus_sents_ls.append(aline_clean)\n","\n","# Test\n","print(f'Raw Lines length: {len(corpus_lines_ls)}')\n","print(f' Clean Sentences: {len(corpus_sents_ls)}')\n","\n","line_ct = 10\n","print(f'First {line_ct} clean Sentences : --------------------\\n')\n","for i,aline in enumerate(corpus_sents_ls[:line_ct]):\n","  print(f'Line #{i}:\\n    {aline}')\n","print(f'\\n\\nLast {line_ct} clean Sentences: -------------------\\n')\n","for i,aline in enumerate(corpus_sents_ls[-line_ct:]):\n","  print(f'Line #{i}: {aline}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x8ERYqQW7U9q"},"outputs":[],"source":["# Create Sentence DataFrame\n","\n","sent_no_ls = list(range(len(corpus_sents_ls)))\n","\n","corpus_sents_df = pd.DataFrame({'sent_no': sent_no_ls, 'sent_raw': corpus_sents_ls})\n","corpus_sents_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70VO_w4Oql1a"},"outputs":[],"source":["# Compute Sentence text statistics\n","\n","corpus_sents_df['sent_clean'] = corpus_sents_df['sent_raw'].apply(lambda x: clean_text(x))\n","corpus_sents_df['token_len'] = corpus_sents_df['sent_clean'].apply(lambda x: len(x.split()))\n","corpus_sents_df['char_len'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x))\n","corpus_sents_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"YFC8GTnw6HrG"},"source":["#### **Create Paragraph DataFrame: [corpus_parags_df]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oGog8ZW16bbr"},"outputs":[],"source":["# Read Corpus into a single string then split into raw Paragraphs\n","\n","corpus_parags_ls, corpus_parags_raw_ls, corpus_raw_str = corpus2parags(CORPUS_FILENAME)\n","print(f'Found #{len(corpus_parags_ls)} paragraphs\\n')\n","\n","print('\\nThe first 5 Paragraphs of the Corpus (first 10 chars):')\n","print('-----------------------------------\\n')\n","corpus_parags_ls[:5][:10]\n","print('\\n')\n","print('\\n\\nThe last 5 Paragraphs of the Corpus (first 10 chars):')\n","print('-----------------------------------\\n')\n","corpus_parags_ls[-5:][:10]\n","print('\\n')\n","\n","n_shortest = 10\n","print(f'The {n_shortest} shortest Paragraphs in the Corpus are:')\n","print('--------------------------------------------')\n","temp_parags_ls = sorted(corpus_parags_ls, key=lambda x: (len(x), x))\n","for i, asent in enumerate(temp_parags_ls[:n_shortest]):\n","  print(f'Shortest Paragraph #{i}: {asent}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ZdfH-rTXwwT"},"outputs":[],"source":["# Verify Paragraphs found and Sentence-Paragraph matches\n","\n","# TODO: Upgrade if warranted (requires updating x2parags functions)\n","\"\"\"\n","print(f'{len(corpus_parags_ls)} Paragraphs found in Corpus')\n","print(f'{len(sentences_section_ls)} Sentences found in a Section')\n","\n","sentences_nosection_ct = len(sentences_nosection_ls)\n","\n","if (sentences_nosection_ct > 0):\n","  print(f'\\n    WARNING: The following {sentences_nosection_ct} Sentences were NOT FOUND in any Section')\n","  print(f'             If these are important/numerous, go back and edit/correct source Corpus text file and rerun this notebook\\n')\n","\n","  for i, asent in enumerate(sentences_nosection_ls):\n","    print(f'    #{i}: {asent}\\n')\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VAwfcbYJnwtq"},"outputs":[],"source":["# Verify Paragraph count and sample\n","\n","len(corpus_parags_ls)\n","print('\\n')\n","\n","parag_no_ls = range(len(corpus_parags_ls))\n","len(corpus_parags_ls)\n","print('\\n')\n","\n","corpus_parags_ls[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnJQvKkyHfVc"},"outputs":[],"source":["# Create DataFrame from list of Paragraphs extracted from Corpus\n","\n","corpus_parags_df = pd.DataFrame({'parag_no':parag_no_ls, 'parag_raw':corpus_parags_raw_ls, 'parag_clean':[clean_text(x) for x in corpus_parags_ls]})\n","corpus_parags_df['parag_raw'] = corpus_parags_df['parag_raw'].astype('string')\n","corpus_parags_df.head(2)\n","corpus_parags_df.tail(2)\n","corpus_parags_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOvIFAsgdgK2"},"outputs":[],"source":["corpus_parags_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TrQ4YmkK6HrH"},"outputs":[],"source":["# For each Paragraph, compute the start, mid and end Sentence Number\n","\n","# NOTE: ~5 minutes runtime\n","\n","# NOTE: May fail on any paragraph/sentence with dirty text and require\n","#       multiple iterations of fix/run cycles\n","\n","# Used raw paragraph data to update Master Corpus DataFrame with Paragraph Info\n","\n","# Filter out all the structural/metatag and blank/punctuation only lines\n","#    and save all semantically meaningful Sentences in corpus_sents_ls\n","\n","corpus_sents2parag_ls = []\n","corpus_sents2parag_reject_ls = []\n","parag_sents_tup_ls = []\n","sent_no_current = 0\n","\n","parag_no_current = 0\n","sent_no_current = 0\n","\n","flag_previous_miss = False\n","\n","corpus_parag_ct = len(corpus_parags_ls)\n","\n","def get_sent2paragno(asent_no, asent_raw_str):\n","  '''\n","  Given a sent_no and sent_raw_str\n","  Search and return the corresponding parag_no that contains the sent_raw_str\n","      (return -1 if not found)\n","  '''\n","\n","  global parag_no_current\n","  global flag_previous_miss\n","  # NOTE: Dependencies on local vars outside this def and global vars corpus_parags_ls\n","  # global parag_no_current\n","\n","  # Loop over every paragraph until we find matching sentence (or fail)\n","  while parag_no_current < corpus_parag_ct:\n","\n","    print(f'Sentence #{asent_no}, Text: {asent_raw_str}')\n","    print(f'    Starting search at Paragraph #{parag_no_current}')\n","    print(f'    Paragraph Text:\\n    {corpus_parags_ls[parag_no_current]}')\n","    parag_str = corpus_parags_ls[parag_no_current]\n","\n","    # Search for Sentence string in current Paragraph string\n","    # if re.search(asent_raw_str, re.escape(parag_str)):\n","    # problems with embedded parenthesis\n","    # noparen_table = str.maketrans({'(':' ', ')':' ', '[':' ', ']':' ', '?':' ', '\"':' ', \"'\":\" \"})\n","    # asent_noparens_str = asent_raw_str.translate(noparen_table)\n","    # parag_noparens_str = parag_str.translate(noparen_table)\n","    asent_noparens_str = re.sub('[^0-9a-zA-Z]+', ' ', asent_raw_str)\n","    parag_noparens_str = re.sub('[^0-9a-zA-Z]+', ' ', parag_str)\n","\n","    if re.search(asent_noparens_str, parag_noparens_str):\n","      print(f'    Found it!')\n","      flag_previous_miss = False\n","      return parag_no_current\n","    else:\n","      if flag_previous_miss == True:\n","        # Sentence not found in 2 consecutive Paragraphs, skip this Sentence\n","        print(f'    Miss: Skip this Sentence and go set current paragraph back 1')\n","        parag_no_current -= 1\n","        flag_previous_miss = False\n","        return -1\n","      else:\n","        # Sentence not found in current Paragraph, so try next one\n","        print(f'     Miss: Sentence not found in current Paragraph, try next one')\n","        parag_no_current += 1\n","        flag_previous_miss = True\n","\n","\n","  # At this point we searched both the current and next Paragraphs for Sentence \n","  #     without success so return error code\n","  return -1 \n","\n","# Step through every Sentence and find the Paragraph Number it lies within\n","# corpus_sents_df['parag_no'] = corpus_sents_df.apply(lambda x: get_sent2paragno(x.sent_no, x.sent_raw), axis=1)\n","parag_no_last_match = 0\n","for idx, row in corpus_sents_df.iterrows():\n","  parag_no_current = parag_no_last_match\n","  asent_no = row['sent_no']\n","  asent_str = row['sent_raw']\n","  print(f'idx #{idx}, sent_no: {asent_no}\\n    {asent_str}')\n","  \n","  asent_parag_no = get_sent2paragno(asent_no, asent_str)\n","  print(f'back from searching for Sentence in all Paragraphs with result: {asent_parag_no}')\n","  if asent_parag_no < 0:\n","    # print(f'FAIL: Did not find current Sentence so skip to next Sentence\\n\\n')\n","    corpus_sents2parag_reject_ls.append((asent_no, corpus_sents_ls[asent_no]))\n","    parag_no_current = parag_no_last_match\n","  else:\n","    # print(f'SUCCESS: Sentence #{asent_no} found in Paragraph #{asent_parag_no}\\n\\n')\n","    corpus_sents2parag_ls.append((asent_no, asent_parag_no))\n","    parag_no_last_match = parag_no_current\n","\n","  # if idx > 20:\n","  #   break\n","\n","# Test\n","\"\"\"\n","print(f'Raw Lines length: {len(corpus_lines_ls)}')\n","print(f' Clean Sentences: {len(corpus_sents_ls)}')\n","\n","line_ct = 10\n","print(f'First {line_ct} clean Sentences : --------------------\\n')\n","for i,aline in enumerate(corpus_sents_ls[:line_ct]):\n","  print(f'Line #{i}:\\n    {aline}')\n","print(f'\\n\\nLast {line_ct} clean Sentences: -------------------\\n')\n","for i,aline in enumerate(corpus_sents_ls[-line_ct:]):\n","  print(f'Line #{i}: {aline}')\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJxi6BUWqj-l"},"outputs":[],"source":["# Any Sentences not found in Corpus Paragraphs?\n","#  If not empty list, manually check and edit corpus if many/significant Sentences rejected\n","\n","sentences_noparag_ct = len(corpus_sents2parag_reject_ls)\n","\n","if sentences_noparag_ct > 0:\n","  print(f'\\n    WARNING: The following {sentences_noparag_ct} Sentences were NOT FOUND in any Paragraph')\n","  print(f'             If these are important/numerous, go back and edit/correct source Corpus text file and rerun this notebook\\n')\n","\n","  for i, asent in enumerate(corpus_sents2parag_reject_ls):\n","    print(f'    #{i}: {asent}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9FrWNj80PkD"},"outputs":[],"source":["corpus_sents_df.iloc[200]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UAnLu0k1PAr"},"outputs":[],"source":["corpus_sents_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KGqcTKBtF4QU"},"outputs":[],"source":["# Create a list of the Sentence Nos associated with each Paragraph in the Corpus\n","\n","sent_parag_no_ls = [sentnoparagno_tp[1] for sentnoparagno_tp in corpus_sents2parag_ls]\n","\n","corpus_sent_ct = corpus_sents_df.shape[0]\n","parags_sent_ct = len(sent_parag_no_ls)\n","if (corpus_sent_ct == parags_sent_ct):\n","  print(f'GOOD, all {corpus_sent_ct} Sentences were matched in one of the {corpus_parags_df.shape[0]} Paragraphs\\n')\n","else:\n","  print(f'WARNING: only {parags_sent_ct} Sentences were matched in one of the {corpus_parags_df.shape[0]} Paragraphs')\n","  print(f'         {corpus_sent_ct - parags_sent_ct} Sentences were not matched\\n')\n","\n","print(f'First 10 Sentences belong to these Paragraph:\\n    {sent_parag_no_ls[:10]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7-1oBkJKeJH"},"outputs":[],"source":["# Verfiy the data about to be used to update the master corpus_all_df DataFrame with Paragraph No data\n","\n","corpus_sents2parag_ls[3233:3237]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2vZk07BkeNw"},"outputs":[],"source":["# corpus_all_df.drop(columns=['parag_no'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29tMZKR4KERa"},"outputs":[],"source":["# Update the master corpus_all_df DataFrame with Paragraph No for each Sentence\n","\n","# WARNING: Only execute once (insert Column/Series into DataFrame)\n","\n","parag_no_ser = pd.Series(parag_no_ls)\n","corpus_sents_df.insert(loc=1, column='parag_no', value=sent_parag_no_ls)\n","\n","corpus_sents_df.head(50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QsOgtjM71eKE"},"outputs":[],"source":["corpus_sents_df.iloc[300:320]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tK5kKZbVl3jR"},"outputs":[],"source":["# corpus_parags_df.drop(columns=['sent_no_start', 'sent_no_mid','sent_no_end'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmLLzsNtECDt"},"outputs":[],"source":["corpus_parags_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVkStzTrlxll"},"outputs":[],"source":["# Verify the Paragraph only DataFrame\n","\n","corpus_parags_df.head(2)\n","corpus_parags_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JkHIEWqEKz66"},"outputs":[],"source":["# Calculate the start, mid and end Sentence No for each Paragraph\n","\n","parag_sent_no_start_ls = np.array(corpus_sents_df.groupby('parag_no')['sent_no'].min())\n","parag_sent_no_end_ls = np.array(corpus_sents_df.groupby('parag_no')['sent_no'].max())\n","\n","def my_mid(anum, bnum):\n","  mid_no = (anum - bnum)//2 + bnum\n","\n","  return mid_no\n","\n","print('\\nParagraph Start Sentence no: -----')\n","print(parag_sent_no_start_ls)\n","\n","parag_sent_no_mid_ls = list(map(my_mid, parag_sent_no_end_ls, parag_sent_no_start_ls))\n","print('\\nParagraph Mid-Sentence no: -----')\n","print(parag_sent_no_mid_ls)\n","\n","print('\\nParagraph End Sentence no: -----')\n","print(parag_sent_no_end_ls)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PbSi16c1Kz6_"},"outputs":[],"source":["# If necessary, delete prior columns to update DataFrame with new data\n","\n","# corpus_parags_df.drop(columns=['parag_no_start'], inplace=True)\n","# corpus_parags_df.drop(columns=['parag_no_mid'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkdeR-S_Kz7B"},"outputs":[],"source":["# Insert 3 new columns on start,mid and end Sentence No for each Paragraph\n","\n","corpus_parags_df.insert(1, 'sent_no_start', parag_sent_no_start_ls)\n","corpus_parags_df.insert(2, 'sent_no_mid', parag_sent_no_mid_ls)\n","corpus_parags_df.insert(3, 'sent_no_end', parag_sent_no_end_ls)\n","\n","corpus_parags_df.head()\n","                       "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9uIWYnCLSr7"},"outputs":[],"source":["# Create a clean text version of each Paragraph\n","\n","# corpus_parags_df = pd.DataFrame({'parag_no':parag_no_ls, 'parag_raw':corpus_parags_ls})\n","corpus_parags_df['parag_clean'] = corpus_parags_df['parag_raw'].apply(lambda x: clean_text(x))\n","corpus_parags_df['parag_clean'] = corpus_parags_df['parag_clean'].astype('string')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gza5pqW3LSsF"},"outputs":[],"source":["# Compute Paragraph text Statistics\n","\n","corpus_parags_df['token_len'] = corpus_parags_df['parag_clean'].apply(lambda x: len(x.split()))\n","corpus_parags_df['char_len'] = corpus_parags_df['parag_raw'].apply(lambda x: len(x))\n","corpus_parags_df.head(2)\n","corpus_parags_df.info()"]},{"cell_type":"markdown","metadata":{"id":"rOjESpbc6Izc"},"source":["#### **Create Section DataFrame: [corpus_sects_df]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_vITL5uo1ABR"},"outputs":[],"source":["# If Corpus has Sections, read Corpus and split into raw Sections\n","#   else just copy Chapter data as pseudo Sections\n","\n","corpus_sects_ls = []    # List of all Section Numbers\n","sent_sectno_ls = []     # Section Number for EVERY Matching Sentence in Corpus found in a Section\n","sent_no_sectno_ls = []  # Sentence Number for for ANY Unmatched Sentence in Corpus NOT found in any Section\n","\n","\n","if SECTION_HEADINGS == 'None':\n","  # Just copy Chapter info\n","  corpus_sects_ls = [x for x in corpus_chaps_ls]\n","  print(f'No Sections in {CORPUS_FULL},\\n    so using Chapters as pseudo-Sections.')\n","  sent_sectno_ls = sent_chap_no_ls # Every Sentence in Corpus belongs to the same SectionNo as CorpusNo\n","\n","else:\n","\n","  # Read corpus into a single string then split into raw Section\n","\n","  corpus_sects_ls, sent_sectno_ls, sent_no_sectno_ls = corpus2sects(CORPUS_FILENAME)\n","  print(f'Found #{len(sent_sectno_ls)} Section\\n')\n","\n","  print('\\nThe first 5 Section of the Corpus (first 10 chars):')\n","  print('-----------------------------------\\n')\n","  sent_sectno_ls[:5][:10]\n","  print('\\n')\n","  print('\\n\\nThe last 5 Section of the Corpus (first 10 chars):')\n","  print('-----------------------------------\\n')\n","  sent_sectno_ls[-5:][:10]\n","  print('\\n')\n","\n","  n_shortest = 10\n","  print(f'The {n_shortest} shortest Section in the Corpus are:')\n","  print('--------------------------------------------')\n","  temp_sects_ls = sorted(sent_sectno_ls, key=lambda x: (len(x), x))\n","  for i, asent in enumerate(temp_sects_ls[:n_shortest]):\n","    print(f'Shortest Section #{i}: {asent}')\n","\n","\n","  # Calculate Section Informationif SECTION_HEADINGS != 'None':\n","  # encoding = 'windows-1252', 'utf-8', 'cp1252', 'iso-8859-1'\n","  # with open(corpus_filename, \"r\", encoding='cp1252') as infp:\n","  # with open(corpus_filename, \"r\", encoding='cp1252') as infp:\n","  # with open(corpus_filename, \"r\", encoding='cp1252') as infp:\n","\n","  \"\"\"\n","  with open(corpus_filename, \"r\", encoding='utf-8') as infp:\n","    corpus_raw_str = infp.read()\n","\n","  len(corpus_raw_str)\n","\n","  # Extract and process Sections from Corpus\n","  corpus_sects_ls, corpus_str_raw = corpus2sects(corpus_filename)\n","\n","  print('\\n\\nAFTER ----------')\n","  print(f'len(corpus_raw_str): {len(corpus_raw_str)}')\n","  print(\"\\n\\n-----\")\n","  print(f'len(corpus_sects_ls): {len(corpus_sects_ls)}\\n\\n')\n","  \"\"\"\n","  \"\"\"\n","  print(\"\\n\\n-----\")\n","  print(f'corpus_sects_ls[0]:\\n\\n    {corpus_sects_ls[0]}')\n","  print(\"\\n\\n-----\")\n","  print(f'corpus_sects_ls[1]:\\n\\n    {corpus_sects_ls[1]}')\n","  print(\"\\n\\n-----\")\n","  print(f'corpus_sects_ls[2]:\\n\\n    {corpus_sects_ls[2]}')\n","  print(\"\\n\\n-----\")\n","  print(f'corpus_sects_ls[-2]:\\n\\n    {corpus_sects_ls[-2]}')\n","  print(\"\\n\\n-----\")\n","  print(f'corpus_sects_ls[-1]:\\n\\n    {corpus_sects_ls[-1]}')\n","  \"\"\";\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1MJfVfpKOU-"},"outputs":[],"source":["# Create a list of the Sentence Nos associated with each Section in the Corpus\n","\n","# sent_chap_no_ls = [sentnochapno_tp[1] for sentnochapno_tp in corpus_sents2chap_ls]\n","\n","corpus_sent_ct = corpus_sents_df.shape[0]\n","sect_sent_ct = len(sent_sectno_ls)\n","if (corpus_sent_ct == sect_sent_ct):\n","  print(f'GOOD, all {corpus_sent_ct} Sentences were matched in one of the {corpus_sects_df.shape[0]} Sections\\n')\n","else:\n","  print(f'WARNING: only {sect_sent_ct} Sentences were matched in one of the {corpus_sects_df.shape[0]} Sections')\n","  print(f'         {corpus_sent_ct - sect_sent_ct} Sentences were not matched\\n')\n","\n","\n","# print(f'There are {corpus_sents_df.shape[0]} Sentences in the Corpus')\n","# print(f'{len(sent_chap_no_ls)} Sentences have been associated with {corpus_chaps_df.shape[0]} Sections\\n')\n","\n","print(f'First 10 Sentences belong to these Sections:\\n    {sent_sectno_ls[:10]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wqKwpPs9JC0R"},"outputs":[],"source":["# Verify Sections found and Sentence-Section matches\n","\n","print(f'{len(corpus_sects_ls)} Sections found in Corpus')\n","print(f'{len(sent_sectno_ls)} Sentences found in a Section')\n","\n","sentences_nosection_ct = len(sent_no_sectno_ls)\n","\n","if (sentences_nosection_ct > 0):\n","  print(f'\\n    WARNING: The following {sentences_nosection_ct} Sentences were NOT FOUND in any Section')\n","  print(f'             If these are important/numerous, go back and edit/correct source Corpus text file and rerun this notebook\\n')\n","\n","  for i, asent in enumerate(sentences_nosection_ls):\n","    print(f'    #{i}: {asent}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7JFQTmEgK66I"},"outputs":[],"source":["# Create DataFrame from list of Sections extracted from Corpus\n","\n","sect_no_ls = list(range(len(corpus_sects_ls)))\n","corpus_sects_df = pd.DataFrame({'sect_no':sect_no_ls, 'sect_raw':corpus_sects_ls})\n","corpus_sects_df['sect_raw'] = corpus_sects_df['sect_raw'].astype('string')\n","corpus_sects_df.head(1)\n","corpus_sects_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjN6omWrifWm"},"outputs":[],"source":["# Create a list of the Sentence Nos associated with each Section in the Corpus\n","\n","# sentno_sectno_ls = [sentno2sectno_tp[0] for sentno2sectno_tp in sentences_section_ls]\n","# sentstr_sectno_ls = [sentno2sectno_tp[1] for sentno2sectno_tp in sentences_section_ls]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KM-JEKECfSa"},"outputs":[],"source":["# corpus_sents_df.drop(columns=['sect_no'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpAlcbpQru0t"},"outputs":[],"source":["# Add Section No for each Sentence in Master DataFrame corpus_all_df\n","# ONLY RUN THIS CODE CELL ONCE\n","\n","# Test if already exists, if not execute\n","corpus_sents_df.insert(3, 'sect_no', sent_sectno_ls)  # This can only be run once\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"353_Nn7pM1FX"},"outputs":[],"source":["# Verify\n","\n","corpus_sents_df.head(2)\n","corpus_sents_df.tail(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpE0UiORYZy6"},"outputs":[],"source":["# Verfiy correct Section Nos using the Sentence No boundaries found in previous code cell\n","#   Search for iloc index ranges containing 1 or more Section boundaries to check correctness\n","\n","corpus_sents_df.iloc[300:500]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xVNIhlQpNZiV"},"outputs":[],"source":["corpus_sects_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1yG53MVwkNHB"},"outputs":[],"source":["# Calculate the start, mid and end Sentence No for each Section\n","\n","sect_sent_no_start_ls = np.array(corpus_sents_df.groupby('sect_no')['sent_no'].min())\n","sect_sent_no_end_ls = np.array(corpus_sents_df.groupby('sect_no')['sent_no'].max())\n","\n","def my_mid(anum, bnum):\n","  mid_no = (anum - bnum)//2 + bnum\n","\n","  return mid_no\n","\n","print('\\nSection start sentence no: -----')\n","print(sect_sent_no_start_ls)\n","\n","sect_sent_no_mid_ls = list(map(my_mid, sect_sent_no_end_ls, sect_sent_no_start_ls))\n","print('\\nSection mid-sentence no: -----')\n","print(sect_sent_no_mid_ls)\n","\n","print('\\nSection end sentence no: -----')\n","print(sect_sent_no_end_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUofWkm0kNHC"},"outputs":[],"source":["# If necessary, delete prior columns to update DataFrame with new data\n","\n","# corpus_chaps_df.drop(columns=['parag_no_start'], inplace=True)\n","# corpus_chaps_df.drop(columns=['parag_no_mid'], inplace=True)\n","# corpus_chaps_df.drop(columns=['parag_no_end'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0HxCqi2XkNHC"},"outputs":[],"source":["# Verify DataFrame before update\n","\n","corpus_sects_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhfgZuiPkNHD"},"outputs":[],"source":["# Insert 3 new columns on start,mid and end Sentence No for each Section\n","\n","corpus_sects_df.insert(1, 'sent_no_start', sect_sent_no_start_ls)\n","corpus_sects_df.insert(2, 'sent_no_mid', sect_sent_no_mid_ls)\n","corpus_sects_df.insert(3, 'sent_no_end', sect_sent_no_end_ls)\n","\n","corpus_sects_df.loc[:, corpus_sects_df.columns != 'sect_raw']\n","# corpus_sects_df.loc[:, ['sect_no', 'sect_no_start', 'sect_no_mid', 'sect_no_end']]\n","corpus_sects_df.info()\n","\n","                       "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iObHkdhmkNHF"},"outputs":[],"source":["# Create a clean text version of each Paragraph\n","\n","corpus_sects_df['sect_clean'] = corpus_sects_df['sect_raw'].apply(lambda x: clean_text(x))\n","corpus_sects_df['sect_clean'] = corpus_sects_df['sect_clean'].astype('string')\n","\n","# corpus_sects_df.loc[:, corpus_sects_df.columns != 'sect_raw']\n","corpus_sects_df.filter(like='_no')\n","# corpus_sects_df.loc[:, ['sect_no', 'sect_no_start', 'sect_no_mid', 'sect_no_end']]\n","corpus_sects_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFDwD6M-kNHH"},"outputs":[],"source":["# Compute Paragraph text Statistics\n","\n","corpus_sects_df['token_len'] = corpus_sects_df['sect_clean'].apply(lambda x: len(x.split()))\n","corpus_sects_df['char_len'] = corpus_sects_df['sect_raw'].apply(lambda x: len(x))\n","corpus_sects_df.loc[:, list(set(corpus_sects_df.columns) - set(['sect_raw', 'sect_clean']))]\n","corpus_sects_df.info()"]},{"cell_type":"markdown","metadata":{"id":"6SdZ4e3ALYdD"},"source":["### **Add Descriptive Statistics and Clean Raw Text**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWu8_C6PQ4iE"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7au9Zy2kR0NK"},"outputs":[],"source":["# TODO: Verfiy and eal with any NaN entries\n","#   all Sentences with NaN or '' Raw Text\n","\"\"\"\n","\n","# Sentences\n","# Let's take a look at the updated text\n","corpus_sents_df['sent_clean'] = corpus_sents_df['sent_raw'].apply(lambda x: text_clean(x))\n","# Ensure to drop all Sentences with NaN or '' Raw Textcorpus_sents_df.replace(\"\", np.nan, regex=True, inplace=True)\n","corpus_sents_df.dropna(how='any', axis=0, subset=['sent_raw'], inplace=True)\n","corpus_sents_df.dropna(how='any', axis=0, subset=['sent_clean'], inplace=True)\n","\n","print('\\nCompare Raw and Cleaned Sentences:')\n","print('--------------------------------------')\n","corpus_sents_df.head(2)\n","\n","\n","# Paragraphs\n","# Let's take a look at the updated text\n","corpus_parags_df['parag_clean'] = corpus_parags_df['parag_raw'].apply(lambda x: text_clean(x))\n","# Ensure to drop all Sentences with NaN or '' Raw Text\n","corpus_parags_df.replace(\"\", np.nan, regex=True, inplace=True)\n","corpus_parags_df.dropna(how='any', axis=0, subset=['parag_raw'], inplace=True)\n","\n","print('\\nCompare Raw and Cleaned Paragraphs:')\n","print('--------------------------------------')\n","corpus_parags_df.head(2)\n","\n","\n","# Sections\n","# Let's take a look at the updated text\n","corpus_sects_df['sect_clean'] = corpus_sects_df['sect_raw'].apply(lambda x: text_clean(x))\n","# Ensure to drop all Sentences with NaN or '' Raw Text\n","corpus_sects_df.replace(\"\", np.nan, regex=True, inplace=True)\n","corpus_sects_df.dropna(how='any', axis=0, subset=['sect_raw'], inplace=True)\n","\n","print('\\nCompare Raw and Cleaned Sections:')\n","print('--------------------------------------')\n","# corpus_sects_df.head(2)\n","\n","\n","# Chapters\n","# Let's take a look at the updated text\n","corpus_chaps_df['chap_clean'] = corpus_chaps_df['chap_raw'].apply(lambda x: text_clean(x))\n","# Ensure to drop all Chapters with NaN or '' Raw Text\n","corpus_chaps_df.replace(\"\", np.nan, regex=True, inplace=True)\n","corpus_chaps_df.dropna(how='any', axis=0, subset=['chap_raw'], inplace=True)\n","\n","print('\\nCompare Raw and Cleaned Chapters:')\n","print('--------------------------------------')\n","# corpus_sects_df.head(2)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mf99apfwKPAO"},"outputs":[],"source":["# Verify shapes of all 4 Baseline 4 Models\n","\n","print(f'corpus_sents_df.shape: {corpus_sents_df.shape}')\n","print(f'corpus_parags_df.shape: {corpus_parags_df.shape}')\n","print(f'corpus_sects_df.shape: {corpus_sects_df.shape}')\n","print(f'corpus_chaps_df.shape: {corpus_chaps_df.shape}')\n","\n","\"\"\"\n","SButler Odyssey\n","\n","corpus_sents_df.shape: (2445, 8)\n","corpus_parags_df.shape: (1051, 8)\n","corpus_sects_df.shape: (24, 8)\n","corpus_chaps_df.shape: (24, 8)\n","\"\"\";\n"]},{"cell_type":"markdown","metadata":{"id":"jHscLkclSYqN"},"source":["##**Save Preprocess Corpus DataFrames**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEzo8eltSvWS"},"outputs":[],"source":["# Save Corpus DataFrames\n","\n","save_dataframes()"]},{"cell_type":"markdown","metadata":{"id":"yL8R_ANtfYG6"},"source":["# (Optional) EDA Raw Text Features: Interactive"]},{"cell_type":"markdown","metadata":{"id":"1njcRD-jgGJh"},"source":["**(Optional) Can Skip Ahead to: 'EDA of Raw Text and Extracted Features'**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ti9jQK7grxO"},"outputs":[],"source":["# Review Cleaned Up Sentences\n","\n","corpus_sents_df.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1TN5ksy55kXy"},"outputs":[],"source":["# Summary Statistics\n","\n","corpus_sents_df.describe()\n","corpus_sents_df['token_len'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxvxkbUGzkfy"},"outputs":[],"source":["# Create histogram of Paragraph lengths\n","\n","sns.histplot(data=corpus_sents_df['char_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Lengths');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_paraglen.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqUtGz8UjY2b"},"outputs":[],"source":["# Plot histogram of Sentence lengths\n","\n","sns.histplot(data=corpus_sents_df['token_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Lengths')\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_sentlen.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OoBrucYR9Xc"},"outputs":[],"source":["# SELECT CORPUS TYPE\n","# TODO: Customized Preprocessing (e.g. Tweets) by Corpus Type\n","\n","# Novel, Tweets, Chat Transcript\n","\n","# Processing Options\n","\n","# Apply first level cleaning"]},{"cell_type":"markdown","metadata":{"id":"j2tIua7tTSRz"},"source":["# (Optional) Manually Create Sentiment Arc Baseline"]},{"cell_type":"markdown","metadata":{"id":"dU11XOZIPalR"},"source":["***Can skip to Section [Load Sentiment Polarities...] or [Calculate VADER...]***"]},{"cell_type":"markdown","metadata":{"id":"Cke3OowdWk6I"},"source":["**Interactively Enter Cruxes and Edge Cases**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wv7Foe3oTMmz"},"outputs":[],"source":["# Setup data structures for endpoints of Sentiment Time Series\n","\n","#   [-1.0 to +1.0] = [v.neg, neg, neutral, pos, v.pos]\n","\n","corpus_man_crux_ols = []  # working datastructure to dynamically build ordered list of manually selected Crux Points\n","corpus_man_cruxes_odt = OrderedDict() # Once all manual Crux points selected, this will be working data structure\n","\n","corpus_sents_len = corpus_sents_df.shape[0] - 1\n","\n","corpus_parags_len = corpus_sents_df.parag_no.max() # make sure no omissions/repeats/skips\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-_ylgiAc6Aw"},"outputs":[],"source":["# <INPUT> Set the Begining and Ending Sentiment Values (Manual Versions)\n","\n","# Start of Corpus Sentiment Analysis Time Series\n","Corpus_Starting_Sentiment = -0.1 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n","# corpus_sa_begin = Corpus_Starting_Sentiment\n","\n","# End of Corpus Sentiment Analysis Time Series\n","Corpus_Ending_Sentiment = -1 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n","# corpus_sa_end = Corpus_Ending_Sentiment\n","\n","corpus_man_crux_ols = [tuple((0, Corpus_Starting_Sentiment)), tuple((corpus_sents_len, Corpus_Ending_Sentiment))]\n","# corpus_man_cruxes_dt[0.] = corpus_sa_begin\n","# corpus_man_cruxes_dt[float(corpus_sents_len)] = corpus_sa_end\n","\n","print(f'Manual Cruxes with Start/End: {corpus_man_crux_ols}')"]},{"cell_type":"markdown","metadata":{"id":"BmcLrzyUw93d"},"source":["**Seach for Key Words that suggest Min/Max Sentiment Crux**\n","* Specific to the Novel: Introduction of Pivotal Character, Scene, Factual Reveal, McGuffin, etc...\n","* General to Events/Themes: Death, Birth, Fight, Accident, Money, Sex, etc... "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UESgwwIAY28T"},"outputs":[],"source":["# <INPUT> Search Corpus for Line No of Peaks/Valleys\n","# TODO: Better Vis\n","Search_String = \"Death\" #@param {type:\"string\"}\n","if (Search_String == \"\"):\n","  search_str = \"accident\"\n","else:\n","  search_str = Search_String.lower()\n","\n","# search the list of cleaned paragraphs\n","# results_ls = [x for x in search_match_ls if re.search(subs, x)]\n","\n","# creating and passsing series to new column\n","match_sents_ser = corpus_sents_df[\"sent_clean\"].str.find(search_str)\n","\n","# print(f'Found #{len(match_index>0)} Matches')\n","match_sents_df = corpus_sents_df.loc[match_sents_ser > 0]\n","print(f'Found #{match_sents_df.shape[0]} Matching Sentences')\n","print('------------------------------------')\n","# print(f'  {match_sents_df}')\n","match_sents_df[['sent_no', 'parag_no', 'sent_raw', 'token_len']]"]},{"cell_type":"markdown","metadata":{"id":"3oVYBfz6-EVo"},"source":["**Get Context for Matched Sentence by Retrieving Surrounding Paragraph**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7AvYQqT5TsdV"},"outputs":[],"source":["# Extract Surrounding Paragraphs for context on matching Sentences\n","\n","def get_parag4sentno(asent_no):\n","  '''\n","  Return the original raw paragraph containing a \n","  given sentence number.\n","  '''\n","  # parag_df = pd.DataFrame()\n","  # print(f'Passed in sent_no: {asent_no}')\n","  aparag_no = int(corpus_sents_df.loc[corpus_sents_df['sent_no'] == asent_no]['parag_no'])\n","  # print(f'  This sent_no {asent_no} is in parag_no: {aparag_no}')\n","  aparag_str = corpus_sents_df.loc[corpus_sents_df['parag_no'] == aparag_no]['sent_raw'].str.cat() # ['sent_clean']\n","  # sentno_parag_df = corpus_sents_df[corpus_sents_df['sent_no']==asent_no]\n","  # print(f'Sent #{asent_no} is in the paragraph: ')\n","  # print(aparag)\n","  # print(f'returning aparag_no: [{aparag_no}]: {aparag}')\n","  return aparag_no, aparag_str\n","\n","'''\n","# Testing\n","asent_no = 7\n","print(f'Searching for paragraph containing Sentence #{asent_no}')\n","\n","aparag_no, aparag_str = get_parag4sentno(asent_no)\n","print(f'\\n  Found in Paragraph #{aparag_no} \\n\\n{aparag_str}')\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnHz08NyDroK"},"outputs":[],"source":["# Extract Surrounding Paragraphs for context on matching Sentences\n","\n","def get_parag_str(aparag_no):\n","  '''\n","  Return the original raw paragraph containing a \n","  given sentence number.\n","  '''\n","  # parag_df = pd.DataFrame()\n","  # print(f'Passed in sent_no: {asent_no}')\n","  # aparag_no = int(corpus_sents_df.loc[corpus_sents_df['sent_no'] == asent_no]['parag_no'])\n","  # print(f'  This sent_no {asent_no} is in parag_no: {aparag_no}')\n","  aparag_str = corpus_sents_df.loc[corpus_sents_df['parag_no'] == aparag_no]['sent_raw'].str.cat() # ['sent_clean']\n","  # sentno_parag_df = corpus_sents_df[corpus_sents_df['sent_no']==asent_no]\n","  # print(f'Sent #{asent_no} is in the paragraph: ')\n","  # print(aparag)\n","  # print(f'returning aparag_no: [{aparag_no}]: {aparag}')\n","  return aparag_str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ByKLYZDsK123"},"outputs":[],"source":["# Summarize current status of manually selected Crux Points\n","# TODO:\n","\n","def crux_sum_short():\n","  print(f'\\nOrdered list of all manually selected Crux Points')\n","  print('---------------------------------------')\n","  for i, acrux_tp in enumerate(corpus_man_crux_ols):\n","    asent_no, asent_pol = acrux_tp\n","    asent_str = corpus_sents_df[corpus_sents_df.sent_no==asent_no].sent_raw.str.cat()\n","    # print(f'Type: {type(asent_str)}')\n","    print(f'Sent No {asent_no:4d}: Polarity: {asent_pol}\\nText: {asent_str}\\n\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mz0FLzt2UeSe"},"outputs":[],"source":["# Summarize current manually selected Crux Points\n","\n","def crux_summary():\n","  print(f'\\nOrdered list of all manually selected Crux Points')\n","  print('---------------------------------------\\n\\n')\n","  for i, acrux_tp in enumerate(corpus_man_crux_ols):\n","    asent_no, asent_pol = acrux_tp\n","    asent_str = corpus_sents_df[corpus_sents_df.sent_no==asent_no].sent_raw.str.cat()\n","    # print(f'Type: {type(asent_str)}')\n","    print(f'Sent No {asent_no:4d}: Polarity: {asent_pol}')\n","    print('------------------------------')\n","    print(f'Text: {asent_str}\\n\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GiUmseNw3CkN"},"outputs":[],"source":["# View the Paragraph containing your Matching Sentence:\n","\n","def get_nparags_context(crux_sent_no, parag_ct):\n","\n","  parag_win = int(parag_ct)\n","  parag_crux_str = ''\n","\n","  parag_crux_no = 0\n","\n","  if (crux_sent_no < 0) | (crux_sent_no > corpus_sents_len):\n","    print(f'ERROR: Pick a Sentence No between 0-{corpus_sents_len-1}')\n","  else:\n","    # get_sent_no = crux_sent_no\n","    # print(f'Retrieving Sentence No: {get_sent_no}')\n","    # print('----------')\n","\n","    parag_crux_no, aparag_str = get_parag4sentno(crux_sent_no)\n","    if parag_win == 1:\n","      print(f'Match #{i}: Sentence No. {asent_no} found in Paragraph No. {parag_crux_no}')\n","      print('----------------------------')\n","      print(f'Sentence:\\n')\n","      # print(f'     {corpus_sents_df[corpus_sents_df.sent_no == crux_sent_no]}\\n\\n')\n","      corpus_sents_df[corpus_sents_df.sent_no == crux_sent_no]\n","      print('----------------------------')\n","      print(f'Paragraph Context:\\n')\n","      print(f'     {aparag_str}\\n\\n')\n","    else:\n","      parag_half_win = int((parag_win-1)/2)\n","      parag_start = parag_crux_no - parag_half_win\n","      parag_end = parag_crux_no + parag_half_win\n","      print(f'Retrieving {parag_ct} Contextual Paragraphs Nos {parag_start} to {parag_end}')\n","      print(f'  for Crux Point centered on Sentence No {crux_sent_no}')\n","      for i in range(parag_start, parag_end + 1, 1):\n","        if i == parag_crux_no:\n","          print(f'\\n   ---------------------------------------------------------')\n","          print(f'** Crux Point Paragraph #{i} with Sentence No. {crux_sent_no} **')\n","          print(f'   ---------------------------------------------------------')\n","          parag_crux_str = get_parag_str(i)\n","          print(parag_crux_str)\n","        else:\n","          print(f'\\n   ----------------------')\n","          print(f'   Regular Paragraph #{i}')\n","          print(f'   ----------------------')\n","          print(get_parag_str(i))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjoXC8Fg3lBP"},"outputs":[],"source":["# Insert new crux point into ordered list: corpus_man_crux_ls\n","\n","# NOTE: For very long lists, use Python simple bisect library (at cost of additional dependency)\n","\n","\n","def insert_ord_tp_list(crux_ord_ols, crux_tp):\n","  '''\n","  Insert new crux tuple: crux_tp = (sent_no, sentiment_polarity)\n","  into ordered list of tuples while maintaining sent_no order\n","  '''\n","  sent_no, senti_pol = crux_tp\n","\n","  # Searching for the position\n","  for i in range(len(crux_ord_ols)):\n","    if crux_ord_ols[i][0] == sent_no:\n","      # Attempting to insert duplicate\n","      return crux_ord_ols\n","    elif crux_ord_ols[i][0] < sent_no:\n","      insert_idx = i\n","    else:\n","      break\n","      \n","  # Inserting n in the list\n","  list = crux_ord_ols[:i] + [crux_tp] + crux_ord_ols[i:]\n","  return list\n","\n","'''\n","# Test\n","crux_test_ls = [(1,0), (5,1), (10,-1)]\n","crux_test_tp = (3,10)\n","  \n","print(insert_ord_tp_list(crux_test_ls, crux_test_tp))\n","''';"]},{"cell_type":"markdown","metadata":{"id":"1PPOLJDZ1wHc"},"source":["**Start of Human in the Loop Manual Crux Point Identification**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h1MLexX57Guc"},"outputs":[],"source":["# Instructions\n","\n","print('Enter a Sentence number based upon your search above to see the ')\n","print('  surrounding Paragraph context.')\n","print('----------------------------------------')\n","print(f'(Enter an integer between 0 and {corpus_sents_len-1})\\n\\n')\n","\n","print('\\n')\n","print('Enter an ODD NUMBER for the Number of surrounding Paragraphs ')\n","print('  around the Sentence No to give Context.')\n","print('----------------------------------------')\n","print(f'(Enter an integer: 3, 5, 7\\n\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exxWtaUPC30l"},"outputs":[],"source":["# Input your Context Retrieval Parameters\n","\n","Sentence_No =  2692#@param {type:\"integer\"}\n","No_Paragraphs_Context = \"3\" #@param [\"1\", \"3\", \"5\"]\n","\n","get_nparags_context(Sentence_No, No_Paragraphs_Context)"]},{"cell_type":"markdown","metadata":{"id":"30fBikIrrgMe"},"source":["**Add Crux to Manually Generated Sentiment Arc**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgljeT6ypNbo"},"outputs":[],"source":["# Instructions to add current sentence as a Crux Point\n","\n","crux_summary()\n","\n","print('--------------------------------------------------------')\n","print(f'INSTRUCTIONS To current Sentence No: {Sentence_No} as a Crux Point')\n","print('--------------------------------------------------------')\n","\n","print(\"\\nCheck this box if you want to add the Sentence/Paragraph above \")\n","print(\"  as a new Min/Max Crux Point with your approximation \")\n","print(\"  for a Sentiment Polarity value between -1.0 to +1.0\\n\\n\")\n","\n","print(f\"Crux Sentence No: {Sentence_No} in Paragraph No: {parag_crux_no}\\n\")\n","print(parag_crux_str)\n","\n","print(\"\\n\\nLeave Add_Sentence_Crux 'unchecked' to not add\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ia1IXChRmyr3"},"outputs":[],"source":["# <INPUT> Option to add this Sentence/Paragraph as a Min/Max Crux Point\n","\n","Sentiment_Polarity = -0.4 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n","Add_Sentence_Crux = True #@param {type:\"boolean\"}\n","\n","\n","# Add Crux if selected and give current summary status\n","\n","if Add_Sentence_Crux == True:\n","  crux_new_tp = tuple((Sentence_No, Sentiment_Polarity))\n","  corpus_man_crux_ols = insert_ord_tp_list(corpus_man_crux_ols, crux_new_tp)\n","  if (corpus_man_crux_ols):\n","    print(f'Successfully inserted new Crux = {crux_new_tp}')\n","    print(f'Added Crux at Sentence No={Sentence_No} with Polarity={Sentiment_Polarity}')\n","    # corpus_man_cruxes_dt[Sentence_No] = Sentiment_Polarity\n","  else:\n","    print(f'ERROR: Could not insert new Crux = {crux_new_tp}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lps67T-lUTi2"},"outputs":[],"source":["# Summary of current Crux Points after addition\n","\n","print('\\n------------------------------------------------------------')\n","print(f'After addition of new Crux Point (Sentence No {Sentence_No})')\n","print('------------------------------------------------------------\\n')\n","\n","crux_summary()"]},{"cell_type":"markdown","metadata":{"id":"fjZT9_0CrzFk"},"source":["**Delete Manually Selected Crux Points**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTX30nokNPBp"},"outputs":[],"source":["len(corpus_man_crux_ols)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxS-J_hPOfkI"},"outputs":[],"source":["crux_tp = (1, 2)\n","a, b = crux_tp\n","print(f'a is {a} and b is {b}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bi7Cu1iErvzc"},"outputs":[],"source":["# Insert new crux point into ordered list: corpus_man_crux_ls\n","\n","# FIX: 20210616 and move to utility functions\n","\n","# NOTE: For very long lists, use Python simple bisect library (at cost of additional dependency)\n","\n","\n","def del_ord_tp_list(acorpus_man_crux_ols, crux_tp):\n","  '''\n","  Insert new crux tuple: crux_tp = (sent_no, sentiment_polarity)\n","  into ordered list of tuples while maintaining sent_no order\n","  '''\n","  crux_ct = len(acorpus_man_crux_ols)\n","  sent_no = crux_tp[0]\n","  print(f'Deleting sent_no: {sent_no} over crux_ls len={len(acorpus_man_crux_ols)}')\n","\n","  # Searching for the positionk\n","  del_idx = -1\n","  for i in range(len(acorpus_man_crux_ols)):\n","    acrux_sent_no = acorpus_man_crux_ols[i][0]\n","    print(f'Crux #{i} is sent_no={acrux_sent_no}')\n","    if acrux_sent_no == sent_no:\n","      print(f'Matching index at {i}')\n","      del_idx = i\n","      \n","  # Delete n in the list\n","  print(f'Deletion index = {del_idx}')\n","  if del_idx == 0:\n","    # Delete the first Crux\n","    list = acorpus_man_crux_ols[1:]\n","    return list\n","  elif del_idx == crux_ct -1:\n","    # Delete the last Crux\n","    list = acorpus_man_crux_ols[:-1]\n","    return list    \n","  elif (del_idx > 0) & (del_idx < crux_ct):\n","    # Delete an interior Crux\n","    before_idx = i - 1\n","    after_idx = i\n","    list = acorpus_man_crux_ols[:before_idx] + acorpus_man_crux_ols[after_idx:]\n","    print(f'Returning list: {list}')\n","    return list\n","  else:\n","    print('No matching Crux tuple found')\n","    return acorpus_man_crux_ols\n","  \n","\n","# Test\n","crux_test_ls = [(1,0), (5,1), (10,-1)]\n","crux_test_tp = (5,5)\n","  \n","print(del_ord_tp_list(crux_test_ls, crux_test_tp))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dI5hZqmSLg5R"},"outputs":[],"source":["# Instructions to Delete a Crux Point\n","\n","crux_summary()\n","\n","print('--------------------------------------------------------')\n","print('INSTRUCTIONS To Delete a Crux Point')\n","print('--------------------------------------------------------')\n","\n","print(\"\\nEnter the Sentence No of a Crux you want to delete.\\n\")\n","print(f'     Current Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n","print(\" Skip this if you want to keep all manually selected Crux Points.\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzowFY-I8U1_"},"outputs":[],"source":["# Instructions\n","\n","print(\"\\nEnter the Sentence No of a Crux you want to delete.\\n\")\n","print(f'     Current Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n","print(\" Skip this if you want to keep all manually selected Crux Points.\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ShFCfOsrvsv"},"outputs":[],"source":["Delete_Sent_No =  777#@param {type:\"integer\"}\n","# Select a Crux to Delete\n","# TODO: Drop down list\n","\n","# corpus_man_crux_ols\n","corpus_man_crux_temp_ols = []\n","\n","crux_sent_set = set([x[0] for x in corpus_man_crux_ols])\n","if not(Delete_Sent_No in crux_sent_set):\n","  print(f'ERROR: {Delete_Sent_No} is not a Crux Point Sentence No')\n","else:\n","  # Keep the same tuple format for uniformity and future features\n","  crux_del_tp = tuple((Delete_Sent_No, 'dummy_sentence'))\n","  print(f'Selected {(crux_del_tp)} to delete')\n","  # corpus_man_crux_temp_ols = \n","  print(f'WTF: {del_ord_tp_list(corpus_man_crux_ols, crux_del_tp)}')\n","  corpus_man_crux_old = del_ord_tp_list(corpus_man_crux_ols, crux_del_tp)\n","  print(f\"corpus_man_crux_ols: {corpus_man_crux_ols}\")\n","  # get_sent_no = Sentence_No\n","  # print(f'Retrieving Sentence No: {get_sent_no}')\n","  # print('----------')\n","  print(f'Updated Crux Points by Sentence No: {corpus_man_crux_ols}\\n\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"rJdNu-oYrwHv"},"source":["**Review Summary of all Manually Selected Crux Points**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmpggAWuvMb1"},"outputs":[],"source":["# Generate Report Summary of All Manually Selected Cruxes\n","\n","f = io.StringIO()\n","with contextlib.redirect_stdout(f):\n","    crux_summary()\n","crux_summary = f.getvalue()\n","\n","# Print Manual Crux Report Summary to Screen\n","\n","# print(crux_summary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ULFfHbxwDfu"},"outputs":[],"source":["# Save Manual Crux Summary Report\n","\n","plot_filename = 'man_cruxes.txt'\n","plotpathfilename_str = gen_pathfiletime(plot_filename)\n","\n","with open(plotpathfilename_str, 'a+') as outfp:\n","  outfp.write(crux_summary)\n","\n","# Verify \n","\n","!ls -alt $plotpathfilename_str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AG_hGPgjxY7X"},"outputs":[],"source":["# Verify Report Content\n","\n","!cat man_cruxes_fscottfitzgerald_thegreatgatsby_20210616214050.txt"]},{"cell_type":"markdown","metadata":{"id":"4zPYRKCex8-U"},"source":["**Clean and Organize Manual Crux Points into new Data Structures**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YICJNt3AXI2d"},"outputs":[],"source":["print(corpus_sents_df[corpus_sents_df['sent_no']==5]['sent_raw'].squeeze())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2bLtpweWQt4"},"outputs":[],"source":["# Convert and assemble all the Crux values in lists to save in a new Crux DataFrame\n","\n","\n","pol_val_ls = [x[1] for x in corpus_man_crux_ols]\n","sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n","parag_no_ls = [(get_parag4sentno(x))[0] for x in sent_no_ls]\n","parag_str_ls = [get_parag_str(x) for x in parag_no_ls]\n","sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n","sent_raw_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_raw'].squeeze() for x in sent_no_ls]\n","sent_raw_ls\n","sent_clean_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_clean'].squeeze() for x in sent_no_ls]\n","sent_clean_ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFEYg7Gfbj9i"},"outputs":[],"source":["# Create a Dict of Crux Points to Tuples (Polarity, Raw Sentence)\n","\n","# First Create the Tuples for each Sentence No (Sentiment Polarity, Raw Text)\n","def merge(list1, list2):\n","    merged_list = tuple(zip(list1, list2)) \n","    return merged_list\n","      \n","crux_tp_ls = merge(pol_val_ls, sent_raw_ls)\n","\n","# Second, Create the Dictionary C\n","corpus_man_cruxes_dt = {sent_no_ls[i]: crux_tp_ls[i] for i in range(len(crux_tp_ls))}\n","\n","# Verify\n","corpus_man_cruxes_dt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtIHecpMdbXa"},"outputs":[],"source":["corpus_sents_df.head()"]},{"cell_type":"markdown","metadata":{"id":"M8qmpFRaTfu2"},"source":["**Plot Interpolated Manual Sentiment Arc**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8ASg7uUZWf3"},"outputs":[],"source":["corpus_man_sa_df = pd.DataFrame({'sent_no':xn, 'sentiment':yn, 'sent_raw':corpus_sents_df.sent_raw.values})\n","corpus_man_sa_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"36VfGi4a47Yl"},"outputs":[],"source":["# Hermite Interpolation with SciPy\n","\n","pol_val_ls = [x[1] for x in corpus_man_crux_ols]\n","sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n","parag_no_ls = [(get_parag4sentno(x))[0] for x in sent_no_ls]\n","parag_str_ls = [get_parag_str(x) for x in parag_no_ls]\n","sent_no_ls = [x[0] for x in corpus_man_crux_ols]\n","sent_raw_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_raw'].squeeze() for x in sent_no_ls]\n","sent_raw_ls\n","sent_clean_ls = [corpus_sents_df[corpus_sents_df['sent_no']==x]['sent_clean'].squeeze() for x in sent_no_ls]\n","sent_clean_ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ElJKawWfZWbv"},"outputs":[],"source":["sent_no_ls\n","pol_val_ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xufA403s3lbF"},"outputs":[],"source":["corpus_man_crux_np = np.asarray(corpus_man_crux_ols)\n","corpus_man_crux_np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdgKbjpi63a9"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMkgU-JhwudG"},"outputs":[],"source":["x2 = np.array(sent_no_ls)\n","y2 = np.array(pol_val_ls)\n","\n","xn = np.linspace(0, corpus_sents_len, corpus_sents_len)\n","yn = interpolate.pchip_interpolate(x2, y2, xn)\n","\n","crux_man_df = pd.DataFrame(\n","    {'sent_no': sent_no_ls,\n","     'pol_val': pol_val_ls\n","     }\n",")\n","\n","# plt.plot(x2, y2, 'ok', label='True values')\n","# plt.plot(xn, yn, label='Hermite Interpolation')\n","\n","# plt.plot(xn, yn4, label='Spline order 4')\n","# plt.plot(xn, yn5, label='Spline order 5')\n","# plt.plot(xn, yn6, label='Spline order 6')\n","# plt.plot(xn, yn7, label='Spline order 7')\n","plt.legend()\n","plt.show()\n","\n","\n","# sns.histplot(data=corpus_sents_df['char_len'], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Lengths')\n","sns.histplot(data=crux_man_df, x='sent_no', y='pol_val', kde=True).set_title(f'{CORPUS_FULL} \\n Manual Cruxes with Hermite Smoothing')\n","\n","\n","# Save graph to file.\n","plot_filename = 'man_crux_plot.png'\n","plotpathfilename_str = gen_pathfiletime(plot_filename)\n","plt.savefig(plotpathfilename_str, format='png', dpi=300)\n"]},{"cell_type":"markdown","metadata":{"id":"WW-VkS2s_ogy"},"source":["**Gaussian Process Regression**\n","\n","* https://blog.dominodatalab.com/fitting-gaussian-process-models-python/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JLFFYDfYpPjn"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"B0BukxX9ZKYA"},"source":["# (Optional) Load Sentiment Polarities: Interactive"]},{"cell_type":"markdown","metadata":{"id":"EhPJ9e-V9NMu"},"source":["***If you upload a file of Sentiment Values you don't have to Calculate them in the following sections***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpx7viqf9AAJ"},"outputs":[],"source":["!ls -altr *.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aw8_mUHI9sdr"},"outputs":[],"source":["# Test\n","\n","files.download('sentiments_raw_all_virginiawoolf_tothelighthouse_20210618161224.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90axHClm9cGZ"},"outputs":[],"source":["# Upload your precomputed Sentiment Values\n","\n","uploaded = files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02NH5nnto2AC"},"outputs":[],"source":["%whos DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QL9JCyCI-Sic"},"outputs":[],"source":["# Verify the file was uploaded correctly\n","\n","newest_csvfile = get_recentfile().split('/')[-1]\n","print(f'The most recently updated *.csv file is: {newest_csvfile}')\n","\n","!head -n 10 $newest_file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTfBj8qdi_vZ"},"outputs":[],"source":["%whos DataFrame\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qgjz-E-mYgzI"},"outputs":[],"source":["# Upload file into DataFrame\n","\n","corpus_test_df = pd.read_csv(newest_csvfile)\n","corpus_test_df['sent_clean'] = corpus_test_df['sent_clean'].astype('string')\n","corpus_test_df['sent_raw'] = corpus_test_df['sent_raw'].astype('string')\n","corpus_test_df.head()\n","corpus_test_df.info()"]},{"cell_type":"markdown","metadata":{"id":"-6ajyb8Jpmzw"},"source":["***Skip to Section <Calculate Median of All...> if SA Loaded***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGA6sQy6RPDC"},"outputs":[],"source":["corpus_lexicons_stats_dt"]},{"cell_type":"markdown","metadata":{"id":"y7CfH00OFkQV"},"source":["# **Either (a) Load Precomputed Sentiment Series or (b) Calculate Sentiment Values**\n","\n","Sentiment Models\n","\n","* VADER [-1.0 to 1.0] zero peak\n","* TextBlob [-1.0 to 1.0] zero peak\n","* Stanza outliers [-1.0 to 199.0] pos, outliers(+peak)\n","* AFINN [-14 (-8 to 8) 20] discrete\n","* SentimentR 11,710 [-5.4 to 8.8] norm\n","* Syuzhet [-5.4 to 8.8] norm\n","* Bing [-100.0 (-20.0 to 20.0) 100] discrete, outliers\n","* Pattern [-1.0 to 1.0] norm\n","* SentiWord [-3.8 to 4.4] norm\n","* SenticNet [-3.8 to 10] norm\n","* NRC [-100.0 (-5.0 to 5.0) 100] zero, outliers"]},{"cell_type":"markdown","metadata":{"id":"CsEbvCoCX7HY"},"source":["## **(b) Compute Baseline Sentiments (Auto)**"]},{"cell_type":"markdown","metadata":{"id":"bmyydBrCYctI"},"source":["### **4 Stastical Machine Learning Classifiers (Optional: Auto)**\n","\n","* Linear Regression\n","* Logistic Regression\n","* Random Forest Classifier\n","* Linear SVC\n","* MultinomialNB\n","\n","Tutorials\n","\n","* https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (Tutorial) \n","* https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_classification.ipynb (github)\n","* https://wellsr.com/python/python-sentiment-analysis-with-sklearn/\n","* https://colab.research.google.com/drive/186bOdu08nv4xHe6VeBgt_aIk9_fziqsX#scrollTo=hpDp3V0Lg-sw"]},{"cell_type":"markdown","metadata":{"id":"RZB6O8_BFaA-"},"source":["#### **Setup**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ym-yOrRbq5N"},"outputs":[],"source":["# Vectorizing text\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","# Validation: https://www.kaggle.com/pocooo/types-of-cross-validation-all-you-need\n","\n","# Simple Cross Fold Validation\n","from sklearn.model_selection import KFold\n","# model=DecisionTreeClassifier()\n","kfold_validation=KFold(10)\n","\n","# import numpy as np\n","from sklearn.model_selection import cross_val_score\n","# results=cross_val_score(model,X,y,cv=kfold_validation)\n","# print(results)\n","# print(np.mean(results))\n","\n","# Stratified CV \n","from sklearn.model_selection import StratifiedKFold\n","skfold=StratifiedKFold(n_splits=5)\n","# model=DecisionTreeClassifier()\n","# scores=cross_val_score(model,X,y,cv=skfold)\n","# print(np.mean(scores))\n","\n","# LOO CV\n","from sklearn.model_selection import LeaveOneOut\n","# model=DecisionTreeClassifier()\n","leave_validation=LeaveOneOut()\n","# results=cross_val_score(model,X,y,cv=leave_validation)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcLbN-4SFiQM"},"outputs":[],"source":["from xgboost import XGBClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZS-7DdAstqfg"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.naive_bayes import MultinomialNB"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMVXlIEQqett"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuzrvLoAqfDV"},"outputs":[],"source":["nlp = spacy.blank('en')\n","\n","nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","# explicitly adding component to pipeline\n","# (recommended - makes it more readable to tell what's going on)\n","# nlp.add_pipe(PySBDFactory(nlp))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LXGMVY3dLlgE"},"outputs":[],"source":["\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aisw4yS2EnL9"},"outputs":[],"source":["# END"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZdO8oGyOMrQk"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","text_clf_SGDClassifier = Pipeline([('vect', CountVectorizer(ngram_range=(2,4), stop_words='english',lowercase=True)),\n","                     ('tfidf', TfidfTransformer()),\n","                     ('clf', SGDClassifier()),\n","])\n","text_clf_SGDClassifier.fit(X, y)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ai2efaWnOWHv"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hH62PhR6OWDI"},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import cross_val_score, StratifiedKFold\n","\n","import plotly\n","import plotly.graph_objs as go\n","# import plotly.offline as ply\n","# plotly.offline.init_notebook_mode()\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","\n","from subprocess import check_output\n","# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RHd2hh9SMrMB"},"outputs":[],"source":["classifier_names = ['Naive Bayes', \n","                    'Decision Tree', \n","                    'Random Forest', \n","                    'Nearest Neighbors', \n","                    'Neural Network']\n","\n","classifiers = [GaussianNB(),\n","               DecisionTreeClassifier(max_depth=10),\n","               RandomForestClassifier(max_depth=10),\n","               KNeighborsClassifier(5),\n","               MLPClassifier()]\n","\n","plot_data=[]\n","\n","clf_data=zip(classifier_names, classifiers)\n","\n","for clf_name, clf in clf_data:\n","    print('Running '+clf_name)\n","    kf=StratifiedKFold(n_splits=10, shuffle=True)\n","    scores=cross_val_score(clf, X, y, cv=kf)\n","    print(scores)\n","    plot_data.append(\n","        go.Scatter(\n","            x=[i+1 for i in range(10)],\n","            y=scores,\n","            mode='lines',\n","            name=clf_name\n","        )\n","    )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RQAiwjNRGUb"},"outputs":[],"source":["plot_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLkfeBvTQZYh"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bY4o2FgRQbOu"},"outputs":[],"source":["layout = go.Layout(\n","    xaxis=dict(\n","        title='Fold no.'\n","    ),\n","    yaxis=dict(\n","        range=[np.min([i['y'] for i in plot_data]), 1],\n","        title='Accuracy'\n","    )\n",")\n","fig=go.Figure(data=plot_data, layout=layout)\n","ply.iplot(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAQ3P3lnOQpX"},"outputs":[],"source":["layout = go.Layout(\n","    xaxis=dict(\n","        title='Fold no.'\n","    ),\n","    yaxis=dict(\n","        range=[np.min([i['y'] for i in plot_data]), 1],\n","        title='Accuracy'\n","    )\n",")\n","fig=go.scatter(data=plot_data, layout=layout)\n","fig.show()\n","# ply.iplot(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dWlJS6T8PXOP"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r4o4HDc3SL8F"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn import model_selection\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","\n","plt.style.use('ggplot')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dslGduteUACr"},"outputs":[],"source":["    KNeighborsClassifier(3),\n","    SVC(kernel=\"rbf\", C=0.025, probability=True),\n","    NuSVC(probability=True),\n","    DecisionTreeClassifier(),\n","    RandomForestClassifier(),\n","    AdaBoostClassifier(),\n","    GradientBoostingClassifier(),\n","    GaussianNB(),\n","    LinearDiscriminantAnalysis(),\n","    QuadraticDiscriminantAnalysis(),\n","    # MultinomialNB(),\n","    MLPClassifier()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Spqkf9VUSL29"},"outputs":[],"source":["seed = 50\n","dataset = datasets.load_wine()\n","# X = dataset.data; y = dataset.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n","kfold = model_selection.KFold(n_splits=10, random_state=seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k08_iXqkSLxk"},"outputs":[],"source":["models = []\n","models.append(('LR', LogisticRegression()))\n","models.append(('LDA', LinearDiscriminantAnalysis()))\n","models.append(('KNN', KNeighborsClassifier()))\n","models.append(('CART', DecisionTreeClassifier()))\n","models.append(('NB', GaussianNB()))\n","models.append(('SVM', SVC()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OC2tlurpTOP4"},"outputs":[],"source":["results = []\n","names = []\n","scoring = 'accuracy'\n","for name, model in models:\n","      kfold = model_selection.KFold(n_splits=10, random_state=seed)\n","      cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n","      results.append(cv_results)\n","      names.append(name)\n","      msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n","      print(msg)\n","\n","\"\"\"\n","\n","LR: 0.862886 (0.005822)\n","LDA: 0.859657 (0.006562)\n","KNN: 0.728571 (0.008688)\n","CART: 0.710829 (0.007842)\n","NB: 0.817286 (0.007659)\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbRGacBAnDyT"},"outputs":[],"source":["models2 = []\n","\n","models2.append(('SVM', SVC()))\n","models2.append(('MultiNB', MultinomialNB()))\n","models2.append(('RFC', RandomForestClassifier()))\n","models2.append(('MultiLP', MLPClassifier()))\n","\n","models2.append(('NuSVC', NuSVC(probability=True)))\n","models2.append(('AdaBC', AdaBoostClassifier()))\n","models2.append(('GradBC', GradientBoostingClassifier()))\n","models2.append(('LinDA', LinearDiscriminantAnalysis()))\n","models2.append(('QuadDA', QuadraticDiscriminantAnalysis()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RpKeO3CcnFcj"},"outputs":[],"source":["results2 = []\n","names2 = []\n","scoring = 'accuracy'\n","for name, model in models2:\n","      kfold = model_selection.KFold(n_splits=10, random_state=seed)\n","      cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n","      results2.append(cv_results)\n","      names2.append(name)\n","      msg = \"%s: %f (%f)\" % (name2, cv_results.mean(), cv_results.std())\n","      print(msg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGom-DXdTROf"},"outputs":[],"source":["fig = plt.figure(figsize=(10,10))\n","fig.suptitle('How to compare sklearn classification algorithms')\n","ax = fig.add_subplot(111)\n","plt.boxplot(results)\n","ax.set_xticklabels(names)\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kJ6HRtBSLsX"},"outputs":[],"source":["results = []\n","names = []\n","scoring = 'accuracy'\n","for name, model in models:\n","      kfold = model_selection.KFold(n_splits=10, random_state=seed)\n","      cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n","      results.append(cv_results)\n","      names.append(name)\n","      msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n","      print(msg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWcwr4sRSLnK"},"outputs":[],"source":["fig = plt.figure(figsize=(10,10))\n","fig.suptitle('How to compare sklearn classification algorithms')\n","ax = fig.add_subplot(111)\n","plt.boxplot(results)\n","ax.set_xticklabels(names)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVJkMiprSLhG"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5o9P-qLSLb2"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IlvJf9cuSLXO"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iW_cTC3KPigo"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import StratifiedShuffleSplit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZz-jk5hPXJI"},"outputs":[],"source":["sss = StratifiedShuffleSplit(labels, 10, test_size=0.2, random_state=23)\n","\n","for train_index, test_index in sss:\n","    X_train, X_test = train.values[train_index], train.values[test_index]\n","    y_train, y_test = labels[train_index], labels[test_index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OVgc73dQAHn"},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WFleRcSOeb_"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, log_loss\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC, NuSVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","\n","classifiers = [\n","    KNeighborsClassifier(3),\n","    SVC(kernel=\"rbf\", C=0.025, probability=True),\n","    NuSVC(probability=True),\n","    DecisionTreeClassifier(),\n","    RandomForestClassifier(),\n","    AdaBoostClassifier(),\n","    GradientBoostingClassifier(),\n","    GaussianNB(),\n","    LinearDiscriminantAnalysis(),\n","    QuadraticDiscriminantAnalysis(),\n","    # MultinomialNB(),\n","    MLPClassifier()]\n","\n","# Logging for Visual Comparison\n","log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n","log = pd.DataFrame(columns=log_cols)\n","\n","for clf in classifiers:\n","    clf.fit(X_train, y_train)\n","    name = clf.__class__.__name__\n","    \n","    print(\"=\"*30)\n","    print(name)\n","    \n","    print('****Results****')\n","    train_predictions = clf.predict(X_test)\n","    acc = accuracy_score(y_test, train_predictions)\n","    print(\"Accuracy: {:.4%}\".format(acc))\n","    \n","    train_predictions = clf.predict_proba(X_test)\n","    ll = log_loss(y_test, train_predictions)\n","    print(\"Log Loss: {}\".format(ll))\n","    \n","    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n","    log = log.append(log_entry)\n","    \n","print(\"=\"*30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4QHEHqgOeW-"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCgNcZExqfDc"},"outputs":[],"source":["# WARNING: This will execute without error even if SpaCy model not loaded (silent error)\n","#          Must choose between En/Fr and consider PySBD sentence segementation as part of nlp pipeline\n","\n","def lemmatize(text):\n","    \"\"\"Perform lemmatization and stopword removal in the clean text\n","       Returns a list of lemmas\n","    \"\"\"\n","    text = ''.join([c for c in text if c.isascii()])\n","    doc = nlp(text)\n","    lemma_list = [str(tok.lemma_).lower() for tok in doc\n","                  if tok.is_alpha and tok.text.lower() not in stopwords_en]\n","    return lemma_list\n","\n","# Test\n","print(lemmatize('I was running late and decided to to stop drinking.'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQqNyV16OQk2"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eUUDZ8H6qfDf"},"outputs":[],"source":["%%time\n","\n","# Method #1: Lemmatize with Pandas apply()\n","\n","# Note: on C.Dickens' Great Expectations\n","\n","# imdb50k_df['text_lemma1'] = imdb50k_df['text_clean'].apply(lemmatize)\n","# imdb50k_df.head(3)\n","\n","\n","# Save checkpoint\n","\n","# imdb50k_df.to_csv(f'mlimdb50k_lemma3_sents_df.csv')\n","# imdb50k_df.to_csv(f'ml{supervised_db}_sents_df.csv')\n","\n","# files.download('mlimdb50k_lemma3_sents_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGISHWdKqfDn"},"outputs":[],"source":["# Method #2: Lemmatize with spacy nlp.pipe\n","\n","def lemmatize_pipe(doc):\n","    lemma_list = [str(tok.lemma_).lower() for tok in doc\n","                  if tok.is_alpha and tok.text.lower() not in stopwords_en] \n","    return lemma_list\n","\n","def preprocess_pipe(texts):\n","    preproc_pipe = []\n","    for doc in nlp.pipe(texts, batch_size=20):\n","        preproc_pipe.append(lemmatize_pipe(doc))\n","    return preproc_pipe\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwnFq1V-qfDp"},"outputs":[],"source":["%%time\n","\n","# Method #2: Lemmatize with spacy nlp.pipe\n","# Note: 22min\n","\n","# imdb50k_df['text_lemma2'] = preprocess_pipe(imdb50k_df['text_clean'])\n","# imdb50k_df.head(3)\n","\n","# imdb50k_df.rename(columns={'text_lemma3':'text_lemma'}, inplace=True)\n","# imdb50k_df.drop(columns=['text_lemma2'], inplace=True)\n","\n","# Save checkpoint\n","\n","# imdb50k_df.to_csv(f'mlimdb50k_lemma2_sents_df.csv')\n","# imdb50k_df.to_csv(f'ml{supervised_db}_sents_df.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6WkbBU9YqfDs"},"outputs":[],"source":["!pip install joblib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FNVjmuGbqfDt"},"outputs":[],"source":["# Method #3: Lemmatize with joblib parallelization\n","\n","from joblib import Parallel, delayed\n","\n","def chunker(iterable, total_length, chunksize):\n","    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n","\n","def flatten(list_of_lists):\n","    \"Flatten a list of lists to a combined list\"\n","    return [item for sublist in list_of_lists for item in sublist]\n","\n","def process_chunk(texts):\n","    preproc_pipe = []\n","    for doc in nlp.pipe(texts, batch_size=20):\n","        preproc_pipe.append(lemmatize_pipe(doc))\n","    return preproc_pipe\n","\n","def preprocess_parallel(texts, chunksize=100):\n","    executor = Parallel(n_jobs=2, backend='multiprocessing', prefer=\"processes\")\n","    do = delayed(process_chunk)\n","    tasks = (do(chunk) for chunk in chunker(texts, len(imdb50k_df), chunksize=chunksize))\n","    result = executor(tasks)\n","    return flatten(result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOWTIHg_qfDu"},"outputs":[],"source":["%%time\n","\n","# Method #3: Lemmatize with joblib parallelization\n","# NOTE: 17m13s \n","\n","# imdb50k_df['text_lemma3'] = preprocess_parallel(imdb50k_df['text_clean'], chunksize=1000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WEb4RV0guDX"},"outputs":[],"source":["\"\"\"\n","\n","if ML_Models_Arc == True:\n","  model_base = 'vader'\n","  model_name = 'vader_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n","\n","if VADER_Arc == True:\n","  # Sentiment evaluation function\n","  sid = SentimentIntensityAnalyzer()\n","\n","  # Test\n","  sid.polarity_scores('hello world'\n","\n","\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"zJT8Z7p1bE-N"},"source":["#### **Preparing Labeled Sentiment Dataset**\n","\n","* SST-2/SST-5\n","* IMDB\n","* Yelp\n","* Sentiment140"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HE1mTRBenRYh"},"outputs":[],"source":["# Upload kaggle credentials *.json file\n","\n","files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiwD6YVXnRUf"},"outputs":[],"source":["!pwd\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Vxm1Pc4GdKg"},"outputs":[],"source":["!mkdir /root/.kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K8C5ikUVngGS"},"outputs":[],"source":["!cp kaggle.json /root/.kaggle/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snpmtQv2GHzz"},"outputs":[],"source":["!ls /root/.kaggle/\n"]},{"cell_type":"markdown","metadata":{"id":"SoAbKdAPmagz"},"source":["**IMDB 50k (Movie)**\n","\n","* https://huggingface.co/datasets/imdb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pg-Ce-V5Endk"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZSTgJsroErYK"},"outputs":[],"source":["!mkdir data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjNs9ErqEu2K"},"outputs":[],"source":["%cd ./data\n","!pwd\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6DzdXynmXzZ"},"outputs":[],"source":["!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AmQ6ePHdE0Gf"},"outputs":[],"source":["!tar -xvf aclImdb_v1.tar.gz\n","!ls -altr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jU86X4wOE_Yl"},"outputs":[],"source":["!ls -altr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YX4HcIUbE_Vb"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNWAauOKE_R6"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YWjGurjNxGz_"},"outputs":[],"source":["supervised_db = 'imdb50k'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqKUuGKOHOXD"},"outputs":[],"source":["!ls -altr *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKUUzsTrHJw1"},"outputs":[],"source":["# Option A: Load in imdb_df\n","\n","imdb50k_df = pd.read_csv(\"sa_train_lemma_imdb50k.csv\")\n","imdb50k_df.head(1)\n","imdb50k_df.info()\n","\n","\"\"\"\n","imdb50k_df[\"polarity\"] = imdb50k_df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n","imdb50k_df[\"text_raw\"] = imdb50k_df[\"review\"].astype('string')\n","imdb50k_df.drop(columns=['sentiment', 'review'], inplace=True)\n","\n","supervised_db = 'imdb50k'\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uA1Rq6Y2qhwV"},"outputs":[],"source":["# Option B: Create IMDB_df\n","\n","!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKQg7q3ary2i"},"outputs":[],"source":["!ls *zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNoXUTsxryxC"},"outputs":[],"source":["!unzip imdb-dataset-of-50k-movie-reviews.zip\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6S-ZShOmsCEx"},"outputs":[],"source":["imdb50k_df = pd.read_csv(\"IMDB Dataset.csv\")\n","imdb50k_df[\"polarity\"] = imdb50k_df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n","imdb50k_df[\"text_raw\"] = imdb50k_df[\"review\"].astype('string')\n","imdb50k_df.drop(columns=['sentiment', 'review'], inplace=True)\n","\n","supervised_db = 'imdb50k'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EBFpI0KusB_h"},"outputs":[],"source":["imdb50k_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"874pXJ8jxDH9"},"outputs":[],"source":["# Remove non-alphanumeric chacters\n","# imdb50k_df['text_lower'] = imdb50k_df['text_raw']\n","\n","pattern = re.compile(r\"[A-Za-z0-9\\-]{3,50}\")\n","imdb50k_df['text_clean'] = imdb50k_df['text_raw'].str.lower().str.strip().str.findall(pattern).str.join(' ')\n","imdb50k_df.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0hlgbsUzvbg"},"outputs":[],"source":["stopwords_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gk1M_iJgdgCG"},"outputs":[],"source":["nlp = spacy.blank('en')\n","\n","nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","# explicitly adding component to pipeline\n","# (recommended - makes it more readable to tell what's going on)\n","# nlp.add_pipe(PySBDFactory(nlp))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTTwI1MsxC4V"},"outputs":[],"source":["# WARNING: This will execute without error even if SpaCy model not loaded (silent error)\n","#          Must choose between En/Fr and consider PySBD sentence segementation as part of nlp pipeline\n","\n","def lemmatize(text):\n","    \"\"\"Perform lemmatization and stopword removal in the clean text\n","       Returns a list of lemmas\n","    \"\"\"\n","    doc = nlp(text)\n","    lemma_list = [str(tok.lemma_).lower() for tok in doc\n","                  if tok.is_alpha and tok.text.lower() not in stopwords_en]\n","    return lemma_list\n","\n","# Test\n","print(lemmatize('I was running late and decided to to stop drinking.'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eKCtw6mzCRv"},"outputs":[],"source":["%%time\n","\n","# Method #1: Lemmatize with Pandas apply()\n","\n","# Note: on C.Dickens' Great Expectations\n","\n","imdb50k_df['text_lemma'] = imdb50k_df['text_clean'].apply(lemmatize)\n","imdb50k_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rb1PTFdB377O"},"outputs":[],"source":["# Save checkpoint\n","\n","imdb50k_df.to_csv(f'mlimdb50k_lemma_sents_df.csv')\n","# imdb50k_df.to_csv(f'ml{supervised_db}_sents_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dEWc92Z7CnE3"},"outputs":[],"source":["!ls -altr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P0wRacWECqgY"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2K83u01NC8Ft"},"outputs":[],"source":["files.download('mlimdb50k_lemma_sents_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkc5w9b6DCJJ"},"outputs":[],"source":["files.download('ml_sents_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JefUrhgQDMiQ"},"outputs":[],"source":["!rm mlimdb50k_lemma2_sents_df.csv\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AA5zr9xr0Pmf"},"outputs":[],"source":["# Method #2: Lemmatize with spacy nlp.pipe\n","\n","def lemmatize_pipe(doc):\n","    lemma_list = [str(tok.lemma_).lower() for tok in doc\n","                  if tok.is_alpha and tok.text.lower() not in stopwords_en] \n","    return lemma_list\n","\n","def preprocess_pipe(texts):\n","    preproc_pipe = []\n","    for doc in nlp.pipe(texts, batch_size=20):\n","        preproc_pipe.append(lemmatize_pipe(doc))\n","    return preproc_pipe\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYZF5fDt0Pdj"},"outputs":[],"source":["%%time\n","\n","# Method #2: Lemmatize with spacy nlp.pipe\n","# Note: 22min\n","\n","imdb50k_df['text_lemma2'] = preprocess_pipe(imdb50k_df['text_clean'])\n","imdb50k_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IvyLp_DEA4q"},"outputs":[],"source":["imdb50k_df.rename(columns={'text_lemma3':'text_lemma'}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"veKXUZgDD8Ae"},"outputs":[],"source":["imdb50k_df.drop(columns=['text_lemma2'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwiRAVYJD3bM"},"outputs":[],"source":["imdb50k_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sVgGKfjIEVC3"},"outputs":[],"source":["imdb50k_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZgnzBM431XH"},"outputs":[],"source":["# Save checkpoint\n","\n","imdb50k_df.to_csv(f'mlimdb50k_lemma2_sents_df.csv')\n","# imdb50k_df.to_csv(f'ml{supervised_db}_sents_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5t3Nkmo01Gyy"},"outputs":[],"source":["!pip install joblib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEQ2iZOS0vOo"},"outputs":[],"source":["# Method #3: Lemmatize with joblib parallelization\n","\n","from joblib import Parallel, delayed\n","\n","def chunker(iterable, total_length, chunksize):\n","    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n","\n","def flatten(list_of_lists):\n","    \"Flatten a list of lists to a combined list\"\n","    return [item for sublist in list_of_lists for item in sublist]\n","\n","def process_chunk(texts):\n","    preproc_pipe = []\n","    for doc in nlp.pipe(texts, batch_size=20):\n","        preproc_pipe.append(lemmatize_pipe(doc))\n","    return preproc_pipe\n","\n","def preprocess_parallel(texts, chunksize=100):\n","    executor = Parallel(n_jobs=2, backend='multiprocessing', prefer=\"processes\")\n","    do = delayed(process_chunk)\n","    tasks = (do(chunk) for chunk in chunker(texts, len(imdb50k_df), chunksize=chunksize))\n","    result = executor(tasks)\n","    return flatten(result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnQ55as-0vEs"},"outputs":[],"source":["%%time\n","\n","# Method #3: Lemmatize with joblib parallelization\n","# NOTE: 17m13s \n","\n","imdb50k_df['text_lemma'] = preprocess_parallel(imdb50k_df['text_clean'], chunksize=1000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2w6i8dv0u_c"},"outputs":[],"source":["# Save checkpoint\n","\n","imdb50k_df.to_csv(f'sa_train_lemma_imdb50k.csv')\n","# imdb50k_df.to_csv(f'ml{supervised_db}_sents_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcqEhl0UCOaX"},"outputs":[],"source":["imdb50k_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UaZQ8pTtFIvI"},"outputs":[],"source":["print(imdb50k_df.iloc[0]['text_lemma'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFuZBs9KFb4E"},"outputs":[],"source":["def list2str(str_ls):\n","  '''\n","  Given a list of string\n","  Return a all strings concatenated, separated by ' '\n","  '''\n","  joined_str = ' '.join(str_ls)\n","\n","  return joined_str\n","\n","imdb50k_df['text_lemma'] = imdb50k_df['text_lemma'].apply(list2str)\n","# imdb50k_df.drop(columns=['text_lemma'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IynLUxIJMUIJ"},"outputs":[],"source":["imdb50k_df.drop(columns=['text_lemma3'],inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlC9MdSNfBVb"},"outputs":[],"source":["imdb50k_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Y2Qrq1bGf7O"},"outputs":[],"source":["# imdb50k_df.rename(columns={'text_lemma':'text_clean'}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifyd8x6OtFo1"},"outputs":[],"source":["# def clean_stemlemma_text(text, stem_fl =False, lemma_fl=True, punct_fl=True, stopword_ls=stopwords_en):\n","# TODO: pandas DataFrame.Series.apply(curried function)\n","\n","# NOTE: SST2: >25m\n","# sst2_sents_df['text_lemma'] = sst2_sents_df['text_raw'].apply(clean_stemlemma_text)\n","\n","# imdb50k_df['text_lower'] = imdb50k_df['text_raw'].str.strip().str.lower()\n","# imdb50k_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDzfd0okE_Yr"},"outputs":[],"source":["# Vectorize IMDB Training dataset with TF-IDF\n","\n","from nltk.corpus import stopwords\n","stopwords_en = stopwords.words('english') # + stopwords.words('french')\n","\n","vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=stopwords_en, max_features=1000)\n","vectors = vectorizer.fit_transform(imdb50k_df.text_clean)\n","words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n","words_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzqocELYE_Yt"},"outputs":[],"source":["# Separate text from labels\n","\n","X = words_df\n","y = imdb50k_df.polarity\n","X.shape\n","print('\\n')\n","y.shape\n","type(y[0])"]},{"cell_type":"markdown","metadata":{"id":"zVg2Vh1QmYJf"},"source":["**SST-5 (Text)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4l9EPgVmqzU"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"HSa0vYLfmg3i"},"source":["**SST-2 (Text)**\n","\n","* https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html (Accelerate with SpaCy pipelines/joblib)"]},{"cell_type":"markdown","metadata":{"id":"RPoZ3onS8u-1"},"source":["**Retrieve via PyTorch TorchText**\n","\n","* https://github.com/shayneobrien/sentiment-classification/blob/master/notebooks/02-naive-bayes-unigram.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXJyGQ-9Eq7Q"},"outputs":[],"source":["supervised_db = 'sst2'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FheAchd8uxd"},"outputs":[],"source":["import torchtext\n","import numpy as np\n","from sklearn.naive_bayes import MultinomialNB\n","from torchtext.vocab import Vectors\n","from tqdm import tqdm_notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZdWe3yQ9EHn"},"outputs":[],"source":["text = torchtext.data.Field(include_lengths = False)\n","label = torchtext.data.Field(sequential=False)\n","train, val, test = torchtext.datasets.SST.splits(text, label, filter_pred=lambda ex: ex.label != 'neutral')\n","text.build_vocab(train)\n","label.build_vocab(train)\n","train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits((train, val, test), batch_size=10, device=-1, repeat = False)\n","url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n","text.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5UhDXd2N9ECt"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"QZrHoKfh2vUC"},"source":["**Retrieve from Kaggle Datasets**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGow41Oz2qpv"},"outputs":[],"source":["!kaggle datasets download -d atulanandjha/stanford-sentiment-treebank-v2-sst2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLs5cDNH2qlM"},"outputs":[],"source":["!unzip stanford-sentiment-treebank-v2-sst2.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OhLqfmQh-xLj"},"outputs":[],"source":["!cat SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/README.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcPO1OjQ2qgx"},"outputs":[],"source":["!ls SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nluKanu2qcU"},"outputs":[],"source":["!head -n 5 ./SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/datasetSentences.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEqbDjgN2qXq"},"outputs":[],"source":["!head -n 5 ./SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/sentiment_labels.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BbomRqan2qTD"},"outputs":[],"source":["sst2_sents_filename = './SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/dictionary.txt'\n","sst2_sents_df = pd.read_csv(sst2_sents_filename, sep='|', header=None) \n","sst2_sents_df.columns = ['text_raw','phrase_id']\n","sst2_sents_df['text_raw'] = sst2_sents_df['text_raw'].astype('string')\n","sst2_sents_df.head(20)\n","sst2_sents_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EP69q79JdZSb"},"outputs":[],"source":["# def clean_stemlemma_text(text, stem_fl =False, lemma_fl=True, punct_fl=True, stopword_ls=stopwords_en):\n","# TODO: pandas DataFrame.Series.apply(curried function)\n","\n","# NOTE: SST2: >25m\n","# sst2_sents_df['text_lemma'] = sst2_sents_df['text_raw'].apply(clean_stemlemma_text)\n","\n","sst2_sents_df['text_lower'] = sst2_sents_df['text_raw'].str.strip().str.lower()\n","sst2_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtswvhMeaEyV"},"outputs":[],"source":["# sst2_sents_df['text_clean'] = sst2_sents_df['text_raw'].apply(clean_stemlemma_text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VxEgiVXGeS_z"},"outputs":[],"source":["# sst2_sents_df['text_lower'] = sst2_sents_df['text_raw'].apply(lambda x: lower(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQ8dmvpR2qNz"},"outputs":[],"source":["sst2_labels_filename = './SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/sentiment_labels.txt'\n","sst2_labels_df = pd.read_csv(sst2_labels_filename, sep='|') \n","sst2_labels_df.columns = ['phrase_id','polarity_fl']\n","sst2_labels_df.head(20)\n","sst2_labels_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sZztnmG_xEF"},"outputs":[],"source":["sst2_df = pd.concat([sst2_sents_df.set_index('phrase_id'),sst2_labels_df.set_index('phrase_id')], axis=1, join='inner')\n","sst2_df.head()\n","sst2_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsOcAE-6Cbid"},"outputs":[],"source":["def polarity_float2int(val_fl):\n","  '''\n","  Given a float between 0.0 and 1.0\n","  Return an integer between 0-5 mapped to every 0.2 interval\n","  '''\n","  val_int = 0\n","  if (val_fl < 0.2):\n","    val_int = 0\n","  elif (0.2 <= val_fl < 0.4):\n","    val_int = 1\n","  elif (0.4 <= val_fl < 0.6):\n","    val_int = 2\n","  elif (0.6 <= val_fl < 0.8):\n","    val_int = 3\n","  elif (0.8 <= val_fl <= 1.0):\n","    val_int = 4\n","  else:\n","    print(f'ERROR: polarity value must be [0.0-1.0] but was set to: {val_fl}')\n","    val_int = -99\n","\n","  return val_int\n","\n","# Test\n","polarity_float2int(0.55)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTyFUTZyDWbD"},"outputs":[],"source":["sst2_df['polarity'] = sst2_df['polarity_fl'].apply(lambda x: polarity_float2int(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VuBCKBbAEDeB"},"outputs":[],"source":["sst2_df.head(10)\n","sst2_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQa4nAmb6H6X"},"outputs":[],"source":["sst2_df.shape\n","sst2_df[sst2_df.polarity.isna()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7y_bnL4Dglpk"},"outputs":[],"source":["\"\"\"\n","\n","from sklearn.feature_extraction import text\n","# from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","stopwords_custom = text.ENGLISH_STOP_WORDS.union([\"bazinga\"])\n","\n","vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words=my_stop_words)\n","\n","X = vectorizer.fit_transform([\"this is an apple.\",\"this is a book.\"])\n","\n","idf_values = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n","\n","# printing the tfidf vectors\n","print(X)\n","\n","# printing the vocabulary\n","print(vectorizer.vocabulary_)\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xw8AN5wghXVT"},"outputs":[],"source":["from sklearn.feature_extraction import text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ETCHx_Z5hXI4"},"outputs":[],"source":["stopwords_custom = text.ENGLISH_STOP_WORDS.union([\"bazinga\"])\n","len(stopwords_custom)\n","print('\\n')\n","type(stopwords_custom)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0uXkt1Ha5-3d"},"outputs":[],"source":["vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words=stopwords_custom, max_features=1000)\n","vectors = vectorizer.fit_transform(sst2_df.text_lower)\n","words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n","words_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DML2PyO5-3e"},"outputs":[],"source":["X = words_df\n","y = sst2_df.polarity\n","X.shape\n","print('\\n')\n","y.shape\n","type(y[0])"]},{"cell_type":"markdown","metadata":{"id":"HGzl5dKk2rST"},"source":["**Retrieve from Huggingface Datasets**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSbf4OyPzkgl"},"outputs":[],"source":["# install datasets\n","!pip install datasets\n","\n","# Make sure that we have a recent version of pyarrow in the session before we continue - otherwise reboot Colab to activate it\n","import pyarrow\n","if int(pyarrow.__version__.split('.')[1]) < 16 and int(pyarrow.__version__.split('.')[0]) == 0:\n","    import os\n","    os.kill(os.getpid(), 9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gH_3O8oEzmB5"},"outputs":[],"source":["# Let's import the library. We typically only need at most four methods:\n","from datasets import list_datasets, list_metrics, load_dataset, load_metric\n","\n","from pprint import pprint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKhOr-XAzl7K"},"outputs":[],"source":["# Currently available datasets and metrics\n","datasets = list_datasets()\n","metrics = list_metrics()\n","\n","print(f\"🤩 Currently {len(datasets)} datasets are available on the hub:\")\n","pprint(datasets, compact=True)\n","print(f\"🤩 Currently {len(metrics)} metrics are available on the hub:\")\n","pprint(metrics, compact=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WEl8TzImj-P"},"outputs":[],"source":["# You can access various attributes of the datasets before downloading them\n","sst_dataset = list_datasets(with_details=True)[datasets.index('sst')]\n","\n","pprint(sst_dataset.__dict__)  # It's a simple python dataclass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UqTkQELdmqgs"},"outputs":[],"source":["# Downloading and loading a dataset\n","dataset = load_dataset('sst', split='validation[:10%]')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjIO3bR_0XBG"},"outputs":[],"source":["# Informations on the dataset (description, citation, size, splits, format...)\n","# are provided in `dataset.info` (a simple python dataclass) and also as direct attributes in the dataset object\n","pprint(dataset.info.__dict__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfG9DIZe0W8r"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBchwJCdG-4z"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzGBqKZmG-rt"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGa8FGa9E-7W"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bA_dToBAE-2B"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aG2_kq-cE-m8"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZP_L_4CuTw-"},"outputs":[],"source":["# Create a version of text that is lemmatized and has both stopwords and punctuation removed \n","# clean_stemlemma_text(text, stem_fl =False, lemma_fl=True, punct_fl=True, stopword_ls=stopwords_en):\n","\n","# TODO: Accelerate with joblib: https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html\n","\n","# NOTE: Xm \n","\n","imdb50k_df['text_lemma'] = imdb50k_df['text_raw'].apply(lambda x : clean_stemlemma_text(x))\n","imdb50k_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04M3cGefwKyl"},"outputs":[],"source":["# reload spacy with minimal components for speed\n","\n","nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n","nlp.add_pipe(nlp.create_pipe('sentencizer'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iomWpSsVwVOF"},"outputs":[],"source":["# use stopword SETS O(1) < LISTS O(n) for performance\n","\n","stopword_custom_set = set(stopword_custom_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20yaHuU5w11S"},"outputs":[],"source":["def cleaner(df):\n","    \"Extract relevant text from DataFrame using a regex\"\n","    # Regex pattern for only alphanumeric, hyphenated text with 3 or more chars\n","    pattern = re.compile(r\"[A-Za-z0-9\\-]{3,50}\")\n","    df['clean'] = df['content'].str.findall(pattern).str.join(' ')\n","    if limit > 0:\n","        return df.iloc[:limit, :].copy()\n","    else:\n","        return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URbYB3pYw1wD"},"outputs":[],"source":["df_preproc = cleaner(df)\n","df_preproc.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SikvRyYLw1r3"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OmOg71lwKtI"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aj-lYZ3GwKjF"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXrmI1r3sOiT"},"outputs":[],"source":["imdb50k_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFSEutU5th5k"},"outputs":[],"source":["stopwords_custom = text.ENGLISH_STOP_WORDS.union([\"bazinga\"])\n","len(stopwords_custom)\n","print('\\n')\n","type(stopwords_custom)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZW3feRLth5l"},"outputs":[],"source":["vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words=stopwords_custom, max_features=1000)\n","vectors = vectorizer.fit_transform(imdb50k_df.text_lower)\n","words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n","words_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMu2M9OntyGk"},"outputs":[],"source":["imdb50k_df.shape\n","print('\\n')\n","imdb50k_df.polarity.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T0_3XDOxtyGm"},"outputs":[],"source":["vectorizer = TfidfVectorizer(max_features=1000)\n","vectors = vectorizer.fit_transform(imdb50k_df.text)\n","words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n","words_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYHtomVHtyGn"},"outputs":[],"source":["X = words_df\n","y = imdb50k_df.polarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YzuarHPWtxzC"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kpd0U4BNtxu7"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlj9bxfOtxoy"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"7hBT6n_jmcr2"},"source":["**Yelp (Restaurant)**\n","\n","* https://www.kaggle.com/suzanaiacob/sentiment-analysis-of-the-yelp-reviews-data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXRwivGimXsV"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQMPluV-mXlD"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"meNQhTTkeIiB"},"source":["**Sentiment140 (Tweets)**\n","\n","* http://help.sentiment140.com/for-students/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wxns2Ni1Yb4Q"},"outputs":[],"source":["# Make data directory if it doesn't exist\n","!mkdir -p data\n","!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/investigating-sentiment-analysis/data/sentiment140-subset.csv.zip -P data\n","!unzip -n -d data data/sentiment140-subset.csv.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_-UTBc6bSIy"},"outputs":[],"source":["sentiment140_df = pd.read_csv(\"data/sentiment140-subset.csv\", nrows=30000)\n","sentiment140_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01BWNOW7bcz0"},"outputs":[],"source":["sentiment140_df.shape\n","print('\\n')\n","sentiment140_df.polarity.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Pyudw8Ybquc"},"outputs":[],"source":["vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=stopwords_en, max_features=1000)\n","vectors = vectorizer.fit_transform(sentiment140_df.text)\n","\n","vectors_np = vectors.toarray()\n","\n","words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n","words_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-C97ACebr3A"},"outputs":[],"source":["X = words_df\n","y = sentiment140_df.polarity"]},{"cell_type":"markdown","metadata":{"id":"vNqksop2eygX"},"source":["#### **Training**\n","\n","Tuning Hyperparameters: Grid and Random Search\n","\n","* https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n","\n","Acceleration:\n","\n","* https://rapids.ai/start.html \n","* https://colab.research.google.com/drive/1rY7Ln6rEE1pOlfSHCYOVaqt8OvDO35J0#forceEdit=true&offline=true&sandboxMode=true (Rapids)\n","* https://github.com/lebedov/scikit-cuda \n","* https://github.com/skorch-dev/skorch (Scikit wrap of PyTorch)"]},{"cell_type":"markdown","metadata":{"id":"_S6_aU98Ckmy"},"source":["**AutoML: MS flaml**\n","\n","* https://github.com/microsoft/FLAML/blob/main/notebook/flaml_automl.ipynb\n","* https://www.youtube.com/watch?v=bJfDJhe-O-c"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPS7SHVJJ0Bs"},"outputs":[],"source":["# !pip install flaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8MSHAEn2qzMt"},"outputs":[],"source":["!pip install flaml[notebook]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5QiH_9pUrRaK"},"outputs":[],"source":["!mkdir test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmQz-X7B96Dr"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7bnmcENq2Wj"},"outputs":[],"source":["# Split into Training and Testing Dataset\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJiEPNRSEg2g"},"outputs":[],"source":["from flaml import AutoML\n","from sklearn.datasets import load_iris\n","# Initialize an AutoML instance\n","automl = AutoML()\n","# Specify automl goal and constraint\n","automl_settings = {\n","    \"time_budget\": 10,  # in seconds\n","    \"metric\": 'accuracy',\n","    \"task\": 'classification',\n","    \"log_file_name\": \"test/iris.log\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sr3K_zFhq2bD"},"outputs":[],"source":["\n","X_train, y_train = load_iris(return_X_y=True)\n","# Train with labeled input data\n","automl.fit(X_train=X_train, y_train=y_train,\n","           **automl_settings)\n","# Predict\n","print(automl.predict_proba(X_train))\n","# Export the best model\n","print(automl.model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8pqoDcO5m2b"},"outputs":[],"source":["y_train.hist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qt4uDl-fsWlC"},"outputs":[],"source":["settings = {\n","    \"time_budget\": 1000,  # total running time in seconds\n","    \"metric\": 'accuracy',  # primary metrics can be chosen from: ['accuracy','roc_auc','f1','log_loss','mae','mse','r2']\n","    \"task\": 'classification',  # task type    \n","    \"log_file_name\": 'airlines_experiment.log',  # flaml log file\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Okke1rN1wvbM"},"outputs":[],"source":["'''The main flaml automl API'''\n","automl.fit(X_train=X_train, y_train=y_train, **settings)\n","\n","\"\"\"\n","\n","[flaml.automl: 08-18 05:53:11] {1461} INFO - selected model: LGBMClassifier(colsample_bytree=0.7733707792852584,\n","               learning_rate=0.11190988982157068, max_bin=128,\n","               min_child_samples=62, n_estimators=701, num_leaves=12,\n","               objective='binary', reg_alpha=0.001291764523034099,\n","               reg_lambda=0.5058442385321611, verbose=-1)\n","[flaml.automl: 08-18 05:53:11] {1184} INFO - fit succeeded\n","[flaml.automl: 08-18 05:53:11] {1185} INFO - Time taken to find the best model: 567.884330034256\n","\n","\n","[flaml.automl: 08-18 05:31:22] {1411} INFO -  at 291.7s,\tbest extra_tree's error=0.2469,\tbest lgbm's error=0.1504\n","[flaml.automl: 08-18 05:31:27] {1438} INFO - retrain extra_tree for 5.0s\n","[flaml.automl: 08-18 05:31:27] {1253} INFO - iteration 46, current learner lrl1\n","No low-cost partial config given to the search algorithm. For cost-frugal search, consider providing low-cost values for cost-related hps via 'low_cost_partial_config'.\n","[flaml.automl: 08-18 05:31:41] {1411} INFO -  at 310.7s,\tbest lrl1's error=0.1454,\tbest lrl1's error=0.1454\n","[flaml.automl: 08-18 05:31:41] {1461} INFO - selected model: LogisticRegression(n_jobs=-1, penalty='l1', solver='saga')\n","[flaml.automl: 08-18 05:31:41] {1184} INFO - fit succeeded\n","[flaml.automl: 08-18 05:31:41] {1185} INFO - Time taken to find the best model: 310.6792550086975\n","[flaml.automl: 08-18 05:31:41] {1191} WARNING - Time taken to find the best model is 104% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaOrLHPMsWZW"},"outputs":[],"source":["''' retrieve best config and best learner'''\n","print('Best ML leaner:', automl.best_estimator)\n","print('Best hyperparmeter config:', automl.best_config)\n","print('Best accuracy on validation data: {0:.4g}'.format(1-automl.best_loss))\n","print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))\n","\n","\"\"\"\n","\n","retrieve best config and best learner\n","\n","Best ML leaner: lgbm\n","Best hyperparmeter config: {'n_estimators': 701, 'num_leaves': 12, 'min_child_samples': 62, 'learning_rate': 0.11190988982157068, 'subsample': 1.0, 'log_max_bin': 8, 'colsample_bytree': 0.7733707792852584, 'reg_alpha': 0.001291764523034099, 'reg_lambda': 0.5058442385321611}\n","Best accuracy on validation data: 0.8516\n","Training duration of best run: 14.68 s\n","\n","Best ML leaner: lrl1\n","Best hyperparmeter config: {'C': 1.0}\n","Best accuracy on validation data: 0.8546\n","Training duration of best run: 13.9 s\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3akuUrlou1qO"},"outputs":[],"source":["automl.model.estimator\n","\n","\"\"\"\n","\n","LogisticRegression(n_jobs=-1, penalty='l1', solver='saga')\n","\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9k2tIm9sWT_"},"outputs":[],"source":["''' compute predictions of testing dataset ''' \n","y_pred = automl.predict(X_test)\n","print('Predicted labels', y_pred)\n","print('True labels', y_test)\n","y_pred_proba = automl.predict_proba(X_test)[:,1]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7nWCA3cwFro"},"outputs":[],"source":["from sklearn.metrics import matthews_corrcoef\n","\n","mcc_y_test_predict = matthews_corrcoef(y_test, y_pred)\n","mcc_y_test_predict\n","\n","\"\"\"\n","\n","0.7274299237200138\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"582R-hBp7M3Z"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"XHXqdHVF-QD-"},"source":["**AutoML: Auto-Sklearn**\n","\n","* https://www.youtube.com/watch?v=CF-GZ9tK_Ik\n","* https://github.com/GauravSahani1417/Kaggle-Datasets/blob/master/Heart_failure_prediction_using_Auto_Sklearn_%F0%9F%A9%BA.ipynb "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mR64ZI27Mud"},"outputs":[],"source":["!pip install auto-sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAp6SYZO7hZw"},"outputs":[],"source":["!pip install scipy>=1.7.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVnzpkq77nxZ"},"outputs":[],"source":["import scipy\n","print(scipy.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WXzXnPs7MnL"},"outputs":[],"source":["from autosklearn.classification import AutoSklearnClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mMzgBVt7Wuj"},"outputs":[],"source":["clf = AutoSklearnClassifier(time_left_for_this_task=1000, \n","                              per_run_time_limit=9, \n","                              ensemble_size=1, \n","                              initial_configurations_via_metalearning=0)\n","# Init training\n","clf.fit(X_train, y_train)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fodvu7kc7WpJ"},"outputs":[],"source":["clf.score(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gsuzebjd7Wji"},"outputs":[],"source":["clf.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uhxWnZ3L7WdB"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, accuracy_score\n","y_pred = clf.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zysNiGqP-tif"},"outputs":[],"source":["conf_matrix = confusion_matrix(y_pred, y_test)\n","sns.heatmap(conf_matrix, annot=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lscmJ8ID-tca"},"outputs":[],"source":["#Performance Measures\n","tn = conf_matrix[0,0]\n","fp = conf_matrix[0,1]\n","tp = conf_matrix[1,1]\n","fn = conf_matrix[1,0]\n","\n","total = tn + fp + tp + fn\n","real_positive = tp + fn\n","real_negative = tn + fp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdadaB_7-3iD"},"outputs":[],"source":["accuracy  = (tp + tn) / total # Accuracy Rate\n","precision = tp / (tp + fp) # Positive Predictive Value\n","recall    = tp / (tp + fn) # True Positive Rate\n","f1score  = 2 * precision * recall / (precision + recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tDDtHX6-3dh"},"outputs":[],"source":["print(f'Accuracy    : {accuracy}')\n","print(f'Precision   : {precision}')\n","print(f'Recall      : {recall}')\n","print(f'F1 score    : {f1score}')"]},{"cell_type":"markdown","metadata":{"id":"I1ZEyU-sLVhk"},"source":["**AutoML: HyperOpt-Sklearn**\n","\n","* https://github.com/hyperopt/hyperopt-sklearn\n","* https://github.com/hyperopt/hyperopt-sklearn/blob/master/notebooks/Demo-Iris.ipynb \n","\n","svc\n","svc_linear\n","svc_rbf\n","svc_poly\n","svc_sigmoid\n","liblinear_svc\n","\n","knn\n","\n","ada_boost\n","gradient_boosting\n","\n","random_forest\n","extra_trees\n","decision_tree\n","\n","sgd\n","\n","xgboost_classification\n","\n","multinomial_nb\n","gaussian_nb\n","\n","passive_aggressive\n","\n","linear_discriminant_analysis\n","quadratic_discriminant_analysis\n","\n","one_vs_rest\n","one_vs_one\n","output_code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVs_kVZF--zj"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"nfDks6td-_Kv"},"source":["**Supervised ML Models**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAqHgshvg-tB"},"outputs":[],"source":["%%time\n","\n","# Create and train a logistic regression\n","# SST2: 1800s/1000it, 20s/10it (default, max_iter=1000)\n","# IMDB50k: 1000it, 1.14s\n","\n","logreg = LogisticRegression(C=1e9, solver='lbfgs', max_iter=10)\n","logreg.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4dnGSxAAGfe"},"outputs":[],"source":["%%time\n","\n","# NOTE: 964s\n","\n","# example of grid searching key hyperparametres for logistic regression\n","# https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n","\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","\n","# define dataset\n","# X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n","\n","# define models and parameters\n","model = LogisticRegression()\n","solvers = ['newton-cg', 'lbfgs', 'liblinear']\n","penalty = ['l2']\n","c_values = [100, 10, 1.0, 0.1, 0.01]\n","\n","# define grid search\n","grid = dict(solver=solvers,penalty=penalty,C=c_values)\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n","grid_result = grid_search.fit(X, y)\n","\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","\n","\n","\"\"\"\n","\n","0.858347 (0.004245) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n","0.858367 (0.004238) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n","0.858360 (0.004251) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n","0.835080 (0.004164) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n","0.835093 (0.004186) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n","0.835287 (0.004264) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n","CPU times: user 27.2 s, sys: 9.56 s, total: 36.8 s\n","Wall time: 16min 4s\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-5yEd1Zg-tB"},"outputs":[],"source":["%%time\n","\n","# Create and train a random forest classifier\n","# SST2: 27m-1500s/50n_est  36s/5n_est (default n_estimators=50)\n","# IMDB50k: 46s (n_est=50)\n","\n","# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 (Hyperparm search)\n","# https://github.com/WillKoehrsen/Machine-Learning-Projects/tree/master/random_forest_explained (github/Jupyter)\n","\n","dforest = RandomForestClassifier(n_estimators=50)\n","dforest.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7PAaPzLAxDN"},"outputs":[],"source":["%%time\n","\n","# NOTE: 9381s\n","\n","# example of grid searching key hyperparameters for RandomForestClassifier\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# define dataset\n","# X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n","\n","# define models and parameters\n","model = RandomForestClassifier()\n","n_estimators = [10, 100, 1000]\n","max_features = ['sqrt', 'log2']\n","\n","# define grid search\n","grid = dict(n_estimators=n_estimators,max_features=max_features)\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n","grid_result = grid_search.fit(X, y)\n","\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","\n","\"\"\"\n","\n","Best: 0.852973 using {'max_features': 'log2', 'n_estimators': 1000}\n","0.785433 (0.005143) with: {'max_features': 'sqrt', 'n_estimators': 10}\n","0.834120 (0.005947) with: {'max_features': 'sqrt', 'n_estimators': 100}\n","0.839340 (0.005137) with: {'max_features': 'sqrt', 'n_estimators': 1000}\n","0.762820 (0.005924) with: {'max_features': 'log2', 'n_estimators': 10}\n","0.842607 (0.005467) with: {'max_features': 'log2', 'n_estimators': 100}\n","0.852973 (0.004983) with: {'max_features': 'log2', 'n_estimators': 1000}\n","CPU times: user 4min 50s, sys: 11.5 s, total: 5min 2s\n","Wall time: 2h 36min 21s\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ve3gAwnPBVNu"},"outputs":[],"source":["# Simple Bagged Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qHg_b15BsBC"},"outputs":[],"source":["%%time\n","\n","# NOTE: >5hrs\n","\n","# example of grid searching key hyperparameters for BaggingClassifier\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import BaggingClassifier\n","\n","# define dataset\n","# X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n","\n","# define models and parameters\n","model = BaggingClassifier()\n","n_estimators = [10, 100, 1000]\n","\n","# define grid search\n","grid = dict(n_estimators=n_estimators)\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n","grid_result = grid_search.fit(X, y)\n","\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-4afRwJCBQxZ"},"outputs":[],"source":["# Simple kNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03j_BjeXBF0G"},"outputs":[],"source":["%%time\n","\n","\n","\n","\n","# example of grid searching key hyperparametres for KNeighborsClassifier\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# define dataset\n","#X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n","\n","# define models and parameters\n","model = KNeighborsClassifier()\n","n_neighbors = range(1, 21, 2)\n","weights = ['uniform', 'distance']\n","metric = ['euclidean', 'manhattan', 'minkowski']\n","\n","# define grid search\n","grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n","grid_result = grid_search.fit(X, y)\n","\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YAErlAgVBaNf"},"outputs":[],"source":["# Simple Ridge Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ko4RsndnA3aQ"},"outputs":[],"source":["%%time\n","\n","# example of grid searching key hyperparametres for ridge classifier\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import RidgeClassifier\n","\n","# define dataset\n","# X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n","\n","# define models and parameters\n","model = RidgeClassifier()\n","alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n","\n","# define grid search\n","grid = dict(alpha=alpha)\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n","grid_result = grid_search.fit(X, y)\n","\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBue6PpPg-tC"},"outputs":[],"source":["%%time\n","# Create and train a linear support vector classifier (LinearSVC)\n","# SST2: 20s \n","# IMDB50k: 1.7s\n","\n","svc = LinearSVC()\n","svc.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fwgE5eJBfnS"},"outputs":[],"source":["%%time\n","\n","# example of grid searching key hyperparametres for SVC\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","\n","# define dataset\n","# X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n","\n","# define model and parameters\n","model = SVC()\n","kernel = ['poly', 'rbf', 'sigmoid']\n","C = [50, 10, 1.0, 0.1, 0.01]\n","gamma = ['scale']\n","\n","# define grid search\n","grid = dict(kernel=kernel,C=C,gamma=gamma)\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n","grid_result = grid_search.fit(X, y)\n","\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptD0DvanCGJs"},"outputs":[],"source":["# Simple Stochastic Gradient Boosting   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FL1uhFpUCGBX"},"outputs":[],"source":["%%time\n","\n","# example of grid searching key hyperparameters for GradientBoostingClassifier\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","# define dataset\n","# X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n","\n","# define models and parameters\n","model = GradientBoostingClassifier()\n","n_estimators = [10, 100, 1000]\n","learning_rate = [0.001, 0.01, 0.1]\n","subsample = [0.5, 0.7, 1.0]\n","max_depth = [3, 7, 9]\n","\n","# define grid search\n","grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n","grid_result = grid_search.fit(X, y)\n","\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k55atR6-JNFo"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQb2_OgGg-tD"},"outputs":[],"source":["%%time\n","\n","# Create and train a multinomial naive bayes classifier (MultinomialNB)\n","# SST2: 1.3s \n","# IMDB50k: 0.24s\n","\n","multinb = MultinomialNB()\n","multinb.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rlXy8nPhJ-nC"},"outputs":[],"source":["\"\"\"\n","\n","https://stats.stackexchange.com/questions/299842/why-grid-search-is-not-performed-for-naive-bayes-classifier/299843\n","\n","Just a thought, I've used NB almost exclusively for NLP tasks. Consider sentiment analysis, the distribution over the word \"fast\" might be {pos:0.8, neg:0.2}. this design is often called \"unigram\" (just one word), however, n-grams (2 or more) will improve accuracy. In general, perplexity is used to evaluate if added model complexity is worth the effort. All that to say, grid search might be useful, if you were trying to optimize the number of n-grams (though this would no longer be \"Naive Bayes\" but rather a conceptual extension.) But outside of NLP, there might be no use for this idea. – jbuddy_13 Jan 2 at 16:41 \n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0Mh-bxNJGED"},"outputs":[],"source":["from sklearn.model_selection import cross_val_predict\n","from sklearn.metrics import confusion_matrix\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2f3ibspoLKnb"},"outputs":[],"source":["# Split into Training and Testing Dataset\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n","# name = df.toxic.name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gj4kbvQgLKhk"},"outputs":[],"source":["# K-fold cross variations\n","\n","# https://towardsdatascience.com/building-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0 (2020) *\n","# https://www.kaggle.com/pocooo/types-of-cross-validation-all-you-need (3mo)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nJqe-pftgh4x"},"outputs":[],"source":["# Hyper parameter grid search using Stratified k-fold search for better l2 penality\n","\n","# https://www.kaggle.com/babbler/cross-validation-considerations (3 yr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyE1gBtGjiXt"},"outputs":[],"source":["# Model Comparisons\n","\n","# https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/ *\n","# https://nulpointerexception.com/2017/12/31/a-tutorial-to-find-best-scikit-classifiers-for-sentiment-analysis/ *\n","# https://www.dezyre.com/recipes/compare-sklearn-classification-algorithms-in-python *\n","# https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html \n","# https://www.kaggle.com/meetnaren/comparing-different-sklearn-classifiers (4yr) *\n","# https://www.kaggle.com/jeffd23/10-classifier-showdown-in-scikit-learn (5yr) *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vmd5IxGLityA"},"outputs":[],"source":["# SA with XGBoost + Pipelines\n","\n","* https://ai.plainenglish.io/sentiment-classification-using-xgboost-7abdaf4771f9\n","* https://github.com/jaotheboss/Drug-Sentiment-Analysis/blob/master/main.py\n","* https://www.kaggle.com/sudhirnl7/simple-naive-bayes-xgboost (4yrs)\n","* https://www.kaggle.com/diveki/classification-with-nlp-xgboost-and-pipelines (3yrs)\n","* https://blessing3ke.medium.com/twitter-sentiment-analysis-with-xgboost-f0016e94d317 (1yr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5M43Z9RLKbs"},"outputs":[],"source":["# ROC/AUC compare classifiers\n","\n","# https://github.com/dformoso/sklearn-classification *\n","# https://colab.research.google.com/github/astg606/py_materials/blob/master/machine_learning/ml_models_scikit-learn.ipynb *\n","# https://colab.research.google.com/github/goswami-rahul/machine-learning/blob/master/UnderstandingMetrics.ipynb *\n","# https://stats.stackexchange.com/questions/133225/how-to-validate-sentiment-classification-and-compare-different-algorithms (6yr)\n","# https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n","# https://www.kaggle.com/nirajvermafcb/comparing-various-ml-models-roc-curve-comparison (4yr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DX2BNSush-e7"},"outputs":[],"source":["# Ensemble Methods\n","\n","* https://scikit-learn.org/stable/modules/ensemble.html?highlight=xgboost\n","* https://journalofbigdata.springeropen.com/articles/10.1186/s40537-018-0152-5\n"]},{"cell_type":"markdown","metadata":{"id":"lRCqXwipFAxp"},"source":["#### **Create Supervised Learning DataFrame**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGe-lstBMyzP"},"outputs":[],"source":["# Setup new DataFrame to hold all Sklearn supervised statistical machine learning models\n","\n","corpus_mlsup_df = pd.DataFrame(corpus_sents_df['sent_clean'])\n","corpus_mlsup_df['sent_clean'] = corpus_mlsup_df['sent_clean'].astype('string')\n","\n","# Remove non-alphanumeric chacters\n","# corpus_mlsup_df['text_lower'] = corpus_mlsup_df['text_raw']\n","\n","pattern = re.compile(r\"[A-Za-z0-9\\-]{3,50}\")\n","corpus_mlsup_df['sent_lemma'] = corpus_mlsup_df['sent_clean'].str.lower().str.strip().str.findall(pattern).str.join(' ')\n","# corpus_mlsup_df.head()\n","\n","# Method #3: Lemmatize with joblib parallelization\n","# NOTE: 17m13s \n","\n","# lemmas_ls = preprocess_parallel(corpus_mlsup_df['sent_lemma'], chunksize=1000)\n","# print(f'type: {type(lemmas_ls[0])}')\n","# corpus_mlsup_df['sent_lemma'] = ' '.join(lemmas_ls)\n","\n","corpus_mlsup_df['sent_lemma'] = preprocess_parallel(corpus_mlsup_df['sent_lemma'], chunksize=1000)\n","corpus_mlsup_df['sent_lemma'] = corpus_mlsup_df['sent_lemma'].apply(lambda x: ' '.join(x))\n","\n","corpus_mlsup_df.info()\n","corpus_mlsup_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ryyJwk5IiAew"},"outputs":[],"source":["# Vectorize our Lemmatized Corpus Sentences\n","\n","# transform, not fit_transform, because we already learned all our words\n","sents_vectors = vectorizer.transform(corpus_mlsup_df.sent_lemma)\n","sents_words_df = pd.DataFrame(sents_vectors.toarray(), columns=vectorizer.get_feature_names())\n","sents_words_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEifwA2zYjLw"},"outputs":[],"source":["print(vectorizer.get_feature_names())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQVo6UDRopJV"},"outputs":[],"source":["print(corpus_mlsup_df.iloc[0]['sent_lemma'])"]},{"cell_type":"markdown","metadata":{"id":"XbvcZo4oiCa7"},"source":["### **Time Series Clustering**\n","\n","Code:\n","\n","* https://towardsdatascience.com/how-to-apply-hierarchical-clustering-to-time-series-a5fe2a7d8447 (DTW) *\n","* https://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf \n","* https://www.sktime.org/en/latest/examples/mrseql.html Classification Mr-SEQL\n","\n","Guidance:\n","\n","* https://stats.stackexchange.com/questions/63546/comparing-hierarchical-clustering-dendrograms-obtained-by-different-distances \n","\n","Ranks:\n","* https://paperswithcode.com/task/time-series-clustering\n","\n","Papers:\n","* https://reader.elsevier.com/reader/sd/pii/S2666827020300013?token=2286F6993FF63B6B3096B72F09503A950095DD5F1C1BB11146BDC0EAAA8E4D942BAD3A216FDBE65978BAE3B5C9F2363F&originRegion=us-east-1&originCreation=20210817184824"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1O0Qmhyh_UO"},"outputs":[],"source":["# Time Series Clustering\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZQkSxI7ziaKG"},"source":["### **Time Series Transformations**\n","\n","Tutorial:\n","* https://opendatascience.com/transforming-skewed-data-for-machine-learning/\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HfB7xgTIh-Y-"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5SgVNEKok1QZ"},"outputs":[],"source":["%%time\n","\n","# Create and train a linear regression\n","# SST2: 1800s/1000it, 20s/10it (default, max_iter=1000)\n","# IMDB50k: 1000it, 1.14s\n","\n","# First get a performance metrics\n","# Test/Train Holdout Validation\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n","\n","\n","linreg = LinearRegression()\n","linreg.fit(X_train, y_train)\n","result = linreg.score(X_test, y_test)\n","print(f'Evaluation (Test/Train holdout): {result}')\n","\n","# K-Fold Validation\n","results=cross_val_score(linreg,X,y,cv=kfold_validation)\n","print(f'Evaluation (K-Fold): {results}')\n","print(np.mean(results))\n","\n","# Stratified Validation\n","skfold=StratifiedKFold(n_splits=5)\n","scores=cross_val_score(linreg,X,y,cv=skfold)\n","print(f'Evaluation (Stratified CV): {np.mean(scores)}')\n","\n","\n","# Run confusion matrix on transformed text and cross-validation predictions\n","from sklearn.model_selection import cross_val_predict\n","from sklearn.metrics import confusion_matrix\n"," \n","sentiment_predict = cross_val_predict(linreg, X, y, cv=5)\n","confusion_mat = confusion_matrix(sentiment, sentiment_predict)\n","\n","\n","# Second, train on entire corpus\n","linreg = LinearRegression()\n","linreg.fit(X, y)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkPwGOxnEaYp"},"outputs":[],"source":["# Stratified Validation\n","skfold=StratifiedKFold(n_splits=5)\n","scores=cross_val_score(linreg,X,y,cv=skfold)\n","print(f'Evaluation (Stratified CV): {np.mean(scores)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yUiOFpVfJCOE"},"outputs":[],"source":["sentiment_predict = cross_val_predict(linreg, X, y, cv=5)\n","confusion_mat = confusion_matrix(y, sentiment_predict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LE_Y86qlJnmq"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyQhiXpnJniK"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCA1SVvmJnd8"},"outputs":[],"source":["# Code source: Gaël Varoquaux\n","#              Andreas Müller\n","# Modified for documentation by Jaques Grobler\n","# License: BSD 3 clause\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import make_moons, make_circles, make_classification\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.gaussian_process.kernels import RBF\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jd0pmghYJrOj"},"outputs":[],"source":["h = .02  # step size in the mesh\n","\n","names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n","         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n","         \"Naive Bayes\", \"QDA\"]\n","\n","classifiers = [\n","    KNeighborsClassifier(3),\n","    SVC(kernel=\"linear\", C=0.025),\n","    SVC(gamma=2, C=1),\n","    GaussianProcessClassifier(1.0 * RBF(1.0)),\n","    DecisionTreeClassifier(max_depth=5),\n","    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n","    MLPClassifier(alpha=1, max_iter=1000),\n","    AdaBoostClassifier(),\n","    GaussianNB(),\n","    QuadraticDiscriminantAnalysis()]\n","\n","X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n","                           random_state=1, n_clusters_per_class=1)\n","rng = np.random.RandomState(2)\n","X += 2 * rng.uniform(size=X.shape)\n","linearly_separable = (X, y)\n","\n","datasets = [make_moons(noise=0.3, random_state=0),\n","            make_circles(noise=0.2, factor=0.5, random_state=1),\n","            linearly_separable\n","            ]\n","\n","figure = plt.figure(figsize=(27, 9))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bWHOskTJly-"},"outputs":[],"source":["i = 1\n","# iterate over datasets\n","for ds_cnt, ds in enumerate(datasets):\n","    # preprocess dataset, split into training and test part\n","    X, y = ds\n","    X = StandardScaler().fit_transform(X)\n","    X_train, X_test, y_train, y_test = \\\n","        train_test_split(X, y, test_size=.4, random_state=42)\n","\n","    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n","    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                         np.arange(y_min, y_max, h))\n","\n","    # just plot the dataset first\n","    cm = plt.cm.RdBu\n","    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n","    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n","    if ds_cnt == 0:\n","        ax.set_title(\"Input data\")\n","    # Plot the training points\n","    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n","               edgecolors='k')\n","    # Plot the testing points\n","    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n","               edgecolors='k')\n","    ax.set_xlim(xx.min(), xx.max())\n","    ax.set_ylim(yy.min(), yy.max())\n","    ax.set_xticks(())\n","    ax.set_yticks(())\n","    i += 1\n","\n","    # iterate over classifiers\n","    for name, clf in zip(names, classifiers):\n","        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n","        clf.fit(X_train, y_train)\n","        score = clf.score(X_test, y_test)\n","\n","        # Plot the decision boundary. For that, we will assign a color to each\n","        # point in the mesh [x_min, x_max]x[y_min, y_max].\n","        if hasattr(clf, \"decision_function\"):\n","            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n","        else:\n","            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n","\n","        # Put the result into a color plot\n","        Z = Z.reshape(xx.shape)\n","        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n","\n","        # Plot the training points\n","        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n","                   edgecolors='k')\n","        # Plot the testing points\n","        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n","                   edgecolors='k', alpha=0.6)\n","\n","        ax.set_xlim(xx.min(), xx.max())\n","        ax.set_ylim(yy.min(), yy.max())\n","        ax.set_xticks(())\n","        ax.set_yticks(())\n","        if ds_cnt == 0:\n","            ax.set_title(name)\n","        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n","                size=15, horizontalalignment='right')\n","        i += 1\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PXznxtcdJlto"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOaUK2gFJlpF"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Euum6F_Encd"},"outputs":[],"source":["classifier1 = svm.SVC(kernel='linear', probability=True)\n","probas_1 = classifier1.fit(X_train, y_train).predict_proba(X_test)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SAkEZMU4evQS"},"source":["#### **Inference**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMhr3YGCAqcQ"},"outputs":[],"source":["# from sklearn.cross_validation import train_test_split\n","\n","X_train, X_test, y_train, y_test  = train_test_split(\n","        X, \n","        y,\n","        train_size=0.80, \n","        random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ej-5lG7Ksh0G"},"outputs":[],"source":["# Set the name of the supervised sentiment classifier training dataset\n","\n","# supervised_db = 'imdb50k'\n","# supervised_db = 'sst2'\n","supervised_db = 'sentiment140'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"draT3Xe9hRLl"},"outputs":[],"source":["# corpus_mlsup_df = pd.DataFrame(corpus_sents_df['sent_clean'])\n","# corpus_mlsup_df['sent_clean'] = corpus_mlsup_df['sent_clean'].astype('string')\n","corpus_mlsup_df.info()\n","corpus_mlsup_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WCYVklLkig2H"},"outputs":[],"source":["sents_words_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QB7t_NbilUe"},"outputs":[],"source":["# Predict using all our models. \n","\n","# Linear Regression predictions + probabilities\n","corpus_mlsup_df[f'linreg_{supervised_db}'] = linreg.predict(sents_words_df)\n","\n","# Logistic Regression predictions + probabilities\n","corpus_mlsup_df[f'logreg_{supervised_db}'] = logreg.predict(sents_words_df)\n","corpus_mlsup_df[f'logreg_proba_{supervised_db}'] = logreg.predict_proba(sents_words_df)[:,1]\n","\n","# Random forest predictions + probabilities\n","corpus_mlsup_df[f'dforest_{supervised_db}'] = dforest.predict(sents_words_df)\n","corpus_mlsup_df[f'dforest_proba_{supervised_db}'] = dforest.predict_proba(sents_words_df)[:,1]\n","\n","# SVC predictions\n","corpus_mlsup_df[f'svc_{supervised_db}'] = svc.predict(sents_words_df)\n","\n","# Bayes predictions + probabilities\n","corpus_mlsup_df[f'multinb_{supervised_db}'] = multinb.predict(sents_words_df)\n","corpus_mlsup_df[f'multinb_proba_{supervised_db}'] = multinb.predict_proba(sents_words_df)[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YdZcUzSvj0si"},"outputs":[],"source":["corpus_mlsup_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qifISzHlrWdC"},"outputs":[],"source":["# Save checkpoint\n","\n","# corpus_mlsup_df.to_csv('ml_sents_df.csv')"]},{"cell_type":"markdown","metadata":{"id":"2XUfmL6p7SsY"},"source":["#### **Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6XnjgZIrvqB"},"outputs":[],"source":["# supervised_db = 'imdb50k'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMhsXr5Mj46e"},"outputs":[],"source":["# Smooth Time Series\n","\n","win_s1per = int(corpus_sents_df.shape[0] * 1/100)\n","\n","corpus_mlsup_df[f'linreg_{supervised_db}_roll10'] = corpus_mlsup_df[f'linreg_{supervised_db}'].rolling(10*win_s1per, center=True).mean()\n","corpus_mlsup_df[f'logreg_proba_{supervised_db}_roll10'] = corpus_mlsup_df[f'logreg_proba_{supervised_db}'].rolling(10*win_s1per, center=True).mean()\n","corpus_mlsup_df[f'dforest_proba_{supervised_db}_roll10'] = corpus_mlsup_df[f'dforest_proba_{supervised_db}'].rolling(10*win_s1per, center=True).mean()\n","corpus_mlsup_df[f'multinb_proba_{supervised_db}_roll10'] = corpus_mlsup_df[f'multinb_proba_{supervised_db}'].rolling(10*win_s1per, center=True).mean()\n","corpus_mlsup_df[f'svc_{supervised_db}_roll10'] = corpus_mlsup_df[f'svc_{supervised_db}'].rolling(10*win_s1per, center=True).mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INJJDXFztcME"},"outputs":[],"source":["corpus_mlsup_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGNSo5mpvGcI"},"outputs":[],"source":["# Standardize Time Series\n","\n","corpus_mlsup_df[f'linreg_{supervised_db}_roll10_stdscaler'] = mean_std_scaler.fit_transform(np.array(corpus_mlsup_df[f'linreg_{supervised_db}_roll10']).reshape(-1, 1))\n","\n","corpus_mlsup_df[f'logreg_proba_{supervised_db}_roll10_stdscaler'] = mean_std_scaler.fit_transform(np.array(corpus_mlsup_df[f'logreg_proba_{supervised_db}_roll10']).reshape(-1, 1))\n","\n","corpus_mlsup_df[f'dforest_proba_{supervised_db}_roll10_stdscaler'] = mean_std_scaler.fit_transform(np.array(corpus_mlsup_df[f'dforest_proba_{supervised_db}_roll10']).reshape(-1, 1))\n","\n","corpus_mlsup_df[f'multinb_proba_{supervised_db}_roll10_stdscaler'] = mean_std_scaler.fit_transform(np.array(corpus_mlsup_df[f'multinb_proba_{supervised_db}_roll10']).reshape(-1, 1))\n","\n","corpus_mlsup_df[f'svc_{supervised_db}_roll10_stdscaler'] = mean_std_scaler.fit_transform(np.array(corpus_mlsup_df[f'svc_{supervised_db}_roll10']).reshape(-1, 1))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oyz_dvkm_X6T"},"outputs":[],"source":["supervised_db = 'sentiment140'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHiTTCLH9qbX"},"outputs":[],"source":["corpus_mlsup_df[f'linreg_{supervised_db}_roll10_stdscaler'].plot(label='Linear Regression')\n","corpus_mlsup_df[f'logreg_proba_{supervised_db}_roll10_stdscaler'].plot(label='Logistic Regression')\n","corpus_mlsup_df[f'dforest_proba_{supervised_db}_roll10_stdscaler'].plot(label='Random Forest')\n","corpus_mlsup_df[f'multinb_proba_{supervised_db}_roll10_stdscaler'].plot(label='Naive Bayes')\n","corpus_mlsup_df[f'svc_{supervised_db}_roll10_stdscaler'].plot(label='SVC')\n","plt.title(f'{CORPUS_FULL}\\n Sentence Sentiment Statistical ML with SMA 10% + StdScaler (TrainingDB: {supervised_db})')\n","plt.legend(loc='best')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SyR6jNKl-Y7B"},"outputs":[],"source":["corpus_mlsup_df[f'linreg_{supervised_db}_roll10_stdscaler'].plot(label='Linear Regression')\n","corpus_mlsup_df[f'logreg_proba_{supervised_db}_roll10_stdscaler'].plot(label='Logistic Regression')\n","corpus_mlsup_df[f'dforest_proba_{supervised_db}_roll10_stdscaler'].plot(label='Random Forest')\n","corpus_mlsup_df[f'multinb_proba_{supervised_db}_roll10_stdscaler'].plot(label='Naive Bayes')\n","corpus_mlsup_df[f'svc_{supervised_db}_roll10_stdscaler'].plot(label='SVC')\n","plt.title(f'{CORPUS_FULL}\\n Sentence Sentiment Statistical ML with SMA 10% + StdScaler (TrainingDB: {supervised_db})')\n","plt.legend(loc='best')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_jx0uTC2FHO"},"outputs":[],"source":["corpus_mlsup_df[f'linreg_{supervised_db}_roll10_stdscaler'].plot(label='Linear Regression')\n","corpus_mlsup_df[f'logreg_proba_{supervised_db}_roll10_stdscaler'].plot(label='Logistic Regression')\n","corpus_mlsup_df[f'dforest_proba_{supervised_db}_roll10_stdscaler'].plot(label='Random Forest')\n","corpus_mlsup_df[f'multinb_proba_{supervised_db}_roll10_stdscaler'].plot(label='Naive Bayes')\n","corpus_mlsup_df[f'svc_{supervised_db}_roll10_stdscaler'].plot(label='SVC')\n","plt.title(f'{CORPUS_FULL}\\n Sentence Sentiment Statistical ML with SMA 10% + StdScaler (TrainingDB: {supervised_db})')\n","plt.legend(loc='best')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ueAfVXa1d0rn"},"outputs":[],"source":["%%time\n","\n","print(\"Training logistic regression\")\n","logreg.fit(X_train, y_train)\n","\n","print(\"Training random forest\")\n","forest.fit(X_train, y_train)\n","\n","print(\"Training SVC\")\n","svc.fit(X_train, y_train)\n","\n","print(\"Training Naive Bayes\")\n","bayes.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"smj2QgkLd0oL"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"49iy8tQNg-tP"},"outputs":[],"source":["# Logistic Regression\n","\n","y_true = y_test\n","y_pred = logreg.predict(X_test)\n","matrix = confusion_matrix(y_true, y_pred)\n","\n","label_names = pd.Series(['negative', 'positive'])\n","pd.DataFrame(matrix,\n","     columns='Predicted ' + label_names,\n","     index='Is ' + label_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T20MMTzBg-tQ"},"outputs":[],"source":["# Random Forest\n","\n","y_true = y_test\n","y_pred = forest.predict(X_test)\n","matrix = confusion_matrix(y_true, y_pred)\n","\n","label_names = pd.Series(['negative', 'positive'])\n","pd.DataFrame(matrix,\n","     columns='Predicted ' + label_names,\n","     index='Is ' + label_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j3P6nAMCg-tS"},"outputs":[],"source":["# SVC\n","\n","y_true = y_test\n","y_pred = svc.predict(X_test)\n","matrix = confusion_matrix(y_true, y_pred)\n","\n","label_names = pd.Series(['negative', 'positive'])\n","pd.DataFrame(matrix,\n","     columns='Predicted ' + label_names,\n","     index='Is ' + label_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGpFNtIgg-tT"},"outputs":[],"source":["# Nultinomial Naive Bayes\n","\n","y_true = y_test\n","y_pred = bayes.predict(X_test)\n","matrix = confusion_matrix(y_true, y_pred)\n","\n","label_names = pd.Series(['negative', 'positive'])\n","pd.DataFrame(matrix,\n","     columns='Predicted ' + label_names,\n","     index='Is ' + label_names)"]},{"cell_type":"markdown","metadata":{"id":"FsrKrn77g-tU"},"source":["**Percentage-based confusion matrices**\n","\n","Those are kind of irritating in that they're just numbers. Let's try percentages instead"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-_N9nsyg-tU"},"outputs":[],"source":["# Logistic Regression\n","\n","y_true = y_test\n","y_pred = logreg.predict(X_test)\n","matrix = confusion_matrix(y_true, y_pred)\n","\n","label_names = pd.Series(['negative', 'positive'])\n","pd.DataFrame(matrix,\n","     columns='Predicted ' + label_names,\n","     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PANO3BE3g-tV"},"outputs":[],"source":["# Random Forest\n","\n","y_true = y_test\n","y_pred = forest.predict(X_test)\n","matrix = confusion_matrix(y_true, y_pred)\n","\n","label_names = pd.Series(['negative', 'positive'])\n","pd.DataFrame(matrix,\n","     columns='Predicted ' + label_names,\n","     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxsX_xxjg-tW"},"outputs":[],"source":["# SVC\n","\n","y_true = y_test\n","y_pred = svc.predict(X_test)\n","matrix = confusion_matrix(y_true, y_pred)\n","\n","label_names = pd.Series(['negative', 'positive'])\n","pd.DataFrame(matrix,\n","     columns='Predicted ' + label_names,\n","     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJyA3W7dg-tW"},"outputs":[],"source":["# Multinomial Naive Bayes\n","\n","y_true = y_test\n","y_pred = bayes.predict(X_test)\n","matrix = confusion_matrix(y_true, y_pred)\n","\n","label_names = pd.Series(['negative', 'positive'])\n","pd.DataFrame(matrix,\n","     columns='Predicted ' + label_names,\n","     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"]},{"cell_type":"markdown","metadata":{"id":"UCfbdBGlhhS-"},"source":["#### **Save Corpus DataFrames**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"524E0SgJiVQq"},"outputs":[],"source":["supervised_db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UE2xijYiWtg"},"outputs":[],"source":["corpus_mlsup_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6J5xizNhdXt"},"outputs":[],"source":["save_dataframes(df_ls=['mlsupervised'])"]},{"cell_type":"markdown","metadata":{"id":"zZK1gA47xzkS"},"source":["### **Select Sentiment Models (Manual)**\n","\n","NOTE:\n","\n","* Stanza (Stanford OpenNLP) can take upto 50 minutes to run\n","\n","* Listed in increasing order of (approx) run time\n","\n","* MPQA/SentiStrength not yet implemented (placeholders only for now)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"L0pStg1gJTZA"},"outputs":[],"source":["SentimentR_Arc = True #@param {type:\"boolean\"}\n","Syuzhet_Arc = True #@param {type:\"boolean\"}\n","Bing_Arc = True #@param {type:\"boolean\"}\n","SenticNet_Arc = True #@param {type:\"boolean\"}\n","SentiWord_Arc = True #@param {type:\"boolean\"}\n","NRC_Arc = True #@param {type:\"boolean\"}\n","AFINN_Arc = True #@param {type:\"boolean\"}\n","VADER_Arc = True #@param {type:\"boolean\"}\n","TextBlob_Arc = True #@param {type:\"boolean\"}\n","Flair_Arc = True #@param {type:\"boolean\"}\n","Pattern_Arc = True #@param {type:\"boolean\"}\n","Stanza_Arc = True #@param {type:\"boolean\"}\n","# MPQA_Arc = False #@param {type:\"boolean\"}\n","# SentiStrength_Arc = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"w0IACAN5JTZC"},"outputs":[],"source":["# Create and Verify custom list of Models to include\n","\n","MODELS_CUSTOM_LS = []\n","\n","if VADER_Arc:\n","  MODELS_CUSTOM_LS.append('vader')\n","if TextBlob_Arc:\n","  MODELS_CUSTOM_LS.append('textblob')\n","if Flair_Arc:\n","  MODELS_CUSTOM_LS.append('flair')\n","if Stanza_Arc:\n","  MODELS_CUSTOM_LS.append('stanza')\n","if SentimentR_Arc:\n","  MODELS_CUSTOM_LS.append('sentimentr')\n","if Syuzhet_Arc:\n","  MODELS_CUSTOM_LS.append('syuzhet')\n","if AFINN_Arc:\n","  MODELS_CUSTOM_LS.append('afinn')\n","if Bing_Arc:\n","  MODELS_CUSTOM_LS.append('bing')\n","if Pattern_Arc:\n","  MODELS_CUSTOM_LS.append('pattern')\n","if SentiWord_Arc:\n","  MODELS_CUSTOM_LS.append('sentiword')\n","if SenticNet_Arc:\n","  MODELS_CUSTOM_LS.append('senticnet')\n","if NRC_Arc:\n","  MODELS_CUSTOM_LS.append('nrc')\n","\n","print(f'Here are the Models we are using to ensemble and save:\\n\\n   {MODELS_CUSTOM_LS}')\n","\n","\"\"\"\n","models_incl_ls = []\n","for amodel in MODELS_CUSTOM_LS:\n","  models_incl_ls.append(amodel[:2])\n","models_incl_str = ''.join(models_incl_ls)\n","\n","print(f'Here is a custom name abbr: {models_incl_str}')\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gw0JPNe6T2ap"},"outputs":[],"source":["# Calculate (win_(x)1per) 1% of Corpus length for smallest (odd-valued) rolling window\n","\n","# Sentences\n","corpus_sents_len = corpus_sents_df.shape[0]\n","\n","win_raw_s1per = int(corpus_sents_len * 0.01)\n","# print(f'1% Rolling Window: {win_raw_s1per}')\n","\n","if win_raw_s1per % 2:\n","  win_s1per = win_raw_s1per\n","else:\n","  win_s1per = win_raw_s1per + 1\n","\n","# Paragraphs\n","# corpus_parags_df = corpus_all_df\n","corpus_parags_len = len(corpus_sents_df['parag_no'].unique())\n","\n","win_raw_p1per = int(corpus_parags_len * 0.01)\n","# print(f'1% Rolling Window: {win_raw_1per}')\n","\n","if win_raw_p1per % 2:\n","  win_p1per = win_raw_p1per\n","else:\n","  win_p1per = win_raw_p1per + 1\n","\n","\n","# Sections\n","\n","# NO NEED FOR SLIDING WINDOW ON SECTIONS\n","\n","\n","print(f'Sentence 1 Percent window: {win_s1per}')\n","print(f'Paragraph 1 Percent window: {win_p1per}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGjfPK9u21HH"},"outputs":[],"source":["# Verify Sentiment Lexicon hash files are accessable\n","\n","lexicons_path = f'/gdrive/MyDrive/{LEXICONS_SUBDIR[1:]}/hash*.csv'\n","!pwd\n","!ls $lexicons_path"]},{"cell_type":"markdown","metadata":{"id":"7tAxuAxU7ueg"},"source":["### **Calculate SentimentR (Jockers-Rinker) Sentiment Polarities (Optional: Auto)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foQeDPQpvyB3"},"outputs":[],"source":["if SentimentR_Arc == True:\n","  model_base = 'sentimentr'\n","\n","  \"\"\"\n","  model_name = 'sentimentr_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n","  \"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iEnJj5rIMcSj"},"outputs":[],"source":["# Verify Lexicon subdirectory and datafiles\n","!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n","\n","print('\\nTop of Dictionary datafile ----------')\n","!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_sentimentr.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gq7dImOJozm5"},"outputs":[],"source":["# Read Lexicon into DataFrame \n","\n","if SentimentR_Arc == True:\n","\n","  lexicon_sentimentr_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_sentimentr.csv')\n","  lexicon_sentimentr_df['x'] = lexicon_sentimentr_df['x'].astype('string')\n","\n","  # Clean/Reorg DataFrame\n","  lexicon_sentimentr_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n","  lexicon_sentimentr_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n","\n","  # Verify\n","  if (PLOT_OUTPUT == 'All'):\n","    lexicon_sentimentr_df.head()\n","    lexicon_sentimentr_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25_Zjja0o_Hx"},"outputs":[],"source":["# Convert DataFrame to Dict[word] = polarity\n","\n","if SentimentR_Arc == True:\n","\n","  id = lexicon_sentimentr_df.word.values\n","  values = lexicon_sentimentr_df.polarity.values\n","\n","  lexicon_sentimentr_dt = dict(zip(id, values))\n","  # lexicon_sentimentr_dt\n","\n","  # Test\n","  sent_test='I hate Mondays.'\n","  print(text2sentiment(sent_test, lexicon_sentimentr_dt))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQYYP2hiO53j"},"outputs":[],"source":["# Verify\n","\n","# corpus_sents_df['sent_clean'].isna().any()\n","# corpus_chaps_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVSImJ3ETSYt"},"outputs":[],"source":["corpus_parags_df.columns\n","print('\\n')\n","corpus_sects_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lxe15XjNwByS"},"outputs":[],"source":["# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","\n","# Sentiment evaluation function\n","def sentiment_sentimentr(text_str):\n","  '''\n","  Given a text string\n","  Return corresponding sentiment value based on sentimentr lexicon\n","  '''\n","  \n","  sentiment_val = text2sentiment(str(text_str), lexicon_sentimentr_dt)\n","\n","  return sentiment_val \n","\n","# Calculate all Sentiment values and variants\n","if SentimentR_Arc == True:\n","  model_base = 'sentimentr'\n","  get_sentiments(model_base=model_base, sentiment_fn=sentiment_sentimentr, sentiment_type='lexicon')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVX6rg5KwBvF"},"outputs":[],"source":["corpus_sents_df.head(2)\n","corpus_sents_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Pkrp6TwY7OI"},"outputs":[],"source":["# Verify there are no empty Sentences\n","\n","corpus_sents_df[corpus_sents_df['token_len'] == 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYeO-NotzY0-"},"outputs":[],"source":["# TODO: Put above at the start of processing each new Corpus\n","\n","corpus_lexicons_stats_dt = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbGz2Yi5wByY"},"outputs":[],"source":["# Get/Set Sentiment Statistics\n","\n","if SentimentR_Arc == True:\n","  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n","  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n","  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n","  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n","\n","  # Validate\n","  corpus_lexicons_stats_dt\n","\n","  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4aDdJcrzlE1"},"outputs":[],"source":["corpus_sents_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOSiIyhbN2-8"},"outputs":[],"source":["# Verify \n","\n","if SentimentR_Arc == True:\n","  # corpus_sents_df['sentimentr'].plot(alpha=0.3)\n","  corpus_sents_df['sentimentr_stdscaler'].plot(alpha=0.1)\n","  corpus_sents_df['sentimentr_medianiqr'].plot(alpha=0.3)\n","  plt.legend(loc='best');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8POYGujomVt"},"outputs":[],"source":["corpus_sents_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6lvswx9yovMV"},"outputs":[],"source":["corpus_parags_df.info()"]},{"cell_type":"markdown","metadata":{"id":"QZjqwTvU76AR"},"source":["### **Calculate Syuzhet (Jockers) Sentiment Polarities (Optional: Auto)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cjOXGQX54lbX"},"outputs":[],"source":["# Define Model names\n","\n","if Syuzhet_Arc == True:\n","  model_base = 'syuzhet'\n","  model_name = 'syuzhet_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sB5fBNQ-QNwk"},"outputs":[],"source":["# Verify Lexicon subdirectory and datafiles\n","!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n","\n","print('\\nTop of Dictionary datafile ----------')\n","!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_syuzhet.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IkhaGcuy4lbg"},"outputs":[],"source":["# Read Lexicon into DataFrame \n","\n","if Syuzhet_Arc == True:\n","\n","  lexicon_syuzhet_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_syuzhet.csv')\n","  lexicon_syuzhet_df['word'] = lexicon_syuzhet_df['word'].astype('string')\n","\n","  # Clean/Reorg DataFrame\n","  lexicon_syuzhet_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n","  lexicon_syuzhet_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n","\n","  # Verify\n","  if (PLOT_OUTPUT == 'All'):\n","    lexicon_syuzhet_df.head()\n","    lexicon_syuzhet_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9IuINuR4lbi"},"outputs":[],"source":["# Convert DataFrame to Dict[word] = polarity\n","\n","if Syuzhet_Arc == True:\n","\n","  id = lexicon_syuzhet_df.word.values\n","  values = lexicon_syuzhet_df.value.values\n","\n","  lexicon_syuzhet_dt = dict(zip(id, values))\n","  # lexicon_sentimentr_dt\n","\n","  # Test\n","  sent_test='I hate Mondays.'\n","  print(text2sentiment(sent_test, lexicon_syuzhet_dt))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qO-txaFL4lbo"},"outputs":[],"source":["# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","\n","# Sentiment evaluation function\n","def sentiment_syuzhet(text_str):\n","  '''\n","  Given a text string\n","  Return corresponding sentiment value based on sentimentr lexicon\n","  '''\n","  \n","  sentiment_val = text2sentiment(str(text_str), lexicon_syuzhet_dt)\n","\n","  return sentiment_val \n","\n","# Calculate all Sentiment values and variants\n","\n","if Syuzhet_Arc == True:\n","  get_sentiments(model_base=model_base, sentiment_fn=sentiment_syuzhet, sentiment_type='lexicon')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RfsLZSo04lbp"},"outputs":[],"source":["# Get/Set Sentiment Statistics\n","\n","if Syuzhet_Arc == True:\n","  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n","  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n","  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n","  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n","\n","  # Validate\n","  corpus_lexicons_stats_dt\n","\n","  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9VFBSXD0vJo"},"outputs":[],"source":["corpus_sents_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-BwrbyXR5Hd"},"outputs":[],"source":["# Verify \n","\n","if Syuzhet_Arc == True:\n","  # corpus_sents_df['syuzhet'].plot(alpha=0.3)\n","  corpus_sents_df['syuzhet_stdscaler'].plot(alpha=0.3)\n","  corpus_sents_df['syuzhet_medianiqr'].plot(alpha=0.1)\n","  plt.legend(loc='best');"]},{"cell_type":"markdown","metadata":{"id":"NA3dWsnF78mi"},"source":["### **Calculate Bing (HuLiu) Sentiment Polarities (Optional: Auto)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wvq1prK7n1k"},"outputs":[],"source":["if Bing_Arc == True:\n","  model_base = 'bing'\n","  model_name = 'bing_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nDfB2f0olded"},"outputs":[],"source":["# Verify Lexicon subdirectory and datafiles\n","!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n","\n","print('\\nTop of Dictionary datafile ----------')\n","!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_bing.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZELGxS8y9PiS"},"outputs":[],"source":["# Read Lexicon into DataFrame \n","\n","if Bing_Arc == True:\n","  \n","  lexicon_bing_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_bing.csv')\n","  lexicon_bing_df['x'] = lexicon_bing_df['x'].astype('string')\n","\n","  # Clean/Reorg DataFrame\n","  lexicon_bing_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n","  lexicon_bing_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n","\n","  # Verify\n","  if (PLOT_OUTPUT == 'All'):\n","    lexicon_bing_df.head()\n","    lexicon_bing_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OJQTaeue9VjI"},"outputs":[],"source":["# Convert DataFrame to Dict[word] = polarity\n","\n","if Bing_Arc == True:\n","\n","  id = lexicon_bing_df.word.values\n","  values = lexicon_bing_df.polarity.values\n","\n","  lexicon_bing_dt = dict(zip(id, values))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Y_Ffzu1m7n10"},"outputs":[],"source":["# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","\n","# Sentiment evaluation function\n","def sentiment_bing(text_str):\n","  '''\n","  Given a text string\n","  Return corresponding sentiment value based on sentimentr lexicon\n","  '''\n","  \n","  sentiment_val = text2sentiment(str(text_str), lexicon_bing_dt)\n","\n","  return sentiment_val \n","\n","# Calculate all Sentiment values and variants\n","if Bing_Arc == True:\n","  get_sentiments(model_base=model_base, sentiment_fn=sentiment_bing, sentiment_type='lexicon')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MWITnL2a9rrx"},"outputs":[],"source":["# Calculate Bing Sentiment [0,1,2]\n","\n","def bing_discrete2continous_sentiment(text):\n","  '''\n","  Given a plain text string, give it to\n","    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n","  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n","    that approximates a normal distribution for all values\n","    In order to get more fine grained measure of overall Sentence sentiment\n","    Sentiment values will be Normalized/Standardized so absolute precision is not required\n","  '''\n","  text_sentiment_total = 0.\n","  text_ls = text.split()\n","  text_len = len(text_ls)\n","  for aword in text_ls:\n","    text_sentiment_total += lex_discrete2continous_sentiment(str(aword), lexicon_bing_dt)\n","  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+1)\n","\n","  return text_sentiment_norm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1xpzm3hi7n1x"},"outputs":[],"source":["# Test\n","\n","if Bing_Arc == True:\n","  sent_test='I hate Mondays.'\n","  print(bing_discrete2continous_sentiment(sent_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5UFEItnO-i7h"},"outputs":[],"source":["# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","\n","# Calculate all Sentiment values and variants\n","if Bing_Arc == True:\n","  get_sentiments(model_base=model_base, sentiment_fn=bing_discrete2continous_sentiment, sentiment_type='function')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"w4thQWz3-i7k"},"outputs":[],"source":["# Get/Set Sentiment Statistics\n","\n","if Bing_Arc == True:\n","  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n","  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n","  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n","  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n","\n","  # Validate\n","  corpus_lexicons_stats_dt\n","\n","  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AKlKnqmWmBEP"},"outputs":[],"source":["# Verify \n","\n","if Bing_Arc == True:\n","  # corpus_sents_df['bing'].plot(alpha=0.3)\n","  corpus_sents_df['bing_stdscaler'].plot(alpha=0.3)\n","  corpus_sents_df['bing_medianiqr'].plot(alpha=0.1)\n","  plt.legend(loc='best');"]},{"cell_type":"markdown","metadata":{"id":"SkNZVk128jV9"},"source":["### **Calculate SentiWord Sentiment Polarities (Optional: Auto)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BnL4ORNp8MgB"},"outputs":[],"source":["if SentiWord_Arc == True:\n","  model_base = 'sentiword'\n","  model_name = 'sentiword_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ootf8xdbnHPx"},"outputs":[],"source":["# Verify Lexicon subdirectory and datafiles\n","!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n","\n","print('\\nTop of Dictionary datafile ----------')\n","!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_sentiword.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jpVmt0fK8xi_"},"outputs":[],"source":["# Read Lexicon into DataFrame \n","\n","if SentiWord_Arc == True:\n","\n","  lexicon_sentiword_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_sentiword.csv')\n","  lexicon_sentiword_df['x'] = lexicon_sentiword_df['x'].astype('string')\n","\n","  # Clean/Reorg DataFrame\n","  lexicon_sentiword_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n","  lexicon_sentiword_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n","\n","  # Verify\n","  if (PLOT_OUTPUT == 'All'):\n","    lexicon_sentiword_df.head()\n","    lexicon_sentiword_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NwtK2M9h879M"},"outputs":[],"source":["# Convert DataFrame to Dict[word] = polarity\n","\n","if SentiWord_Arc == True:\n","\n","  id = lexicon_sentiword_df.word.values\n","  values = lexicon_sentiword_df.polarity.values\n","\n","  lexicon_sentiword_dt = dict(zip(id, values))\n","  # lexicon_sentiword_dt\n","\n","  # Test\n","  sent_test='I hate Mondays.'\n","  print(text2sentiment(sent_test, lexicon_sentiword_dt))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0I-QwB478MgP"},"outputs":[],"source":["# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","\n","# Sentiment evaluation function\n","def sentiment_sentiword(text_str):\n","  '''\n","  Given a text string\n","  Return corresponding sentiment value based on sentimentr lexicon\n","  '''\n","  \n","  sentiment_val = text2sentiment(str(text_str), lexicon_sentiword_dt)\n","\n","  return sentiment_val \n","\n","# Calculate all Sentiment values and variants\n","if SentiWord_Arc == True:\n","  get_sentiments(model_base=model_base, sentiment_fn=sentiment_sentiword, sentiment_type='lexicon')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RbEF88568MgS"},"outputs":[],"source":["# Get/Set Sentiment Statistics\n","\n","if SentiWord_Arc == True:\n","  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n","  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n","  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n","  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n","\n","  # Validate\n","  corpus_lexicons_stats_dt\n","\n","  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"s9GfZ5RQnn4G"},"outputs":[],"source":["# Verify \n","\n","if SentiWord_Arc == True:\n","  # corpus_sents_df['sentiword'].plot(alpha=0.3)\n","  corpus_sents_df['sentiword_stdscaler'].plot(alpha=0.3)\n","  corpus_sents_df['sentiword_medianiqr'].plot(alpha=0.1)\n","  plt.legend(loc='best');"]},{"cell_type":"markdown","metadata":{"id":"dUcANLM_8mtT"},"source":["### **Calculate SenticNet Sentiment Polarities (Optional: Auto)**\n","\n","* https://sentic.net/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3OqhSberA1hZ"},"outputs":[],"source":["if SenticNet_Arc == True:\n","  model_base = 'senticnet'\n","  model_name = 'senticnet_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HsnCzsfFnx7B"},"outputs":[],"source":["# Verify Lexicon subdirectory and datafiles\n","!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n","\n","print('\\nTop of Dictionary datafile ----------')\n","!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_senticnet.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qmMfvQvYBBoM"},"outputs":[],"source":["# Read Lexicon into DataFrame \n","\n","if SenticNet_Arc == True:\n","\n","  lexicon_senticnet_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_senticnet.csv')\n","  lexicon_senticnet_df['x'] = lexicon_senticnet_df['x'].astype('string')\n","\n","  # Clean/Reorg DataFrame\n","  lexicon_senticnet_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n","  lexicon_senticnet_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n","\n","  # Verify\n","  if (PLOT_OUTPUT == 'All'):\n","    lexicon_senticnet_df.head()\n","    lexicon_senticnet_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"c2oS71STBIUS"},"outputs":[],"source":["# Convert DataFrame to Dict[word] = polarity\n","\n","if SenticNet_Arc == True:\n","\n","  id = lexicon_senticnet_df.word.values\n","  values = lexicon_senticnet_df.polarity.values\n","\n","  lexicon_senticnet_dt =dict(zip(id, values))\n","  # lexicon_jockersrinker_dt\n","\n","  # Test\n","  sent_test='I hate Mondays.'\n","  text2sentiment(sent_test, lexicon_senticnet_dt)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NHQwXBl5BlRM"},"outputs":[],"source":["# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","\n","# Sentiment evaluation function\n","def sentiment_senticnet(text_str):\n","  '''\n","  Given a text string\n","  Return corresponding sentiment value based on senticnet lexicon\n","  '''\n","  \n","  sentiment_val = text2sentiment(str(text_str), lexicon_senticnet_dt)\n","\n","  return sentiment_val \n","\n","# Calculate all Sentiment values and variants\n","if SenticNet_Arc == True:\n","  get_sentiments(model_base=model_base, sentiment_fn=sentiment_senticnet, sentiment_type='lexicon')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"raHUj3a4A1hs"},"outputs":[],"source":["# Get/Set Sentiment Statistics\n","\n","if SenticNet_Arc == True:\n","  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n","  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n","  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n","  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n","\n","  # Validate\n","  corpus_lexicons_stats_dt\n","\n","  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"C9lSo0Kmn_T4"},"outputs":[],"source":["# Verify \n","\n","if SenticNet_Arc == True:\n","  # corpus_sents_df['senticnet'].plot(alpha=0.3)\n","  corpus_sents_df['senticnet_stdscaler'].plot(alpha=0.3)\n","  corpus_sents_df['senticnet_medianiqr'].plot(alpha=0.1)\n","  plt.legend(loc='best');"]},{"cell_type":"markdown","metadata":{"id":"Cn4KQYpH3glK"},"source":["### **Calculate NRC Sentiment Polarities (Optional: Auto)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"USx6LRmoCFV8"},"outputs":[],"source":["if NRC_Arc == True:\n","  model_base = 'nrc'\n","  model_name = 'nrc_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_EdOTv3KoFEV"},"outputs":[],"source":["# Verify Lexicon subdirectory and datafiles\n","!ls /gdrive/MyDrive/$LEXICONS_SUBDIR\n","\n","print('\\nTop of Dictionary datafile ----------')\n","!head -n 5  /gdrive/MyDrive/$LEXICONS_SUBDIR/hash_sentiment_nrc.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qFEszSc13glL"},"outputs":[],"source":["# Read Lexicon into DataFrame \n","\n","if NRC_Arc == True:\n","\n","  lexicon_nrc_df = get_lexicon(f'/gdrive/MyDrive/{LEXICONS_SUBDIR}/hash_sentiment_nrc.csv')\n","  lexicon_nrc_df['x'] = lexicon_nrc_df['x'].astype('string')\n","\n","  # Clean/Reorg DataFrame\n","  lexicon_nrc_df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n","  lexicon_nrc_df.rename(columns={'x':'word', 'y':'polarity'}, inplace=True)\n","\n","  # Verify\n","  if (PLOT_OUTPUT == 'All'):\n","    lexicon_nrc_df.head()\n","    lexicon_nrc_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-nAkx4Mv3glL"},"outputs":[],"source":["# Convert DataFrame to Dict[word] = polarity\n","\n","if NRC_Arc == True:\n","\n","  id = lexicon_nrc_df.word.values\n","  values = lexicon_nrc_df.polarity.values\n","\n","  lexicon_nrc_dt =dict(zip(id, values))\n","  # lexicon_jockersrinker_dt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hv1nVSATb7z4"},"outputs":[],"source":["# Calculate NRC Sentiment [0,1,2]\n","\n","def nrc_discrete2continous_sentiment(text):\n","  '''\n","  Given a plain text string, give it to\n","    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n","  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n","    that approximates a normal distribution for all values\n","    In order to get more fine grained measure of overall Sentence sentiment\n","    Sentiment values will be Normalized/Standardized so absolute precision is not required\n","  '''\n","  text_sentiment_total = 0.\n","  text_ls = text.split()\n","  text_len = len(text_ls)\n","  for aword in text_ls:\n","    text_sentiment_total += lex_discrete2continous_sentiment(str(aword), lexicon_nrc_dt)\n","  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+10)\n","\n","  return text_sentiment_norm\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ss3Xixo_CX2q"},"outputs":[],"source":["# Test\n","\n","if NRC_Arc == True:\n","  sent_test='I hate Mondays.'\n","  print(nrc_discrete2continous_sentiment(sent_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hhqnljMSCX2w"},"outputs":[],"source":["# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","\n","# Calculate all Sentiment values and variants\n","if NRC_Arc == True:\n","  get_sentiments(model_base=model_base, sentiment_fn=nrc_discrete2continous_sentiment, sentiment_type='function')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"W2q97Dm-CX2z"},"outputs":[],"source":["# Get/Set Sentiment Statistics\n","\n","if NRC_Arc == True:\n","  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n","  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n","  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n","  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n","\n","  # Validate\n","  corpus_lexicons_stats_dt\n","\n","  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bBK7LQx4oXI-"},"outputs":[],"source":["# Verify \n","\n","if NRC_Arc == True:\n","  # corpus_sents_df['nrc'].plot(alpha=0.3)\n","  corpus_sents_df['nrc_stdscaler'].plot(alpha=0.1)\n","  corpus_sents_df['nrc_medianiqr'].plot(alpha=0.3)\n","  plt.legend(loc='best');"]},{"cell_type":"markdown","metadata":{"id":"jRTjCPLb8cbB"},"source":["### **Calculate Afinn Sentiment Polarities (Optional: Auto)**\n","\n","* https://github.com/fnielsen/afinn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcnxqnyzDCde"},"outputs":[],"source":["if AFINN_Arc == True:\n","  model_base = 'afinn'\n","  model_name = 'afinn_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDG2KxvNBdj6"},"outputs":[],"source":["if AFINN_Arc == True:\n","  !pip install afinn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evnkXWL58CcX"},"outputs":[],"source":["# Install and configure for English\n","\n","if AFINN_Arc == True:\n","  from afinn import Afinn\n","  afinn = Afinn(language='en')\n","\n","  # Test\n","\n","  # afinn.score('I had the worst day.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKbeW_cMXhmI"},"outputs":[],"source":["# Calculate AFINN Sentiment [0,1,2]\n","\n","def afinn_discrete2continous_sentiment(text):\n","  '''\n","  Given a plain text string, give it to\n","    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n","  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n","    that approximates a normal distribution for all values\n","    In order to get more fine grained measure of overall Sentence sentiment\n","    Sentiment values will be Normalized/Standardized so absolute precision is not required\n","  '''\n","  text_sentiment_total = 0.\n","  text_ls = text.split()\n","  text_len = len(text_ls)\n","  for aword in text_ls:\n","    text_sentiment_total += afinn.score(aword)\n","  text_sentiment_norm = text_sentiment_total/(np.log(text_len)+0.1)\n","\n","  return float(text_sentiment_norm)  # return float vs np.float64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kRCd4VpwDO0Q"},"outputs":[],"source":["# Test\n","\n","if AFINN_Arc == True:\n","  sent_test='I hate Mondays.'\n","  print(afinn_discrete2continous_sentiment(sent_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrP_wxB3DO0S"},"outputs":[],"source":["# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","\n","# Calculate all Sentiment values and variants\n","if AFINN_Arc == True:\n","  get_sentiments(model_base=model_base, sentiment_fn=afinn_discrete2continous_sentiment, sentiment_type='function')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgZaMPYKDO0T"},"outputs":[],"source":["# Get/Set Sentiment Statistics\n","\n","if AFINN_Arc == True:\n","  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n","  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n","  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n","  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n","\n","  # Validate\n","  corpus_lexicons_stats_dt\n","\n","  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clUzylGOog5h"},"outputs":[],"source":["# Verify \n","\n","if AFINN_Arc == True:\n","  # corpus_sents_df['afinn'].plot(alpha=0.3)\n","  corpus_sents_df['afinn_stdscaler'].plot(alpha=0.3)\n","  corpus_sents_df['afinn_medianiqr'].plot(alpha=0.1)\n","  plt.legend(loc='best');"]},{"cell_type":"markdown","metadata":{"id":"gAEiglIPDfFI"},"source":["### **Calculate VADER Sentiment Polarities (Optional: Auto)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgkvefzpDgx-"},"outputs":[],"source":["if VADER_Arc == True:\n","  model_base = 'vader'\n","  model_name = 'vader_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wodGtjXhDmZN"},"outputs":[],"source":["if VADER_Arc == True:\n","  # Sentiment evaluation function\n","  sid = SentimentIntensityAnalyzer()\n","\n","  # Test\n","  sid.polarity_scores('hello world')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dS8e25MkDmZP"},"outputs":[],"source":["# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","\n","if VADER_Arc == True:\n","  get_sentiments(model_base=model_base, sentiment_fn=sid.polarity_scores, sentiment_type='compound')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2Azyv2lDmZP"},"outputs":[],"source":["# Get/Set Sentiment Statistics\n","\n","if VADER_Arc == True:\n","  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n","  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n","  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n","  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n","\n","  # Validate\n","  corpus_lexicons_stats_dt\n","\n","  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yq8KNwpjom4X"},"outputs":[],"source":["# Verify \n","\n","if VADER_Arc == True:\n","  # corpus_sents_df['vader'].plot(alpha=0.3)\n","  corpus_sents_df['vader_stdscaler'].plot(alpha=0.1)\n","  corpus_sents_df['vader_medianiqr'].plot(alpha=0.3)\n","  plt.legend(loc='best');"]},{"cell_type":"markdown","metadata":{"id":"iCN4c-G48e7-"},"source":["### **Calculate TextBlob Sentiment Polarities (Optional: Auto)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6MfVWZ34Vg8U"},"outputs":[],"source":["if TextBlob_Arc == True:\n","  model_base = 'textblob'\n","  model_name = 'textblob_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"118Blghk7fjp"},"outputs":[],"source":["if TextBlob_Arc == True:\n","  from textblob import TextBlob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhJsYxPoVhY4"},"outputs":[],"source":["# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","\n","def textblob_sentiment(text_str):\n","  '''\n","  Given a text string\n","  Return a sentiment value between -1.0 to +1.0 using TextBlob\n","  '''\n","  return TextBlob(text_str).sentiment.polarity\n","\n","# Calculate all Sentiment values and variants\n","if TextBlob_Arc == True:\n","  get_sentiments(model_base=model_base, sentiment_fn=textblob_sentiment, sentiment_type='function')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlHRf2aFVhY7"},"outputs":[],"source":["# Get/Set Sentiment Statistics\n","\n","if TextBlob_Arc == True:\n","  get_lexstats(corpus_sents_df, model_name, text_unit='sentence')\n","  get_lexstats(corpus_parags_df, model_name, text_unit='paragraph')\n","  get_lexstats(corpus_sects_df, model_name, text_unit='section')\n","  get_lexstats(corpus_chaps_df, model_name, text_unit='chapter')\n","\n","  # Validate\n","  corpus_lexicons_stats_dt\n","\n","  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMWewkTgord1"},"outputs":[],"source":["# Verify \n","\n","if TextBlob_Arc == True:\n","  # corpus_sents_df['textblob'].plot(alpha=0.3)\n","  corpus_sents_df['textblob_stdscaler'].plot(alpha=0.3)\n","  corpus_sents_df['textblob_medianiqr'].plot(alpha=0.1)\n","  plt.legend(loc='best');"]},{"cell_type":"markdown","metadata":{"id":"G2blGfVlKb_s"},"source":["### **Calculate Pattern Sentiment Polarities (Optional: Auto)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRDctGTdUfKk"},"outputs":[],"source":["# Pattern library has a bug wherein a/several Sentences have far outlying Sentiments\n","#   we need to clip these to within n * MAD\n","\n","if Pattern_Arc == True:\n","  MAD_Clip_Boundary = 5.2 #@param {type:\"slider\", min:1.0, max:10, step:0.1}\n","\n","  # find the limits of 2.5 Median Absolute Deviation of Pattern Time Series\n","\n","  clip_25mad = MAD_Clip_Boundary * robust.mad(corpus_sents_df['pattern'])\n","  print(f'Clip Pattern Series at 2.5 x Median Absolute Deviation (MAD) = {clip_25mad}')\n","\n","  # Create a Temporary DataFrame to test/find best MAD Clipping multiplier for Pattern\n","  temp_df['pattern'] = pd.Series(corpus_sents_df['pattern'].clip(upper=clip_25mad))\n","  temp_df['pattern'].clip(lower=-clip_25mad, inplace=True)\n","\n","  # Verify \n","  temp_df['pattern'].plot();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"co3jaquyZom7"},"outputs":[],"source":["# Once a good Clip_MAD_multiplier if found, update Pattern Time Series with it\n","\n","if Pattern_Arc == True:\n","  corpus_sents_df['pattern'] = temp_df['pattern']\n","  corpus_sents_df['pattern'].plot();"]},{"cell_type":"markdown","metadata":{"id":"wsaziON_Z263"},"source":["### **Calculate Stanza/OpenNLP Sentiment Polarities (Optional: Auto)**\n","\n","* https://github.com/piyushpathak03/NLP-using-STANZA/blob/main/Stanza.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZgGfcCuFnmI"},"outputs":[],"source":["if Stanza_Arc == True:\n","  model_base = 'stanza'\n","  model_name = 'stanza_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoZUi2AwZ_7L"},"outputs":[],"source":["if Stanza_Arc == True:\n","  !pip install stanza"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5txTb6aIZ2tN"},"outputs":[],"source":["%time\n","\n","import stanza\n","\n","if Stanza_Arc == True:\n","  stanza.download('en')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NORYbxsxZ2qg"},"outputs":[],"source":["if Stanza_Arc == True:\n","  nlp = stanza.Pipeline('en', processors='tokenize,sentiment')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtOtBfYwZ2na"},"outputs":[],"source":["# Test stanza directly\n","\n","# doc = nlp('Ram is a bad boy')\n","# for i, sentence in enumerate(doc.sentences):\n","#     print(i, sentence.sentiment)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKnox67kayod"},"outputs":[],"source":["# Calculate Stanza Sentiment [0,1,2]\n","\n","def stanza_discrete2continous_sentiment(text):\n","  '''\n","  Given a plain text string, give it to\n","    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n","  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n","    that approximates a normal distribution for all values\n","    In order to get more fine grained measure of overall Sentence sentiment\n","    Sentiment values will be Normalized/Standardized so absolute precision is not required\n","  '''\n","  text_sentiment_tot = 0.\n","  text_ls = text.split()\n","  text_len = len(text_ls)\n","  for aword in text_ls:\n","    adoc = nlp(aword)\n","    for i, sentence in enumerate(adoc.sentences):\n","      text_sentiment_tot += float(sentence.sentiment)\n","  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.1)\n","\n","  return text_sentiment_norm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNngkBBAF26C"},"outputs":[],"source":["# Test\n","\n","if Stanza_Arc == True:\n","  sent_test='I hate Mondays.'\n","  print(stanza_discrete2continous_sentiment(sent_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MrY2mrhVF26D"},"outputs":[],"source":["# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","\n","# NOTE: requires about 30-50mins (20210708 at 0730) Colab Pro: GPU+RAM \n","#                      2hrs30mins (20210802 at 1330) Colab Pro: CPU only\n","\n","# Calculate all Sentiment values and variants\n","if Stanza_Arc == True:\n","  get_sentiments(model_base=model_base, sentiment_fn=stanza_discrete2continous_sentiment, sentiment_type='function')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSiYC73nF26D"},"outputs":[],"source":["# Get/Set Sentiment Statistics\n","\n","if Stanza_Arc == True:\n","  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n","  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n","  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n","  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n","\n","  # Validate\n","  corpus_lexicons_stats_dt\n","\n","  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S5tUYZA1DExu"},"outputs":[],"source":["# Verify \n","\n","if Stanza_Arc == True:\n","  # corpus_sents_df['stanza'].plot(alpha=0.3)\n","  corpus_sents_df['stanza_stdscaler'].plot(alpha=0.3)\n","  corpus_sents_df['stanza_medianiqr'].plot(alpha=0.1)\n","  plt.legend(loc='best');"]},{"cell_type":"markdown","metadata":{"id":"AIGQgWvyOtg6"},"source":["### **Calculate Flair Sentiment Polarities (Optional: Auto)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWbcaaDDO1J1"},"outputs":[],"source":["if Flair_Arc == True:\n","  model_base = 'flair'\n","  model_name = 'flair_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ooLdktLHO1J3"},"outputs":[],"source":["if Flair_Arc == True:\n","  !pip install flair"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77-3mVyRPJbb"},"outputs":[],"source":["if Flair_Arc == True:\n","  from flair.models import TextClassifier\n","  from flair.data import Sentence\n","\n","  classifier = TextClassifier.load('en-sentiment')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJOopGZhO1J5"},"outputs":[],"source":["# Test\n","\n","if Flair_Arc == True:\n","  sentence = Sentence('The food was great!')\n","  classifier.predict(sentence)\n","\n","  # print sentence with predicted labels\n","  print('Sentence above is: ', sentence.labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7fZT10HQDqd"},"outputs":[],"source":["def get_flairsentiment(text_str):\n","  # TODO: For efficiency, combine sentences in batches as arrays (if possible)\n","  '''\n","  Given a text string\n","  Return a floating point -1.0 to 1.0 value for Sentiment\n","  '''\n","\n","  text_tokenized_obj = Sentence(text_str)\n","  classifier.predict(text_tokenized_obj)\n","\n","  # print(f'Processing text_str: {text_str}')\n","  sentiment_str = str(text_tokenized_obj.labels[0])\n","\n","  sentiment_ls = sentiment_str.split(' ')\n","  \n","  sentiment_sign = sentiment_ls[0]\n","\n","  if sentiment_sign.lower() == 'positive':\n","    sign_multiplier = 1.0\n","  else:\n","    sign_multiplier = -1.0\n","\n","  sentiment_abs = float(sentiment_ls[1][1:-1])\n","\n","  sentiment_fl = sign_multiplier * sentiment_abs \n","\n","  return sentiment_fl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sbgQVQlNSdcE"},"outputs":[],"source":["# Test\n","\n","if Flair_Arc == True:\n","  get_flairsentiment('it is.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WX0PDn5AO1J9"},"outputs":[],"source":["# Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","\n","# Calculate all Sentiment values and variants\n","if Flair_Arc == True:\n","  get_sentiments(model_base=model_base, sentiment_fn=get_flairsentiment, sentiment_type='function')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3v5Q2ota94_r"},"outputs":[],"source":["corpus_sents_df[corpus_sents_df['sent_clean'].str.find('smokeing') != -1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfJEaIw1-QLs"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vlK-dyHO1J-"},"outputs":[],"source":["# Get/Set Sentiment Statistics\n","\n","if Flair_Arc == True:\n","  get_lexstats(corpus_sents_df, model_base, text_unit='sentence')\n","  get_lexstats(corpus_parags_df, model_base, text_unit='paragraph')\n","  get_lexstats(corpus_sects_df, model_base, text_unit='section')\n","  get_lexstats(corpus_chaps_df, model_base, text_unit='chapter')\n","\n","  # Validate\n","  corpus_lexicons_stats_dt\n","\n","  # corpus_lexicons_stats_dt['vader']['sents']['sentiment_max']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VycWC2gDO1J_"},"outputs":[],"source":["# Verify \n","\n","if Flair_Arc == True:\n","  corpus_sents_df['flair'].plot(alpha=0.3)\n","  corpus_sents_df['flair_stdscaler'].plot(alpha=0.3)\n","  corpus_sents_df['flair_medianiqr'].plot(alpha=0.1)\n","  plt.legend(loc='best');"]},{"cell_type":"markdown","metadata":{"id":"yaISmTWT-pev"},"source":["### **Calculate FRENCH FEEL Lexicon Sentiment Polarities (Optional: Auto)**\n","\n","* https://github.com/kujjwal02/French_Tragedies_Dataset/blob/master/French%20Tragedies.ipynb\n","* https://github.com/shalinijaiswalsj09/French-Language-Processing-/blob/master/feelNLP.py (201802271s) FEEL Lexical analysis\n","*https://github.com/nishchaychawla/Sentiment_Analysis_of_french_text (20180226 3s) Sentiment analysis of ftragedy dataset using tidytext and French Expanded Emotion Lexicon\n","* https://github.com/kujjwal02/French_Tragedies_Dataset (20180502 5s) Lexical SA of French Tragedies from Kaggle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMCz0gt7-a6f"},"outputs":[],"source":["lexicon_feel_df = pd.read_csv('https://raw.githubusercontent.com/kujjwal02/French_Tragedies_Dataset/master/FEEL.csv',delimiter=';', index_col=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urA-XaQH-nnz"},"outputs":[],"source":["lexicon_feel_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hW_MJVzawiR3"},"outputs":[],"source":["lexicon_feel_df[lexicon_feel_df.word == 'bien']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IE7MPz6Swh3y"},"outputs":[],"source":["lexicon_feel_df = lexicon_feel_df.drop(2831)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1Go8lTAxTyg"},"outputs":[],"source":["lexicon_feel_df.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HC4PWccVxbSF"},"outputs":[],"source":["lexicon_feel_df.set_index('word')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVA_kK3ZxbJx"},"outputs":[],"source":["lexicon_feel_df.set_index('word').loc['moins']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDos_t5kxuA8"},"outputs":[],"source":["lexicon_feel_df.polarity.value_counts().plot(kind='bar')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqDUqj5Dxt8G"},"outputs":[],"source":["lexicon_feel_df[lexicon_feel_df.word.str.split().map(lambda lst: len(lst)) > 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8gU6G1g1Jar"},"outputs":[],"source":["lexicon_feel_df['length'] = lexicon_feel_df.word.str.split().map(lambda lst: len(lst))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxH8GuKF0WvE"},"outputs":[],"source":["lexicon_feel_dt = lexicon_feel_df.set_index('word')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eBjtuuS34nSY"},"outputs":[],"source":["lexicon_feel_dt.loc['mal']['polarity']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OeVXivA5xt4A"},"outputs":[],"source":["def get_frsentiment(fr_sent):\n","  '''\n","  Given a text sentence in French\n","  Return a sentiment 1.0 (positive) or -1.0 (negative)\n","  '''\n","\n","  fr_sent_clean = fr_sent.translate(str.maketrans('', '', string.punctuation))\n","  asent_sentiment = 0.0\n","  pol_fl = 0.0\n","  token_ls = fr_sent_clean.split()\n","  for i, atoken in enumerate(token_ls):\n","    try:\n","      atoken_sentiment = lexicon_feel_dt.loc[atoken.lower()]['polarity']\n","      print(f'atoken_sentiment: {atoken_sentiment}')\n","      if atoken_sentiment.lower().strip() == 'positive':\n","        pol_fl = 1.0\n","      elif atoken_sentiment.lower().strip() == 'negative':\n","        pol_fl = -1.0\n","      else:\n","        print(f'ERROR: atoken_sentiment = {atoken_sentiment}')\n","\n","      asent_sentiment += pol_fl\n","    except KeyError:\n","      pass\n","\n","  return asent_sentiment\n","\n","# Test\n","\n","test_fr_sent1 = u\"C'est une voiture mal y terribles y horrible.\"\n","test_fr_sent2 = u\"C'est une voiture super y bon y bonne.\"\n","test_fr_sent3 = u\"C'est une voiture.\"\n","\n","print(get_frsentiment(re.escape(test_fr_sent1)))\n","\n","\n","\n","\"\"\"\n","  # for window_size in range(6, 0, -1):\n","  for each in window(sent.split(), window_size):\n","      sub_str = ' '.join(each)\n","      try:\n","          sentiment = lexicon_dict.loc[sub_str]\n","          sent.replace(sub_str, '')\n","          yield sentiment\n","#                 if window_size > 1:\n","#                     print(sub_str, ':', proprty['polarity'])\n","      except KeyError:\n","          pass\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Jjdto_Ixt0D"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivEy4LGOxtwU"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"CcLU3Gr46LLl"},"source":["### **Calculate FRENCH VADER Sentiment Polarities (Optional: Auto)**\n","\n","* https://github.com/thomas7lieues/vader_FR\n","* https://github.com/brunneis/vader-multi "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpSWmVK-6CYW"},"outputs":[],"source":["!pip install vaderSentiment-fr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJCxXQZ67yGx"},"outputs":[],"source":["SIA = SentimentIntensityAnalyzer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1tydph07x-s"},"outputs":[],"source":["phrase = \"Une phrase très cool à analyser\"\n","\n","score = SIA.polarity_scores(phrase)\n","\n","print(score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMG_TtQY_TKI"},"outputs":[],"source":["model_base = 'vaderfr'\n","model_name = 'vaderfr_lnorm_medianiqr'\n","\n","col_medianiqr = f'{model_base}_medianiqr'\n","col_meanstd = f'{model_base}_meanstd'\n","\n","col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n","\n","get_sentiments(model_base=model_base, sentiment_fn=SIA.polarity_scores, sentiment_type='compound', text_prep='raw')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LiWihAaN7x6N"},"outputs":[],"source":["get_sentiments(model_base=model_base, sentiment_fn=SIA.polarity_scores, sentiment_type='compound', text_prep='raw')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d-8wBC4nCiKg"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVjg7jXcCm5E"},"outputs":[],"source":["\n","corpus_sents_df['vaderfr_stdscaler_roll10'] = corpus_sents_df['vaderfr_stdscaler'].rolling(win_s1per*10, center=True).mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDx4aNeDDASj"},"outputs":[],"source":["corpus_sents_df['vaderfr_stdscaler_roll10'].plot()\n","plt.title(f'{CORPUS_FULL}\\nFrench VADER Sentence StdScaler SMA 10%')"]},{"cell_type":"markdown","metadata":{"id":"mCY-BlXr6DNH"},"source":["### **Calculate FRENCH TextBlob Sentiment Polarities (Optional: Auto)**\n","\n","* https://pypi.org/project/textblob-fr/ "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0A2pFRHpLzL"},"outputs":[],"source":["!pip install textblob_fr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tqtw1BE_6AdW"},"outputs":[],"source":["# import re\n","# import spacy\n","# from spacy.lang.fr.stop_words import STOP_WORDS\n","from textblob import Blobber\n","from textblob_fr import PatternTagger, PatternAnalyzer\n","TextBlobFr = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n","# from nltk.tokenize import sent_tokenize\n","# from nltk.tokenize import word_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hu1j1jQlt6Wz"},"outputs":[],"source":["TextBlobFrench_Arc = True\n","\n","if TextBlobFrench_Arc == True:\n","  model_base = 'textblobfr'\n","  model_name = 'textblobfr_lnorm_medianiqr'\n","\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_meanstd = f'{model_base}_meanstd'\n","\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTIDEsrnqW_C"},"outputs":[],"source":["# Test\n","\n","blob2 = TextBlobFr(u\"C'est une voiture terribles.\")\n","blob2.sentiment\n","blob2 = TextBlobFr(u\"C'est une voiture super y bon.\")\n","blob2.sentiment\n","blob2 = TextBlobFr(u\"C'est une voiture.\")\n","blob2.sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqKdowoppQFn"},"outputs":[],"source":["for i in range(5):\n","  test_sent = corpus_sents_df.iloc[i]['sent_raw']\n","  print(f'test_sent: {test_sent}')\n","  test_score = TextBlobFr(test_sent).sentiment[0]\n","  print(test_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lxo3cl-SqJpl"},"outputs":[],"source":["def textblobfr_sentiment(text_str):\n","  '''\n","  Given a text string\n","  Return a sentiment value between -1.0 to +1.0 using TextBlob\n","  '''\n","  return TextBlobFr(text_str).sentiment[0]\n","\n","# Test\n","\n","test_sents_ls = [u\"C'est une voiture terribles.\", u\"C'est une voiture super y bon.\", u\"C'est une voiture.\"]\n","\n","for i,asent in enumerate(test_sents_ls):\n","  print(f'Sent #{i} Sentiment: {textblobfr_sentiment(asent)}\\n     Sentence: {asent}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HHuIWpF8uwGY"},"outputs":[],"source":["# Calculate all Sentiment values and variants\n","if TextBlobFrench_Arc == True:\n","  get_sentiments(model_base=model_base, sentiment_fn=textblobfr_sentiment, sentiment_type='function')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3mSF91nlv8cP"},"outputs":[],"source":["win_s1per = int(1/100 * corpus_sents_df.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvuGwwshvil4"},"outputs":[],"source":["\n","\n","corpus_sents_df['textblobfr_stdscaler_roll10'] = corpus_sents_df['textblobfr_stdscaler'].rolling(10*win_s1per, center=True).mean()\n","corpus_sents_df['textblobfr_stdscaler_roll10'].plot()\n","plt.title(f'{CORPUS_FULL}\\nFrench TextBlob StdScaler SMA 10%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JljMuTggpWya"},"outputs":[],"source":["sentiments = []\n","\n","for i in range(5):\n","  asent = corpus_sents_df.iloc[i]['sent_raw']\n","  sentiment = tb(asent).sentiment[0]\n","  if (sentiment > 0):\n","      sentiments.append('Positif')\n","  elif (sentiment < 0):\n","      sentiments.append('Negatif')\n","  else:\n","      sentiments.append('Neutre') \n","\n","[f'{x}\\n' for x in sentiments]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbo61bCr7PPb"},"outputs":[],"source":["\"\"\"\n","\n","def load_data():\n","  # https://github.com/adepril/CustomerSatisfactionAnalysisOfInsuranceCompanies/blob/main/app.py\n","\n","  df = pd.read_csv(\"https://raw.githubusercontent.com/adepril/datasets/main/insurance-reviews-france-Comments.csv\")\n","  df = df.drop(['Unnamed: 0'],axis=1)\n","  df = df.dropna()\n","  df[\"Comment\"]= df[\"Comment\"].str.lower()\n","\n","  # Word Tokenization and deleting punctuation\n","  comments=[]\n","  for comment in df[\"Comment\"].apply(str):\n","      WordTokenizer = []\n","      for word in  re.sub(\"\\W\",\" \",comment ).split():\n","          WordTokenizer.append(word)\n","      comments.append(WordTokenizer)\n","\n","  #Ajoute une nouvelle colonne\n","  df[\"Word_Tokenizer\"]= comments\n","\n","  # Set new Spacy's Stop Word list by deleting negation word \n","  stop_words=set(STOP_WORDS)\n","\n","  deselect_stop_words = ['n\\'','ne','pas','plus','personne','aucun','ni','aucune','rien']\n","  for w in deselect_stop_words:\n","      if w in stop_words:\n","          stop_words.remove(w)\n","      else:\n","          continue\n","\n","  # Add a new column for comments without StopWords\n","  AllfilteredComment=[]\n","  for comment in df[\"Word_Tokenizer\"]:\n","      filteredComment = [w for w in comment if not ((w in stop_words) or (len(w) == 1))]\n","      AllfilteredComment.append(' '.join(filteredComment))\n","      \n","  df[\"CommentAferPreproc\"]=AllfilteredComment\n","\n","  # Sentiment Analysis with TextBlob\n","  sentiments = []\n","  for i in df[\"CommentAferPreproc\"]:\n","      sentiment = tb(i).sentiment[0]\n","      if (sentiment > 0):\n","          sentiments.append('Positif')\n","      elif (sentiment < 0):\n","          sentiments.append('Negatif')\n","      else:\n","          sentiments.append('Neutre')   \n","\n","  # Ajoute une colonne : Sentiment\n","  df[\"sentiment\"]=sentiments\n","\n","  return df\n","\n","df = load_data()\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZpW9HYk7PBC"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"Rozr6h7x6cmO"},"source":["### **Calculate FRENCH SVM Sentiment Polarities (Optional: Auto)**\n","\n","* https://github.com/amineabdaoui/python-sentiment-classification (20200813) Pretrained SVM Sentiment Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3z39csg6hMx"},"outputs":[],"source":["!git clone https://github.com/amineabdaoui/python-sentiment-classification.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzdeNyxz5Wwe"},"outputs":[],"source":["%cd ./python-sentiment-classification/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QB6_irHF65i2"},"outputs":[],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGQIyG2q65XH"},"outputs":[],"source":["import predictSentiment as ps\n","\n","print(ps.predictFrench('Je suis content, il fait beau c\\'est super ! '))\n","print(ps.predictFrench('Je suis triste, il pleut c\\'est horrible ! '))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5rJwLyU7DER"},"outputs":[],"source":["%cd ..\n","!pwd"]},{"cell_type":"markdown","metadata":{"id":"kfMwbhVMwgXw"},"source":["### **(Optional) Calculate SentimentR and SyuzhetR Sentiments in RStudio**\n","\n","**NOTE** Process in RStudio with the following R Script\n","```\n","# Setup\n","# getwd()\n","# list.files(pattern='*.csv')\n","# setwd('./sentimenttime')\n","\n","###\n","# Begin SyuzhetR Preprocessing\n","library('syuzhet')\n","\n","# import the RAW Sentences.csv, not CLEAN\n","# because SentimentR assigns greater polarity to HATE > hate\n","# TODO: Compare CLEAN vs RAW for SentimentR and VADER for impact of such Heuristics\n","\n","# >>> CUSTOMIZE THIS FOR EACH CORPUS <<<\n","corpus_name = 'ddefoe_robinsoncrusoe' \n","# >>> CUSTOMIZE THIS FOR EACH CORPUS <<<\n","\n","# SentimentTime Preprocessing (e.g. corpus_text_sents_raw_ddefoe_robinsoncrusoe.csv)\n","input_filename_prefix = 'corpus_text_sents_raw_'\n","input_filename_suffix = '.csv'\n","\n","corpus_input_filename = trimws(paste0(input_filename_prefix, corpus_name, input_filename_suffix, sep=' '))\n","\n","###\n","syuzhet_output_prefix = 'sum_sentiments_syuzhetR_4models_sentimenttimeraw_'\n","syuzhet_output_suffix = '.csv'\n","syuzhet_output = trimws(paste0(syuzhet_output_prefix, corpus_name, syuzhet_output_suffix, sep=' '))\n","\n","\n","# Use 4 Models in Syuzhet to parse Corpus and generate 4 Sentiment Time Series\n","\n","# (OPTION A) Read preprocessed Corpus *.csv (Sentence Tokenized and text cleaned by sentimenttime)\n","corpus_str <- read.csv(corpus_input_filename,header=T)$sent_raw\n","\n","# corpus_sents_v <- syuzhet::get_sentences(corpus_str) # SyuzhetR often splits a line with one Sentence into several lines\n","corpus_sents_v <- read.csv(corpus_input_filename, header = TRUE, row.names = 1)\n","\n","# (OPTION B) Read raw textfile using Syuzhet Sentence Tokenizer and text preprocessing\n","# corpus_str <- syuzhet::get_text_as_string(corpus_input)\n","# corpus_sents_v <- syuzhet::get_sentences(corpus_str)\n","\n","# Read in RAW Sentences\n","syuzhet_all_df <- data.frame(sent_raw = corpus_sents_v)\n","\n","# Compute Sentiment values for each Model \n","syuzhet_all_df$syuzhet <- syuzhet::get_sentiment(corpus_sents_v, method='syuzhet')\n","syuzhet_all_df$bing <- syuzhet::get_sentiment(corpus_sents_v, method='bing')\n","syuzhet_all_df$afinn <- syuzhet::get_sentiment(corpus_sents_v, method='afinn')\n","syuzhet_all_df$nrc <- syuzhet::get_sentiment(corpus_sents_v, method='nrc')\n","\n","# Save Syuzhet Results    \n","write.csv(syuzhet_all_df, syuzhet_output)\n","\n","\n","\n","# Begin SentimentR Processing\n","library('sentimentr')\n","\n","# Set Output Sentiments Datafile names\n","sentimentr_output_prefix = 'sum_sentiments_sentimentR_7models_sentimenttimeraw_'\n","sentimentr_output_suffix = '.csv'\n","sentimentr_output = trimws(paste0(sentimentr_output_prefix, corpus_name, sentimentr_output_suffix, sep=' '))\n","\n","# Use vector of RAW sentences already read (originally parsed by SentimentTime.py) \n","# sentimentr_sents_v <- sentimentr::get_sentences(corpus_sents_v)\n","# sentimentr_sents_v <- corpus_sents_v\n","  \n","  \n","# Create data.frame with jockers_rinker sentiments\n","sentimentr_all_df <- data.frame(sent_raw = corpus_sents_v)\n","\n","# sentimentr_sents_v <- read.csv(corpus_input_filename, header = TRUE, row.names = 1)\n","# Create data.frame with jockers_rinker sentiments\n","# sentimentr_all_df <- data.frame(sent_raw = sentimentr_sents_v)\n","\n","# Code from sentimentr.R\n","# \n","# SentimentR function sentiment will not work on native R data types, only\n","#   types of 'get_sentences'/'get_sentences_char' created by passing text\n","#   through the sentence tokenizer textshape::split_sentence\n","# We fool SentimentR by copying and calling it's make_class function in utils.R\n","#   and passing our preprocessed text (sentence tokenization done in Python) to\n","#   ensure that SentimentR has the same number/alignment of Sentences in our Corpus\n","#   as all other Sentiment analysis methods\n","# \n","# get_sentences.character <- function(x, ...) {\n","#   out <- textshape::split_sentence(x, ...)\n","#   make_class(out, \"get_sentences\", \"get_sentences_character\")\n","# }\n","\n","# Code from SentimentR.r (in utils.R)\n","make_class <- function(x, ...) {\n","  class(x) <- unique(c(..., class(x)))    \n","  x\n","}\n","\n","# Add other lexicon sentiments\n","sentimentr_all_df$jockers_rinker <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_jockers_rinker, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n","sentimentr_all_df$jockers <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_jockers, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n","sentimentr_all_df$huliu <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_huliu, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n","sentimentr_all_df$lmcd <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_loughran_mcdonald, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n","sentimentr_all_df$nrc <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_nrc, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n","sentimentr_all_df$senticnet <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_senticnet, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n","sentimentr_all_df$sentiword <- sentimentr::sentiment(make_class(corpus_sents_v, \"get_sentences\", \"get_sentences_character\"), polarity_dt=lexicon::hash_sentiment_sentiword, hyphen=\" \", neutral.nonverb.like=TRUE)$sentiment\n","\n","write.csv(sentimentr_all_df, sentimentr_output)\n","```"]},{"cell_type":"markdown","metadata":{"id":"KNCGf1KZEpld"},"source":["## **Summary**\n","\n","Baseline Models\n","\n","* VADER [-1.0 to 1.0] zero peak\n","* TextBlob [-1.0 to 1.0] zero peak\n","* Stanza outliers [-1.0 to 199.0] pos, outliers(+peak)\n","* AFINN [-14 (-8 to 8) 20] discrete\n","* SentimentR 11,710 [-5.4 to 8.8] norm\n","* Syuzhet [-5.4 to 8.8] norm\n","* Bing [-100.0 (-20.0 to 20.0) 100] discrete, outliers\n","* Pattern [-1.0 to 1.0] norm\n","* SentiWord [-3.8 to 4.4] norm\n","* SenticNet [-3.8 to 10] norm\n","* NRC [-100.0 (-5.0 to 5.0) 100] zero, outliers\n","\n","SentimentR Models\n","\n","* Jockers_Rinker\n","* Jockers\n","* HuLiu\n","* NRC\n","* Loughran-McDonald\n","* SenticNet\n","* SentiWord\n","\n","SyuzhetR Models\n","\n","* Syuzhet\n","* Bing\n","* AFINN\n","* NRC\n","\n","Tranformer Models\n","\n","* NLPTown\n","* RoBERTa Large 15 Datasets\n","* BERT Yelp Dataset\n","* BERT Code Switching Hinglish\n","* IMDB 2-way \n","* Huggingface Default (Distilled BERT)\n","* T5 IMDB 50k Dataset\n","* RoBERTa XML 8 Languages"]},{"cell_type":"markdown","metadata":{"id":"6_j92DSgyhGc"},"source":["# **END OF WORKING**"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["_R11BEJ_47yA","8r3-sp6k47yE","QJO7kLz-47yF","EA1yTaY_9Qod","7dPPrZwyIIze","X229IbToHwa2","ik_8vPz36AaK","XUvKJEybUIeP","yXwKR4gA8Ouk","3YJJcvDVnUuT","qjb60CVnz5SF","PVCkjat0vffd","YFC8GTnw6HrG","j2tIua7tTSRz","XbvcZo4oiCa7","QZjqwTvU76AR","NA3dWsnF78mi","SkNZVk128jV9","dUcANLM_8mtT","Cn4KQYpH3glK","jRTjCPLb8cbB","gAEiglIPDfFI","iCN4c-G48e7-","AIGQgWvyOtg6"],"name":"master_sentimentarcs_part3_lex_ml_dnn_py.ipynb","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}