{"cells":[{"cell_type":"markdown","metadata":{"id":"pweLtPomQH13"},"source":["# **Huggingface Transformers Sentiment Analysis at the Command Line**\n","\n","Jon Chun\n","19 Jun 2019\n","\n","References:\n","\n","* https://github.com/barissayil/SentimentAnalysis"]},{"cell_type":"markdown","metadata":{"id":"ZnKuJjHQhPqN"},"source":["# Configuration (Auto)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8158,"status":"ok","timestamp":1631848669661,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"yyIA7Cwi_8km","outputId":"9cca75f4-c294-47f3-a099-c40c4efad501"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers[sentencepiece]\n","  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n","\u001b[K     |████████████████████████████████| 2.8 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.6.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.19.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 38.4 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 51.9 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.62.0)\n","Collecting huggingface-hub>=0.0.12\n","  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 48.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n","Collecting sentencepiece==0.1.91\n","  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 37.4 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers[sentencepiece]) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers[sentencepiece]) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.5.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2021.5.30)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, sentencepiece\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.45 sentencepiece-0.1.91 tokenizers-0.10.3 transformers-4.10.2\n"]}],"source":["!pip install transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2pwRf4iKBDpo"},"outputs":[],"source":["# !pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S5nj2VBR9qUE"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WpchZD9DID6N"},"outputs":[],"source":["from datetime import datetime"]},{"cell_type":"markdown","metadata":{"id":"qjow_h7Z_Izs"},"source":["**Configure Jupyter Notebook**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nD1cyqWsfjxA"},"outputs":[],"source":["# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"smPLNS9Q_Izw"},"outputs":[],"source":["# Configure Jupyter\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8fo_vRE_IcG"},"outputs":[],"source":["# SKIP TO NEXT SECTION"]},{"cell_type":"markdown","metadata":{"id":"8KvaPoGshPqR"},"source":["**Global Configuration Constants**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qpcNAP8QhPqZ"},"outputs":[],"source":["# Hardcoded Sentiment Analysis Models\n","\n","MODELS_LS = ['vader','textblob','stanza','afinn','bing','sentimentr','syuzhet','pattern','sentiword','senticnet','nrc']\n","\n","MODELS_LEX_LS = ['vader','textblob','stanza','afinn','bing','sentimentr','syuzhet','pattern','sentiword','senticnet','nrc']\n","MODELS_TRANS_LS = ['distilbertsst', 'nlptown','roberta_lg15','albertbv2','bertuc_gapps','bert_imdb']\n","MODELS_ALL_LS = MODELS_LEX_LS + MODELS_TRANS_LS\n","\n","# Minimum lengths for Sentences and Paragraphs\n","#   (Shorter Sents/Parags will be deleted)\n","\n","MIN_PARAG_LEN = 2\n","MIN_SENT_LEN = 2\n","\n","# Min/Max statistics on each lexicon's sentiment values applied to corpus\n","corpus_lexicons_stats_dt = {}\n","\n","\n","# Hardcoded Sentiment Analysis Models\n","\n","\n","            \n","# Minimum lengths for Sentences and Paragraphs\n","#   (Shorter Sents/Parags will be deleted)\n","\n","MIN_CHAP_LEN = 5000\n","MIN_SECT_LEN = 5000  # Minimum char length to be included in section DataFrame\n","MIN_PARAG_LEN = 2\n","MIN_SENT_LEN = 2\n","\n","# Min/Max statistics on each lexicon's sentiment values applied to corpus\n","corpus_lexicons_stats_dt = {}\n","corpus_cruxes_dt = {}\n","\n","# Crux Points Dict key:model, value:list of crux point tuples (x,y)\n","corpus_cruxes_all_dt = {}\n"]},{"cell_type":"markdown","metadata":{"id":"IcyAYgXYhPqd"},"source":["**Install Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6967,"status":"ok","timestamp":1631848676622,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"OMfCRQoThsIb","outputId":"ff08bad4-c91f-45c4-94fe-160dea7b8aa1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1631848676623,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"TBpDherrhPqf","outputId":"0eb2bd49-82fb-4c6c-8ede-ed53979fa2be"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n"]}],"source":["# INSTALL LIBRARIES\n","\n","!pip install sklearn"]},{"cell_type":"markdown","metadata":{"id":"6G0L_vDHhPqj"},"source":["**Import Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fE8zqIVqhPql"},"outputs":[],"source":["import os\n","import sys\n","import io\n","import glob\n","import contextlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eSclnGRhPqn"},"outputs":[],"source":["# IMPORT LIBRARIES\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LXP_vjHhPqp"},"outputs":[],"source":["import re\n","import string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kemoFwHMhPqs"},"outputs":[],"source":["import collections\n","from collections import OrderedDict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iY9v27SEhPqw"},"outputs":[],"source":["# Import libraries for logging\n","\n","import logging\n","from datetime import datetime\n","import time                     # (TODO: check no dependencies and delete)\n","from time import gmtime, strftime"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2156,"status":"ok","timestamp":1631848678770,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"UAW172t3hPqx","outputId":"db490e67-c146-4015-c1bc-5b5c00a8d7a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":16,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":16,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":16,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","\n","# Download for sentence tokenization\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')\n","\n","# Download for nltk/VADER sentiment analysis\n","nltk.download('vader_lexicon')\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7s6T7Vy_QLUb"},"outputs":[],"source":["# DTW\n","\n","import json\n","import numpy.fft\n","from decimal import Decimal\n","import math\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjlNsDRYhPqz"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler   # To normalize time series\n","from sklearn.preprocessing import StandardScaler # To sandardize time series"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cYbDnivUhPqz"},"outputs":[],"source":["# Smoothing\n","\n","from scipy import interpolate\n","from scipy.interpolate import CubicSpline\n","from scipy import signal\n","from scipy.signal import argrelextrema\n","import scipy.stats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSQjLhE3hPq0"},"outputs":[],"source":["from statsmodels.nonparametric.smoothers_lowess import lowess as sm_lowess\n","from statsmodels import robust"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YxKnWu8cC-L"},"outputs":[],"source":["# !pip install dtaidistance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90ug8zzjzQDo"},"outputs":[],"source":["# DTW\n","\n","# from dtaidistance import dtw\n","# from dtaidistance import clustering\n","# from dtaidistance import dtw_visualisation as dtwvis"]},{"cell_type":"markdown","metadata":{"id":"Dc7oR0-HhPq1"},"source":["**Configure Jupyter Notebook**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9SLVjdjhPq2"},"outputs":[],"source":["# Configure Jupyter\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = [16, 8]\n","plt.rcParams['figure.dpi'] = 100\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from ipywidgets import widgets, interactive\n","\n","# Configure Google Colab\n","\n","%load_ext google.colab.data_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GS_El2PiQlyP"},"outputs":[],"source":["# Text wrap\n","\n","from IPython.display import HTML\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"]},{"cell_type":"markdown","metadata":{"id":"qZCr4ctahPq4"},"source":["**Configuration Details Snapshot**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1631848678775,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"s8F_ACDghPq4","outputId":"85648a23-7bce-4c42-9f3d-195834616a96"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Snap Shot of Time, Machine, Data and Library/Version Blueprint\n","# TODO:"]},{"cell_type":"markdown","metadata":{"id":"9wiSBHxoOGZz"},"source":["# Pick ONE Method (a) or (b) to Get Corpus Textfile\n","\n","**Choose either (a) OR (b), not both**"]},{"cell_type":"markdown","metadata":{"id":"6KRfiXXQOZcq"},"source":["## **Connect to Google gDrive**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":49949,"status":"ok","timestamp":1631848728700,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"M3CRP-whhJkI","outputId":"ef062044-0fbc-420e-d638-8ac3b78199c2"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Mounted at /gdrive\n","/gdrive/MyDrive\n"]}],"source":["# Connect to Google gDrive\n","\n","# Flag to indicate first run through code \n","flag_first_run = True\n","\n","from google.colab import drive, files\n","drive.mount('/gdrive')\n","%cd /gdrive/MyDrive/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":375,"status":"ok","timestamp":1631866905939,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"s7-pXiQBw3Oy","outputId":"a8df4e97-d683-42a3-c5dc-6b5223345b8c"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["/gdrive/MyDrive/research/2021/sa_book_code/books_sa/mshelley_frankenstein\n"]}],"source":["# Select the Corpus subdirectory on your Google gDrive\n","\n","# Done\n","\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/fbaum_thewonderfulwizardofoz\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/cdickens_achristmascarol\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/cdickens_greatexpectations\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/ddefoe_robinsoncrusoe\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/emforster_howardsend\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/fdouglass_narrativelifeslave\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/fscottfitzgerald_thegreatgatsby\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/geliot_middlemarch\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/hjames_portraitofalady\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/homer_odyssey\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/imcewan_machineslikeme\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/jausten_prideandprejudice\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/jconrad_heartofdarkness\" #@param {type:\"string\"} \n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/jjoyce_portraitoftheartist\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/jkrowling_harrypotter\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/mproust_time\" #@param {type:\"string\"}\n","gdrive_subdir = \"./research/2021/sa_book_code/books_sa/mshelley_frankenstein\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/staugustine_confessions\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/tmorrison_beloved\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_mrsdalloway\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_thewaves\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vwoolf_orlando\" #@param {type:\"string\"}\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/vnabokov_palefire\" #@param {type:\"string\"}\n","\n","# Current\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/homer_odyssey\" #@param {type:\"string\"}\n","\n","# To do\n","# gdrive_subdir = \"./research/2021/sa_book_code/books_sa/geliot_middlemarch\" #@param {type:\"string\"}\n","\n","CORPUS_SUBDIR = gdrive_subdir\n","corpus_filename = CORPUS_SUBDIR\n","\n","# Change to working subdirectory\n","if flag_first_run == True:\n","  full_path_str = gdrive_subdir\n","  flag_first_run = False\n","else:\n","  full_path_str = f'/gdrive/MyDrive{gdrive_subdir[1:]}'\n","\n","%cd $full_path_str\n"]},{"cell_type":"markdown","metadata":{"id":"wEHyy5rI6OWC"},"source":["### **Option (a) Load Corpus Raw Text DataFrames**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1631866920081,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"f5XvvhZg6hTf","outputId":"ec0c72d3-fb45-4473-b850-bf0a2ba9ac84"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["mshelley_frankenstein\n"]}],"source":["# Get DataFrame filenames\n","\n","corpus_root = gdrive_subdir.split('/')[-1]\n","print(corpus_root)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"elapsed":514,"status":"ok","timestamp":1631866923712,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"_OTvpJHkyA9E","outputId":"8c28695d-1154-4836-c3b1-e3cfd5e856ea"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["/gdrive/MyDrive/research/2021/sa_book_code/books_sa/mshelley_frankenstein\n","-rw------- 1 root root  428446 Sep 17 00:36 mshelley_frankenstein.txt\n","-rw------- 1 root root  510187 Sep 17 01:42 sum_sentiments_syuzhetR_4models_mshelley_frankenstein.csv\n","-rw------- 1 root root  797623 Sep 17 01:42 sum_sentiments_sentimentR_7models_mshelley_frankenstein.csv\n","-rw------- 1 root root 2756961 Sep 17 01:51 sum_sentiments_sents_syuzhetr_mshelley_frankenstein.csv\n","-rw------- 1 root root 3815266 Sep 17 01:51 sum_sentiments_sents_sentimentr_mshelley_frankenstein.csv\n","-rw------- 1 root root  436149 Sep 17 01:52 corpus_text_sents_raw_mshelley_frankenstein.csv\n","-rw------- 1 root root  422352 Sep 17 01:52 corpus_text_sents_clean_mshelley_frankenstein.csv\n","-rw------- 1 root root 5865397 Sep 17 01:52 corpus_sents_baseline_mshelley_frankenstein.csv\n","-rw------- 1 root root  850343 Sep 17 01:52 corpus_sects_baseline_mshelley_frankenstein.csv\n","-rw------- 1 root root 1723015 Sep 17 01:52 corpus_parags_baseline_mshelley_frankenstein.csv\n","-rw------- 1 root root  856406 Sep 17 01:52 corpus_chaps_baseline_mshelley_frankenstein.csv\n"]}],"source":["!pwd\n","!ls -altr *"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":585,"status":"ok","timestamp":1631866954452,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"Plx6fOSn6UnD","outputId":"4dba22ba-c838-4faa-a2b0-271f4df6fd04"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Index(['sent_no', 'sent_raw'], dtype='object')"]},"execution_count":95,"metadata":{},"output_type":"execute_result"},{"data":{"text/plain":["(3282, 2)"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["# (Optional) Read Corpus Sentence Text Datafiles \n","\n","corpus_sents_text_filename = f'corpus_text_sents_raw_mshelley_frankenstein.csv'\n","# corpus_sents_text_filename = f'corpus_text_sentences_raw_ddefoe_robinsoncrusoe.csv'\n","\n","\n","corpus_sents_trans_df = pd.read_csv(corpus_sents_text_filename)\n","\n","corpus_sents_trans_df.rename(columns={'Unnamed: 0':'sent_no'}, inplace=True)\n","\n","corpus_sents_trans_df.columns\n","corpus_sents_trans_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":548},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1631866956534,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"eYTy77DT6U-I","outputId":"aacbb143-65c8-4ebe-b81c-0dd914e060b8"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.module+javascript":"\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"Letter\"],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n\"_To Mrs. Saville, England._\"],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n\"St. Petersburgh, Dec. th, .\"],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings.\"],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n\"I arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking.\"]],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"string\", \"sent_raw\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ","text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent_no</th>\n","      <th>sent_raw</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Letter</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>_To Mrs. Saville, England._</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>St. Petersburgh, Dec. th, .</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>You will rejoice to hear that no disaster has ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>I arrived here yesterday, and my first task is...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sent_no                                           sent_raw\n","0        0                                             Letter\n","1        1                        _To Mrs. Saville, England._\n","2        2                        St. Petersburgh, Dec. th, .\n","3        3  You will rejoice to hear that no disaster has ...\n","4        4  I arrived here yesterday, and my first task is..."]},"execution_count":96,"metadata":{},"output_type":"execute_result"},{"data":{"application/vnd.google.colaboratory.module+javascript":"\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 3277,\n            'f': \"3277\",\n        },\n{\n            'v': 3277,\n            'f': \"3277\",\n        },\n\"The light of that conflagration will fade away; my ashes will be swept into the sea by the winds.\"],\n [{\n            'v': 3278,\n            'f': \"3278\",\n        },\n{\n            'v': 3278,\n            'f': \"3278\",\n        },\n\"My spirit will sleep in peace, or if it thinks, it will not surely think thus.\"],\n [{\n            'v': 3279,\n            'f': \"3279\",\n        },\n{\n            'v': 3279,\n            'f': \"3279\",\n        },\n\"Farewell.\"],\n [{\n            'v': 3280,\n            'f': \"3280\",\n        },\n{\n            'v': 3280,\n            'f': \"3280\",\n        },\n\"He sprang from the cabin-window as he said this, upon the ice raft which lay close to the vessel.\"],\n [{\n            'v': 3281,\n            'f': \"3281\",\n        },\n{\n            'v': 3281,\n            'f': \"3281\",\n        },\n\"He was soon borne away by the waves and lost in darkness and distance.\"]],\n        columns: [[\"number\", \"index\"], [\"number\", \"sent_no\"], [\"string\", \"sent_raw\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ","text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent_no</th>\n","      <th>sent_raw</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3277</th>\n","      <td>3277</td>\n","      <td>The light of that conflagration will fade away...</td>\n","    </tr>\n","    <tr>\n","      <th>3278</th>\n","      <td>3278</td>\n","      <td>My spirit will sleep in peace, or if it thinks...</td>\n","    </tr>\n","    <tr>\n","      <th>3279</th>\n","      <td>3279</td>\n","      <td>Farewell.</td>\n","    </tr>\n","    <tr>\n","      <th>3280</th>\n","      <td>3280</td>\n","      <td>He sprang from the cabin-window as he said thi...</td>\n","    </tr>\n","    <tr>\n","      <th>3281</th>\n","      <td>3281</td>\n","      <td>He was soon borne away by the waves and lost i...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      sent_no                                           sent_raw\n","3277     3277  The light of that conflagration will fade away...\n","3278     3278  My spirit will sleep in peace, or if it thinks...\n","3279     3279                                          Farewell.\n","3280     3280  He sprang from the cabin-window as he said thi...\n","3281     3281  He was soon borne away by the waves and lost i..."]},"execution_count":96,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3282 entries, 0 to 3281\n","Data columns (total 2 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   sent_no   3282 non-null   int64 \n"," 1   sent_raw  3282 non-null   object\n","dtypes: int64(1), object(1)\n","memory usage: 51.4+ KB\n"]}],"source":["corpus_sents_trans_df.head()\n","corpus_sents_trans_df.tail()\n","corpus_sents_trans_df.info()"]},{"cell_type":"markdown","metadata":{"id":"HGEwrTS3d_pX"},"source":["### **Option (b): Load Previously Computed Transformer Sentiment Datasets**\n","\n","***Only do this if your Google subdirectory doesn't already contain a plain text file of your Corpus or you wish to overwrite it and use a newer version***"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"elapsed":394,"status":"ok","timestamp":1631867156654,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"3oVDQzaf3Kaq","outputId":"3f54ffa3-09cc-41cf-cbd5-2f1161f315e7"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-rw------- 1 root root  510187 Sep 17 01:42 sum_sentiments_syuzhetR_4models_mshelley_frankenstein.csv\n","-rw------- 1 root root  797623 Sep 17 01:42 sum_sentiments_sentimentR_7models_mshelley_frankenstein.csv\n","-rw------- 1 root root 2756961 Sep 17 01:51 sum_sentiments_sents_syuzhetr_mshelley_frankenstein.csv\n","-rw------- 1 root root 3815266 Sep 17 01:51 sum_sentiments_sents_sentimentr_mshelley_frankenstein.csv\n","-rw------- 1 root root  436149 Sep 17 01:52 corpus_text_sents_raw_mshelley_frankenstein.csv\n","-rw------- 1 root root  422352 Sep 17 01:52 corpus_text_sents_clean_mshelley_frankenstein.csv\n","-rw------- 1 root root 5865397 Sep 17 01:52 corpus_sents_baseline_mshelley_frankenstein.csv\n","-rw------- 1 root root  850343 Sep 17 01:52 corpus_sects_baseline_mshelley_frankenstein.csv\n","-rw------- 1 root root 1723015 Sep 17 01:52 corpus_parags_baseline_mshelley_frankenstein.csv\n","-rw------- 1 root root  856406 Sep 17 01:52 corpus_chaps_baseline_mshelley_frankenstein.csv\n"]}],"source":["!ls -altr *.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1631867174357,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"4D-2OUHE6U6c","outputId":"cb7fe009-2510-4c8b-94e0-cb385f2c1326"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-rw------- 1 root root 5865397 Sep 17 01:52 corpus_sents_baseline_mshelley_frankenstein.csv\n"]}],"source":["!ls -altr corpus_sents_baseline_*.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":481},"executionInfo":{"elapsed":925,"status":"ok","timestamp":1631867184997,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"-_eJHPZNBbXK","outputId":"dec226e8-59b1-42b5-f9a4-8df703b015d0"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>sent_no</th>\n","      <th>parag_no</th>\n","      <th>sect_no</th>\n","      <th>sent_raw</th>\n","      <th>sent_clean</th>\n","      <th>char_len</th>\n","      <th>token_len</th>\n","      <th>sentimentr</th>\n","      <th>sentimentr_stdscaler</th>\n","      <th>sentimentr_medianiqr</th>\n","      <th>sentimentr_lnorm_stdscaler</th>\n","      <th>sentimentr_lnorm_medianiqr</th>\n","      <th>syuzhet</th>\n","      <th>syuzhet_stdscaler</th>\n","      <th>syuzhet_medianiqr</th>\n","      <th>syuzhet_lnorm_stdscaler</th>\n","      <th>syuzhet_lnorm_medianiqr</th>\n","      <th>bing</th>\n","      <th>bing_stdscaler</th>\n","      <th>bing_medianiqr</th>\n","      <th>bing_lnorm_stdscaler</th>\n","      <th>bing_lnorm_medianiqr</th>\n","      <th>sentiword</th>\n","      <th>sentiword_stdscaler</th>\n","      <th>sentiword_medianiqr</th>\n","      <th>sentiword_lnorm_stdscaler</th>\n","      <th>sentiword_lnorm_medianiqr</th>\n","      <th>senticnet</th>\n","      <th>senticnet_stdscaler</th>\n","      <th>senticnet_medianiqr</th>\n","      <th>senticnet_lnorm_stdscaler</th>\n","      <th>senticnet_lnorm_medianiqr</th>\n","      <th>nrc</th>\n","      <th>nrc_stdscaler</th>\n","      <th>nrc_medianiqr</th>\n","      <th>nrc_lnorm_stdscaler</th>\n","      <th>nrc_lnorm_medianiqr</th>\n","      <th>afinn</th>\n","      <th>afinn_stdscaler</th>\n","      <th>...</th>\n","      <th>textblob_lnorm_medianiqr</th>\n","      <th>pattern</th>\n","      <th>pattern_stdscaler</th>\n","      <th>pattern_medianiqr</th>\n","      <th>pattern_lnorm_stdscaler</th>\n","      <th>pattern_lnorm_medianiqr</th>\n","      <th>stanza</th>\n","      <th>stanza_stdscaler</th>\n","      <th>stanza_medianiqr</th>\n","      <th>stanza_lnorm_stdscaler</th>\n","      <th>stanza_lnorm_medianiqr</th>\n","      <th>flair</th>\n","      <th>flair_stdscaler</th>\n","      <th>flair_medianiqr</th>\n","      <th>flair_lnorm_stdscaler</th>\n","      <th>flair_lnorm_medianiqr</th>\n","      <th>sentimentr_roll100</th>\n","      <th>sentimentr_stdscaler_roll10</th>\n","      <th>syuzhet_roll100</th>\n","      <th>syuzhet_stdscaler_roll10</th>\n","      <th>bing_roll100</th>\n","      <th>bing_stdscaler_roll10</th>\n","      <th>sentiword_roll100</th>\n","      <th>sentiword_stdscaler_roll10</th>\n","      <th>senticnet_roll100</th>\n","      <th>senticnet_stdscaler_roll10</th>\n","      <th>nrc_roll100</th>\n","      <th>nrc_stdscaler_roll10</th>\n","      <th>afinn_roll100</th>\n","      <th>afinn_stdscaler_roll10</th>\n","      <th>vader_roll100</th>\n","      <th>vader_stdscaler_roll10</th>\n","      <th>textblob_roll100</th>\n","      <th>textblob_stdscaler_roll10</th>\n","      <th>flair_roll100</th>\n","      <th>flair_stdscaler_roll10</th>\n","      <th>pattern_roll100</th>\n","      <th>pattern_stdscaler_roll10</th>\n","      <th>stanza_roll100</th>\n","      <th>stanza_stdscaler_roll10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Letter</td>\n","      <td>letter</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>-0.017066</td>\n","      <td>0.000000</td>\n","      <td>-0.017066</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.039</td>\n","      <td>-0.285565</td>\n","      <td>-0.167174</td>\n","      <td>-0.285565</td>\n","      <td>0.245632</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.013530</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>10.000000</td>\n","      <td>0.943158</td>\n","      <td>0.813018</td>\n","      <td>0.943158</td>\n","      <td>96.262640</td>\n","      <td>-0.5913</td>\n","      <td>-0.641646</td>\n","      <td>-0.565404</td>\n","      <td>-0.641646</td>\n","      <td>-6.797046</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>_To Mrs. Saville, England._</td>\n","      <td>to mrs saville england</td>\n","      <td>27</td>\n","      <td>4</td>\n","      <td>0.00</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>-0.017066</td>\n","      <td>0.000000</td>\n","      <td>-0.017066</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.000</td>\n","      <td>-0.311172</td>\n","      <td>-0.188763</td>\n","      <td>-0.311172</td>\n","      <td>-0.198164</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.013530</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>2.691257</td>\n","      <td>-1.444950</td>\n","      <td>-1.034394</td>\n","      <td>-1.444950</td>\n","      <td>3.479559</td>\n","      <td>0.8427</td>\n","      <td>0.902033</td>\n","      <td>0.162708</td>\n","      <td>0.902033</td>\n","      <td>2.239383</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>St. Petersburgh, Dec. th, .</td>\n","      <td>st petersburgh december th</td>\n","      <td>27</td>\n","      <td>5</td>\n","      <td>0.00</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>-0.017066</td>\n","      <td>0.000000</td>\n","      <td>-0.017066</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.799</td>\n","      <td>0.213444</td>\n","      <td>0.253529</td>\n","      <td>0.213444</td>\n","      <td>1.620261</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.013530</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>2.691257</td>\n","      <td>-1.444950</td>\n","      <td>-1.034394</td>\n","      <td>-1.444950</td>\n","      <td>2.140982</td>\n","      <td>0.9341</td>\n","      <td>1.000424</td>\n","      <td>0.209117</td>\n","      <td>1.000424</td>\n","      <td>1.970591</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>You will rejoice to hear that no disaster has ...</td>\n","      <td>you will rejoice to hear that no disaster has ...</td>\n","      <td>143</td>\n","      <td>23</td>\n","      <td>-0.40</td>\n","      <td>-0.293782</td>\n","      <td>-0.242424</td>\n","      <td>-0.293782</td>\n","      <td>-0.203385</td>\n","      <td>-0.40</td>\n","      <td>-0.282833</td>\n","      <td>-0.242424</td>\n","      <td>-0.282833</td>\n","      <td>-0.20745</td>\n","      <td>-24.180907</td>\n","      <td>-0.455440</td>\n","      <td>-0.491201</td>\n","      <td>-0.455440</td>\n","      <td>-0.518981</td>\n","      <td>-0.187500</td>\n","      <td>-0.105724</td>\n","      <td>-0.195556</td>\n","      <td>-0.105724</td>\n","      <td>-0.178792</td>\n","      <td>2.154</td>\n","      <td>1.103125</td>\n","      <td>1.003598</td>\n","      <td>1.103125</td>\n","      <td>0.867540</td>\n","      <td>-7.612961</td>\n","      <td>-0.625949</td>\n","      <td>-0.485653</td>\n","      <td>-0.625949</td>\n","      <td>-0.419321</td>\n","      <td>-0.618144</td>\n","      <td>-0.392231</td>\n","      <td>...</td>\n","      <td>-2.958494</td>\n","      <td>-0.317915</td>\n","      <td>-0.076285</td>\n","      <td>-2.177818</td>\n","      <td>-0.076285</td>\n","      <td>-2.520593</td>\n","      <td>6.490508</td>\n","      <td>-0.203557</td>\n","      <td>-0.074067</td>\n","      <td>-0.203557</td>\n","      <td>-0.406154</td>\n","      <td>-0.6601</td>\n","      <td>-0.715708</td>\n","      <td>-0.600338</td>\n","      <td>-0.715708</td>\n","      <td>-0.457827</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>I arrived here yesterday, and my first task is...</td>\n","      <td>i arrived here yesterday and my first task is ...</td>\n","      <td>145</td>\n","      <td>26</td>\n","      <td>2.55</td>\n","      <td>1.638930</td>\n","      <td>1.545455</td>\n","      <td>1.638930</td>\n","      <td>1.146975</td>\n","      <td>2.55</td>\n","      <td>1.677196</td>\n","      <td>1.545455</td>\n","      <td>1.677196</td>\n","      <td>1.16990</td>\n","      <td>70.454016</td>\n","      <td>1.822038</td>\n","      <td>1.431175</td>\n","      <td>1.822038</td>\n","      <td>1.337640</td>\n","      <td>2.083333</td>\n","      <td>2.637415</td>\n","      <td>2.226667</td>\n","      <td>2.637415</td>\n","      <td>1.780112</td>\n","      <td>3.143</td>\n","      <td>1.752494</td>\n","      <td>1.551066</td>\n","      <td>1.752494</td>\n","      <td>1.177428</td>\n","      <td>30.170243</td>\n","      <td>2.046460</td>\n","      <td>1.924647</td>\n","      <td>2.046460</td>\n","      <td>1.470030</td>\n","      <td>1.786726</td>\n","      <td>1.186369</td>\n","      <td>...</td>\n","      <td>1.439421</td>\n","      <td>0.168294</td>\n","      <td>0.085208</td>\n","      <td>1.152865</td>\n","      <td>0.085208</td>\n","      <td>1.180359</td>\n","      <td>8.338057</td>\n","      <td>0.400124</td>\n","      <td>0.392933</td>\n","      <td>0.400124</td>\n","      <td>-0.023186</td>\n","      <td>0.9813</td>\n","      <td>1.051234</td>\n","      <td>0.233082</td>\n","      <td>1.051234</td>\n","      <td>0.290826</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 93 columns</p>\n","</div>"],"text/plain":["   Unnamed: 0  sent_no  ...  stanza_roll100  stanza_stdscaler_roll10\n","0           0        0  ...             NaN                      NaN\n","1           1        1  ...             NaN                      NaN\n","2           2        2  ...             NaN                      NaN\n","3           3        3  ...             NaN                      NaN\n","4           4        4  ...             NaN                      NaN\n","\n","[5 rows x 93 columns]"]},"execution_count":104,"metadata":{},"output_type":"execute_result"}],"source":["sum_sentiments_transformer_series = 'corpus_sents_baseline_mshelley_frankenstein.csv'\n","\n","corpus_sents_trans_df = pd.read_csv(sum_sentiments_transformer_series)\n","corpus_sents_trans_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1631867197655,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"JjK4RFqw6UyA","outputId":"a2647049-718e-49c9-e5bc-5c940395a5bb"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent_no</th>\n","      <th>parag_no</th>\n","      <th>sect_no</th>\n","      <th>sent_raw</th>\n","      <th>sent_clean</th>\n","      <th>char_len</th>\n","      <th>token_len</th>\n","      <th>sentimentr</th>\n","      <th>sentimentr_stdscaler</th>\n","      <th>sentimentr_medianiqr</th>\n","      <th>sentimentr_lnorm_stdscaler</th>\n","      <th>sentimentr_lnorm_medianiqr</th>\n","      <th>syuzhet</th>\n","      <th>syuzhet_stdscaler</th>\n","      <th>syuzhet_medianiqr</th>\n","      <th>syuzhet_lnorm_stdscaler</th>\n","      <th>syuzhet_lnorm_medianiqr</th>\n","      <th>bing</th>\n","      <th>bing_stdscaler</th>\n","      <th>bing_medianiqr</th>\n","      <th>bing_lnorm_stdscaler</th>\n","      <th>bing_lnorm_medianiqr</th>\n","      <th>sentiword</th>\n","      <th>sentiword_stdscaler</th>\n","      <th>sentiword_medianiqr</th>\n","      <th>sentiword_lnorm_stdscaler</th>\n","      <th>sentiword_lnorm_medianiqr</th>\n","      <th>senticnet</th>\n","      <th>senticnet_stdscaler</th>\n","      <th>senticnet_medianiqr</th>\n","      <th>senticnet_lnorm_stdscaler</th>\n","      <th>senticnet_lnorm_medianiqr</th>\n","      <th>nrc</th>\n","      <th>nrc_stdscaler</th>\n","      <th>nrc_medianiqr</th>\n","      <th>nrc_lnorm_stdscaler</th>\n","      <th>nrc_lnorm_medianiqr</th>\n","      <th>afinn</th>\n","      <th>afinn_stdscaler</th>\n","      <th>afinn_medianiqr</th>\n","      <th>...</th>\n","      <th>textblob_lnorm_medianiqr</th>\n","      <th>pattern</th>\n","      <th>pattern_stdscaler</th>\n","      <th>pattern_medianiqr</th>\n","      <th>pattern_lnorm_stdscaler</th>\n","      <th>pattern_lnorm_medianiqr</th>\n","      <th>stanza</th>\n","      <th>stanza_stdscaler</th>\n","      <th>stanza_medianiqr</th>\n","      <th>stanza_lnorm_stdscaler</th>\n","      <th>stanza_lnorm_medianiqr</th>\n","      <th>flair</th>\n","      <th>flair_stdscaler</th>\n","      <th>flair_medianiqr</th>\n","      <th>flair_lnorm_stdscaler</th>\n","      <th>flair_lnorm_medianiqr</th>\n","      <th>sentimentr_roll100</th>\n","      <th>sentimentr_stdscaler_roll10</th>\n","      <th>syuzhet_roll100</th>\n","      <th>syuzhet_stdscaler_roll10</th>\n","      <th>bing_roll100</th>\n","      <th>bing_stdscaler_roll10</th>\n","      <th>sentiword_roll100</th>\n","      <th>sentiword_stdscaler_roll10</th>\n","      <th>senticnet_roll100</th>\n","      <th>senticnet_stdscaler_roll10</th>\n","      <th>nrc_roll100</th>\n","      <th>nrc_stdscaler_roll10</th>\n","      <th>afinn_roll100</th>\n","      <th>afinn_stdscaler_roll10</th>\n","      <th>vader_roll100</th>\n","      <th>vader_stdscaler_roll10</th>\n","      <th>textblob_roll100</th>\n","      <th>textblob_stdscaler_roll10</th>\n","      <th>flair_roll100</th>\n","      <th>flair_stdscaler_roll10</th>\n","      <th>pattern_roll100</th>\n","      <th>pattern_stdscaler_roll10</th>\n","      <th>stanza_roll100</th>\n","      <th>stanza_stdscaler_roll10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Letter</td>\n","      <td>letter</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>-0.017066</td>\n","      <td>0.000000</td>\n","      <td>-0.017066</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.039</td>\n","      <td>-0.285565</td>\n","      <td>-0.167174</td>\n","      <td>-0.285565</td>\n","      <td>0.245632</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.013530</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>10.000000</td>\n","      <td>0.943158</td>\n","      <td>0.813018</td>\n","      <td>0.943158</td>\n","      <td>96.262640</td>\n","      <td>-0.5913</td>\n","      <td>-0.641646</td>\n","      <td>-0.565404</td>\n","      <td>-0.641646</td>\n","      <td>-6.797046</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>_To Mrs. Saville, England._</td>\n","      <td>to mrs saville england</td>\n","      <td>27</td>\n","      <td>4</td>\n","      <td>0.00</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>-0.017066</td>\n","      <td>0.000000</td>\n","      <td>-0.017066</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.000</td>\n","      <td>-0.311172</td>\n","      <td>-0.188763</td>\n","      <td>-0.311172</td>\n","      <td>-0.198164</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.013530</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>2.691257</td>\n","      <td>-1.444950</td>\n","      <td>-1.034394</td>\n","      <td>-1.444950</td>\n","      <td>3.479559</td>\n","      <td>0.8427</td>\n","      <td>0.902033</td>\n","      <td>0.162708</td>\n","      <td>0.902033</td>\n","      <td>2.239383</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>St. Petersburgh, Dec. th, .</td>\n","      <td>st petersburgh december th</td>\n","      <td>27</td>\n","      <td>5</td>\n","      <td>0.00</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>-0.017066</td>\n","      <td>0.000000</td>\n","      <td>-0.017066</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.799</td>\n","      <td>0.213444</td>\n","      <td>0.253529</td>\n","      <td>0.213444</td>\n","      <td>1.620261</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.013530</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>2.691257</td>\n","      <td>-1.444950</td>\n","      <td>-1.034394</td>\n","      <td>-1.444950</td>\n","      <td>2.140982</td>\n","      <td>0.9341</td>\n","      <td>1.000424</td>\n","      <td>0.209117</td>\n","      <td>1.000424</td>\n","      <td>1.970591</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>You will rejoice to hear that no disaster has ...</td>\n","      <td>you will rejoice to hear that no disaster has ...</td>\n","      <td>143</td>\n","      <td>23</td>\n","      <td>-0.40</td>\n","      <td>-0.293782</td>\n","      <td>-0.242424</td>\n","      <td>-0.293782</td>\n","      <td>-0.203385</td>\n","      <td>-0.40</td>\n","      <td>-0.282833</td>\n","      <td>-0.242424</td>\n","      <td>-0.282833</td>\n","      <td>-0.20745</td>\n","      <td>-24.180907</td>\n","      <td>-0.455440</td>\n","      <td>-0.491201</td>\n","      <td>-0.455440</td>\n","      <td>-0.518981</td>\n","      <td>-0.187500</td>\n","      <td>-0.105724</td>\n","      <td>-0.195556</td>\n","      <td>-0.105724</td>\n","      <td>-0.178792</td>\n","      <td>2.154</td>\n","      <td>1.103125</td>\n","      <td>1.003598</td>\n","      <td>1.103125</td>\n","      <td>0.867540</td>\n","      <td>-7.612961</td>\n","      <td>-0.625949</td>\n","      <td>-0.485653</td>\n","      <td>-0.625949</td>\n","      <td>-0.419321</td>\n","      <td>-0.618144</td>\n","      <td>-0.392231</td>\n","      <td>-0.492871</td>\n","      <td>...</td>\n","      <td>-2.958494</td>\n","      <td>-0.317915</td>\n","      <td>-0.076285</td>\n","      <td>-2.177818</td>\n","      <td>-0.076285</td>\n","      <td>-2.520593</td>\n","      <td>6.490508</td>\n","      <td>-0.203557</td>\n","      <td>-0.074067</td>\n","      <td>-0.203557</td>\n","      <td>-0.406154</td>\n","      <td>-0.6601</td>\n","      <td>-0.715708</td>\n","      <td>-0.600338</td>\n","      <td>-0.715708</td>\n","      <td>-0.457827</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>I arrived here yesterday, and my first task is...</td>\n","      <td>i arrived here yesterday and my first task is ...</td>\n","      <td>145</td>\n","      <td>26</td>\n","      <td>2.55</td>\n","      <td>1.638930</td>\n","      <td>1.545455</td>\n","      <td>1.638930</td>\n","      <td>1.146975</td>\n","      <td>2.55</td>\n","      <td>1.677196</td>\n","      <td>1.545455</td>\n","      <td>1.677196</td>\n","      <td>1.16990</td>\n","      <td>70.454016</td>\n","      <td>1.822038</td>\n","      <td>1.431175</td>\n","      <td>1.822038</td>\n","      <td>1.337640</td>\n","      <td>2.083333</td>\n","      <td>2.637415</td>\n","      <td>2.226667</td>\n","      <td>2.637415</td>\n","      <td>1.780112</td>\n","      <td>3.143</td>\n","      <td>1.752494</td>\n","      <td>1.551066</td>\n","      <td>1.752494</td>\n","      <td>1.177428</td>\n","      <td>30.170243</td>\n","      <td>2.046460</td>\n","      <td>1.924647</td>\n","      <td>2.046460</td>\n","      <td>1.470030</td>\n","      <td>1.786726</td>\n","      <td>1.186369</td>\n","      <td>1.424628</td>\n","      <td>...</td>\n","      <td>1.439421</td>\n","      <td>0.168294</td>\n","      <td>0.085208</td>\n","      <td>1.152865</td>\n","      <td>0.085208</td>\n","      <td>1.180359</td>\n","      <td>8.338057</td>\n","      <td>0.400124</td>\n","      <td>0.392933</td>\n","      <td>0.400124</td>\n","      <td>-0.023186</td>\n","      <td>0.9813</td>\n","      <td>1.051234</td>\n","      <td>0.233082</td>\n","      <td>1.051234</td>\n","      <td>0.290826</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 92 columns</p>\n","</div>"],"text/plain":["   sent_no  parag_no  ...  stanza_roll100 stanza_stdscaler_roll10\n","0        0         0  ...             NaN                     NaN\n","1        1         1  ...             NaN                     NaN\n","2        2         2  ...             NaN                     NaN\n","3        3         3  ...             NaN                     NaN\n","4        4         3  ...             NaN                     NaN\n","\n","[5 rows x 92 columns]"]},"execution_count":105,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3282 entries, 0 to 3281\n","Data columns (total 92 columns):\n"," #   Column                       Non-Null Count  Dtype  \n","---  ------                       --------------  -----  \n"," 0   sent_no                      3282 non-null   int64  \n"," 1   parag_no                     3282 non-null   int64  \n"," 2   sect_no                      3282 non-null   int64  \n"," 3   sent_raw                     3282 non-null   string \n"," 4   sent_clean                   3282 non-null   object \n"," 5   char_len                     3282 non-null   int64  \n"," 6   token_len                    3282 non-null   int64  \n"," 7   sentimentr                   3282 non-null   float64\n"," 8   sentimentr_stdscaler         3282 non-null   float64\n"," 9   sentimentr_medianiqr         3282 non-null   float64\n"," 10  sentimentr_lnorm_stdscaler   3282 non-null   float64\n"," 11  sentimentr_lnorm_medianiqr   3282 non-null   float64\n"," 12  syuzhet                      3282 non-null   float64\n"," 13  syuzhet_stdscaler            3282 non-null   float64\n"," 14  syuzhet_medianiqr            3282 non-null   float64\n"," 15  syuzhet_lnorm_stdscaler      3282 non-null   float64\n"," 16  syuzhet_lnorm_medianiqr      3282 non-null   float64\n"," 17  bing                         3282 non-null   float64\n"," 18  bing_stdscaler               3282 non-null   float64\n"," 19  bing_medianiqr               3282 non-null   float64\n"," 20  bing_lnorm_stdscaler         3282 non-null   float64\n"," 21  bing_lnorm_medianiqr         3282 non-null   float64\n"," 22  sentiword                    3282 non-null   float64\n"," 23  sentiword_stdscaler          3282 non-null   float64\n"," 24  sentiword_medianiqr          3282 non-null   float64\n"," 25  sentiword_lnorm_stdscaler    3282 non-null   float64\n"," 26  sentiword_lnorm_medianiqr    3282 non-null   float64\n"," 27  senticnet                    3282 non-null   float64\n"," 28  senticnet_stdscaler          3282 non-null   float64\n"," 29  senticnet_medianiqr          3282 non-null   float64\n"," 30  senticnet_lnorm_stdscaler    3282 non-null   float64\n"," 31  senticnet_lnorm_medianiqr    3282 non-null   float64\n"," 32  nrc                          3282 non-null   float64\n"," 33  nrc_stdscaler                3282 non-null   float64\n"," 34  nrc_medianiqr                3282 non-null   float64\n"," 35  nrc_lnorm_stdscaler          3282 non-null   float64\n"," 36  nrc_lnorm_medianiqr          3282 non-null   float64\n"," 37  afinn                        3282 non-null   float64\n"," 38  afinn_stdscaler              3282 non-null   float64\n"," 39  afinn_medianiqr              3282 non-null   float64\n"," 40  afinn_lnorm_stdscaler        3282 non-null   float64\n"," 41  afinn_lnorm_medianiqr        3282 non-null   float64\n"," 42  scores                       3282 non-null   object \n"," 43  vader                        3282 non-null   float64\n"," 44  vader_stdscaler              3282 non-null   float64\n"," 45  vader_medianiqr              3282 non-null   float64\n"," 46  vader_lnorm_stdscaler        3282 non-null   float64\n"," 47  vader_lnorm_medianiqr        3282 non-null   float64\n"," 48  textblob                     3282 non-null   float64\n"," 49  textblob_stdscaler           3282 non-null   float64\n"," 50  textblob_medianiqr           3282 non-null   float64\n"," 51  textblob_lnorm_stdscaler     3282 non-null   float64\n"," 52  textblob_lnorm_medianiqr     3282 non-null   float64\n"," 53  pattern                      3282 non-null   float64\n"," 54  pattern_stdscaler            3282 non-null   float64\n"," 55  pattern_medianiqr            3282 non-null   float64\n"," 56  pattern_lnorm_stdscaler      3282 non-null   float64\n"," 57  pattern_lnorm_medianiqr      3282 non-null   float64\n"," 58  stanza                       3282 non-null   float64\n"," 59  stanza_stdscaler             3282 non-null   float64\n"," 60  stanza_medianiqr             3282 non-null   float64\n"," 61  stanza_lnorm_stdscaler       3282 non-null   float64\n"," 62  stanza_lnorm_medianiqr       3282 non-null   float64\n"," 63  flair                        3282 non-null   float64\n"," 64  flair_stdscaler              3282 non-null   float64\n"," 65  flair_medianiqr              3282 non-null   float64\n"," 66  flair_lnorm_stdscaler        3282 non-null   float64\n"," 67  flair_lnorm_medianiqr        3282 non-null   float64\n"," 68  sentimentr_roll100           2955 non-null   float64\n"," 69  sentimentr_stdscaler_roll10  2955 non-null   float64\n"," 70  syuzhet_roll100              2955 non-null   float64\n"," 71  syuzhet_stdscaler_roll10     2955 non-null   float64\n"," 72  bing_roll100                 2955 non-null   float64\n"," 73  bing_stdscaler_roll10        2955 non-null   float64\n"," 74  sentiword_roll100            2955 non-null   float64\n"," 75  sentiword_stdscaler_roll10   2955 non-null   float64\n"," 76  senticnet_roll100            2955 non-null   float64\n"," 77  senticnet_stdscaler_roll10   2955 non-null   float64\n"," 78  nrc_roll100                  2955 non-null   float64\n"," 79  nrc_stdscaler_roll10         2955 non-null   float64\n"," 80  afinn_roll100                2955 non-null   float64\n"," 81  afinn_stdscaler_roll10       2955 non-null   float64\n"," 82  vader_roll100                2955 non-null   float64\n"," 83  vader_stdscaler_roll10       2955 non-null   float64\n"," 84  textblob_roll100             2955 non-null   float64\n"," 85  textblob_stdscaler_roll10    2955 non-null   float64\n"," 86  flair_roll100                2955 non-null   float64\n"," 87  flair_stdscaler_roll10       2955 non-null   float64\n"," 88  pattern_roll100              2955 non-null   float64\n"," 89  pattern_stdscaler_roll10     2955 non-null   float64\n"," 90  stanza_roll100               2955 non-null   float64\n"," 91  stanza_stdscaler_roll10      2955 non-null   float64\n","dtypes: float64(84), int64(5), object(2), string(1)\n","memory usage: 2.3+ MB\n"]}],"source":["corpus_sents_trans_df.drop(columns=['Unnamed: 0'], inplace=True)\n","corpus_sents_trans_df['sent_raw'] = corpus_sents_trans_df['sent_raw'].astype('string')\n","corpus_sents_trans_df.head()\n","corpus_sents_trans_df.info()"]},{"cell_type":"markdown","metadata":{"id":"e5LIdnknhJkO"},"source":["### **Option (c): Upload Corpus Sentiment Transformer Sentiment Datafiles**\n","\n","***Only do this if your Google subdirectory doesn't already contain a plain text file of your Corpus or you wish to overwrite it and use a newer version***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bH6dlB2fO6Ln"},"outputs":[],"source":["# Execute this code cell to upload plain text file of corpus\n","#   Should be *.txt format with paragraphs separated by at least 2 newlines\n","\n","uploaded = files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3LORQ4fRGBW"},"outputs":[],"source":["# Verify file was uploaded\n","\n","# Get uploaded filename\n","corpus_filename = list(uploaded.keys())[0]\n","print(f'Uploaded Corpus filename is: {corpus_filename}')\n","CORPUS_FILENAME = corpus_filename\n","\n","!ls -al $corpus_filename"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZTbaNN26N2S"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"FCZmu5ifh7Sn"},"source":["# **Configuration (Manual)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":629,"status":"ok","timestamp":1631867206535,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"GseUJ-Vbh7So","outputId":"b2224c0a-80da-4760-a9fb-45b76d4df3cc"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["/gdrive/My Drive/research/2021/sa_book_code/books_sa/mshelley_frankenstein\n","mshelley_frankenstein.txt\n"]}],"source":["# Verify subdirectory change\n","\n","!pwd\n","!ls *.txt\n","\n","# TODO: Intelligently automate the filling of form based upon directory"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"elapsed":331,"status":"ok","timestamp":1631867208538,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"sO_OPyHlCORt","outputId":"cd6d90fa-6821-4524-dcb4-0dbc0db064e6"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-rw------- 1 root root  428446 Sep 17 00:36 mshelley_frankenstein.txt\n","-rw------- 1 root root  510187 Sep 17 01:42 sum_sentiments_syuzhetR_4models_mshelley_frankenstein.csv\n","-rw------- 1 root root  797623 Sep 17 01:42 sum_sentiments_sentimentR_7models_mshelley_frankenstein.csv\n","-rw------- 1 root root 2756961 Sep 17 01:51 sum_sentiments_sents_syuzhetr_mshelley_frankenstein.csv\n","-rw------- 1 root root 3815266 Sep 17 01:51 sum_sentiments_sents_sentimentr_mshelley_frankenstein.csv\n","-rw------- 1 root root  436149 Sep 17 01:52 corpus_text_sents_raw_mshelley_frankenstein.csv\n","-rw------- 1 root root  422352 Sep 17 01:52 corpus_text_sents_clean_mshelley_frankenstein.csv\n","-rw------- 1 root root 5865397 Sep 17 01:52 corpus_sents_baseline_mshelley_frankenstein.csv\n","-rw------- 1 root root  850343 Sep 17 01:52 corpus_sects_baseline_mshelley_frankenstein.csv\n","-rw------- 1 root root 1723015 Sep 17 01:52 corpus_parags_baseline_mshelley_frankenstein.csv\n","-rw------- 1 root root  856406 Sep 17 01:52 corpus_chaps_baseline_mshelley_frankenstein.csv\n"]}],"source":["!ls -altr *\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1631867084888,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"4rHxlFgL2qkn","outputId":"53ea1134-2eb6-4d2f-e1af-9d742367fc2a"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["head: cannot open 'mshelley_frankenstein.csv' for reading: No such file or directory\n"]}],"source":["!head -n 10 mshelley_frankenstein.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"elapsed":598,"status":"ok","timestamp":1631867223203,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"NUuzQBqkxV4u","outputId":"e66ae48f-b5ae-4489-cfc6-97cfd3e58ba2"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Working Corpus Datafile: ------------------------------ \n","\n","    ./research/2021/sa_book_code/books_sa/mshelley_frankenstein\n","\n","Full Corpus Title/Author: ------------------------------ \n","\n","    Frankenstein by: Mary Shelley\n","\n","CHAPTER Headings: ------------------------------ \n","\n","    CHAPTER\n","\n","SECTION Headings: ------------------------------ \n","\n","    None\n","\n","Corpus file information: ------------------------------ \n","\n","-rw------- 1 root root 428446 Sep 17 00:36 mshelley_frankenstein.txt\n"]}],"source":["# CORPUS_TITLE = 'Beloved' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Toni Morrison\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"tmorrison_beloved.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/tmorrison_belovedy\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'A Christmas Carol' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Charles Dickens\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"cdickens_achristmascarol.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/cdickens_achristmascarol\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Confessions' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Saint Augustine\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"staugustine_confessions.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/staugustine_confessions\"  #@param {type:\"string\"}\n","\n","CORPUS_TITLE = 'Frankenstein' #@param {type:\"string\"}\n","CORPUS_AUTHOR = \"Mary Shelley\" #@param {type:\"string\"}\n","CORPUS_FILENAME = \"mshelley_frankenstein.txt\" #@param {type:\"string\"}\n","CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/mshelley_frankenstein\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Great Expectations' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Charles Dickens\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"cdickens_greatexpectations.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/cdickens_greatexpectations\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Heart of Darkness' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Joseph Conrad\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"jconrad_heartofdarkness.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/jconrad_heartofdarkness\" #@param {type:\"string\"}\n","\n","# ORPUS_TITLE = 'Howards End' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"EM Forster\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"emforster_howardsend.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/emforster_howardsend\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Machines Like Me' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Ian McEwan\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"imcewan_machineslikeme.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/imcewan_machineslikeme\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Middlemarch' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"George Eliot\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"geliot_middlemarch_wprelude.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/geliot_middlemarch\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Frankenstein' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Mary Shelley\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"mshelley_frankenstein.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/mshelley_frankenstein\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Mrs. Dalloway' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"vwoolf_mrsdalloway.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_mrsdalloway\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Narrative Life of Frederick Douglass' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Frederick Douglass\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"fdouglass_narrative.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/fdouglass_narrativelifeslave\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Orlando' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"vwoolf_orlando.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_orlando\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Palefire - Commentary' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Vladimir Nabokov\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"vnabokov_palefire_commentary.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vnabokov_palefire\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Portrait of a Lady' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Henry James\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"hjames_portraitofalady.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/hjames_portraitofalady\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Portrait of the Artist as a Young Man' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"James Joyce\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"jjoyce_portraitoftheartist.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/jjoyce_portraitoftheartist\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Pride and Prejudice' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Jane Austen\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"jausten_prideandprejudice.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/jausten_prideandprejudice\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'Robinson Crusoe' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Daniel Defoe\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"ddefoe_robinsoncrusoe.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/ddefoe_robinsoncrusoe\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Great Gatsby' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"F. Scott Fitzgerald\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"fscottfitzgerald_thegreatgatsby.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/fscottfitzgerald_thegreatgatsby\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Socerers Stone' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"J.K. Rowling\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"hpotter1_sorcerersstone.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/jkrowling_harrypotter\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Wonderful Wizard of Oz' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Frank Baum\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"fbaum_thewonderfulwizardofoz.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/fbaum_thewonderfulwizardofoz\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Waves' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"vwoolf_thewaves.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_thewaves\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'To The Lighthouse' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Virginia Woolf\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"vwoolf_tothelighthouse.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/vwoolf_tothelighthouse\" #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Odyssey' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Homer SButler\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"sbutler_odyssey.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/sbutler_odyssey\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Odyssey' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Homer EWilson\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"ewilson_odyssey.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/homer_odyssey\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Guermantes Way - English' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Marcel Proust\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"mproust_3guermantesway_mtreharne_en.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/mproust_time\"  #@param {type:\"string\"}\n","\n","# CORPUS_TITLE = 'The Guermantes Way - French' #@param {type:\"string\"}\n","# CORPUS_AUTHOR = \"Marcel Proust\" #@param {type:\"string\"}\n","# CORPUS_FILENAME = \"mproust_guermantes_fr.txt\" #@param {type:\"string\"}\n","# CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/mproust_time\"  #@param {type:\"string\"}\n","\n","CORPUS_LANGUAGE = \"English\" #@param [\"English\", \"French\"]\n","\n","CHAPTER_HEADINGS = \"CHAPTER\" #@param [\"CHAPTER\", \"BOOK\", \"None\"]\n","CHAPTER_NUMBERING = \"Arabic (1,2,...)\" #@param [\"Arabic (1,2,...)\", \"Roman (I,II,...)\"]\n","SECTION_HEADINGS = \"None\" #@param [\"SECTION (ArabicNo)\", \"SECTION (RomanNo)\", \"----- (Hyphens)\", \"None\"]\n","\n","LEXICONS_SUBDIR = \"./research/2021/sa_book_code/books_sa/lexicons\" #@param {type:\"string\"}\n","\n","CORPUS_FULL = f'{CORPUS_TITLE} by: {CORPUS_AUTHOR}'\n","\n","PLOT_OUTPUT = \"Major\" #@param [\"None\", \"Major\", \"All\"]\n","\n","FILE_OUTPUT = \"Major\" #@param [\"None\", \"Major\", \"All\"]\n","\n","\n","gdrive_subdir = CORPUS_SUBDIR\n","corpus_filename = CORPUS_FILENAME\n","CORPUS_LANGUAGE = CORPUS_LANGUAGE.lower()\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","author_abbr_str = (CORPUS_AUTHOR.split(' ')[0][0]+CORPUS_AUTHOR.split(' ')[1]).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","title_str = re.sub(r'[^A-Za-z0-9]','', title_str).lower()\n","\n","print(f'\\nWorking Corpus Datafile: ------------------------------ \\n\\n    {CORPUS_SUBDIR}')\n","print(f'\\nFull Corpus Title/Author: ------------------------------ \\n\\n    {CORPUS_FULL}')\n","\n","\n","if CHAPTER_HEADINGS == 'CHAPTER':\n","  if CHAPTER_NUMBERING == \"Arabic (1,2,...)\":\n","    # pattern_chap = r'CHAPTER [0123456789]{1,2} ' # [\\.]?[^\\n]*'\n","    pattern_chap = r'CHAPTER [0123456789]{1,2}[.]?[^\\n]*' # [os.return]*'\n","  elif CHAPTER_NUMBERING == \"Roman (I,II,...)\":\n","    pattern_chap = r'CHAPTER[\\s]{1,5}[IVXL]{1,10}[.:]?[\\s]+' # [^\\n]+'\n","    # pattern_chap = r'CHAPTER[\\s]{1,}[IVXL]{1,10}[.:]?[^\\n\\r]*'\n","  else:\n","    print(f'ERROR: Illegal CHAPTER_NUMBERING value = {CHAPTER_NUMBERING}')\n","\n","elif CHAPTER_HEADINGS == 'BOOK':\n","  if CHAPTER_NUMBERING == \"Arabic (1,2,...)\":\n","    pattern_chap = r'BOOK [0123456789]{1,2}[.]?[^\\n]*'\n","  elif CHAPTER_NUMBERING == \"Roman (I,II,...)\":\n","    pattern_chap = r'[\\s]*BOOK[\\s]{1,5}[IVXL]{1,10}[.:]?[\\s]+' # [.:]?[\\s]*[^\\n]*[\\n\\r]+' # ]{0,1}[^\\n]*' # [^\\n]*' # Problems with embedded 'Book'\n","  else:\n","    print(f'ERROR: Illegal CHAPTER_NUMBERING value = {CHAPTER_NUMBERING}')\n","\n","elif CHAPTER_HEADINGS == \"None\":\n","  pattern_chap = r'CHAPTER [0123456789]{1,2}[.]?[^\\n]*'\n","\n","else:\n","  print(f'ERROR: Illegal CHAPTER_HEADINGS value = {CHAPTER_HEADINGS}')\n","\n","# Default Section RegEx Pattern\n","pattern_sect = 'SECTION [0123456789]{1,2}[^\\n]*'\n","\n","if SECTION_HEADINGS == 'SECTION (ArabicNo)':\n","  # pattern_sect = r'SECTION [0-9]{1,2} [^\\n]*'\n","  # TODO: [^\\n] gets parsed into [^\\\\n] causing problems, so simplify\n","  pattern_sect = r'SECTION [0123456789]{1,2}[.:]?[^\\n]*'\n","elif SECTION_HEADINGS == 'SECTION (RomanNo)':\n","  pattern_sect = r'SECTION [IVXL]{1,10}[.:]?[^\\n\\r]+' # } [A-Z \\.-:—;-’\\'\"]*[\\n]*'\n","elif SECTION_HEADINGS == '----- (Hyphens)':\n","  pattern_sect = r'^[- ]{3,}[^\\n]*'\n","elif SECTION_HEADINGS == 'None':\n","  pass\n","else:\n","  print(f'ERROR: Illegal SECTION_HEADING value = {SECTION_HEADINGS}')\n","\n","print(f'\\nCHAPTER Headings: ------------------------------ \\n\\n    {CHAPTER_HEADINGS}')\n","\n","print(f'\\nSECTION Headings: ------------------------------ \\n\\n    {SECTION_HEADINGS}')\n","\n","\n","print(f'\\nCorpus file information: ------------------------------ \\n')\n","!ls -al $CORPUS_FILENAME\n","\n","# Verify contents of Corpus File is Correctly Formatted\n","#   \n","# TODO: ./utils/verify_format.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HnjA0Q5DpcNh"},"outputs":[],"source":["!pwd"]},{"cell_type":"markdown","metadata":{"id":"OXnCgU3UAhxz"},"source":["# **Utility Functions**"]},{"cell_type":"markdown","metadata":{"id":"CXPeszvsqSvu"},"source":["## **General Setup**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":332,"status":"ok","timestamp":1631867237040,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"z1TbyqENqR89","outputId":"e90c11d1-b873-4cd6-fd9c-3d49efa98a1d"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import pipeline"]},{"cell_type":"markdown","metadata":{"id":"egqtbEOICSGG"},"source":["## **Sentiment Translations**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1631867334944,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"40d0v2MD_zU0","outputId":"2ef1b511-3716-4d65-e467-d34ad6029881"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["\"\"\"\n","def adj_polarityprobability2float(pol_str, prob_fl):\n","  '''\n","  Given a Polarity string (Negative or Positive) and a Probability float (0.0-1.0)\n","  Return a Sentiment float value (-1.0 to 1.0)\n","  '''\n","  sign_fl = 1.0\n","  if pol_str.lower().startswith('neg'):\n","    # print(f'pol_str: {pol_str} is Negative')\n","    sign_fl = -1.0\n","  elif pol_str.lower().startswith('pos'):\n","    # print(f'pol_str: {pol_str} is Positive')\n","    pass\n","  else:\n","    print(f'ERROR: pol_str: {pol_str} is neither Negative nor Positive')\n","    sign_fl = 0.0\n","\n","  return sign_fl * prob_fl\n","\n","# Test\n","# polprob2sentiment('Positive', 0.91)\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1631867465434,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"OvGgV_xzaece","outputId":"54ae7041-fd1f-43cf-9246-d5a1e048e183"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["def twoway_probability2sentiment(text_str, sentiment_2polarity_fn, pol_labels=['negative','positive']):\n","  '''\n","  Given a text string, sentiment_fn (return 0.0 to 1.0) and a list of 2 labels for negative and positive classes\n","    e.g. CamemBERT returns (LABEL_0/LABEL_1) for (Negative/Positive)\n","         xxx returns (LABEL_1/LABEL_2) for (Negative/Positive)\n","         xxx returns (NEGATIVE/POSITIVE)\n","         xxx returns (Neg/Pos)\n","  Get return a sign adjusted sentiment score -1.0 to 1.0\n","  '''\n","\n","  model_score = sentiment_2polarity_fn(text_str)\n","  pol_str = model_score[0]['label']\n","\n","  # print(f'pol_str = {pol_str} and is type:{type(pol_str)}')\n","  score_fl = float(model_score[0]['score'])\n","  # print(f'score_fl = {score_fl} and is type{type(score_fl)}')\n","\n","  # print(f'pol_str.lower: {pol_str.lower()} and pol_labels[0]: {pol_labels[0]}')\n","  if (pol_str.lower() in pol_labels[0].lower()):\n","    # print('negative')\n","    sign_fl = -1.0\n","  elif (pol_str.lower() in pol_labels[1].lower()):\n","    # print('positive')\n","    sign_fl = 1.0\n","  else:\n","    print(f'ERROR polarity string: {pol_str} must be one of two values (e.g. [Nn]egative|[Pp]ositive)')\n","    return -99\n","    \n","  return sign_fl * score_fl\n","\n","# Test\n","# test_fl = wrapper_polprob2sentiment('I hate your guts you bastard!') # sentiment_analysis('section')[0]['label'],sentiment_analysis('section')[0]['score']))\n","# print(f'test_fl: {test_fl}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1631867335262,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"jh8VOpCZ_zQ0","outputId":"4b12e200-599a-4448-8671-0c8d5266b1f1"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["\"\"\"\n","\n","def twoway_probability2sentiment(text_str, sentiment_2polarity_fn):\n","  '''\n","  Given a text string and sentiment_fn that returns ['negative|positive', float(0.0-1.0)]\n","  Get return a sign adjusted sentiment score -1.0 to 1.0\n","  '''\n","  model_score = sentiment_2polarity_fn(text_str)\n","  pol_str = model_score[0]['label']\n","  # print(f'pol_str = {pol_str} and is type:{type(pol_str)}')\n","  score_fl = float(model_score[0]['score'])\n","  # print(f'score_fl = {score_fl} and is type{type(score_fl)}')\n","\n","  if (pol_str.lower().startswith('neg')) | (pol_str in ['LABEL_1','0']):\n","    # print('negative')\n","    sign_fl = -1.0\n","  elif (pol_str.lower().startswith('pos')) | (pol_str in ['LABEL_2','1']):\n","    # print('positive')\n","    sign_fl = 1.0\n","  else:\n","    print(f'ERROR polarity string: {pol_str} must be one of two values (e.g. [Nn]egative|[Pp]ositive)')\n","\n","  return sign_fl * score_fl\n","\n","# Test\n","# test_fl = wrapper_polprob2sentiment('I hate your guts you bastard!') # sentiment_analysis('section')[0]['label'],sentiment_analysis('section')[0]['score']))\n","# print(f'test_fl: {test_fl}')\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1631867335263,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"Zi-N1Z9Q1DFf","outputId":"0db3f955-f319-49d6-925a-d9e24895a971"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["def threeway_probability2sentiment(text_str, sentiment_2polarity_fn):\n","  '''\n","  Given a text string and sentiment_fn that returns ['negative|positive|neutral', float(0.0-1.0)]\n","  Get return a sign adjusted sentiment score -1.0 to 1.0\n","  '''\n","  sign_fl = 1.0\n","  \n","  # Special case for Pysentimiento\n","  score_pysentimiento_fl = -99.0    # Use Pysentimeinto score as flag with val -99.0\n","  if False: # sentiment_2polarity_fn == analyzer.predict:\n","    # from pysentimiento import SentimentAnalyzer\n","    # analyzer = SentimentAnalyzer(lang=\"en\")\n","    # print('Using Pysentimiento')\n","    text_str_ls = text_str.split()[:125]\n","    text_125_str = ' '.join(text_str_ls)\n","    pol_object = analyzer.predict(text_125_str)\n","    pol_str = pol_object.output\n","    if pol_str == 'NEG':\n","      sign_fl = -1.0\n","    elif pol_str == 'NEU':\n","      sign_fl = 1.0\n","    else:\n","      # Polarity is 'POS' by default\n","      sign_fl = 1.0\n","    score_pysentimiento_fl = sign_fl * pol_object.probas[pol_str]\n","\n","    # Distribute the Neutral values between -0.5 and +0.5\n","    if pol_str == 'NEU':\n","      score_pysentimiento_fl = score_pysentimiento_fl - 0.5\n","\n","  # General case for other 3-way sentiment models\n","  else:\n","    # print('Not using Pysentimiento')\n","    model_score = sentiment_2polarity_fn(text_str)\n","    pol_str = model_score[0]['label']\n","    # print(f'pol_str = {pol_str} and is type:{type(pol_str)}')\n","    score_fl = float(model_score[0]['score'])\n","    # print(f'score_fl = {score_fl} and is type{type(score_fl)}')\n","\n","    if (pol_str.lower().startswith('neu')) | (pol_str in ['NEU','LABEL_0']):\n","      # print('negative')\n","      if score_fl < 0.5:\n","        sign_fl = -1.0\n","      else:\n","        sign_fl = +1.0\n","      adj_base = 0.0\n","    elif (pol_str.lower().startswith('neg')) | (pol_str in ['NEG','LABEL_1']):\n","      # print('positive')\n","      sign_fl = -1.0\n","      adj_base = -1.0\n","    elif (pol_str.lower().startswith('pos')) | (pol_str in ['POS','LABEL_2']):\n","      # print('positive')\n","      sign_fl = 1.0\n","      adj_base = 1.0\n","    else:\n","      print(f'ERROR polarity string: {pol_str} must be one of two values (e.g. [Nn]egative|[Pp]ositive)')\n","\n","  if score_pysentimiento_fl == -99.0:\n","    adj_score = (sign_fl * score_fl) + adj_base\n","  else:\n","    adj_score = score_pysentimiento_fl\n","\n","  return adj_score # , adj_base\n","\n","# Test\n","# test_fl = wrapper_polprob2sentiment('I hate your guts you bastard!') # sentiment_analysis('section')[0]['label'],sentiment_analysis('section')[0]['score']))\n","# print(f'test_fl: {test_fl}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1631867335265,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"4SguJaRegHZY","outputId":"61df2842-1ed9-4d6a-ca2b-ec9d526c2185"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["def fiveway_probability2sentiment(text_str, sentiment_5star_fn):\n","  '''\n","  Given a text string and sentiment_fn that returns '1 star' to '5 stars' rating with probability]\n","  Get return a sign adjusted sentiment score 0.0 to 5.0\n","  '''\n","\n","  model_score = sentiment_5star_fn(text_str)\n","  pol_str = model_score[0]['label']\n","  # print(f'pol_str = [{pol_str}]')\n","  if pol_str in ['1 star','LABEL_0']:\n","    score_base = 0.0\n","  elif pol_str in ['2 stars','LABEL_1']:\n","    score_base = 1.0\n","  elif pol_str in ['3 stars','LABEL_2']:\n","    score_base = 2.0\n","  elif pol_str in ['4 stars','LABEL_3']:\n","    score_base = 3.0\n","  elif pol_str in ['5 stars','LABEL_4']:\n","    score_base = 4.0\n","  else:\n","    print(f\"ERROR: polarity string = {pol_str} must be in [1-5] 'stars'\")\n","    score_base = 2.0\n","\n","  score_fl = score_base + model_score[0]['score']\n","\n","  return score_fl\n","\n","# Test\n","# test_fl = wrapper_polprob2sentiment('I hate your guts you bastard!') # sentiment_analysis('section')[0]['label'],sentiment_analysis('section')[0]['score']))\n","# print(f'test_fl: {test_fl}')"]},{"cell_type":"markdown","metadata":{"id":"d5_WhpYp_3Fi"},"source":["\n","# **Transformer Sentiment Models**\n","\n","* https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwvKBT1YhS2x"},"outputs":[],"source":["# Be sure you have loaded in Corpus Sentences into corpus_sents_trans_df at top of Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":464},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1631867244061,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"1-FVXy1HxmBr","outputId":"bc587b10-4dc3-49f8-8c12-bd2563f19bda"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent_no</th>\n","      <th>parag_no</th>\n","      <th>sect_no</th>\n","      <th>sent_raw</th>\n","      <th>sent_clean</th>\n","      <th>char_len</th>\n","      <th>token_len</th>\n","      <th>sentimentr</th>\n","      <th>sentimentr_stdscaler</th>\n","      <th>sentimentr_medianiqr</th>\n","      <th>sentimentr_lnorm_stdscaler</th>\n","      <th>sentimentr_lnorm_medianiqr</th>\n","      <th>syuzhet</th>\n","      <th>syuzhet_stdscaler</th>\n","      <th>syuzhet_medianiqr</th>\n","      <th>syuzhet_lnorm_stdscaler</th>\n","      <th>syuzhet_lnorm_medianiqr</th>\n","      <th>bing</th>\n","      <th>bing_stdscaler</th>\n","      <th>bing_medianiqr</th>\n","      <th>bing_lnorm_stdscaler</th>\n","      <th>bing_lnorm_medianiqr</th>\n","      <th>sentiword</th>\n","      <th>sentiword_stdscaler</th>\n","      <th>sentiword_medianiqr</th>\n","      <th>sentiword_lnorm_stdscaler</th>\n","      <th>sentiword_lnorm_medianiqr</th>\n","      <th>senticnet</th>\n","      <th>senticnet_stdscaler</th>\n","      <th>senticnet_medianiqr</th>\n","      <th>senticnet_lnorm_stdscaler</th>\n","      <th>senticnet_lnorm_medianiqr</th>\n","      <th>nrc</th>\n","      <th>nrc_stdscaler</th>\n","      <th>nrc_medianiqr</th>\n","      <th>nrc_lnorm_stdscaler</th>\n","      <th>nrc_lnorm_medianiqr</th>\n","      <th>afinn</th>\n","      <th>afinn_stdscaler</th>\n","      <th>afinn_medianiqr</th>\n","      <th>...</th>\n","      <th>textblob_lnorm_medianiqr</th>\n","      <th>pattern</th>\n","      <th>pattern_stdscaler</th>\n","      <th>pattern_medianiqr</th>\n","      <th>pattern_lnorm_stdscaler</th>\n","      <th>pattern_lnorm_medianiqr</th>\n","      <th>stanza</th>\n","      <th>stanza_stdscaler</th>\n","      <th>stanza_medianiqr</th>\n","      <th>stanza_lnorm_stdscaler</th>\n","      <th>stanza_lnorm_medianiqr</th>\n","      <th>flair</th>\n","      <th>flair_stdscaler</th>\n","      <th>flair_medianiqr</th>\n","      <th>flair_lnorm_stdscaler</th>\n","      <th>flair_lnorm_medianiqr</th>\n","      <th>sentimentr_roll100</th>\n","      <th>sentimentr_stdscaler_roll10</th>\n","      <th>syuzhet_roll100</th>\n","      <th>syuzhet_stdscaler_roll10</th>\n","      <th>bing_roll100</th>\n","      <th>bing_stdscaler_roll10</th>\n","      <th>sentiword_roll100</th>\n","      <th>sentiword_stdscaler_roll10</th>\n","      <th>senticnet_roll100</th>\n","      <th>senticnet_stdscaler_roll10</th>\n","      <th>nrc_roll100</th>\n","      <th>nrc_stdscaler_roll10</th>\n","      <th>afinn_roll100</th>\n","      <th>afinn_stdscaler_roll10</th>\n","      <th>vader_roll100</th>\n","      <th>vader_stdscaler_roll10</th>\n","      <th>textblob_roll100</th>\n","      <th>textblob_stdscaler_roll10</th>\n","      <th>flair_roll100</th>\n","      <th>flair_stdscaler_roll10</th>\n","      <th>pattern_roll100</th>\n","      <th>pattern_stdscaler_roll10</th>\n","      <th>stanza_roll100</th>\n","      <th>stanza_stdscaler_roll10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Letter</td>\n","      <td>letter</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>-0.017066</td>\n","      <td>0.000000</td>\n","      <td>-0.017066</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.039</td>\n","      <td>-0.285565</td>\n","      <td>-0.167174</td>\n","      <td>-0.285565</td>\n","      <td>0.245632</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.013530</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>10.000000</td>\n","      <td>0.943158</td>\n","      <td>0.813018</td>\n","      <td>0.943158</td>\n","      <td>96.262640</td>\n","      <td>-0.5913</td>\n","      <td>-0.641646</td>\n","      <td>-0.565404</td>\n","      <td>-0.641646</td>\n","      <td>-6.797046</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>_To Mrs. Saville, England._</td>\n","      <td>to mrs saville england</td>\n","      <td>27</td>\n","      <td>4</td>\n","      <td>0.00</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>-0.017066</td>\n","      <td>0.000000</td>\n","      <td>-0.017066</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.000</td>\n","      <td>-0.311172</td>\n","      <td>-0.188763</td>\n","      <td>-0.311172</td>\n","      <td>-0.198164</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.013530</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>2.691257</td>\n","      <td>-1.444950</td>\n","      <td>-1.034394</td>\n","      <td>-1.444950</td>\n","      <td>3.479559</td>\n","      <td>0.8427</td>\n","      <td>0.902033</td>\n","      <td>0.162708</td>\n","      <td>0.902033</td>\n","      <td>2.239383</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>St. Petersburgh, Dec. th, .</td>\n","      <td>st petersburgh december th</td>\n","      <td>27</td>\n","      <td>5</td>\n","      <td>0.00</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>-0.031720</td>\n","      <td>0.000000</td>\n","      <td>0.00</td>\n","      <td>-0.017066</td>\n","      <td>0.000000</td>\n","      <td>-0.017066</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.799</td>\n","      <td>0.213444</td>\n","      <td>0.253529</td>\n","      <td>0.213444</td>\n","      <td>1.620261</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.013530</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>0.029310</td>\n","      <td>0.000000</td>\n","      <td>2.691257</td>\n","      <td>-1.444950</td>\n","      <td>-1.034394</td>\n","      <td>-1.444950</td>\n","      <td>2.140982</td>\n","      <td>0.9341</td>\n","      <td>1.000424</td>\n","      <td>0.209117</td>\n","      <td>1.000424</td>\n","      <td>1.970591</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>You will rejoice to hear that no disaster has ...</td>\n","      <td>you will rejoice to hear that no disaster has ...</td>\n","      <td>143</td>\n","      <td>23</td>\n","      <td>-0.40</td>\n","      <td>-0.293782</td>\n","      <td>-0.242424</td>\n","      <td>-0.293782</td>\n","      <td>-0.203385</td>\n","      <td>-0.40</td>\n","      <td>-0.282833</td>\n","      <td>-0.242424</td>\n","      <td>-0.282833</td>\n","      <td>-0.20745</td>\n","      <td>-24.180907</td>\n","      <td>-0.455440</td>\n","      <td>-0.491201</td>\n","      <td>-0.455440</td>\n","      <td>-0.518981</td>\n","      <td>-0.187500</td>\n","      <td>-0.105724</td>\n","      <td>-0.195556</td>\n","      <td>-0.105724</td>\n","      <td>-0.178792</td>\n","      <td>2.154</td>\n","      <td>1.103125</td>\n","      <td>1.003598</td>\n","      <td>1.103125</td>\n","      <td>0.867540</td>\n","      <td>-7.612961</td>\n","      <td>-0.625949</td>\n","      <td>-0.485653</td>\n","      <td>-0.625949</td>\n","      <td>-0.419321</td>\n","      <td>-0.618144</td>\n","      <td>-0.392231</td>\n","      <td>-0.492871</td>\n","      <td>...</td>\n","      <td>-2.958494</td>\n","      <td>-0.317915</td>\n","      <td>-0.076285</td>\n","      <td>-2.177818</td>\n","      <td>-0.076285</td>\n","      <td>-2.520593</td>\n","      <td>6.490508</td>\n","      <td>-0.203557</td>\n","      <td>-0.074067</td>\n","      <td>-0.203557</td>\n","      <td>-0.406154</td>\n","      <td>-0.6601</td>\n","      <td>-0.715708</td>\n","      <td>-0.600338</td>\n","      <td>-0.715708</td>\n","      <td>-0.457827</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>I arrived here yesterday, and my first task is...</td>\n","      <td>i arrived here yesterday and my first task is ...</td>\n","      <td>145</td>\n","      <td>26</td>\n","      <td>2.55</td>\n","      <td>1.638930</td>\n","      <td>1.545455</td>\n","      <td>1.638930</td>\n","      <td>1.146975</td>\n","      <td>2.55</td>\n","      <td>1.677196</td>\n","      <td>1.545455</td>\n","      <td>1.677196</td>\n","      <td>1.16990</td>\n","      <td>70.454016</td>\n","      <td>1.822038</td>\n","      <td>1.431175</td>\n","      <td>1.822038</td>\n","      <td>1.337640</td>\n","      <td>2.083333</td>\n","      <td>2.637415</td>\n","      <td>2.226667</td>\n","      <td>2.637415</td>\n","      <td>1.780112</td>\n","      <td>3.143</td>\n","      <td>1.752494</td>\n","      <td>1.551066</td>\n","      <td>1.752494</td>\n","      <td>1.177428</td>\n","      <td>30.170243</td>\n","      <td>2.046460</td>\n","      <td>1.924647</td>\n","      <td>2.046460</td>\n","      <td>1.470030</td>\n","      <td>1.786726</td>\n","      <td>1.186369</td>\n","      <td>1.424628</td>\n","      <td>...</td>\n","      <td>1.439421</td>\n","      <td>0.168294</td>\n","      <td>0.085208</td>\n","      <td>1.152865</td>\n","      <td>0.085208</td>\n","      <td>1.180359</td>\n","      <td>8.338057</td>\n","      <td>0.400124</td>\n","      <td>0.392933</td>\n","      <td>0.400124</td>\n","      <td>-0.023186</td>\n","      <td>0.9813</td>\n","      <td>1.051234</td>\n","      <td>0.233082</td>\n","      <td>1.051234</td>\n","      <td>0.290826</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 92 columns</p>\n","</div>"],"text/plain":["   sent_no  parag_no  ...  stanza_roll100 stanza_stdscaler_roll10\n","0        0         0  ...             NaN                     NaN\n","1        1         1  ...             NaN                     NaN\n","2        2         2  ...             NaN                     NaN\n","3        3         3  ...             NaN                     NaN\n","4        4         3  ...             NaN                     NaN\n","\n","[5 rows x 92 columns]"]},"execution_count":115,"metadata":{},"output_type":"execute_result"}],"source":["corpus_sents_trans_df.head()"]},{"cell_type":"markdown","metadata":{"id":"MJbeiU1R_z5y"},"source":["## **(5-way) RoBERTa Large 15 Datasets**\n","\n","* https://huggingface.co/siebert/sentiment-roberta-large-english\n","* https://huggingface.co/roberta-base "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"executionInfo":{"elapsed":37020,"status":"ok","timestamp":1631868169103,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"AyZI9cVw_z5z","outputId":"3194c3d4-e6aa-4496-c69e-80451c4b2dab"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8431f91a4074bbcb2bfe82b049d04c3","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/687 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df723023550f4ec4aee35e89f89c7b6a","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69242f900b4146799d85eb297d905b2c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/256 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41e76eecedf84dc68355c28112bff793","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a67fc74b19c449559e0a2ddf60479043","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e5ad99c48de0447c8ea4e5806626002f","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[{'label': 'POSITIVE', 'score': 0.9988656044006348}]\n"]}],"source":["from transformers import pipeline\n","\n","sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n","print(sentiment_analysis(\"I love this!\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1631868169104,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"cvxllmCZGnCD","outputId":"0709a5a5-22f0-4ca5-e019-0f87a8e5edff"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9985595345497131}]"]},"execution_count":151,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'NEGATIVE', 'score': 0.9992327690124512}]"]},"execution_count":151,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9937145113945007}]"]},"execution_count":151,"metadata":{},"output_type":"execute_result"}],"source":["# Direct Test\n","\n","sentiment_analysis('I love wonderful good things')\n","print('\\n')\n","sentiment_analysis('I hate your guts you filthy bastard')\n","print('\\n')\n","sentiment_analysis('It is')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"elapsed":293,"status":"ok","timestamp":1631868169383,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"pUR-y7lvGnCD","outputId":"664afa8e-59a9-45a5-886f-3d7e590cd3d8"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["0.9985595345497131"]},"execution_count":152,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["-1.999458372592926"]},"execution_count":152,"metadata":{},"output_type":"execute_result"}],"source":["# Test\n","\n","model_adj_score = twoway_probability2sentiment('I love wonderful good things', sentiment_analysis, pol_labels=['NEGATIVE','POSITIVE'])\n","model_adj_score\n","print('\\n')\n","model_adj_score = threeway_probability2sentiment('It is not good', sentiment_analysis) # , pol_labels=['NEGATIVE','POSITIVE'])\n","model_adj_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1631868169696,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"H8Z7-N5MGnCD","outputId":"3e07efbf-a041-4477-9734-a6ac6f48ab44"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["-0.9986940026283264"]},"execution_count":153,"metadata":{},"output_type":"execute_result"}],"source":["# Test\n","model_adj_score = twoway_probability2sentiment('I hate your guts you bastard', sentiment_analysis, pol_labels=['NEGATIVE','POSITIVE'])\n","model_adj_score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":529},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1631868169698,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"mzcpQKpTXfuE","outputId":"b4d4823b-f2cb-4702-80a8-318177a88e30"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent_no</th>\n","      <th>parag_no</th>\n","      <th>sect_no</th>\n","      <th>sent_raw</th>\n","      <th>sent_clean</th>\n","      <th>char_len</th>\n","      <th>token_len</th>\n","      <th>sentimentr</th>\n","      <th>sentimentr_stdscaler</th>\n","      <th>sentimentr_medianiqr</th>\n","      <th>sentimentr_lnorm_stdscaler</th>\n","      <th>sentimentr_lnorm_medianiqr</th>\n","      <th>syuzhet</th>\n","      <th>syuzhet_stdscaler</th>\n","      <th>syuzhet_medianiqr</th>\n","      <th>syuzhet_lnorm_stdscaler</th>\n","      <th>syuzhet_lnorm_medianiqr</th>\n","      <th>bing</th>\n","      <th>bing_stdscaler</th>\n","      <th>bing_medianiqr</th>\n","      <th>bing_lnorm_stdscaler</th>\n","      <th>bing_lnorm_medianiqr</th>\n","      <th>sentiword</th>\n","      <th>sentiword_stdscaler</th>\n","      <th>sentiword_medianiqr</th>\n","      <th>sentiword_lnorm_stdscaler</th>\n","      <th>sentiword_lnorm_medianiqr</th>\n","      <th>senticnet</th>\n","      <th>senticnet_stdscaler</th>\n","      <th>senticnet_medianiqr</th>\n","      <th>senticnet_lnorm_stdscaler</th>\n","      <th>senticnet_lnorm_medianiqr</th>\n","      <th>nrc</th>\n","      <th>nrc_stdscaler</th>\n","      <th>nrc_medianiqr</th>\n","      <th>nrc_lnorm_stdscaler</th>\n","      <th>nrc_lnorm_medianiqr</th>\n","      <th>afinn</th>\n","      <th>afinn_stdscaler</th>\n","      <th>afinn_medianiqr</th>\n","      <th>...</th>\n","      <th>textblob_lnorm_medianiqr</th>\n","      <th>pattern</th>\n","      <th>pattern_stdscaler</th>\n","      <th>pattern_medianiqr</th>\n","      <th>pattern_lnorm_stdscaler</th>\n","      <th>pattern_lnorm_medianiqr</th>\n","      <th>stanza</th>\n","      <th>stanza_stdscaler</th>\n","      <th>stanza_medianiqr</th>\n","      <th>stanza_lnorm_stdscaler</th>\n","      <th>stanza_lnorm_medianiqr</th>\n","      <th>flair</th>\n","      <th>flair_stdscaler</th>\n","      <th>flair_medianiqr</th>\n","      <th>flair_lnorm_stdscaler</th>\n","      <th>flair_lnorm_medianiqr</th>\n","      <th>sentimentr_roll100</th>\n","      <th>sentimentr_stdscaler_roll10</th>\n","      <th>syuzhet_roll100</th>\n","      <th>syuzhet_stdscaler_roll10</th>\n","      <th>bing_roll100</th>\n","      <th>bing_stdscaler_roll10</th>\n","      <th>sentiword_roll100</th>\n","      <th>sentiword_stdscaler_roll10</th>\n","      <th>senticnet_roll100</th>\n","      <th>senticnet_stdscaler_roll10</th>\n","      <th>nrc_roll100</th>\n","      <th>nrc_stdscaler_roll10</th>\n","      <th>afinn_roll100</th>\n","      <th>afinn_stdscaler_roll10</th>\n","      <th>vader_roll100</th>\n","      <th>vader_stdscaler_roll10</th>\n","      <th>textblob_roll100</th>\n","      <th>textblob_stdscaler_roll10</th>\n","      <th>flair_roll100</th>\n","      <th>flair_stdscaler_roll10</th>\n","      <th>pattern_roll100</th>\n","      <th>pattern_stdscaler_roll10</th>\n","      <th>stanza_roll100</th>\n","      <th>stanza_stdscaler_roll10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Letter</td>\n","      <td>letter</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>-0.03172</td>\n","      <td>0.0</td>\n","      <td>-0.03172</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.017066</td>\n","      <td>0.0</td>\n","      <td>-0.017066</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.126496</td>\n","      <td>0.0</td>\n","      <td>0.126496</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.039</td>\n","      <td>-0.285565</td>\n","      <td>-0.167174</td>\n","      <td>-0.285565</td>\n","      <td>0.245632</td>\n","      <td>0.0</td>\n","      <td>-0.087484</td>\n","      <td>0.0</td>\n","      <td>-0.087484</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.01353</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.02931</td>\n","      <td>0.0</td>\n","      <td>0.02931</td>\n","      <td>0.0</td>\n","      <td>10.000000</td>\n","      <td>0.943158</td>\n","      <td>0.813018</td>\n","      <td>0.943158</td>\n","      <td>96.262640</td>\n","      <td>-0.5913</td>\n","      <td>-0.641646</td>\n","      <td>-0.565404</td>\n","      <td>-0.641646</td>\n","      <td>-6.797046</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>_To Mrs. Saville, England._</td>\n","      <td>to mrs saville england</td>\n","      <td>27</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>-0.03172</td>\n","      <td>0.0</td>\n","      <td>-0.03172</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.017066</td>\n","      <td>0.0</td>\n","      <td>-0.017066</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.126496</td>\n","      <td>0.0</td>\n","      <td>0.126496</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.000</td>\n","      <td>-0.311172</td>\n","      <td>-0.188763</td>\n","      <td>-0.311172</td>\n","      <td>-0.198164</td>\n","      <td>0.0</td>\n","      <td>-0.087484</td>\n","      <td>0.0</td>\n","      <td>-0.087484</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.01353</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.02931</td>\n","      <td>0.0</td>\n","      <td>0.02931</td>\n","      <td>0.0</td>\n","      <td>2.691257</td>\n","      <td>-1.444950</td>\n","      <td>-1.034394</td>\n","      <td>-1.444950</td>\n","      <td>3.479559</td>\n","      <td>0.8427</td>\n","      <td>0.902033</td>\n","      <td>0.162708</td>\n","      <td>0.902033</td>\n","      <td>2.239383</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 92 columns</p>\n","</div>"],"text/plain":["   sent_no  parag_no  ...  stanza_roll100 stanza_stdscaler_roll10\n","0        0         0  ...             NaN                     NaN\n","1        1         1  ...             NaN                     NaN\n","\n","[2 rows x 92 columns]"]},"execution_count":154,"metadata":{},"output_type":"execute_result"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent_no</th>\n","      <th>parag_no</th>\n","      <th>sect_no</th>\n","      <th>sent_raw</th>\n","      <th>sent_clean</th>\n","      <th>char_len</th>\n","      <th>token_len</th>\n","      <th>sentimentr</th>\n","      <th>sentimentr_stdscaler</th>\n","      <th>sentimentr_medianiqr</th>\n","      <th>sentimentr_lnorm_stdscaler</th>\n","      <th>sentimentr_lnorm_medianiqr</th>\n","      <th>syuzhet</th>\n","      <th>syuzhet_stdscaler</th>\n","      <th>syuzhet_medianiqr</th>\n","      <th>syuzhet_lnorm_stdscaler</th>\n","      <th>syuzhet_lnorm_medianiqr</th>\n","      <th>bing</th>\n","      <th>bing_stdscaler</th>\n","      <th>bing_medianiqr</th>\n","      <th>bing_lnorm_stdscaler</th>\n","      <th>bing_lnorm_medianiqr</th>\n","      <th>sentiword</th>\n","      <th>sentiword_stdscaler</th>\n","      <th>sentiword_medianiqr</th>\n","      <th>sentiword_lnorm_stdscaler</th>\n","      <th>sentiword_lnorm_medianiqr</th>\n","      <th>senticnet</th>\n","      <th>senticnet_stdscaler</th>\n","      <th>senticnet_medianiqr</th>\n","      <th>senticnet_lnorm_stdscaler</th>\n","      <th>senticnet_lnorm_medianiqr</th>\n","      <th>nrc</th>\n","      <th>nrc_stdscaler</th>\n","      <th>nrc_medianiqr</th>\n","      <th>nrc_lnorm_stdscaler</th>\n","      <th>nrc_lnorm_medianiqr</th>\n","      <th>afinn</th>\n","      <th>afinn_stdscaler</th>\n","      <th>afinn_medianiqr</th>\n","      <th>...</th>\n","      <th>textblob_lnorm_medianiqr</th>\n","      <th>pattern</th>\n","      <th>pattern_stdscaler</th>\n","      <th>pattern_medianiqr</th>\n","      <th>pattern_lnorm_stdscaler</th>\n","      <th>pattern_lnorm_medianiqr</th>\n","      <th>stanza</th>\n","      <th>stanza_stdscaler</th>\n","      <th>stanza_medianiqr</th>\n","      <th>stanza_lnorm_stdscaler</th>\n","      <th>stanza_lnorm_medianiqr</th>\n","      <th>flair</th>\n","      <th>flair_stdscaler</th>\n","      <th>flair_medianiqr</th>\n","      <th>flair_lnorm_stdscaler</th>\n","      <th>flair_lnorm_medianiqr</th>\n","      <th>sentimentr_roll100</th>\n","      <th>sentimentr_stdscaler_roll10</th>\n","      <th>syuzhet_roll100</th>\n","      <th>syuzhet_stdscaler_roll10</th>\n","      <th>bing_roll100</th>\n","      <th>bing_stdscaler_roll10</th>\n","      <th>sentiword_roll100</th>\n","      <th>sentiword_stdscaler_roll10</th>\n","      <th>senticnet_roll100</th>\n","      <th>senticnet_stdscaler_roll10</th>\n","      <th>nrc_roll100</th>\n","      <th>nrc_stdscaler_roll10</th>\n","      <th>afinn_roll100</th>\n","      <th>afinn_stdscaler_roll10</th>\n","      <th>vader_roll100</th>\n","      <th>vader_stdscaler_roll10</th>\n","      <th>textblob_roll100</th>\n","      <th>textblob_stdscaler_roll10</th>\n","      <th>flair_roll100</th>\n","      <th>flair_stdscaler_roll10</th>\n","      <th>pattern_roll100</th>\n","      <th>pattern_stdscaler_roll10</th>\n","      <th>stanza_roll100</th>\n","      <th>stanza_stdscaler_roll10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3280</th>\n","      <td>3282</td>\n","      <td>764</td>\n","      <td>24</td>\n","      <td>He sprang from the cabin-window as he said thi...</td>\n","      <td>he sprang from the cabin window as he said thi...</td>\n","      <td>97</td>\n","      <td>19</td>\n","      <td>-0.25</td>\n","      <td>-0.195509</td>\n","      <td>-0.151515</td>\n","      <td>-0.195509</td>\n","      <td>-0.153877</td>\n","      <td>-0.25</td>\n","      <td>-0.183170</td>\n","      <td>-0.151515</td>\n","      <td>-0.183170</td>\n","      <td>-0.156953</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.126496</td>\n","      <td>0.000000</td>\n","      <td>0.0500</td>\n","      <td>0.181173</td>\n","      <td>0.057778</td>\n","      <td>0.181173</td>\n","      <td>0.060495</td>\n","      <td>-0.566</td>\n","      <td>-0.682803</td>\n","      <td>-0.502076</td>\n","      <td>-0.682803</td>\n","      <td>-0.537149</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>-0.087484</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.013530</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.02931</td>\n","      <td>0.0</td>\n","      <td>0.02931</td>\n","      <td>0.0</td>\n","      <td>6.460507</td>\n","      <td>-0.213360</td>\n","      <td>-0.081650</td>\n","      <td>-0.213360</td>\n","      <td>0.169122</td>\n","      <td>-0.9373</td>\n","      <td>-1.014110</td>\n","      <td>-0.741086</td>\n","      <td>-1.014110</td>\n","      <td>-0.690298</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3281</th>\n","      <td>3283</td>\n","      <td>764</td>\n","      <td>24</td>\n","      <td>He was soon borne away by the waves and lost i...</td>\n","      <td>he was soon borne away by the waves and lost i...</td>\n","      <td>70</td>\n","      <td>14</td>\n","      <td>-1.50</td>\n","      <td>-1.014455</td>\n","      <td>-0.909091</td>\n","      <td>-1.014455</td>\n","      <td>-1.252998</td>\n","      <td>-1.50</td>\n","      <td>-1.013691</td>\n","      <td>-0.909091</td>\n","      <td>-1.013691</td>\n","      <td>-1.278042</td>\n","      <td>-54.959288</td>\n","      <td>-1.196151</td>\n","      <td>-1.116421</td>\n","      <td>-1.196151</td>\n","      <td>-1.937849</td>\n","      <td>-1.6875</td>\n","      <td>-1.917706</td>\n","      <td>-1.795556</td>\n","      <td>-1.917706</td>\n","      <td>-2.672535</td>\n","      <td>-1.636</td>\n","      <td>-1.385355</td>\n","      <td>-1.094381</td>\n","      <td>-1.385355</td>\n","      <td>-1.527926</td>\n","      <td>-15.823965</td>\n","      <td>-1.206714</td>\n","      <td>-1.009457</td>\n","      <td>-1.206714</td>\n","      <td>-1.431884</td>\n","      <td>-1.460356</td>\n","      <td>-0.945075</td>\n","      <td>-1.164401</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.02931</td>\n","      <td>0.0</td>\n","      <td>0.02931</td>\n","      <td>0.0</td>\n","      <td>4.746158</td>\n","      <td>-0.773517</td>\n","      <td>-0.514982</td>\n","      <td>-0.773517</td>\n","      <td>0.159022</td>\n","      <td>-0.6195</td>\n","      <td>-0.672003</td>\n","      <td>-0.579723</td>\n","      <td>-0.672003</td>\n","      <td>-0.633040</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 92 columns</p>\n","</div>"],"text/plain":["      sent_no  parag_no  ...  stanza_roll100 stanza_stdscaler_roll10\n","3280     3282       764  ...             NaN                     NaN\n","3281     3283       764  ...             NaN                     NaN\n","\n","[2 rows x 92 columns]"]},"execution_count":154,"metadata":{},"output_type":"execute_result"}],"source":["# Verify Transformer DataFrame content\n","\n","corpus_sents_trans_df.head(2)\n","corpus_sents_trans_df.tail(2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":17},"id":"UC_Ah0QAC9Th","outputId":"34eba305-37aa-4031-f9ff-b88c8c155e9c"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: user 30min 29s, sys: 6.62 s, total: 30min 36s\n","Wall time: 15min 15s\n"]}],"source":["%%time\n","\n","# NOTE: ~15-20m\n","#       28m50s mins (20210915 at 13:26) Colab Pro: GPU+RAM (jausten_prideandprejudice)\n","#       ? m ? s mins (20210915 at 13:26) Colab Pro: GPU+RAM (mshelley_frankenstein)\n","\n","# corpus_sents_trans_df['roberta15lg'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: twoway_probability2sentiment(x[:510], sentiment_analysis, pol_labels=['NEGATIVE','POSITIVE']))\n","# corpus_sents_trans_df['roberta15lg'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: twoway_probability2sentiment(x[:510], sentiment_analysis, pol_labels=['Negative','Positive']))\n","corpus_sents_trans_df['roberta15lg'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: threeway_probability2sentiment(x[:510], sentiment_analysis)) # , pol_labels=['Negative','Positive']))\n","corpus_sents_trans_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HRu-URq6HMtj","outputId":"61e21aa9-3979-4e37-928a-9b78a8473dee"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent_no</th>\n","      <th>parag_no</th>\n","      <th>sect_no</th>\n","      <th>sent_raw</th>\n","      <th>sent_clean</th>\n","      <th>char_len</th>\n","      <th>token_len</th>\n","      <th>sentimentr</th>\n","      <th>sentimentr_stdscaler</th>\n","      <th>sentimentr_medianiqr</th>\n","      <th>sentimentr_lnorm_stdscaler</th>\n","      <th>sentimentr_lnorm_medianiqr</th>\n","      <th>syuzhet</th>\n","      <th>syuzhet_stdscaler</th>\n","      <th>syuzhet_medianiqr</th>\n","      <th>syuzhet_lnorm_stdscaler</th>\n","      <th>syuzhet_lnorm_medianiqr</th>\n","      <th>bing</th>\n","      <th>bing_stdscaler</th>\n","      <th>bing_medianiqr</th>\n","      <th>bing_lnorm_stdscaler</th>\n","      <th>bing_lnorm_medianiqr</th>\n","      <th>sentiword</th>\n","      <th>sentiword_stdscaler</th>\n","      <th>sentiword_medianiqr</th>\n","      <th>sentiword_lnorm_stdscaler</th>\n","      <th>sentiword_lnorm_medianiqr</th>\n","      <th>senticnet</th>\n","      <th>senticnet_stdscaler</th>\n","      <th>senticnet_medianiqr</th>\n","      <th>senticnet_lnorm_stdscaler</th>\n","      <th>senticnet_lnorm_medianiqr</th>\n","      <th>nrc</th>\n","      <th>nrc_stdscaler</th>\n","      <th>nrc_medianiqr</th>\n","      <th>nrc_lnorm_stdscaler</th>\n","      <th>nrc_lnorm_medianiqr</th>\n","      <th>afinn</th>\n","      <th>afinn_stdscaler</th>\n","      <th>afinn_medianiqr</th>\n","      <th>...</th>\n","      <th>pattern</th>\n","      <th>pattern_stdscaler</th>\n","      <th>pattern_medianiqr</th>\n","      <th>pattern_lnorm_stdscaler</th>\n","      <th>pattern_lnorm_medianiqr</th>\n","      <th>stanza</th>\n","      <th>stanza_stdscaler</th>\n","      <th>stanza_medianiqr</th>\n","      <th>stanza_lnorm_stdscaler</th>\n","      <th>stanza_lnorm_medianiqr</th>\n","      <th>flair</th>\n","      <th>flair_stdscaler</th>\n","      <th>flair_medianiqr</th>\n","      <th>flair_lnorm_stdscaler</th>\n","      <th>flair_lnorm_medianiqr</th>\n","      <th>sentimentr_roll100</th>\n","      <th>sentimentr_stdscaler_roll10</th>\n","      <th>syuzhet_roll100</th>\n","      <th>syuzhet_stdscaler_roll10</th>\n","      <th>bing_roll100</th>\n","      <th>bing_stdscaler_roll10</th>\n","      <th>sentiword_roll100</th>\n","      <th>sentiword_stdscaler_roll10</th>\n","      <th>senticnet_roll100</th>\n","      <th>senticnet_stdscaler_roll10</th>\n","      <th>nrc_roll100</th>\n","      <th>nrc_stdscaler_roll10</th>\n","      <th>afinn_roll100</th>\n","      <th>afinn_stdscaler_roll10</th>\n","      <th>vader_roll100</th>\n","      <th>vader_stdscaler_roll10</th>\n","      <th>textblob_roll100</th>\n","      <th>textblob_stdscaler_roll10</th>\n","      <th>flair_roll100</th>\n","      <th>flair_stdscaler_roll10</th>\n","      <th>pattern_roll100</th>\n","      <th>pattern_stdscaler_roll10</th>\n","      <th>stanza_roll100</th>\n","      <th>stanza_stdscaler_roll10</th>\n","      <th>roberta15lg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Letter</td>\n","      <td>letter</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>-0.03172</td>\n","      <td>0.0</td>\n","      <td>-0.03172</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.017066</td>\n","      <td>0.0</td>\n","      <td>-0.017066</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.126496</td>\n","      <td>0.0</td>\n","      <td>0.126496</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.039</td>\n","      <td>-0.285565</td>\n","      <td>-0.167174</td>\n","      <td>-0.285565</td>\n","      <td>0.245632</td>\n","      <td>0.0</td>\n","      <td>-0.087484</td>\n","      <td>0.0</td>\n","      <td>-0.087484</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.01353</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.02931</td>\n","      <td>0.0</td>\n","      <td>0.02931</td>\n","      <td>0.0</td>\n","      <td>10.000000</td>\n","      <td>0.943158</td>\n","      <td>0.813018</td>\n","      <td>0.943158</td>\n","      <td>96.262640</td>\n","      <td>-0.5913</td>\n","      <td>-0.641646</td>\n","      <td>-0.565404</td>\n","      <td>-0.641646</td>\n","      <td>-6.797046</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.801378</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>_To Mrs. Saville, England._</td>\n","      <td>to mrs saville england</td>\n","      <td>27</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>-0.03172</td>\n","      <td>0.0</td>\n","      <td>-0.03172</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.017066</td>\n","      <td>0.0</td>\n","      <td>-0.017066</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.126496</td>\n","      <td>0.0</td>\n","      <td>0.126496</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.120774</td>\n","      <td>0.004444</td>\n","      <td>0.120774</td>\n","      <td>0.002101</td>\n","      <td>0.000</td>\n","      <td>-0.311172</td>\n","      <td>-0.188763</td>\n","      <td>-0.311172</td>\n","      <td>-0.198164</td>\n","      <td>0.0</td>\n","      <td>-0.087484</td>\n","      <td>0.0</td>\n","      <td>-0.087484</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.01353</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.02931</td>\n","      <td>0.0</td>\n","      <td>0.02931</td>\n","      <td>0.0</td>\n","      <td>2.691257</td>\n","      <td>-1.444950</td>\n","      <td>-1.034394</td>\n","      <td>-1.444950</td>\n","      <td>3.479559</td>\n","      <td>0.8427</td>\n","      <td>0.902033</td>\n","      <td>0.162708</td>\n","      <td>0.902033</td>\n","      <td>2.239383</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.985770</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 93 columns</p>\n","</div>"],"text/plain":["   sent_no  parag_no  ...  stanza_stdscaler_roll10 roberta15lg\n","0        0         0  ...                      NaN    1.801378\n","1        1         1  ...                      NaN    1.985770\n","\n","[2 rows x 93 columns]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["corpus_sents_trans_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jMRsbnRQ6tlr","outputId":"58460a41-20a7-43c4-9b5f-c4ab12caeaa5"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'mshelley'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["author_abbr_str"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"X5mXF7xh6vFx","outputId":"eac77b31-7aa3-471d-8da7-e468f29c3222"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'frankenstein'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["title_str"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"248Y5RqLvVAk","outputId":"e0442471-f7c9-4370-8124-95e338135bbc"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving to file: sum_sentiments_sents_trans_mshelley_frankenstein.csv\n"]}],"source":["# Save Preprocessed Corpus Sentences DataFrame\n","\n","# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Sentences\n","corpus_sents_filename = f'sum_sentiments_sents_trans_{author_abbr_str}_{title_str}.csv'\n","print(f'Saving to file: {corpus_sents_filename}')\n","\n","corpus_sents_trans_df.to_csv(corpus_sents_filename)"]},{"cell_type":"markdown","metadata":{"id":"ReDQHLzGrQG8"},"source":["## **(5-way) Yelp Sentiment Finetuned**\n","\n","* https://huggingface.co/gilf/english-yelp-sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VH-vhk2htCea","outputId":"e2bbf7b3-2b1b-44ec-90ed-57306c57a787"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72c81c9c4fb5493cba25d1fa45ed6496","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ea2892eb82c4c0f99d00c99850e1e9e","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/748 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42e5b219c56a48f591ccc2dca1247f28","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dee620c814234ff6b223b280cec3bdb0","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47d9b3f512e546dfa16fe99bb7a01305","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/433M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"gilf/english-yelp-sentiment\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"gilf/english-yelp-sentiment\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nKh2PaQCtKD0","outputId":"9cdc46b9-211f-42b7-aeb3-256fbe38403b"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"gilf/english-yelp-sentiment\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1631867951637,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"neF3vOLtcpnu","outputId":"698eee35-82bc-4611-af20-999547c3c22d"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'label': 'Positive', 'score': 0.9455917477607727}]"]},"execution_count":143,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'Neutral', 'score': 0.4261215627193451}]"]},"execution_count":143,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'Negative', 'score': 0.9544662237167358}]"]},"execution_count":143,"metadata":{},"output_type":"execute_result"}],"source":["# Test Directly\n","\n","test_result_pos = sentiment_analysis('I absolutely love this great and wonderful opportunity to enjoy lovely and good things')\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = sentiment_analysis('blank')\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = sentiment_analysis('I hate bad evil dislike')\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MUzc-0VVcpnw","outputId":"2cf8bb89-d6b5-47cc-e2bf-c4ba982c4b51"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Positive Sentence Polarity: 4.547934353351593\n","    Text: I love wonderful good things\n","\n","Neutral Sentence Polarity: 2.7140876054763794\n","    Text: It is what it is\n","\n","Negative Sentence Polarity: 0.9732653498649597\n","    Text: I hate your stinking guts you filthy lying stealing cheating bastard.\n"]}],"source":["# Test Indirectly\n","\n","sentence_str = 'I love wonderful good things'\n","polarity = fiveway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nPositive Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'It is what it is'\n","polarity = fiveway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNeutral Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity = fiveway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNegative Sentence Polarity: {polarity}\\n    Text: {sentence_str}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"htzE-x6FtKD4","outputId":"29855f55-b480-4117-d2db-040c0ee2cff2"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: user 15min 43s, sys: 5.46 s, total: 15min 48s\n","Wall time: 7min 53s\n"]}],"source":["%%time\n","\n","# NOTE: 2 minutes up\n","#       7m53s mins (20210915 at 13:26) Colab Pro: GPU+RAM (jausten_prideandprejudice)\n","#       ? m ? s mins (20210915 at 13:26) Colab Pro: GPU+RAM (mshelley_frankenstein)\n","\n","corpus_sents_trans_df['yelp'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: fiveway_probability2sentiment(x[:510], sentiment_analysis))\n","corpus_sents_trans_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDP6THISvZQA"},"outputs":[],"source":["# Save Preprocessed Corpus Sentences DataFrame\n","\n","# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Sentences\n","corpus_sents_filename = f'sum_sentiments_sents_trans_{author_abbr_str}_{title_str}.csv'\n","print(f'Saving to file: {corpus_sents_filename}')\n","\n","corpus_sents_trans_df.to_csv(corpus_sents_filename)"]},{"cell_type":"markdown","metadata":{"id":"G3anSxJbAIuM"},"source":["## **(5-way) MULTILINGUAL NLPTown BERT**\n","\n","* https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Eli4ig1yF8ZE","outputId":"24325af5-cc88-4e13-e5ac-437a79fd3b6c"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ae67f7cba174eb8a8a8228d9069a510","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/953 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a19f63c1acc42598caf840168e9dfdc","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/669M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bbc7a67160954438af8061d187d3c842","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb79de58cbce48bbbec57a33db7d9143","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/872k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bda2ff754f0946cba0e5c90493c8e006","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[{'label': '5 stars', 'score': 0.9236246943473816}]\n"]}],"source":["from transformers import pipeline\n","\n","nlptown_sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","print(nlptown_sentiment_analysis(\"I love this!\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"l1jEtCl5nDJI","outputId":"25280afb-31d7-4c1b-91ca-996019044b54"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hku9v3XDelJT","outputId":"d342c53a-88da-4099-a773-075982118666"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'label': '5 stars', 'score': 0.9404123425483704}]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': '1 star', 'score': 0.3800172507762909}]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': '1 star', 'score': 0.6309829354286194}]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# Test Directly\n","test_result_pos = sentiment_analysis('I absolutely love this great and wonderful opportunity to enjoy lovely and good things')\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = sentiment_analysis('blank')\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = sentiment_analysis('I hate bad evil dislike')\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wKZezUMwelJW","outputId":"31c748dd-9cb7-4fa1-e727-068758d499f2"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Positive Sentence Polarity: 4.767589509487152\n","    Text: I love wonderful good things\n","\n","Neutral Sentence Polarity: 4.464488178491592\n","    Text: It is what it is\n","\n","Negative Sentence Polarity: 0.7658354043960571\n","    Text: I hate your stinking guts you filthy lying stealing cheating bastard.\n"]}],"source":["# Test Indirectly\n","\n","sentence_str = 'I love wonderful good things'\n","polarity = fiveway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nPositive Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'It is what it is'\n","polarity = fiveway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNeutral Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity = fiveway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNegative Sentence Polarity: {polarity}\\n    Text: {sentence_str}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"daWYS67-Gb3j","outputId":"ecaa19c3-baeb-40e9-d1c9-e1b94617adc2"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'label': '5 stars', 'score': 0.7675895094871521}]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1 star'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# Test Directly\n","\n","nlptown_sentiment_analysis('I love wonderful good things')\n","print('\\n')\n","test_result = nlptown_sentiment_analysis('I hate your guts you filthy bastard')\n","test_result[0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"o4K-BVlfGb3i","outputId":"cbf74cac-7907-4c93-984b-75327e69d7fc"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["4.767589509487152"]},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["0.7658354043960571"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# Test Indirectly\n","\n","fiveway_probability2sentiment('I love wonderful good things', nlptown_sentiment_analysis)\n","print('\\n')\n","fiveway_probability2sentiment('I hate your stinking guts you filthy lying stealing cheating bastard.', nlptown_sentiment_analysis)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hTDuKzqtGbiI"},"outputs":[],"source":["%%time\n","\n","# NOTE: ~5-7 minutes runtime\n","#       28m50s mins (20210915 at 13:26) Colab Pro: GPU+RAM (jausten_prideandprejudice)\n","\n","corpus_sents_trans_df['nlptown'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: fiveway_probability2sentiment(x[:510], sentiment_analysis))\n","corpus_sents_trans_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjy05TZkF8R7"},"outputs":[],"source":["# END"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUHUCR73EyEq"},"outputs":[],"source":["# import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aggiwOjcAIj1"},"outputs":[],"source":["\"\"\"\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3IygkQlETVr"},"outputs":[],"source":["\"\"\"\n","\n","# Predict Tokens\n","tokens = tokenizer.encode(\"It wasn't the worst i've seen, in fact, it was the opposite\", return_tensors='pt')\n","# tokens[0]\n","# tokenizer.decode(tokens[0])\n","result = model(tokens)\n","result\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VTSZTwFIEsMP"},"outputs":[],"source":["\"\"\"\n","\n","predict_sentiment = int(torch.argmax(result.logits))+1\n","predict_sentiment\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j21_x-ReEsMQ"},"outputs":[],"source":["\"\"\"\n","\n","def nlptown_sentiment_score(text):\n","  '''\n","  Given a text string (sentence or paragraph)\n","  Return a floating point sentiment value\n","  '''\n","\n","  # tokens = tokenizer.encode(text, return_tensors='pt')\n","  # result = model(tokens)\n","  # sentiment_int = int(torch.argmax(result.logits))+1\n","  # sentiment_fl = sentiment_int + result.logits[sentiment_int-1]\n","  # return sentiment_fl\n","\n","  tokens = tokenizer.encode(text, return_tensors='pt')\n","  result = model(tokens)\n","  type(result)\n","  prob_ls = list(result.logits)[0].tolist()\n","  # print(f'prob_ls: {prob_ls}')\n","  # prob_ls_sum = sum(prob_ls)\n","  prob_ls_sum = sum(map(abs, prob_ls))\n","  prob_norm_ls = [abs(i/prob_ls_sum) for i in prob_ls]\n","  # prob_ls_min = min(prob_ls)\n","  # prob_ls_max = max(prob_ls)\n","  # prob_norm_ls = [(x-prob_ls_min)/(prob_ls_max-prob_ls_min) for x in prob_ls]\n","  # print(f'prob_norm_ls {prob_norm_ls}')\n","  prob_int = int(torch.argmax(result.logits))\n","  # print(f'prob_int {prob_int}')\n","  prob_frac = abs(float(prob_norm_ls[prob_int]))\n","  # print(f'prob_frac {prob_frac}')\n","  \n","  return prob_int + prob_frac # int(torch.argmax(result.logits))+1\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d350bXAREsMS"},"outputs":[],"source":["# nlptown_sentiment_score('i love the smell of beautiful flowers, the make me happy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iz25gCr1EsMU"},"outputs":[],"source":["%time\n","\n","# NOTE: 10m Long-running process\n","\n","# Calculate Sentence Sentiment Scores using the NLPTown BERT fine-grained, fine-tuned, multi-lingual model\n","\n","# https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n","\n","# This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in \n","#    six languages: English, Dutch, German, French, Spanish and Italian. \n","#    It predicts the sentiment of the review as a number of stars (between 1 and 5).\n","\n","# corpus_sents_df['nlptown'] = corpus_sents_df['sent_raw'].astype('str').apply(lambda x: nlptown_sentiment_score(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8k1lybi9vawS"},"outputs":[],"source":["# Save Preprocessed Corpus Sentences DataFrame\n","\n","# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Sentences\n","corpus_sents_filename = f'sum_sentiments_sents_trans_{author_abbr_str}_{title_str}.csv'\n","print(f'Saving to file: {corpus_sents_filename}')\n","\n","corpus_sents_trans_df.to_csv(corpus_sents_filename)"]},{"cell_type":"markdown","metadata":{"id":"8opUJxLkunNu"},"source":["## **Huggingface Distill BERT SST**\n","\n","* https://www.machinecurve.com/index.php/2020/12/23/easy-sentiment-analysis-with-machine-learning-and-huggingface-transformers/\n","\n","* https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aikcv-m1unNu"},"outputs":[],"source":["from transformers import pipeline\n","\n","classifier = pipeline('sentiment-analysis')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUZFcwfdunNv"},"outputs":[],"source":["results = classifier([\"We are very happy to show you the 🤗 Transformers library.\",\n","                      \"We hope you don't hate it.\"])\n","\n","for result in results:\n","  print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n","\n","print('\\n')\n","type(results[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSvSydxzmnie"},"outputs":[],"source":["# Test Directly\n","\n","test_result_pos = classifier('I absolutely love this great and wonderful opportunity to enjoy lovely and good things')\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = classifier('blank')\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = classifier('I hate bad evil dislike')\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XT9wTlxmniY"},"outputs":[],"source":["# Test Indirectly\n","\n","sentence_str = 'I love wonderful good things'\n","polarity = twoway_probability2sentiment(sentence_str, classifier, pol_labels=['NEGATIVE','POSITIVE'])\n","print(f'\\nPositive Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'is'\n","polarity = twoway_probability2sentiment(sentence_str, classifier, pol_labels=['NEGATIVE','POSITIVE'])\n","print(f'\\nNeutral Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity = twoway_probability2sentiment(sentence_str, classifier, pol_labels=['NEGATIVE','POSITIVE'])\n","print(f'\\nNegative Sentence Polarity: {polarity}\\n    Text: {sentence_str}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7YwZpPKnLkp"},"outputs":[],"source":["%%time\n","\n","# NOTE: !5 minutes runtime\n","#       28m50s mins (20210915 at 13:26) Colab Pro: GPU+RAM (jausten_prideandprejudice)\n","\n","corpus_sents_trans_df['huggingface'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: twoway_probability2sentiment(x[:510], classifier, pol_labels=['NEGATIVE','POSITIVE']))\n","corpus_sents_trans_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIbdb5AivcKo"},"outputs":[],"source":["# Save Preprocessed Corpus Sentences DataFrame\n","\n","# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Sentences\n","corpus_sents_filename = f'sum_sentiments_sents_trans_{author_abbr_str}_{title_str}.csv'\n","print(f'Saving to file: {corpus_sents_filename}')\n","\n","corpus_sents_trans_df.to_csv(corpus_sents_filename)"]},{"cell_type":"markdown","metadata":{"id":"qEOydcZpum2h"},"source":["## **(3-way) BERT Multilingual Mixed Code Hinglish**\n","\n","* https://huggingface.co/rohanrajpal/bert-base-multilingual-codemixed-cased-sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177},"executionInfo":{"elapsed":20085,"status":"ok","timestamp":1631865053448,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"O1XwIQu6wYh7","outputId":"4d3a1703-6c19-4efd-c78a-34605d746972"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"814d3627dafb4c66b34ffd8a72d6f3f1","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40356d4d53764bd59aaed46a97dbac56","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/828 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"378d56da02774a35a7b91269d418d505","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b78b9588b1c4b9896e23d3aaf6859ec","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"67b524c35d5c4ad9a13d40edad4f35d3","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/712M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"rohanrajpal/bert-base-multilingual-codemixed-cased-sentiment\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"rohanrajpal/bert-base-multilingual-codemixed-cased-sentiment\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":5776,"status":"ok","timestamp":1631865059217,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"fg7mKXnBwuGP","outputId":"8e95b637-6061-4b34-9bbb-ba9ef1c2d1ba"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"rohanrajpal/bert-base-multilingual-codemixed-cased-sentiment\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":217,"status":"ok","timestamp":1631865059422,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"Pu5QlXKxe-_U","outputId":"36f66ba5-2b91-47c1-c3a4-8723b6de46cc"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'label': 'LABEL_2', 'score': 0.9715988636016846}]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'LABEL_0', 'score': 0.8095260262489319}]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'LABEL_1', 'score': 0.9319002032279968}]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["# Test Directly\n","\n","test_result_pos = sentiment_analysis('I absolutely love this great and wonderful opportunity to enjoy lovely and good things')\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = sentiment_analysis('blank')\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = sentiment_analysis('I hate bad evil dislike')\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1631865059630,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"JjwtoQtme-_V","outputId":"f5930e80-f272-4856-92cc-fa15f88042f0"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Positive Sentence Polarity: 1.971106469631195\n","    Text: I love wonderful good things\n","\n","Neutral Sentence Polarity: 1.7358180284500122\n","    Text: It is what it is\n","\n","Negative Sentence Polarity: -1.8960636258125305\n","    Text: I hate your stinking guts you filthy lying stealing cheating bastard.\n"]}],"source":["# Test Indirectly\n","\n","sentence_str = 'I love wonderful good things'\n","polarity = threeway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nPositive Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'It is what it is'\n","polarity = threeway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNeutral Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity = threeway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNegative Sentence Polarity: {polarity}\\n    Text: {sentence_str}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":437629,"status":"ok","timestamp":1631865501388,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"v5D_Sk0ufGw_","outputId":"687a0d16-7cac-4e3f-ad56-7f2f38946006"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: user 14min 32s, sys: 4.72 s, total: 14min 36s\n","Wall time: 7min 17s\n"]}],"source":["%%time\n","\n","# NOTE: ~6-10 minutes runtime\n","#       7m17s mins (20210915 at 13:26) Colab Pro: GPU+RAM (jausten_prideandprejudice)\n","\n","corpus_sents_trans_df['hinglish'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: threeway_probability2sentiment(x[:510], sentiment_analysis))\n","corpus_sents_trans_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":204,"status":"ok","timestamp":1631865502889,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"nGaXaUtpveiI","outputId":"dfda7019-2380-4933-c17e-1b8863a96a39"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving to file: sum_sentiments_sents_trans_vnabokov_palefire-cantos.csv\n"]}],"source":["# Save Preprocessed Corpus Sentences DataFrame\n","\n","# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Sentences\n","corpus_sents_filename = f'sum_sentiments_sents_trans_{author_abbr_str}_{title_str}.csv'\n","print(f'Saving to file: {corpus_sents_filename}')\n","\n","corpus_sents_trans_df.to_csv(corpus_sents_filename)"]},{"cell_type":"markdown","metadata":{"id":"LEC93OI0um9e"},"source":["## **(2-way) IMDB Sentiment**\n","\n","* https://huggingface.co/abhishek/autonlp-imdb_sentiment_classification-31154 (metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"executionInfo":{"elapsed":13911,"status":"ok","timestamp":1631865517038,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"63dihgd3um9e","outputId":"41af0594-2d46-4fcf-89be-1e77b0d3d471"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44115756e0f3406197f73478fc3686e6","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/283 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f39a7f8bd00242c991cad756cf6e80dd","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/816 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5096c5aabc2848e28ef1a361d6015fdd","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b5cbd07c2af545858f967de24d70dacb","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"957780322adb4da7830299e3def10483","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc121da3e8f64dd4a89b262a2cbd1fb0","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/329M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"abhishek/autonlp-imdb_sentiment_classification-31154\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"abhishek/autonlp-imdb_sentiment_classification-31154\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":5287,"status":"ok","timestamp":1631865522290,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"S6_94blh3OsO","outputId":"3b484d00-22f4-44ea-9133-b82d362b984b"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"abhishek/autonlp-imdb_sentiment_classification-31154\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1631865522292,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"kZNW4_DF3OsP","outputId":"6709abbf-b032-4e3a-c2fd-c33d6ad5ad62"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'label': '1', 'score': 0.9978368878364563}]"]},"execution_count":60,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': '0', 'score': 0.9467214345932007}]"]},"execution_count":60,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': '0', 'score': 0.9924196600914001}]"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["# Test Directly\n","\n","test_result_pos = sentiment_analysis('I absolutely love this great and wonderful opportunity to enjoy lovely and good things')\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = sentiment_analysis('blank')\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = sentiment_analysis('I hate bad evil dislike')\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1631865522294,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"ij5wvR5v3OsQ","outputId":"c9903363-4c0d-4977-e6da-33b615f15957"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Positive Sentence Polarity: 0.9855077862739563\n","    Text: I love wonderful good things\n","\n","Neutral Sentence Polarity: 0.9945908784866333\n","    Text: It is what it is\n","\n","Negative Sentence Polarity: -0.7816630601882935\n","    Text: I hate your stinking guts you filthy lying stealing cheating bastard.\n"]}],"source":["# Test Indirectly\n","\n","sentence_str = 'I love wonderful good things'\n","polarity = twoway_probability2sentiment(sentence_str, sentiment_analysis, pol_labels=['0','1'])\n","print(f'\\nPositive Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'It is what it is'\n","polarity = twoway_probability2sentiment(sentence_str, sentiment_analysis, pol_labels=['0','1'])\n","print(f'\\nNeutral Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity = twoway_probability2sentiment(sentence_str, sentiment_analysis, pol_labels=['0','1'])\n","print(f'\\nNegative Sentence Polarity: {polarity}\\n    Text: {sentence_str}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":220277,"status":"ok","timestamp":1631865742550,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"Km_pbXl-um9f","outputId":"777971f4-a55f-4e60-d14c-8e67e7f0a8ba"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: user 7min 5s, sys: 2.44 s, total: 7min 7s\n","Wall time: 3min 33s\n"]}],"source":["%%time\n","\n","# NOTE: ~5 minutes runtime\n","#       3m33s mins (20210915 at 13:26) Colab Pro: GPU+RAM (jausten_prideandprejudice)\n","\n","corpus_sents_trans_df['imdb2way'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: twoway_probability2sentiment(x[:510], sentiment_analysis, pol_labels=['0','1']))\n","corpus_sents_trans_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1631865742552,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"_LqoM9MhvlP9","outputId":"59a2712d-0ca1-41e7-9a89-8f7bf2a39e46"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving to file: sum_sentiments_sents_trans_vnabokov_palefire-cantos.csv\n"]}],"source":["# Save Preprocessed Corpus Sentences DataFrame\n","\n","# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Sentences\n","corpus_sents_filename = f'sum_sentiments_sents_trans_{author_abbr_str}_{title_str}.csv'\n","print(f'Saving to file: {corpus_sents_filename}')\n","\n","corpus_sents_trans_df.to_csv(corpus_sents_filename)"]},{"cell_type":"markdown","metadata":{"id":"cPtgYODJ4EIc"},"source":["## **(Binary) T5Base 50k Finetuned IMDB Sentiment Extraction**\n","\n","* https://huggingface.co/mrm8488/t5-small-finetuned-imdb-sentiment \n","* https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177},"executionInfo":{"elapsed":7375,"status":"ok","timestamp":1631865749898,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"GNrSlyz44CSG","outputId":"98923855-5d84-4724-954d-7f8b8c04351c"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7aa64bc0c2e844ee9eee5e64508592c8","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae61ff2158db47cea55a18e0922139ec","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a2b0a3820dc426395f66c00f2476703","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e21c0790a28c4bdfb833197de3914a55","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91b910e6df474d178e09cb7d40aa6b6c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-small-finetuned-imdb-sentiment\")\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-small-finetuned-imdb-sentiment\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1631865749900,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"aSvbl425DpSf","outputId":"6d70f37e-9a96-4393-c62b-66dc02445959"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["def get_t5imdb50k_sentiment(text):\n","  '''\n","  Given a plain text string\n","  Return a binary integer [-1,1] (negative,positive) sentiment value\n","  '''\n","\n","  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n","\n","  output = model.generate(input_ids=input_ids,\n","               max_length=2)\n","\n","  dec = [tokenizer.decode(ids) for ids in output]\n","  label = dec[0]\n","\n","  if 'positive' in label:\n","    score_int = 1\n","  elif 'negative' in label:\n","    score_int = -1\n","  else:\n","    score_int = 0\n","\n","  return score_int\n","\n","# Test\n","# get_sentiment(\"I dislike a lot that film\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1631865749902,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"_I_CM0rh4RGc","outputId":"bab2eece-0a64-4ca8-f35a-38b2310a8c38"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["1"]},"execution_count":66,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["-1"]},"execution_count":66,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["-1"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["# Test Directly\n","\n","test_result_pos = get_t5imdb50k_sentiment('I absolutely love this great and wonderful opportunity to enjoy lovely and good things')\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = get_t5imdb50k_sentiment('blank')\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = get_t5imdb50k_sentiment('I hate bad evil dislike')\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"elapsed":603,"status":"ok","timestamp":1631865750487,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"p1hEDsxL4RGg","outputId":"0d4ace26-b966-41e3-bc52-3cf2d2330fe2"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Positive Sentence Polarity: 1\n","    Text: I love wonderful good things\n","\n","Neutral Sentence Polarity: 0\n","    Text: This\n","\n","Negative Sentence Polarity: -1\n","    Text: I hate your stinking guts you filthy lying stealing cheating bastard.\n"]}],"source":["# Test Indirectly\n","\n","# Production\n","sentence_str = 'I love wonderful good things'\n","polarity = get_t5imdb50k_sentiment(sentence_str)\n","print(f'\\nPositive Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'This'\n","polarity = get_t5imdb50k_sentiment(sentence_str)\n","print(f'\\nNeutral Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity = get_t5imdb50k_sentiment(sentence_str)\n","print(f'\\nNegative Sentence Polarity: {polarity}\\n    Text: {sentence_str}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":195796,"status":"ok","timestamp":1631865946277,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"e4rF4WiD4RGn","outputId":"bdd94f55-9594-4bf2-8bb2-d99e54ae5522"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: user 6min 30s, sys: 2.62 s, total: 6min 32s\n","Wall time: 3min 16s\n"]}],"source":["%%time\n","\n","# NOTE: ~5 minutes runtime\n","#       3m16s mins (20210915 at 13:26) Colab Pro: GPU+RAM (jausten_prideandprejudice)\n","\n","corpus_sents_trans_df['t5imdb50k'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: get_t5imdb50k_sentiment(x[:510]))\n","corpus_sents_trans_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1631865947137,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"d5uHZjR8vp4e","outputId":"c2eafa69-31a6-459c-9544-c00f43d8fa94"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving to file: sum_sentiments_sents_trans_vnabokov_palefire-cantos.csv\n"]}],"source":["# Save Preprocessed Corpus Sentences DataFrame\n","\n","# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Sentences\n","corpus_sents_filename = f'sum_sentiments_sents_trans_{author_abbr_str}_{title_str}.csv'\n","print(f'Saving to file: {corpus_sents_filename}')\n","\n","corpus_sents_trans_df.to_csv(corpus_sents_filename)"]},{"cell_type":"markdown","metadata":{"id":"ilhcBc9JAOqv"},"source":["## **(3-way: slow) MULTILINGUAL RoBERTa XLM Twitter 8 Languages**\n","\n","* http://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1631865947139,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"ut5wRKUqmtv7","outputId":"2ddd0f5e-22e8-4f07-8ffe-df1496a44601"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# !pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":27841,"status":"ok","timestamp":1631865974970,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"O4FKwsFA-pwT","outputId":"243bd6d9-4903-4bab-86d7-bfba09e56ccd"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"010b57dcbea04ea5b00fa3bfaf97217c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/841 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46d13176b0d54df1920acd4142a3f69d","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3f0af4e84fc4579923d008faa5b1045","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"717242f5f0384c29861ad09bd183602f","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":7766,"status":"ok","timestamp":1631865982723,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"UFhnYSZt-vIy","outputId":"e8563155-3e6d-440c-d954-23d472934ecb"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1631865982725,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"of2fJeMO-vIz","outputId":"0d4852f7-fa7b-42cd-e4dc-6972115743f6"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'label': 'Positive', 'score': 0.9455917477607727}]"]},"execution_count":73,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'Neutral', 'score': 0.4261215627193451}]"]},"execution_count":73,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'Negative', 'score': 0.9544662237167358}]"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["# Test Directly\n","\n","test_result_pos = sentiment_analysis('I absolutely love this great and wonderful opportunity to enjoy lovely and good things')\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = sentiment_analysis('blank')\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = sentiment_analysis('I hate bad evil dislike')\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":182},"executionInfo":{"elapsed":345,"status":"ok","timestamp":1631865983056,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"arPEoFgM-vI1","outputId":"b74ea824-ba08-4dc3-b915-00a59754c477"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Positive Sentence Polarity: 1.9224474430084229\n","    Text: I love wonderful good things\n","\n","Neutral Sentence Polarity: 0.5882502794265747\n","    Text: It is what it is\n","\n","Negative Sentence Polarity: -1.9525753259658813\n","    Text: I hate your stinking guts you filthy lying stealing cheating bastard.\n"]}],"source":["# Test English\n","\n","sentence_str = 'I love wonderful good things'\n","polarity = threeway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nPositive Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'It is what it is'\n","polarity = threeway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNeutral Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity = threeway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNegative Sentence Polarity: {polarity}\\n    Text: {sentence_str}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"executionInfo":{"elapsed":285,"status":"ok","timestamp":1631865983332,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"gtme1CESqhCt","outputId":"91b006d0-fede-46f0-d1d3-bd8547c7b68e"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'label': 'Negative', 'score': 0.961158275604248}]"]},"execution_count":75,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'Positive', 'score': 0.7114393711090088}]"]},"execution_count":75,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'Negative', 'score': 0.7386776804924011}]"]},"execution_count":75,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'Positive', 'score': 0.9386938810348511}]"]},"execution_count":75,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'Neutral', 'score': 0.5012816190719604}]"]},"execution_count":75,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'Negative', 'score': 0.9402057528495789}]"]},"execution_count":75,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n"]},{"data":{"text/plain":["[{'label': 'Negative', 'score': 0.9703734517097473}]"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["# Test French\n","\n","test_result_pos = sentiment_analysis(\"Je déteste le livre.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"Bien.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"Mal.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"J'aime et j'apprécie les journées ensoleillées avec des enfants rieurs qui jouent joyeusement pendant l'été insouciant.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"Il est.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = sentiment_analysis(\"Cette phrase est vide.\")\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = sentiment_analysis(\"Je déteste et méprise le mal affreux qui infecte notre organisation.\")\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":440732,"status":"ok","timestamp":1631866424059,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"pMQw-bt--vI3","outputId":"cec67a00-7998-4efb-9a9b-f521b5e003ff"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: user 14min 38s, sys: 4.75 s, total: 14min 43s\n","Wall time: 7min 20s\n"]}],"source":["%%time\n","\n","# NOTE: started 9:05\n","#       28m50s mins (20210915 at 13:26) Colab Pro: GPU+RAM (jausten_prideandprejudice)\n","\n","corpus_sents_trans_df['robertaxml8lang'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: threeway_probability2sentiment(x[:510], sentiment_analysis))\n","corpus_sents_trans_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Z0RiS7E_BoW"},"outputs":[],"source":["# Save Preprocessed Corpus Sentences DataFrame\n","\n","# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Sentences\n","corpus_sents_filename = f'sum_sentiments_sents_trans_{author_abbr_str}_{title_str}.csv'\n","print(f'Saving to file: {corpus_sents_filename}')\n","\n","corpus_sents_trans_df.to_csv(corpus_sents_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":400,"status":"ok","timestamp":1631866580899,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"ztIRsexX05o4","outputId":"d3e478ca-7bf9-4f5e-cdd9-583fb56fbff2"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'sum_sentiments_sents_trans_vnabokov_palefire-cantos.csv'"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["corpus_sents_filename"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":174,"status":"ok","timestamp":1631866640019,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"q0lIYxIN1EYg","outputId":"dbe75952-fac6-452b-bf42-24ad5c0aeba6"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["!mv sum_sentiments_sents_trans_vnabokov_palefire-cantos.csv sum_sentiments_sents_trans_jausten_prideandprejudice.csvreader"]},{"cell_type":"markdown","metadata":{"id":"2JG2p4RPv22T"},"source":["## **END ENGLISH**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fV187gLvGQf"},"outputs":[],"source":["\"\"\"\n","from transformers import AutoModelForSequenceClassification\n","from transformers import TFAutoModelForSequenceClassification\n","from transformers import AutoTokenizer, AutoConfig\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3VuooxfSvGMu"},"outputs":[],"source":["# import numpy as np\n","# from scipy.special import softmax"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKkKWgr2vPgY"},"outputs":[],"source":["# Preprocess text (username and link placeholders)\n","def preprocess(text):\n","    new_text = []\n","    for t in text.split(\" \"):\n","        t = '@user' if t.startswith('@') and len(t) > 1 else t\n","        t = 'http' if t.startswith('http') else t\n","        new_text.append(t)\n","    return \" \".join(new_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVKq0lMmvKeo"},"outputs":[],"source":["\"\"\"\n","\n","MODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","config = AutoConfig.from_pretrained(MODEL)\n","\n","# PT\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n","model.save_pretrained(MODEL)\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3HpEOkRAAIbL"},"outputs":[],"source":["\"\"\"\n","\n","text = \"Good night 😊\"\n","text = preprocess(text)\n","encoded_input = tokenizer(text, return_tensors='pt')\n","output = model(**encoded_input)\n","scores = output[0][0].detach().numpy()\n","scores = softmax(scores)\n","\n","# # TF\n","# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n","# model.save_pretrained(MODEL)\n","\n","# text = \"Good night 😊\"\n","# encoded_input = tokenizer(text, return_tensors='tf')\n","# output = model(encoded_input)\n","# scores = output[0][0].numpy()\n","# scores = softmax(scores)\n","\n","# Print labels and scores\n","ranking = np.argsort(scores)\n","ranking = ranking[::-1]\n","for i in range(scores.shape[0]):\n","    l = config.id2label[ranking[i]]\n","    s = scores[ranking[i]]\n","    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"tYV2eKBIXM-s"},"source":["## **(0.0-1.0) FRENCH Multilingual NLPTown**\n","\n","* https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"piny4Uu7XJ7j"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_hpjpUjXbYW"},"outputs":[],"source":["sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzRFUF89XbT4"},"outputs":[],"source":["# Test Directly\n","test_result_pos = sentiment_analysis(\"Je déteste le livre.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"Bien.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"Mal.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"J'aime et j'apprécie les journées ensoleillées avec des enfants rieurs qui jouent joyeusement pendant l'été insouciant.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"Il est.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = sentiment_analysis(\"Cette phrase est vide.\")\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = sentiment_analysis(\"Je déteste et méprise le mal affreux qui infecte notre organisation.\")\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDZXcTdFFWvF"},"outputs":[],"source":["corpus_sents_trans_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-qcOH6cFpLx"},"outputs":[],"source":["win_s1per = int(1/100 * corpus_sents_trans_df.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVvpkoSsGIJQ"},"outputs":[],"source":["corpus_sents_trans_df['nlptown'][:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-ETCrbvFeXZ"},"outputs":[],"source":["corpus_sents_trans_df['nlptown_roll10'] = corpus_sents_trans_df['nlptown'].rolling(10*win_s1per, center=True).mean()\n","\n","corpus_sents_trans_df['robertaxml8lang_roll10'] = corpus_sents_trans_df['robertaxml8lang'].rolling(10*win_s1per, center=True).mean()\n","\n","corpus_sents_trans_df['camembert_roll10'] = corpus_sents_trans_df['camembert'].rolling(10*win_s1per, center=True).mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PXkf8lzsF4xX"},"outputs":[],"source":["corpus_sents_trans_df['nlptown_roll10'].plot()\n","plt.title(f'{CORPUS_FULL}\\nMultilingual NLPTown SMA 10%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSI5F353Gq37"},"outputs":[],"source":["corpus_sents_trans_df['robertaxml8lang_roll10'].plot()\n","plt.title(f'{CORPUS_FULL}\\nMultilingual RoBERTa XML 8 Languages SMA 10%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CyCZjbEGqsW"},"outputs":[],"source":["corpus_sents_trans_df['camembert_roll10'].plot()\n","plt.title(f'{CORPUS_FULL}\\nFrench CamemBERT Transformer SMA 10%')"]},{"cell_type":"markdown","metadata":{"id":"UGAe-sduTDF0"},"source":["## **(0.0-1.0) FRENCH CamemBERT (tuned on movie reviews)**\n","\n","* https://huggingface.co/mrm8488/camembert-base-finetuned-movie-review-sentiment-analysis\n","\n","\n","Tf/Keras - Not Working (_C not installed)\n","* https://github.com/TheophileBlard/french-sentiment-analysis-with-bert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IaWhb6DhYTkj"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/camembert-base-finetuned-movie-review-sentiment-analysis\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/camembert-base-finetuned-movie-review-sentiment-analysis\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3y3BykQuYWjB"},"outputs":[],"source":["sentiment_analysis = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n","\n","print(sentiment_analysis(\"Je suis plutôt confiant.\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ooAvkb4YWfW"},"outputs":[],"source":["# Test Directly\n","test_result_pos = sentiment_analysis(\"Je déteste le livre.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"Bien.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"Mal.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"J'aime et j'apprécie les journées ensoleillées avec des enfants rieurs qui jouent joyeusement pendant l'été insouciant.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"Il est.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = sentiment_analysis(\"Cette phrase est vide.\")\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = sentiment_analysis(\"Je déteste et méprise le mal affreux qui infecte notre organisation.\")\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pcAQholeYrWM"},"outputs":[],"source":["# Test \n","\n","test_str = \"Je déteste et méprise le mal affreux qui infecte notre organisation.\"\n","# test_str = \"J'aime et j'apprécie les journées ensoleillées avec des enfants rieurs qui jouent joyeusement pendant l'été insouciant.\"\n","\n","twoway_probability2sentiment(test_str, sentiment_analysis, pol_labels=['LABEL_0','LABEL_1'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbaQeM8OYrSr"},"outputs":[],"source":["# NOTE: ~5-7 minutes runtime\n","\n","corpus_sents_trans_df['camembert'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: twoway_probability2sentiment(x[:510], sentiment_analysis, pol_labels=['LABEL_0','LABEL_1']))\n","corpus_sents_trans_df.head()\n"]},{"cell_type":"markdown","metadata":{"id":"_mfeuiNwSGUY"},"source":["## **(0.0-1.0) (POOR) FRENCH FlauBERT**\n","\n","* https://huggingface.co/models?search=sentiment\n","* https://huggingface.co/DemangeJeremy/4-sentiments-with-flaubert?text=Je+t%27appr%C3%A9cie+beaucoup.+Je+t%27aime. \n","\n","Returns [0.0 to 1.0]:\n","* MIXED\n","* NEGATIVE\n","* OBJECTIVE\n","* POSITIVE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOBM3wJHSGCj"},"outputs":[],"source":["# !pip install Cython"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIs3t6VQS_lf"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import pipeline\n","\n","loaded_tokenizer = AutoTokenizer.from_pretrained('flaubert/flaubert_large_cased')\n","loaded_model = AutoModelForSequenceClassification.from_pretrained(\"DemangeJeremy/4-sentiments-with-flaubert\")\n","\n","sentiment_analysis = pipeline('sentiment-analysis', model=loaded_model, tokenizer=loaded_tokenizer)\n","\n","print(sentiment_analysis(\"Je suis plutôt confiant.\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WjdHgsuEUf6h"},"outputs":[],"source":["sentiment_analysis = pipeline('sentiment-analysis', model=loaded_model, tokenizer=loaded_tokenizer)\n","\n","print(sentiment_analysis(\"Je suis plutôt confiant.\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CX94DgOyUOkt"},"outputs":[],"source":["# Test Directly\n","test_result_pos = sentiment_analysis(\"Je déteste le livre.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"Bien.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"Mal.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"J'aime et j'apprécie les journées ensoleillées avec des enfants rieurs qui jouent joyeusement pendant l'été insouciant.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_pos = sentiment_analysis(\"Il est.\")\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = sentiment_analysis(\"Cette phrase est vide.\")\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = sentiment_analysis(\"Je déteste et méprise le mal affreux qui infecte notre organisation.\")\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPK0vbUMUOkx"},"outputs":[],"source":["# Test Indirectly\n","\n","sentence_str = 'I love wonderful good things'\n","polarity = threeway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nPositive Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'It is what it is'\n","polarity = threeway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNeutral Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity = threeway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNegative Sentence Polarity: {polarity}\\n    Text: {sentence_str}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYwmD18iUOk2"},"outputs":[],"source":["# NOTE: started 9:05\n","\n","corpus_sents_trans_df['robertaxml8lang'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: threeway_probability2sentiment(x[:510], sentiment_analysis))\n","corpus_sents_trans_df.head()"]},{"cell_type":"markdown","metadata":{"id":"WBQ_RprIG7V0"},"source":["# **Save Results to File**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2IdIDpEc-MXX"},"outputs":[],"source":["corpus_sents_trans_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6dLZGXcOGF7"},"outputs":[],"source":["corpus_sents_trans_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63Qgj12kmOn-"},"outputs":[],"source":["title_str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlGeRUMC_zL0"},"outputs":[],"source":["# Save Preprocessed Corpus Sentences DataFrame\n","\n","# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Sentences\n","corpus_sents_filename = f'sum_sentiments_sents_trans_{author_abbr_str}_{title_str}.csv'\n","print(f'Saving to file: {corpus_sents_filename}')\n","\n","corpus_sents_trans_df.to_csv(corpus_sents_filename)\n","\n","# Paragraphs\n","# corpus_parags_filename = f'corpus_parags_clean_{author_str}_{title_str}_{datetime_now}.csv'\n","# print(f'Saving to file: {corpus_parags_filename}')\n","\n","# corpus_parags_df.to_csv(corpus_parags_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kjh0P5OWzOlY"},"outputs":[],"source":["# Save Preprocessed Corpus Sentences DataFrame\n","\n","# author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Sentences\n","corpus_sents_filename = f'sum_sentiments_sents_trans_{author_abbr_str}_{title_str}.csv'\n","print(f'Saving to file: {corpus_sents_filename}')\n","\n","corpus_sents_trans_df.to_csv(corpus_sents_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3OD0madIGme"},"outputs":[],"source":["# Verify\n","\n","# !head -n 5 sum_sentiments_sents_trans_hsbutler_theodyssey.csv"]},{"cell_type":"markdown","metadata":{"id":"wYjstRyu2t-9"},"source":["# **END OF WORKING TRANSFORMERS**"]},{"cell_type":"markdown","metadata":{"id":"neXSDXd1L5Gd"},"source":["## **(5-way: slow) T5 Small IMDB**\n","\n","* https://huggingface.co/mrm8488/t5-small-finetuned-imdb-sentiment\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vb7bOcVmL4Aa"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-small-finetuned-imdb-sentiment\")\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-small-finetuned-imdb-sentiment\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdyYdynfL4w_"},"outputs":[],"source":["def t5smimdb_sentiment(text):\n","  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n","\n","  output = model.generate(input_ids=input_ids,\n","               max_length=2)\n","\n","  dec = [tokenizer.decode(ids) for ids in output]\n","  label = dec[0]\n","  return label\n","\n","get_sentiment(\"I dislike a lot that film\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZvlxyt5Mf6q"},"outputs":[],"source":["# Test Directly\n","test_result_pos = t5smimdb_sentiment('I absolutely love this great and wonderful opportunity to enjoy lovely and good things')\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = t5smimdb_sentiment('blank')\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = t5smimdb_sentiment('I hate bad evil dislike')\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OsCKPFfJMf6r"},"outputs":[],"source":["# Test Indirectly\n","\n","sentence_str = 'I love wonderful good things'\n","polarity = twoway_probability2sentiment(sentence_str, classifier)\n","print(f'\\nPositive Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'is'\n","polarity = twoway_probability2sentiment(sentence_str, classifier)\n","print(f'\\nNeutral Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity = twoway_probability2sentiment(sentence_str, classifier)\n","print(f'\\nNegative Sentence Polarity: {polarity}\\n    Text: {sentence_str}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L9CpjCJhMf6s"},"outputs":[],"source":["# NOTE: started 8:42\n","\n","corpus_sents_trans_df['huggingface'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: twoway_probability2sentiment(x[:510], classifier))\n","corpus_sents_trans_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZS-V6lK_MGI9"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"14X-LOG3AMcM"},"source":["## **(5-way: slow) T5Base Finetuned Span Sentiment Extraction**\n","\n","* https://huggingface.co/mrm8488/t5-base-finetuned-span-sentiment-extraction\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iD6GyhmfAIfs"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-span-sentiment-extraction\")\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-span-sentiment-extraction\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_13a-R7E2xR"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGbkBZCWE2td"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"AkRjCJFHuWMu"},"source":["## **SpaCy BERT**\n","\n","* https://explosion.ai/blog/spacy-transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CtdrxXr9ulwU"},"outputs":[],"source":["!pip install spacy-transformers\n","!python -m spacy download en_core_web_trf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1JnZEelcuqu4"},"outputs":[],"source":["import spacy\n","import torch\n","import numpy\n","from numpy.testing import assert_almost_equal\n","\n","is_using_gpu = spacy.prefer_gpu()\n","if is_using_gpu:\n","    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n","\n","nlp = spacy.load(\"en_core_web_trf\")\n","doc = nlp(\"Here is some text to encode.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Me9N6s1Fve1f"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wJRLKgxvely"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"QINAbw-_rQAf"},"source":["## **BERT SST**\n","\n","* https://huggingface.co/barissayil/bert-sentiment-analysis-sst\n","* NOTE: Should be fine-tuned, not the same as Huggingface default"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2v-yvPkr9Fr"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"barissayil/bert-sentiment-analysis-sst\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"barissayil/bert-sentiment-analysis-sst\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBoX4f9RsPKQ"},"outputs":[],"source":["sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"barissayil/bert-sentiment-analysis-sst\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ran6_lZbsC-u"},"outputs":[],"source":["# Test Directly\n","test_result_pos = sentiment_analysis('I love wonderful good things')\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neg = sentiment_analysis('I hate your guts you filthy bastard')\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBs0HKmcsC-w"},"outputs":[],"source":["# Test Indirectly\n","\n","fivestar_probability2sentiment('I love wonderful good things', nlptown_sentiment_analysis)\n","print('\\n')\n","fivestar_probability2sentiment('I hate your stinking guts you filthy lying stealing cheating bastard.', nlptown_sentiment_analysis)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTpjYyomsC-y"},"outputs":[],"source":["# NOTE: started 4:13\n","\n","corpus_sents_trans_df['nlptown'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: fivestar_probability2sentiment(x[:510], sentiment_analysis))\n","corpus_sents_trans_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjJcFfY4r89r"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBmAiIdIrQAi"},"outputs":[],"source":["corpus_sents_trans_df.head()"]},{"cell_type":"markdown","metadata":{"id":"ICiq1NpK7S_R"},"source":["## **(3-way) BERTweet Base Bilingual Pysentimiento**\n","\n","* https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis\n","\n","* https://github.com/pysentimiento/pysentimiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdmxZWmR8DBV"},"outputs":[],"source":["!pip install pysentimiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a43FYVFVCYKa"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8rx1bfDCPz0"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qze7DLeKrVNz"},"outputs":[],"source":["from pysentimiento import SentimentAnalyzer\n","# from pysentimiento import EmotionAnalyzer\n","\n","# Bilingual es/en for both sentiment/emotions\n","analyzer = SentimentAnalyzer(lang=\"en\")\n","# emotion_analyzer = EmotionAnalyzer(lang=\"en\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TK9KvrpAj85l"},"outputs":[],"source":["def get_pysentimiento_sentiment(text):\n","  '''\n","  Given a plain text string\n","  Return a three-way sentiment integer [-1.5 to +1.5] (negative,positive) sentiment value\n","      with each neg,neu,pos equally distributed over the same range (-1.5 to -0.5, -0.5 to 0.5, 0.5 to 1.5)\n","  '''\n","\n","    # from pysentimiento import SentimentAnalyzer\n","    # analyzer = SentimentAnalyzer(lang=\"en\")\n","    # print('Using Pysentimiento')\n","    text_str_ls = text_str.split()[:125]\n","    text_125_str = ' '.join(text_str_ls)\n","    pol_object = analyzer.predict(text_125_str)\n","    pol_str = pol_object.output\n","    if pol_str == 'NEG':\n","      sign_fl = -1.0\n","      score_base = -0.5\n","    elif pol_str == 'NEU':\n","      sign_fl = 1.0\n","      socre_base = 0\n","    else:\n","      # Polarity is 'POS' by default\n","      sign_fl = 1.0\n","      score_base = 0.5\n","\n","    score_fl = (sign_fl * pol_object.probas[pol_str]) + score_base\n","\n","\n","  return score_fl\n","\n","\n","\n","  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n","\n","  output = model.generate(input_ids=input_ids,\n","               max_length=2)\n","\n","  dec = [tokenizer.decode(ids) for ids in output]\n","  label = dec[0]\n","\n","  if 'positive' in label:\n","    score_int = 1\n","  elif 'negative' in label:\n","    score_int = -1\n","  else:\n","    score_int = 0\n","\n","  return score_int\n","\n","\n","get_sentiment(\"I dislike a lot that film\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6kvmMw1FiY8"},"outputs":[],"source":["# Test Directly\n","test_result_pos = analyzer.predict('I absolutely love this great and wonderful opportunity to enjoy lovely and good things')\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = analyzer.predict('blank')\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = analyzer.predict('I hate bad evil dislike')\n","test_result_neg # [0]['label']\n","print('\\n')\n","\n","test_result_neg.output\n","print('\\n')\n","type(test_result_neg.probas)\n","test_result_neg.probas['NEG']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVT70Olt8KpX"},"outputs":[],"source":["# Test Indirectly\n","\n","sentence_str = 'I love wonderful good things'\n","polarity = threeway_probability2sentiment(sentence_str, analyzer.predict)\n","print(f'\\nPositive Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'This'\n","polarity = threeway_probability2sentiment(sentence_str, analyzer.predict)\n","print(f'\\nNeutral Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity = threeway_probability2sentiment(sentence_str, analyzer.predict)\n","print(f'\\nNegative Sentence Polarity: {polarity}\\n    Text: {sentence_str}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aNnoGfmCFiZB"},"outputs":[],"source":["# Test Indirectly\n","\n","sentence_str = 'I love wonderful good things'\n","polarity = threeway_probability2sentiment(sentence_str, analyzer.predict)\n","print(f'\\nPositive Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'It is what it is'\n","polarity = threeway_probability2sentiment(sentence_str, analyzer.predict)\n","print(f'\\nNeutral Sentence Polarity: {polarity}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity = threeway_probability2sentiment(sentence_str, analyzer.predict)\n","print(f'\\nNegative Sentence Polarity: {polarity}\\n    Text: {sentence_str}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3XjM5Ew1FiZF"},"outputs":[],"source":["# NOTE: started 5:42\n","\n","corpus_sents_trans_df['pysentimiento'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: threeway_probability2sentiment((' '.join(x.split()).split()[:124], analyzer.predict))\n","corpus_sents_trans_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUPTLGYvRrzO"},"outputs":[],"source":["corpus_sents_trans_df.head()"]},{"cell_type":"markdown","metadata":{"id":"0eEqeDMnoenp"},"source":["## **(3-way) FinEstEn 3Multilingual**\n","\n","* https://huggingface.co/EMBEDDIA/finest-bert\n","\n","* https://arxiv.org/pdf/2006.07890.pdf\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqSQO33job6J"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForMaskedLM\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"EMBEDDIA/finest-bert\")\n","\n","model = AutoModelForMaskedLM.from_pretrained(\"EMBEDDIA/finest-bert\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKYSvqnvob06"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"agjB8LPHobva"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"8np-yWt-5kgT"},"source":["## **(5-way) (No-Tigrinya) RoBERTa Base Sentiment**\n","\n","* https://huggingface.co/fgaim/roberta-base-ti-sentiment (no metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZBlyeoH5hdi"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","  \n","tokenizer = AutoTokenizer.from_pretrained(\"fgaim/roberta-base-ti-sentiment\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"fgaim/roberta-base-ti-sentiment\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OwycfKv_559i"},"outputs":[],"source":["sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"fgaim/roberta-base-ti-sentiment\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJgD5-v5559k"},"outputs":[],"source":["# Test Directly\n","test_result_pos = sentiment_analysis('I absolutely love this great and wonderful opportunity to enjoy lovely and good things')\n","test_result_pos # [0]['label']\n","print('\\n')\n","test_result_neu = sentiment_analysis('blank')\n","test_result_neu # [0]['label']\n","print('\\n')\n","test_result_neg = sentiment_analysis('I hate bad evil dislike')\n","test_result_neg # [0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJtTZdUp559n"},"outputs":[],"source":["# Test Indirectly\n","\n","\"\"\"\n","# Testing\n","sentence_str = 'I love wonderful good things'\n","polarity, base_adj = fiveway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nPositive Sentence Polarity: {polarity} and Adjustment: {base_adj}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'This'\n","polarity, base_adj = fiveway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNeutral Sentence Polarity: {polarity} and Adjustment: {base_adj}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity, base_adj = fiveway_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNegative Sentence Polarity: {polarity} and Adjustment: {base_adj}\\n    Text: {sentence_str}')\n","\"\"\";\n","\n","sentence_str = 'I love wonderful good things'\n","polarity = threepolarity_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nPositive Sentence Polarity: {polarity} and Adjustment: {base_adj}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'This'\n","polarity = threepolarity_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNeutral Sentence Polarity: {polarity} and Adjustment: {base_adj}\\n    Text: {sentence_str}')\n","\n","sentence_str = 'I hate your stinking guts you filthy lying stealing cheating bastard.'\n","polarity = threepolarity_probability2sentiment(sentence_str, sentiment_analysis)\n","print(f'\\nNegative Sentence Polarity: {polarity} and Adjustment: {base_adj}\\n    Text: {sentence_str}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5ROns07559p"},"outputs":[],"source":["# NOTE: started 5:10\n","\n","corpus_sents_trans_df['hinglish'] = corpus_sents_trans_df['sent_raw'].apply(lambda x: threepolarity_probability2sentiment(x, sentiment_analysis))\n","corpus_sents_trans_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XnMtqS6E5hVJ"},"outputs":[],"source":["%whos DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6e2nqc3PN4HF"},"outputs":[],"source":["corpus_sents_trans_df.shape"]},{"cell_type":"markdown","metadata":{"id":"8owpM75RILKn"},"source":["# **Utility Functions (Auto)**"]},{"cell_type":"markdown","metadata":{"id":"aMLbyx6gIPqj"},"source":["## **Files**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MaRD7RL9IOeg"},"outputs":[],"source":["# Generate full path and timestamp for new filepath/filename\n","\n","def gen_pathfiletime(file_str, subdir_str=''):\n","\n","  # Geenreate compressed author and title substrings\n","  author_raw_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","  title_raw_str = ''.join(CORPUS_TITLE.split()).lower()\n","\n","  # Generate current/unique datetime string\n","  datetime_str = str(datetime.now().strftime('%Y%m%d%H%M%S'))\n","\n","  # Built fullpath+filename string\n","  file_base, file_ext = file_str.split('.')\n","\n","  author_str = re.sub('[^A-Za-z0-9]+', '', author_raw_str)\n","  title_str = re.sub('[^A-Za-z0-9]+', '', title_raw_str)\n","\n","  full_filepath_str = f'{subdir_str}{file_base}_{author_str}_{title_str}_{datetime_str}.{file_ext}'\n","\n","  # print(f'Returning from gen_savepath() with full_filepath={full_filepath}')\n","\n","  return full_filepath_str\n","\n","# Test\n","# pathfilename_str = gen_pathfiletime('hist_paraglen.png')\n","# print(pathfilename_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VaBHFZBtKut8"},"outputs":[],"source":["# Tokenize into Sentences\n","\n","def parag2sents(corpus_parags_ls):\n","  '''\n","  Given a list of paragraphs,\n","  Return a list of lists of Sentences [sent_no, parag_no, asent(text)]\n","  '''\n","\n","  sent_no = 0\n","  # sent_base = 0\n","  corpus_sents_row_ls = []\n","  for parag_no,aparag in enumerate(corpus_parags_ls):\n","    sents_ls = sent_tokenize(aparag)\n","    # Delete (whitespace only) sentences\n","    sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n","    # Delete (punctuation only) sentences\n","    sents_ls = [x for x in sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_SENT_LEN]\n","    # Delete numbers (int or float) sentences\n","    # TODO: may want to keep\n","    for s,asent in enumerate(sents_ls):\n","      corpus_sents_row_ls.append([sent_no, parag_no, asent])\n","      sent_no += 1\n","\n","    # print(f'Returning with corpus_sents_row_ls length = {len(corpus_sents_row_ls)}')\n","  \n","  return corpus_sents_row_ls\n","\n","# Test\n","\n","'''\n","print(f'Length {len(corpus_parags_raw_ls)}')\n","corpus_sents_row_ls = parag2sents(corpus_parags_raw_ls)\n","\n","print(f'First row {corpus_sents_row_ls[0]}')\n","print('\\n')\n","print(f'Last row {corpus_sents_row_ls[-1]}')\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9FDtpQ6QaEJ"},"outputs":[],"source":["#This function converts to lower-case, removes square bracket, removes numbers and punctuation\n"," \n","def text_clean(text):\n","    text = text.lower()\n","    text = re.sub('\\[.*?\\]', ' ', text)\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    text = re.sub('[\\n]', ' ', text)  # Replace newline with space\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xf5a5ClONwiA"},"outputs":[],"source":["# Read corpus into a single string then split into paragraphs\n","\n","'''\n","if len(corpus_filename) == 0:\n","  # If now file uploaded, use the file in Google gDrive\n","  corpus_filename = CORPUS_FILENAME\n","else:\n","  # The uploaded file has priority over the gDrive Corpus file\n","  pass\n","'''\n","\n","def read_corpus_parags(corpus_filename):\n","  '''\n","  Given a corpus_filename (assuming already %cd into correct subdir)\n","  Return a list of min preprocessed raw paragraphs (corpus_parags_raw_temp_ls)\n","  '''\n","\n","  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n","    corpus_raw_str = infp.read()\n","\n","  corpus_parags_raw_temp_ls = corpus_raw_str.split('\\n\\n')\n","  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_raw_temp_ls)}')\n","\n","  # Strip excess whitespace and drop empty lines\n","  corpus_parags_raw_temp_ls = [x.strip() for x in corpus_parags_raw_temp_ls if len(x.strip()) > MIN_PARAG_LEN]\n","  print(f'Corpus Paragraph -(whitespace only) Count: {len(corpus_parags_raw_temp_ls)}')\n","\n","  # Drop lines that only contain punctuation (e.g. '\"', '.', '...', etc)\n","  corpus_parags_raw_temp_ls = [x for x in corpus_parags_raw_temp_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n","  print(f'Corpus Paragraph -(punctuation only) Count: {len(corpus_parags_raw_temp_ls)}')\n","\n","  return corpus_parags_raw_temp_ls\n","\n","# Test  \n","'''\n","corpus_parags_raw_ls = read_corpus_parags(CORPUS_FILENAME)\n","print(f'We found #{len(corpus_parags_raw_ls)} lines\\n')\n","\n","print('\\nThe first 10 lines of the Corpus:')\n","print('-----------------------------------\\n')\n","corpus_parags_raw_ls[:10]\n","\n","print('\\nThe last 10 lines of the Corpus:')\n","print('-----------------------------------\\n')\n","corpus_parags_raw_ls[-10:]\n","print('\\n')\n","print(sorted(corpus_parags_raw_ls, key=lambda x: (len(x), x)))\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afWoWCZ7R_U9"},"outputs":[],"source":["# Verify saved under newest filename\n","\n","def get_recentfile(file_type='csv'):\n","  '''\n","  Given a file extension type,\n","  Return the most recently created file of that type \n","  in the current directory\n","  '''\n","  file_pattern = \"./*.\" + file_type\n","  print(f'file_pattern: {file_pattern}')\n","  list_of_files = glob.glob(file_pattern) # * means all if need specific format then *.csv\n","  latest_file = max(list_of_files, key=os.path.getmtime)\n","\n","  return latest_file\n","\n","# Test\n","\n","# get_recentfile('txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tC6Cp8gLosRF"},"outputs":[],"source":["!pip install chardet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUFV0iy80-uU"},"outputs":[],"source":["import chardet\n","name = b\"\\x4a\\x6f\\x73\\xe9\"\n","detection = chardet.detect(name)\n","print(detection)\n","encoding = detection[\"encoding\"]\n","print(name.decode(encoding))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTiQPOc61kE6"},"outputs":[],"source":["!pip install cchardet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiB1fC7F1kAu"},"outputs":[],"source":["# https://dev.to/bowmanjd/character-encodings-and-detection-with-python-chardet-and-cchardet-4hj7\n","\n","import cchardet as chardet\n","\n","from pathlib import Path\n","import sys\n","\n","def get_file_encoding(filename):\n","    \"\"\"Detect encoding and return decoded text, encoding, and confidence level.\"\"\"\n","    filepath = Path(filename)\n","\n","    # We must read as binary (bytes) because we don't yet know encoding\n","    blob = filepath.read_bytes()\n","\n","    detection = chardet.detect(blob)\n","    encoding = detection[\"encoding\"]\n","    confidence = detection[\"confidence\"]\n","    text = blob.decode(encoding)\n","\n","    return text, encoding, confidence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaNNcdnt14-0"},"outputs":[],"source":["!ls -altr *.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HWf02Tw3G6t"},"outputs":[],"source":["CORPUS_FILENAME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKfIQRxB1438"},"outputs":[],"source":["# Try to discover Corpus text Encoding scheme (default to 'utf-8', but often 'iso-8859-1', 'windows-1252', 'cp1252', or 'ascii')\n","CORPUS_ENCODING = 'utf-8'\n","corpus_str, CORPUS_ENCODING, encoding_confidence = get_file_encoding(CORPUS_FILENAME)\n","\n","# print(text)\n","# print(f\"Encoding was detected as {CORPUS_ENCODING}.\")\n","# print(f'             Confidence: {encoding_confidence}')\n","\n","if encoding_confidence > 0.6:\n","  print(f\"{encoding_confidence*100:.2f}% confidence Encoding = '{CORPUS_ENCODING}' for '{CORPUS_FILENAME}'\")\n","else:\n","  print(f\"ERROR: Less than 60% confidence estimating Encoding scheme for '{CORPUS_FILENAME}'\")\n","  print(f\"       Only {encoding_confidence*100:.2f}% confidence Encoding = '{CORPUS_ENCODING}'\")\n","  print(f\"       Manually verify corpus file '{CORPUS_FILENAME}' encoding, set as GLOBAL_CONSTATANT and rerun\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"co93ggd_1-V5"},"outputs":[],"source":["CORPUS_ENCODING"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uXhd9liD0CmT"},"outputs":[],"source":["print(\"\\x73\\x70\\x61\\x6d\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tvbya1m0CiY"},"outputs":[],"source":["b\"\\x73\\x70\\x61\\x6d\".decode(\"ascii\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gD4Q3Rh0K-e"},"outputs":[],"source":["b\"\\x4a\\x6f\\x73\\xe9\".decode(\"iso-8859-1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Lm1vGz40K4M"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"JrxFyQaPzjBf"},"source":["**Start of New Files Section**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2ZKrOg_8Pcz"},"outputs":[],"source":["def corpus2chaps(corpus_filename):\n","  '''\n","  Given a corpus_filename (assuming already %cd into correct subdir)\n","  Return a list of min preprocessed raw CHAPTERs (corpus_parags_raw_temp_ls)\n","  '''\n","\n","  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n","    corpus_raw_str = infp.read()\n","\n","  # Filter out SECTION [\\d]{1,2}[^\\n]* patterns from raw text corpus\n","  pattern = r'SECTION [\\d]{1,2}[^\\n]*'\n","  # Replace all occurrences of character s with an empty string\n","  corpus_raw_str = re.sub(pattern, '', corpus_raw_str)\n","\n","  # print(f'len(corpus_raw_str) = {len(corpus_raw_str)}')\n","  corpus_chaps_ls = re.split(r'(CHAPTER [\\d]{1,2}[^\\n]*)', corpus_raw_str, flags=re.I) # , flags=re.I)\n","    \n","  # Strip off whitespace\n","  corpus_chaps_ls = [x.strip() for x in corpus_chaps_ls]\n","\n","  # Filter out empty lines\n","  corpus_chaps_ls = [x for x in corpus_chaps_ls if not (len(x.strip()) <= MIN_PARAG_LEN)]\n","\n","  # Filter out CHAPTER lines\n","  corpus_chaps_ls = [x for x in corpus_chaps_ls if not (x.strip().startswith('CHAPTER '))]\n","\n","  return corpus_chaps_ls, corpus_raw_str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5nrHLut8AEl"},"outputs":[],"source":["def corpus2sects(corpus_filename):\n","  '''\n","  Given a corpus_filename (assuming already %cd into correct subdir)\n","  Return a list of min preprocessed raw sections/CHAPTERs (corpus_parags_raw_temp_ls)\n","  '''\n","\n","  with open(corpus_filename, \"r\", encoding='cp1252', errors='ignore') as infp: # encoding='utf-8', errors='ignore') as infp: # encoding=CORPUS_ENCODING) as infp:\n","    corpus_raw_str = infp.read()\n","\n","  corpus_sects_ls = re.split(r'(CHAPTER [\\d]{1,2}[^\\n]*|SECTION [\\d]{1,2}[^\\n]*|-----)', corpus_raw_str)\n","\n","  # Strip off whitespace\n","  corpus_sects_ls = [x.strip() for x in corpus_sects_ls]\n","\n","  # Filter out empty lines\n","  corpus_sects_ls = [x for x in corpus_sects_ls if not (len(x.strip()) <= MIN_PARAG_LEN)]\n","\n","  # Filter out the Section separator '-----' lines\n","  corpus_sects_ls = [x for x in corpus_sects_ls if not (x.strip().startswith('----- '))]\n","\n","  # Filter out the Section separator 'SECTION ' lines\n","  corpus_sects_ls = [x for x in corpus_sects_ls if not (x.startswith('SECTION '))]\n","\n","  # Filter out the Chapter separator 'CHAPTER ' lines\n","  # Keep for now, messy but enables proper SECTION assignments to appropraite CHAPTERs\n","  corpus_sects_ls = [x for x in corpus_sects_ls if not (x.startswith('CHAPTER '))]\n","\n","  return corpus_sects_ls, corpus_raw_str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_wm00VG-SEw"},"outputs":[],"source":["CORPUS_FILENAME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GzK5s6W-iJu"},"outputs":[],"source":["!ls -altr *hand_clean.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_0uSZS09yRe"},"outputs":[],"source":["CORPUS_ENCODING.lower()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51uchS9F9egl"},"outputs":[],"source":["corpus_sects_ls, corpus_str_raw = corpus2sects(CORPUS_FILENAME)\n","print(f'Length corpus_str_raw: {len(corpus_sects_ls)}')\n","print(f'Length corpus_sects_ls: {len(corpus_sects_ls)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8z8ZY5n8BRs"},"outputs":[],"source":["def corpus2parags(corpus_filename):\n","  '''\n","  Given a corpus_filename (assuming already %cd into correct subdir)\n","  Return a list of min preprocessed raw paragraphs (corpus_parags_ls)\n","  '''\n","\n","  with open(corpus_filename, \"r\", encoding=CORPUS_ENCODING) as infp:\n","    corpus_raw_str = infp.read()\n","\n","  corpus_parags_ls = re.split(r'[\\n]{2,}', corpus_raw_str)\n","  print(f'Corpus Paragraph Raw Count: {len(corpus_parags_ls)}')\n","\n","  # Strip off whitespace\n","  corpus_parags_ls = [x.strip() for x in corpus_parags_ls]\n","\n","  # Filter out empty lines\n","  corpus_parags_ls = [x for x in corpus_parags_ls if (len(x.strip()) >= MIN_PARAG_LEN)]\n","\n","  # Filter out lines containing only punctuation (e.g. '\"', '.', '...', etc)\n","  parag_before_punctstrip_ct = len(corpus_parags_ls)\n","  corpus_parags_ls = [x for x in corpus_parags_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_PARAG_LEN]\n","  print(f'Punctuation only Paragraph Count: {len(corpus_parags_ls) - parag_before_punctstrip_ct}')\n","\n","  # Filter out the Section separator '-----' lines\n","  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.strip().startswith('----- '))]\n","\n","  # Filter out the Section separator 'SECTION ' lines\n","  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('SECTION '))]\n","\n","  # Filter out the Chapter separator 'CHAPTER ' lines\n","  corpus_parags_ls = [x for x in corpus_parags_ls if not (x.startswith('CHAPTER '))]\n","\n","  return corpus_parags_ls, corpus_raw_str\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RX1gjU7S8esY"},"outputs":[],"source":["def parag2sents(corpus_parags_ls):\n","  '''\n","  Given a list of paragraphs,\n","  Return a list of lists of Sentences [sent_no, parag_no, asent(text)]\n","  '''\n","\n","  sent_no = 0\n","  # sent_base = 0\n","  corpus_sents_ls = []\n","  for parag_no,aparag in enumerate(corpus_parags_ls):\n","    sents_ls = sent_tokenize(aparag)\n","    # Delete (whitespace only) sentences\n","    sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n","    # Delete (punctuation only) sentences\n","    sents_ls = [x for x in sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_SENT_LEN]\n","    # Delete numbers (int or float) sentences\n","    sents_ls = [x for x in sents_ls if not (x.strip().isnumeric())]\n","    # TODO: may want to keep\n","    for s,asent in enumerate(sents_ls):\n","      corpus_sents_ls.append([sent_no, parag_no, asent])\n","      sent_no += 1\n","\n","    # print(f'Returning with corpus_sents_ls length = {len(corpus_sents_ls)}')\n","  \n","  return corpus_sents_ls\n"]},{"cell_type":"markdown","metadata":{"id":"nJ16-mUnzeB4"},"source":["**End of Files Section**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQXX_b2eozGE"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLASKwzZF-Lh"},"outputs":[],"source":["def get_sentiments(model_base, sentiment_fn, sentiment_type='lexicon'):\n","  '''\n","  Given a model_base name and sentiment evaluation function\n","  Calculate all the Sentence, Paragraph, Section and Chapter Sentiment Scores and Standardized variants\n","  '''\n","\n","  # Calculate Sentiment Polarities\n","\n","  if sentiment_type == 'lexicon':\n","    print(f'Processing Lexicon/Sentences...')\n","    corpus_sents_df[model_base] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n","    print(f'Processing Lexicon/Paragraphs...')\n","    corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n","    print(f'Processing Lexicon/Sections...')\n","    corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n","    print(f'Processing Lexicon/Chapters...')\n","    corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n","  \n","  elif sentiment_type == 'compound':\n","    # VADER\n","\n","    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean\n","    print(f'Processing Lexicon/Sentences...')\n","    corpus_sents_df['scores'] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n","    print(f'Processing Lexicon/Paragraphs...')\n","    corpus_parags_df['scores'] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n","    print(f'Processing Lexicon/Sections...')\n","    corpus_sects_df['scores'] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n","    print(f'Processing Lexicon/Chapters...')\n","    corpus_chaps_df['scores'] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n","\n","    # Extract Compound Sentiment\n","    corpus_sents_df[model_base]  = corpus_sents_df['scores'].apply(lambda score_dict: score_dict['compound'])\n","    corpus_parags_df[model_base]  = corpus_parags_df['scores'].apply(lambda score_dict: score_dict['compound'])\n","    corpus_sects_df[model_base]  = corpus_sects_df['scores'].apply(lambda score_dict: score_dict['compound'])\n","    corpus_chaps_df[model_base]  = corpus_chaps_df['scores'].apply(lambda score_dict: score_dict['compound'])\n","\n","  elif sentiment_type == 'function':\n","    # TextBlob\n","\n","    # Calculate dictionary of {neg/neu/pos/compound} values for sent_clean\n","    print(f'Processing Lexicon/Sentences...')\n","    corpus_sents_df[model_base] = corpus_sents_df['sent_clean'].apply(lambda text: sentiment_fn(str(text)))\n","    print(f'Processing Lexicon/Paragraphs...')\n","    corpus_parags_df[model_base] = corpus_parags_df['parag_clean'].apply(lambda text: sentiment_fn(str(text)))\n","    print(f'Processing Lexicon/Sections...')\n","    corpus_sects_df[model_base] = corpus_sects_df['sect_clean'].apply(lambda text: sentiment_fn(str(text)))\n","    print(f'Processing Lexicon/Chapters...')\n","    corpus_chaps_df[model_base] = corpus_chaps_df['chap_clean'].apply(lambda text: sentiment_fn(str(text)))\n","\n","  else:\n","    print(f'ERROR: sentiment_type={sentiment_type} but must be one of (lexicon, compound, function)')\n","    return\n","\n","  # Create new column names\n","  col_meanstd = f'{model_base}_meanstd'\n","  col_medianiqr = f'{model_base}_medianiqr'\n","  col_lnorm_meanstd = f'{model_base}_lnorm_meanstd'\n","  col_lnorm_medianiqr = f'{model_base}_lnorm_medianiqr'\n","\n","\n","  # Get Chapter Standardization with MeanSTD and RobustStandardization with MedianIQRScaling\n","  corpus_chaps_df[col_meanstd]  = mean_std_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n","  corpus_chaps_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n","  # Normalize the Chapter Sentiment by dividing by Chapter Length\n","  chaps_len_ls = list(corpus_chaps_df['token_len'])\n","  chaps_sentiment_ls = list(corpus_chaps_df[model_base])\n","  chaps_sentiment_norm_ls = [chaps_sentiment_ls[i]/chaps_len_ls[i] for i in range(len(chaps_len_ls))]\n","  # RobustStandardize Chapter sentiment values\n","  corpus_chaps_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n","  corpus_chaps_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_chaps_df[model_base]).reshape(-1, 1))\n","  corpus_chaps_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(chaps_sentiment_norm_ls)).reshape(-1, 1))\n","\n","  # Get Section Standardization with MeanSTD and RobustStandardization with MedianIQRScaling\n","  corpus_sects_df[col_meanstd]  = mean_std_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n","  corpus_sects_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n","  # Normalize the Section Sentiment by dividing by Section Length\n","  sects_len_ls = list(corpus_sects_df['token_len'])\n","  sects_sentiment_ls = list(corpus_sects_df[model_base])\n","  sects_sentiment_norm_ls = [sects_sentiment_ls[i]/sects_len_ls[i] for i in range(len(sects_len_ls))]\n","  # RobustStandardize Section sentiment values\n","  corpus_sects_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n","  corpus_sects_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sects_df[model_base]).reshape(-1, 1))\n","  corpus_sects_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sects_sentiment_norm_ls)).reshape(-1, 1))\n","\n","\n","  # Normalize the Paragraph Sentiment by dividing by Chapter Length\n","  parags_len_ls = list(corpus_parags_df['token_len'])\n","  parags_sentiment_ls = list(corpus_parags_df[model_base])\n","  parags_sentiment_norm_ls = [parags_sentiment_ls[i]/parags_len_ls[i] for i in range(len(parags_len_ls))]\n","  # RobustStandardize Paragraph sentiment values\n","  corpus_parags_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n","  corpus_parags_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_parags_df[model_base]).reshape(-1, 1))\n","  corpus_parags_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(parags_sentiment_norm_ls)).reshape(-1, 1))\n","\n","  # Normalize the Sentence Sentiment by dividing by Chapter Length\n","  sents_len_ls = list(corpus_sents_df['token_len'])\n","  sents_sentiment_ls = list(corpus_sents_df[model_base])\n","  sents_sentiment_norm_ls = [sents_sentiment_ls[i]/sents_len_ls[i] for i in range(len(sents_len_ls))]\n","  # RobustStandardize Sentence sentiment values\n","  corpus_sents_df[col_lnorm_meanstd]  = mean_std_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n","  corpus_sents_df[col_medianiqr]  = median_iqr_scaler.fit_transform(np.array(corpus_sents_df[model_base]).reshape(-1, 1))\n","  corpus_sents_df[col_lnorm_medianiqr]  = median_iqr_scaler.fit_transform(np.array(pd.Series(sents_sentiment_norm_ls)).reshape(-1, 1))\n","\n","  return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nt8ewj2uozf3"},"outputs":[],"source":["# Read in lexicon at given path into Dict[word]=polarity\n","\n","def get_lexicon(lexicon_name, lexicon_format=2):\n","    \"\"\"\n","    Read sentiment lexicon.csv file at lexicon_path\n","    into appropriate Dict[word]=polarity\n","\n","    1. lexicon_dt[word] = <polarity value>\n","\n","    Args:\n","        sa_lib (str, optional): [description]. Defaults to 'syuzhet'.\n","    \"\"\"\n","    \n","    # global lexicon_df\n","\n","    lexicon_df = pd.DataFrame()\n","    \n","    # print(os.getcwd())\n","    \"\"\"\n","    lexicons_ls = os.listdir('../sa_lexicons/')\n","    if (lexicon_name in lexicons_ls):\n","      print(f'Found {lexicon_name} in lexicon_directory)')\n","    # print(glob.glob('*.csv'))\n","    cp_cmd = f'copy ../sa_lexicons/{lexicon_name} ./'\n","    print(f'cp_cmd = {cp_cmd}')\n","    os.system(cp_cmd)\n","    os.system('cp ../sa_lexicons/' + lexicon_name.strip() + ' ./')\n","    os.listdir('.')  \n","    \"\"\";\n","    \n","    try:\n","      lexicon_df = pd.read_csv(lexicon_name)\n","      lexicon_df.info()\n","      # lexicon_df = lexicon_tmp_df.copy()\n","      # print(lexicon_df.head())\n","      return lexicon_df\n","    except:\n","      print(f'ERROR: Cannot read lexicon.csv at {lexicon_name}')\n","      return -1\n","\n","'''\n","    print\n","    if (sa_lexicon == 'default'):\n","        lexicon_df = pd.read_csv(LEXICON_PATH)\n","        lexicon_df.columns = ['index_no', 'word', 'polarity']\n","        lexicon_df.drop(['index_no'], axis=1, inplace=True)\n","        lexicon_df.dropna(inplace=True)\n","        lexicon_dt = lexicon_df.set_index('word').T.to_dict('list')\n","        # unlist the polarity to type: float\n","        for key in lexicon_dt:\n","            lexicon_dt[key] = float(lexicon_dt[key][0])\n","        \n","    ### print(f\"Exit get_sa_lex() with {len(lexicon_dt.keys())} entries in syuzhet_dt\")\n","    return lexicon_dt\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATK_FSVGozf4"},"outputs":[],"source":["# Sentence to Sentiment Polarity according to passed in Lexicon Dictionary\n","\n","def text2sentiment(text_str, lexicon_dt):\n","  '''\n","  Given a text_str and lexicon_dt, calculate \n","  the sentimety polarity.\n","  '''\n","\n","  # Remove all not alphanumeric and whitespace characters\n","  text_str = re.sub(r'[^\\w\\s]', '', text_str) \n","\n","  text_str = text_str.strip().lower()\n","  if (len(text_str) < 1):\n","      print(f\"ERROR: text2sentiment() given empty/null/invalid string: {text_str}\")\n","\n","  text_ls = text_str.split()\n","  # print(f'text_ls: {text_ls}')\n","\n","  # Accumulated Total Sentiment Polarity for entire Sentence\n","  text_sa_tot = 0.0\n","\n","  for aword in text_ls:\n","      # print(f'getting sa for word: {aword}')\n","      try:\n","          word_sa_fl = float(lexicon_dt[aword])\n","          text_sa_tot += word_sa_fl\n","          # print(f\">>{aword} has a sentiment value of {word_sa_fl}\")\n","      except TypeError: # KeyError:\n","          # aword is not in lexicon so it adds 0 to the sentence sa sum\n","          # print(f\"TypeError: cannot convert {lexicon_dt[aword]} to float\")\n","          continue\n","      except KeyError:\n","          # print(f\"KeyError: missing key {aword} in defaultdict syuzhet_dt\")\n","          continue\n","      except:\n","          e = sys.exc_info()[0]\n","          # print(f\"ERROR {e}: sent2lex_sa() cannot catch aword indexing into syuzhet_dt error\")\n","  \n","  # print(f\"Leaving sent2lex_sa() with sentence sa value = {str(text_sa_tot)}\")\n","  \n","  return text_sa_tot\n","\n","\n","# Test\n","\n","# sent2sentiment('I hate and despise and abhor and dislike and am disgusted by Mondays.', lexicon_jockersrinker_dt)\n","# sent2sentiment('hate Mondays.', lexicon_jockersrinker_dt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6AEBZzvEz4a"},"outputs":[],"source":["def plot_smas(section_view=True, model_name='vader', text_unit='sentence', wins_ls=[20], alpha=0.5, subtitle_str='', y_height=0, save2file=False):\n","  '''\n","  Given a model, text_unit\n","  Plot a SMA using default values and wrapping the function get_smas()\n","  '''\n","\n","  if (section_view == True) and not any(x == text_unit for x in ['sentence', 'paragraph']):\n","    print(f'ERROR: You can only plot SMA within a Section with Sentence or Paragraph text units')\n","    return -99\n","\n","  if text_unit == 'sentence':\n","    if section_view == False:\n","      ts_df = corpus_sents_df\n","    else:\n","      ts_df = section_sents_df\n","    wins_ls = [5,10,20]\n","  elif text_unit == 'paragraph':\n","    if section_view == False:\n","      ts_df = corpus_parags_df\n","    else:\n","      ts_df = section_parags_df\n","    wins_ls = [5,10,20]\n","  elif text_unit == 'section':\n","    ts_df = corpus_sects_df\n","    wins_ls=[20]\n","  else:\n","    print(f'ERROR: {text_unit} must be sentence, paragraph or section')\n","\n","  sectno_loc = ts_df[model_name].min()\n","\n","  if section_view ==False:\n","    # At Section boundries draw blue vertical lines \n","    section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n","    for i, sent_no in enumerate(section_boundries_ls):\n","      plt.text(sent_no, y_height, f'Sec#{i}', alpha=0.2, rotation=90)\n","      plt.axvline(sent_no, color='blue', alpha=0.1)\n","      # 'BigNews1', xy=(sent_no, 0.5), xytext=(-10, 25), textcoords='offset points',                   rotation=90, va='bottom', ha='center', annotation_clip=True)\n","\n","      # plt.text(sent_no, -.5, 'goodbye',rotation=90, zorder=0)\n","\n","    # At Chapter boundaries draw red vertical lines\n","    chapter_boundries_ls = list(corpus_chaps_df['sent_no_start'])\n","    for i, sent_no in enumerate(chapter_boundries_ls):\n","      plt.axvline(sent_no, color='navy', alpha=0.1)\n","      # plt.text(sent_no, .5, 'hello', rotation=90, zorder=0)\n","\n","  get_smas(ts_df, model_name=model_name, text_unit=text_unit, wins_ls=wins_ls, alpha=alpha, subtitle_str=subtitle_str, save2file=save2file)\n","\n","  if (save2file == True):\n","    # Save graph to file.\n","    plot_filename = f'plot_sma_sents_{model_name}.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DtiQokMozf5"},"outputs":[],"source":["# SMA 5% Sentiment of Sentence Sentiment\n","\n","def get_smas(ts_df, model_name, text_unit='sentence', wins_ls=[5,10], alpha=0.5, scale_factor=1., subtitle_str='', mean_adj=0., do_plot=True, save2file=False):\n","  '''\n","  Given a model_name and time series DataFrame and list of win_rolls in percentages\n","  Return the rolling means of the time series using the window sizes in win_rolls\n","  '''\n","\n","  temp_roll_df = pd.DataFrame() # TODO: save sma rolling values into temp_df and return this value\n","\n","  win_1per = int(ts_df.shape[0]*0.01)\n","  if text_unit ==  'sentence':\n","    # win_1per = win_s1per\n","    x_idx = 'sent_no'\n","    fname_abbr = 'sents'\n","  elif text_unit == 'paragraph':\n","    # win_1per = win_p1per\n","    x_idx = 'parag_no'\n","    fname_abbr = 'parags'\n","  elif text_unit == 'section':\n","    win_1per = 1\n","    wins_ls = [int(0.1 * corpus_sects_df.shape[0])]  # Edge case to deal with very few Section data points\n","    x_idx = 'sect_no'\n","    fname_abbr = 'sects'\n","  else:\n","    print(f'ERROR: text_unit={text_unit} but must be either sentence, paragraph or section')\n","  \n","  for i, awin_size in enumerate(wins_ls):\n","    if len(str(awin_size)) == 1:\n","      awin_str = '0'+str(awin_size)+'0'\n","    else:\n","      awin_str = str(awin_size)+ '0'\n","    col_roll_str = f'{model_name}_mean_roll{awin_str}'\n","    win_size = awin_size*win_1per\n","    ts_df[col_roll_str] = ts_df[model_name].rolling(window=win_size, center=True).mean()\n","  \n","    if do_plot == True:\n","      alabel = f'{model_name} (win={awin_size})'\n","      ts_df['y_scaled'] = ts_df[col_roll_str]*scale_factor + mean_adj \n","      sns.lineplot(data=ts_df, x=x_idx, y='y_scaled', legend='brief', label=alabel, alpha=alpha)\n","      \n","  plt.title(f'{CORPUS_FULL} (Model: {model_name}: {subtitle_str}) \\nSMA Smoothed {text_unit} Sentiment Plot (windows={wins_ls})')\n","  # plt.legend(loc='best')\n","\n","  if save2file == True:\n","    # Save graph to file.\n","    plot_filename = f'plot_{fname_abbr}_sa_mean_050100sma.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return temp_roll_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2f9r-cHozf6"},"outputs":[],"source":["def get_lexstats(ts_df, model_name, text_unit='sentence'):\n","  '''\n","  Given a model name\n","  calculate, store and return time series stats\n","  '''\n","  \n","  global corpus_lexicons_stats_dt\n","\n","  temp_dt = {}\n","  \n","  if text_unit == 'sentence':\n","    stat_idx = f'{model_name}_sents'\n","  elif text_unit == 'paragraph':\n","    stat_idx = f'{model_name}_parags'\n","  elif text_unit == 'section':\n","    stat_idx = f'{model_name}_sects'\n","  elif text_unit == 'chapter':\n","    stat_idx = f'{model_name}_chaps'\n","  else:\n","    print(f'ERROR: {text_unit} must either be sentence, paragraph, or section')\n","\n","  sentiment_min = ts_df[model_name].min()\n","  sentiment_max = ts_df[model_name].max()\n","\n","  temp_dt = {'sentiment_min' : sentiment_min,\n","             'sentiment_max' : sentiment_max}\n","\n","  corpus_lexicons_stats_dt[stat_idx] = temp_dt\n","                                     \n","  return \n","\n","# Test\n","# get_lexstats('afinn')\n","# corpus_lexicons_stats_dt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGfwZfCKozf6"},"outputs":[],"source":["def lex_discrete2continous_sentiment(text, lexicon):\n","  '''\n","  Given a plain text string, give it to\n","    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n","  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n","    that approximates a normal distribution for all values\n","    In order to get more fine grained measure of overall Sentence sentiment\n","    Sentiment values will be Normalized/Standardized so absolute precision is not required\n","  '''\n","  text_sentiment_tot = 0.\n","  text_ls = text.split()\n","  text_len = len(text_ls)\n","  for aword in text_ls:\n","    word_sentiment = text2sentiment(str(aword), lexicon)\n","    text_sentiment_tot += word_sentiment\n","  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.01)\n","\n","  return text_sentiment_norm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73JukYyaozf7"},"outputs":[],"source":["def clip_outliers(floats_ser):\n","  '''\n","  Given a pd.Series of float values\n","  Return a list with outliers removed, values limited within 3 median absolute deviations from median\n","  '''\n","  # https://www.statsmodels.org/stable/generated/statsmodels.robust.scale.mad.html#statsmodels.robust.scale.mad\n","\n","  # Old mean/std, less robust\n","  # ser_std = floats_ser.std()\n","  # ser_median = floats_ser.mean() # TODO: more robust: asym/outliers -> median/IQR or median/median abs deviation\n","\n","  floats_np = np.array(floats_ser)\n","  ser_median = floats_ser.median()\n","  ser_mad = robust.mad(floats_np)\n","  print(f'ser_median = {ser_median}')\n","  print(f'ser_mad = {ser_mad}')\n","\n","  if ser_mad == 0:\n","    # for TS with small ranges (e.g. -1.0 to +1.0) Median Abs Deviation = 0\n","    #   so pass back the original time series\n","    floats_clip_ls = list(floats_ser)\n","\n","  else:\n","    ser_oldmax = floats_ser.max()\n","    ser_oldmin = floats_ser.min()\n","    print(f'ser_max = {ser_oldmax}')\n","    print(f'ser_min = {ser_oldmin}')\n","\n","    ser_upperlim = ser_median + 2.5*ser_mad\n","    ser_lowerlim = ser_median - 2.5*ser_mad\n","    print(f'ser_upperlim = {ser_upperlim}')\n","    print(f'ser_lowerlim = {ser_lowerlim}')\n","\n","    # Clip outliers to max or min values\n","    floats_clip_ls = np.clip(floats_np, ser_lowerlim, ser_upperlim)\n","    # print(f'max floast_ls {floats_ls.max()}')\n","\n","    # def map2range(value, low, high, new_low, new_high):\n","    #   '''map a value from one range to another'''\n","    #   return value * 1.0 / (high - low + 1) * (new_high - new_low + 1)\n","\n","    # Map all float values to range [-1.0 to 1.0]\n","    # floats_clip_sig_ls = [map2range(i, ser_oldmin, ser_oldmax, ser_upperlim, ser_lowerlim) for i in floats_clip_ls]\n","\n","    # listmax_fl = float(max(floats_ls))\n","    # floats_ls = [i/listmax_fl for i in floats_ls]\n","    #floats_ls = [1/(1+math.exp(-i)) for i in floats_ls]\n","\n","  return floats_clip_ls  # floats_clip_sig_ls\n","\n","# Test\n","# Will not work on first run as corpus_sents_df is not defined yet\n","'''\n","data = np.array([1, 4, 4, 7, 12, 13, 16, 19, 22, 24])\n","test_ls = clip_outliers(corpus_sents_df['vader'])\n","print(f'new min is {min(test_ls)}')\n","print(f'new max is {max(test_ls)}')\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpmQEz6UozBV"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"XUvKJEybUIeP"},"source":["## **Sentiment**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJNepWBIkVjm"},"outputs":[],"source":["# Read in lexicon at given path into Dict[word]=polarity\n","\n","def get_lexicon(lexicon_name, lexicon_format=2):\n","    \"\"\"\n","    Read sentiment lexicon.csv file at lexicon_path\n","    into appropriate Dict[word]=polarity\n","\n","    1. lexicon_dt[word] = <polarity value>\n","\n","    Args:\n","        sa_lib (str, optional): [description]. Defaults to 'syuzhet'.\n","    \"\"\"\n","    \n","    # global lexicon_df\n","\n","    lexicon_df = pd.DataFrame()\n","    \n","    # print(os.getcwd())\n","    lexicons_ls = os.listdir('../sa_lexicons/')\n","    if (lexicon_name in lexicons_ls):\n","      print(f'Found {lexicon_name} in lexicon_directory)')\n","    # print(glob.glob('*.csv'))\n","    cp_cmd = f'copy ../sa_lexicons/{lexicon_name} ./'\n","    print(f'cp_cmd = {cp_cmd}')\n","    os.system(cp_cmd)\n","    os.system('cp ../sa_lexicons/' + lexicon_name.strip() + ' ./')\n","    os.listdir('.')  \n","\n","    try:\n","      lexicon_df = pd.read_csv(lexicon_name)\n","      lexicon_df.info()\n","      # lexicon_df = lexicon_tmp_df.copy()\n","      # print(lexicon_df.head())\n","      return lexicon_df\n","    except:\n","      print(f'ERROR: Cannot read lexicon.csv at {lexicon_name}')\n","      return -1\n","\n","'''\n","    print\n","    if (sa_lexicon == 'default'):\n","        lexicon_df = pd.read_csv(LEXICON_PATH)\n","        lexicon_df.columns = ['index_no', 'word', 'polarity']\n","        lexicon_df.drop(['index_no'], axis=1, inplace=True)\n","        lexicon_df.dropna(inplace=True)\n","        lexicon_dt = lexicon_df.set_index('word').T.to_dict('list')\n","        # unlist the polarity to type: float\n","        for key in lexicon_dt:\n","            lexicon_dt[key] = float(lexicon_dt[key][0])\n","        \n","    ### print(f\"Exit get_sa_lex() with {len(lexicon_dt.keys())} entries in syuzhet_dt\")\n","    return lexicon_dt\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRPV5D7L4kGi"},"outputs":[],"source":["def sentiment_sents2parags(ts_df, model_name='roberta_lg15'):\n","  '''\n","  Given a DataFrame and a model_name column containing float sentiment values (one row per Sentence)\n","  Return a list of model paragraph sentiment values by aggregated/summing sentence sentiment values\n","  '''\n","  \n","  # global corpus_parags_df # Don't create hidden entanglement with global vars if not necessary\n","\n","  parags_sentiment_ls = []\n","  parag_ptr = 0\n","  parag_sentiment_tot = 0\n","\n","  for index, row in corpus_sents_df.iterrows():\n","    this_sent_no = row['sent_no']\n","    this_parag_no = row['parag_no']\n","    this_sentiment = row[model_name]\n","    # print(f'Sent #{this_sent_no}, Parag #{this_parag_no}, Sentiment: {this_sentiment}')\n","    # print(row['sent_no'], row['parag_no'], row['roberta_lg15'])\n","    if parag_ptr == this_parag_no:\n","      parag_sentiment_tot += this_sentiment\n","    else:\n","      # corpus_parags_df.iloc[this_parag_no][model_name] = parag_sentiment_tot # See above note\n","      parags_sentiment_ls.append(parag_sentiment_tot)\n","      parag_sentiment_tot = this_sentiment\n","      parag_ptr += 1\n","\n","  parags_sentiment_ls.append(parag_sentiment_tot) # Add the last remaining Paragraph Sentiment Value \n","\n","  return parags_sentiment_ls\n","\n","# Test\n","\"\"\"\n","parags_sentiment_ls = sentiment_sents2parags(corpus_sents_df)\n","len(parags_sentiment_ls)\n","\n","corpus_parags_df['parag_no'].duplicated().any()\n","corpus_parags_df['parag_raw'].astype('str').apply(lambda x: len(x)==0).any()\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bN81l1AudIN"},"outputs":[],"source":["def polprob2sentiment(pol_str, prob_fl):\n","  '''\n","  Given a Polarity string (Negative or Positive) and a Probability float (0.0-1.0)\n","  Return a Sentiment float value (-1.0 to 1.0)\n","  '''\n","  sign_fl = 1.0\n","  if pol_str.lower().startswith('neg'):\n","    # print(f'pol_str: {pol_str} is Negative')\n","    sign_fl = -1.0\n","  elif pol_str.lower().startswith('pos'):\n","    # print(f'pol_str: {pol_str} is Positive')\n","    pass\n","  else:\n","    print(f'ERROR: pol_str: {pol_str} is neither Negative nor Positive')\n","    sign_fl = 0.0\n","\n","  return sign_fl * prob_fl\n","\n","# Test\n","polprob2sentiment('Positive', 0.91)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWdzLF0jwI-u"},"outputs":[],"source":["# Sentence to Sentiment Polarity according to passed in Lexicon Dictionary\n","\n","def text2sentiment(text_str, lexicon_dt):\n","  '''\n","  Given a text_str and lexicon_dt, calculate \n","  the sentimety polarity.\n","  '''\n","\n","  # Remove all not alphanumeric and whitespace characters\n","  text_str = re.sub(r'[^\\w\\s]', '', text_str) \n","\n","  text_str = text_str.strip().lower()\n","  if (len(text_str) < 1):\n","      print(f\"ERROR: text2sentiment() given empty/null/invalid string: {text_str}\")\n","\n","  text_ls = text_str.split()\n","  # print(f'text_ls: {text_ls}')\n","\n","  # Accumulated Total Sentiment Polarity for entire Sentence\n","  text_sa_tot = 0.0\n","\n","  for aword in text_ls:\n","      # print(f'getting sa for word: {aword}')\n","      try:\n","          word_sa_fl = float(lexicon_dt[aword])\n","          text_sa_tot += word_sa_fl\n","          # print(f\">>{aword} has a sentiment value of {word_sa_fl}\")\n","      except TypeError: # KeyError:\n","          # aword is not in lexicon so it adds 0 to the sentence sa sum\n","          # print(f\"TypeError: cannot convert {lexicon_dt[aword]} to float\")\n","          continue\n","      except KeyError:\n","          # print(f\"KeyError: missing key {aword} in defaultdict syuzhet_dt\")\n","          continue\n","      except:\n","          e = sys.exc_info()[0]\n","          # print(f\"ERROR {e}: sent2lex_sa() cannot catch aword indexing into syuzhet_dt error\")\n","  \n","  # print(f\"Leaving sent2lex_sa() with sentence sa value = {str(text_sa_tot)}\")\n","  \n","  return text_sa_tot\n","\n","\n","# Test\n","\n","# sent2sentiment('I hate and despise and abhor and dislike and am disgusted by Mondays.', lexicon_jockersrinker_dt)\n","# sent2sentiment('hate Mondays.', lexicon_jockersrinker_dt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltdJn-7ePNM9"},"outputs":[],"source":["def lex_discrete2continous_sentiment(text, lexicon):\n","  '''\n","  Given a plain text string, give it to\n","    Stanford Stanza (OpenNLP) to calculate sentiment for each word on a 3 point scale 0-2\n","  Return a sentiment value for the entire sentence (sum of word sentiments/log(len of sentence)) \n","    that approximates a normal distribution for all values\n","    In order to get more fine grained measure of overall Sentence sentiment\n","    Sentiment values will be Normalized/Standardized so absolute precision is not required\n","  '''\n","  text_sentiment_tot = 0.\n","  text_ls = text.split()\n","  text_len = len(text_ls)\n","  for aword in text_ls:\n","    word_sentiment = text2sentiment(str(aword), lexicon)\n","    text_sentiment_tot += word_sentiment\n","  text_sentiment_norm = text_sentiment_tot/(np.log(text_len)+0.01)\n","\n","  return text_sentiment_norm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULzfHeDK8udN"},"outputs":[],"source":["def get_lexstats(ts_df, model_name, text_unit='sentence'):\n","  '''\n","  Given a model name\n","  calculate, store and return time series stats\n","  '''\n","  \n","  global corpus_lexicons_stats_dt\n","\n","  temp_dt = {}\n","  \n","  if text_unit == 'sentence':\n","    stat_idx = f'{model_name}-sents'\n","  elif text_unit == 'paragraph':\n","    stat_idx = f'{model_name}-parags'\n","  else:\n","    print(f'ERROR: {text_unit} must either be sentence or paragraph')\n","\n","  sentiment_min = ts_df[model_name].min()\n","  sentiment_max = ts_df[model_name].max()\n","\n","  temp_dt = {'sentiment_min' : sentiment_min,\n","             'sentiment_max' : sentiment_max}\n","\n","  corpus_lexicons_stats_dt[stat_idx] = temp_dt\n","                                     \n","  return \n","\n","# Test\n","# get_lexstats('afinn')\n","# corpus_lexicons_stats_dt"]},{"cell_type":"markdown","metadata":{"id":"RvE7bVifo0qy"},"source":["## **Normalize, Standardize and Outliers**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIo6-zGKnZps"},"outputs":[],"source":["def norm2negpos1(data_ser):\n","  '''\n","  Given a series of floating number\n","  Return a a list of same values normed between -1.0 and +1.0\n","  '''\n","  # data_np = np.matrix(data_ser)\n","\n","  scaler=MinMaxScaler(feature_range=(-1.0, 1.0))\n","  temp_ser = scaler.fit_transform(np.matrix(data_ser))\n","  \n","  return temp_ser\n","\n","# Test\n","'''\n","temp_np = norm2negpos1(corpus_all_df[['xlnet_sst5']])\n","print(type(temp_np))\n","temp_np.shape\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XpVHeYKYnUhU"},"outputs":[],"source":["def standardize_ts(data_ser):\n","  '''\n","  Given a series of floating number\n","  Return a a list of same values normed between -1.0 and +1.0\n","  '''\n","  # data_np = np.matrix(data_ser)\n","\n","  std_scaler = StandardScaler()\n","  df_std = std_scaler.fit_transform(np.array(data_ser))\n","  \n","  return df_std\n","\n","# Test\n","'''\n","temp_np = norm2negpos1(corpus_all_df[['xlnet_sst5']])\n","print(type(temp_np))\n","temp_np.shape\n","temp_np\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6xMI98l8uaH"},"outputs":[],"source":["def clip_outliers(floats_ser):\n","  '''\n","  Given a pd.Series of float values\n","  Return a list with outliers removed, values limited within 3 median absolute deviations from median\n","  '''\n","  # https://www.statsmodels.org/stable/generated/statsmodels.robust.scale.mad.html#statsmodels.robust.scale.mad\n","\n","  # Old mean/std, less robust\n","  # ser_std = floats_ser.std()\n","  # ser_median = floats_ser.mean() # TODO: more robust: asym/outliers -> median/IQR or median/median abs deviation\n","\n","  floats_np = np.array(floats_ser)\n","  ser_median = floats_ser.median()\n","  ser_mad = robust.mad(floats_np)\n","  print(f'ser_median = {ser_median}')\n","  print(f'ser_mad = {ser_mad}')\n","\n","  if ser_mad == 0:\n","    # for TS with small ranges (e.g. -1.0 to +1.0) Median Abs Deviation = 0\n","    #   so pass back the original time series\n","    floats_clip_ls = list(floats_ser)\n","\n","  else:\n","    ser_oldmax = floats_ser.max()\n","    ser_oldmin = floats_ser.min()\n","    print(f'ser_max = {ser_oldmax}')\n","    print(f'ser_min = {ser_oldmin}')\n","\n","    ser_upperlim = ser_median + 2.5*ser_mad\n","    ser_lowerlim = ser_median - 2.5*ser_mad\n","    print(f'ser_upperlim = {ser_upperlim}')\n","    print(f'ser_lowerlim = {ser_lowerlim}')\n","\n","    # Clip outliers to max or min values\n","    floats_clip_ls = np.clip(floats_np, ser_lowerlim, ser_upperlim)\n","    # print(f'max floast_ls {floats_ls.max()}')\n","\n","    # def map2range(value, low, high, new_low, new_high):\n","    #   '''map a value from one range to another'''\n","    #   return value * 1.0 / (high - low + 1) * (new_high - new_low + 1)\n","\n","    # Map all float values to range [-1.0 to 1.0]\n","    # floats_clip_sig_ls = [map2range(i, ser_oldmin, ser_oldmax, ser_upperlim, ser_lowerlim) for i in floats_clip_ls]\n","\n","    # listmax_fl = float(max(floats_ls))\n","    # floats_ls = [i/listmax_fl for i in floats_ls]\n","    #floats_ls = [1/(1+math.exp(-i)) for i in floats_ls]\n","\n","  return floats_clip_ls  # floats_clip_sig_ls\n","\n","# Test\n","# Will not work on first run as corpus_sents_df is not defined yet\n","'''\n","data = np.array([1, 4, 4, 7, 12, 13, 16, 19, 22, 24])\n","test_ls = clip_outliers(corpus_sents_df['vader'])\n","print(f'new min is {min(test_ls)}')\n","print(f'new max is {max(test_ls)}')\n","''';"]},{"cell_type":"markdown","metadata":{"id":"V8amlNSGoRwe"},"source":["## **Smoothing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcAMyjyn8ugj"},"outputs":[],"source":["# SMA 5% Sentiment of Sentence Sentiment\n","\n","def get_smas(ts_df, model_name, text_unit='sentence', win_ls=[5,10], scale_factor=1., mean_adj=0., do_plot=True, save2file=False):\n","  '''\n","  Given a model_name and time series DataFrame and list of win_rolls in percentages\n","  Return the rolling means of the time series using the window sizes in win_rolls\n","  '''\n","\n","  temp_roll_df = pd.DataFrame() # TODO: save sma rolling values into temp_df and return this value\n","\n","  if text_unit ==  'sentence':\n","    win_1per = win_s1per\n","    x_idx = 'sent_no'\n","    fname_abbr = 'sents'\n","  elif text_unit == 'paragraph':\n","    win_1per = win_p1per\n","    x_idx = 'parag_no'\n","    fname_abbr = 'parags'\n","  else:\n","    print(f'ERROR: text_unit={text_unit} but must be either sentence or paragraph')\n","  \n","  for i, awin_size in enumerate(win_ls):\n","    if len(str(awin_size)) == 1:\n","      awin_str = '0'+str(awin_size)+'0'\n","    else:\n","      awin_str = str(awin_size)+ '0'\n","    col_roll_str = f'{model_name}_mean_roll{awin_str}'\n","    ts_df[col_roll_str] = ts_df[model_name].rolling(awin_size*win_1per, center=True).mean()\n","  \n","    if do_plot == True:\n","      alabel = f'{model_name} (win={awin_size})'\n","      ts_df['y_scaled'] = ts_df[col_roll_str]*scale_factor + mean_adj \n","      sns.lineplot(data=ts_df, x=x_idx, y='y_scaled', legend='brief', label=alabel)\n","      \n","  plt.title(f'{CORPUS_FULL} (Model: {model_name}) \\nSMA Smoothed {text_unit} Sentiment Plot (windows={win_ls})')\n","  # plt.legend(loc='best')\n","\n","  if save2file == True:\n","    # Save graph to file.\n","    plot_filename = f'plot_{fname_abbr}_sa_050100mean_afinn.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return temp_roll_df;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fgb_FFPs9vMA"},"outputs":[],"source":["def plot_lowess(ts_df, df_cols_ls, do_plot=True, afrac=1./10, ait=5):\n","  '''\n","  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n","  Return a DataFrame with LOWESS values\n","  If 'plot=True', also output plot\n","  '''\n","\n","  # global corpus_sents_df\n","\n","  lowess_df = pd.DataFrame()\n","\n","  for i,acol in enumerate(df_cols_ls):\n","    sm_x, sm_y = sm_lowess(endog=ts_df[acol].values, exog=ts_df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n","    col_new = f'{acol}_lowess'\n","    lowess_df[col_new] = pd.Series(sm_y)\n","    if do_plot:\n","      afrac_label = f'{afrac:.3f}'\n","      plt.plot(sm_x, sm_y, label=afrac_label, alpha=0.5, linewidth=2)\n","\n","      frac_str = str(round(100*afrac))\n","      plt.title(f'{CORPUS_FULL} \\n LOWESS (frac={frac_str} Sentence Sentiment ({sa_model}, frac={afrac})')\n","      plt.legend(title='LOWESS fraction')\n","\n","  return lowess_df\n","\n","# Test\n","\"\"\"\n","new_lowess_col = f'{sa_model}_lowess'\n","my_frac = 1./10\n","my_frac_per = round(100*my_frac)\n","new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n","corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], afrac=my_frac)\n","corpus_sents_df.head()\n","\"\"\";"]},{"cell_type":"markdown","metadata":{"id":"Mjff-H5tokgI"},"source":["## **Classifiers**"]},{"cell_type":"markdown","metadata":{"id":"yXwKR4gA8Ouk"},"source":["## **Pandas**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8Hf8nU98uXI"},"outputs":[],"source":["def rename_cols(ts_df, col_old_ls, suffix_str='_raw'):\n","  '''\n","  Given a DataFrame, list of columns in DataFrame and a suffix,\n","  Return a Dictionary mapping old col names to new col name (orig+suffix)\n","  '''\n","\n","  col_new_ls = []\n","  for acol in col_old_ls:\n","    acol_new = acol + suffix_str\n","    col_new_ls.append(acol_new)\n","\n","  # Create dict for col mapping: keys=old col names, value=new col names\n","  col_rename_dt = dict(zip(col_old_ls, col_new_ls))\n","\n","  # ts_df.rename(columns=col_rename_dt, errors=\"raise\")\n","\n","  return col_rename_dt\n","\n","# test_ls = [col for col in corpus_sents_df.columns if not(renaming_fun(col) is None)]\n","# print(f'test_ls: {test_ls}')\n","\n","# Test\n","# col_rename_dt = rename_cols(corpus_sents_df, sentiment_only_cols_ls)\n","# col_rename_dt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c45yulj-EzyU"},"outputs":[],"source":["def get_cols_regex(ts_df, find_regex, ignore_regex, strict_match=False):\n","  ''' \n","  Given a DataFrame and 2 regex string to match/find and ignore/discard (strict_match: return if substring only)\n","  Return all the column names that match that regex\n","  '''\n","\n","  cols_ls = ts_df.columns\n","  cols_match = []\n","\n","  if strict_match:\n","    find_regex = f'^{find_regex}$'\n","\n","  find_comp = re.compile(find_regex)\n","\n","  for acol in cols_ls:\n","    match_str = re.search(find_comp, acol)\n","    if match_str:\n","      ignore_str = re.search(ignore_regex, acol)\n","      if ignore_str:\n","        continue\n","        # print(f'Ignore: {acol}')\n","        pass\n","      else: \n","        # print(f'Found {match_str.group(0)} in {acol}')\n","        cols_match.append(acol)\n","\n","  return cols_match\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3YJJcvDVnUuT"},"source":["## **Stanford ASAP**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txDkWap8QpcV"},"outputs":[],"source":["# ASAP smooth_simple and support functions\n","\n","def moving_average(data, _range):\n","    ret = np.cumsum(data, dtype=float)\n","    ret[_range:] = ret[_range:] - ret[:-_range]\n","    return ret[_range - 1:] / _range\n","\n","def SMA(data, _range, slide):\n","    ret = moving_average(data, _range)[::slide]\n","    return list(ret)\n","\n","def kurtosis(values):\n","    return scipy.stats.kurtosis(values)\n","\n","def roughness(vals):\n","    return np.std(np.diff(vals))\n","\n","def smooth_simple(data, max_window=5, resolution=None):\n","    data = np.array(data)\n","    # Preaggregate according to resolution\n","    window_size = 1\n","    slide_size = 1\n","    if resolution:\n","        slide_size = int(len(data) / resolution)\n","        if slide_size > 1:\n","            data = SMA(data, slide_size, slide_size)\n","    orig_kurt   = kurtosis(data)\n","    min_obj     = roughness(data)\n","    range_lim = int(len(data) / max_window + 1)  # 20210621 insert: Fix JChun\n","    for w in range(2, range_lim):                #          edit:   range_lim\n","        w_int = int(w)                     \n","        smoothed = SMA(data, w_int, 1)   \n","        if kurtosis(smoothed) >= orig_kurt:\n","            r = roughness(smoothed)\n","            if r < min_obj:\n","                min_obj = r\n","                window_size = w\n","    return window_size, slide_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clpvH9GOQpcZ"},"outputs":[],"source":["# ASAP asap_smooth and utility functions\n","\n","class Metrics(object):\n","    def __init__(self, values):\n","        self.set_values( values )\n","\n","    def set_values(self, values):\n","        self.values = values\n","        self.r = self.k = None\n","\n","    @property\n","    def kurtosis(self):\n","        if self.k is None:\n","            self.k = scipy.stats.kurtosis(self.values)\n","        return self.k\n","\n","    @property\n","    def roughness(self):\n","        if self.r is None:\n","            self.r = np.std(np.diff(self.values))\n","        return self.r\n","\n","class ACF(Metrics):\n","    CORR_THRESH = 0.2\n","    def __init__(self, values, max_lag=None):\n","        super(ACF, self).__init__(values)\n","        if max_lag is None:\n","            max_lag = len(values) / 5\n","        self.max_lag = int(max_lag)\n","        self.max_acf = 0.0\n","\n","        # Calculate autocorrelation via FFT\n","        # Demean\n","        demeaned = values - np.mean(values)\n","        # Pad data to power of 2 \n","        l = int(2.0 ** (int(math.log(len(demeaned),2.0)) + 1))\n","        padded = np.append(demeaned, ([0.0] * (l - len(demeaned))))    \n","        # FFT and inverse FFT\n","        F_f = numpy.fft.fft( padded )\n","        R_t = numpy.fft.ifft( F_f * np.conjugate(F_f) )\n","        max_lag = int(max_lag)\n","        self.correlations = R_t[:max_lag].real / R_t[0].real \n","        \n","        # Find autocorrelation peaks \n","        self.peaks = []\n","        if len(self.correlations) >1 :\n","            positive = self.correlations[1] > self.correlations[0]\n","            max = 1\n","            for i in range(2, len(self.correlations)):\n","                if not positive and self.correlations[i] > self.correlations[i-1]:\n","                    max = i\n","                    positive = not positive\n","                elif positive and self.correlations[i] > self.correlations[max]:\n","                    max = i\n","                elif positive and self.correlations[i] < self.correlations[i-1]:\n","                    if max > 1 and self.correlations[max] > self.CORR_THRESH:\n","                        self.peaks.append(max)\n","                        if self.correlations[max] > self.max_acf:\n","                            self.max_acf = self.correlations[max]\n","                    positive = not positive\n","        # If there is no autocorrelation peak within the MAX_WINDOW boundary,\n","        # try windows from the largest to the smallest \n","        if len(self.peaks) <= 1:\n","            self.peaks = range(2, len(self.correlations))\n","                    \n","def moving_average(data, _range):\n","    ret = np.cumsum(data)\n","    ret[_range:] = ret[_range:] - ret[:-_range]\n","    return ret[_range - 1:] / _range\n","\n","def SMA(data, _range, slide):\n","    ret = moving_average(data, _range)[::slide]\n","    return list(ret)\n","                    \n","def binary_search(head,tail,data,min_obj,orig_kurt,window_size):\n","    while head <= tail:\n","        w = int(round((head + tail) / 2.0))\n","        smoothed = SMA(data,w,1)\n","        metrics  = Metrics(smoothed)\n","        if metrics.kurtosis >= orig_kurt:\n","            if metrics.roughness < min_obj:\n","                window_size = w\n","                min_obj = metrics.roughness\n","            head = w + 1\n","        else:\n","            tail = w - 1\n","    return window_size\n","\n","def smooth_ASAP(data, max_window=5, resolution=None):\n","    data = np.array(data)\n","    # Preaggregate according to resolution\n","    slide_size = 1\n","    window_size = 1\n","    if resolution and len(data) >= 2 * resolution:\n","        slide_size = int(len(data) / resolution)  # 20210621 JChun\n","        data = SMA(data, slide_size, slide_size)\n","    acf         = ACF(data, max_lag=len(data) / max_window)\n","    peaks       = acf.peaks\n","    orig_kurt   = acf.kurtosis\n","    min_obj     = acf.roughness\n","    lb          = 1\n","    largest_feasible = -1\n","    tail = int(len(data) / max_window)  # 20210621 JChun\n","    for i in range(len(peaks) - 1, -1, -1):\n","        w = peaks[i]\n","\n","        if w < lb or w == 1:\n","            break\n","        elif math.sqrt(1 - acf.correlations[w]) * window_size > math.sqrt(1 - acf.correlations[window_size]) * w:\n","            continue\n","\n","        smoothed = SMA(data, w, 1)\n","        metrics = Metrics(smoothed)\n","        if metrics.roughness < min_obj and metrics.kurtosis >= orig_kurt:\n","            min_obj = metrics.roughness\n","            window_size = w\n","            lb = round( max(w*math.sqrt( (acf.max_acf -1) / (acf.correlations[w]-1) ), lb) )\n","    if largest_feasible > 0:\n","        if largest_feasible < len(peaks) - 2:\n","            tail = peaks[largest_feasible + 1]\n","        lb = max(lb, peaks[largest_feasible] + 1)\n","\n","    window_size = binary_search(lb, tail, data, min_obj, orig_kurt, window_size)\n","    return window_size, slide_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ds1890fsQpcc"},"outputs":[],"source":["# ASAP utility function to read data from CSV (not used)\n","\n","def load_csv(fname, input_column=1):\n","    import csv\n","    with open(fname, 'r') as ifh:\n","        icsv = csv.reader(ifh)\n","        if sys.version_info.major == 2:\n","            header = icsv.next()\n","        else:\n","            header = next(icsv)\n","        rows = list(icsv)\n","\n","        try:\n","            data = [ float(x[input_column]) for x in rows ]\n","        except ValueError:\n","            print(\"couldn't convert input-column={0} float\".format(input_column))\n","            if rows:\n","                print(\"first row:\")\n","                for idx,x in enumerate(rows[0]):\n","                    print('  column {:3d}: {}'.format(idx,x))\n","            exit(1)\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfDHASdngiVX"},"outputs":[],"source":["# ASAP Simple (Brute Force)\n","\"\"\"\n","def moving_average(data, _range):\n","    ret = np.cumsum(data, dtype=float)\n","    ret[_range:] = ret[_range:] - ret[:-_range]\n","    return ret[_range - 1:] / _range\n","\n","def SMA(data, _range, slide):\n","    ret = moving_average(data, _range)[::slide]\n","    return list(ret)\n","\n","def kurtosis(values):\n","    return scipy.stats.kurtosis(values)\n","\n","def roughness(vals):\n","    return np.std(np.diff(vals))\n","\n","def smooth_simple(data, max_window=5, resolution=None):\n","    data = np.array(data)\n","    # Preaggregate according to resolution\n","    window_size = 1\n","    slide_size = 1\n","    if resolution:\n","        slide_size = int(len(data) / resolution)\n","        if slide_size > 1:\n","            data = SMA(data, slide_size, slide_size)\n","    orig_kurt   = kurtosis(data)\n","    min_obj     = roughness(data)\n","    for w in range(2, int(len(data) / max_window + 1)):\n","        smoothed = SMA(data, w, 1)\n","        if kurtosis(smoothed) >= orig_kurt:\n","            r = roughness(smoothed)\n","            if r < min_obj:\n","                min_obj = r\n","                window_size = w\n","    return window_size, slide_size\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XK_lQShlgiVc"},"outputs":[],"source":["# ASAP Plot time series before and after smoothing\n","\n","def plot(data, window_size, slide_size, plot_title):\n","    plt.clf()\n","    plt.figure()\n","    data = SMA(data, slide_size, slide_size)\n","    method_names = [\"SMA Smoothed\", \"ASAP Smoothed\"]\n","    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n","    # smoothed = SMA(data, window_size, 1)\n","    smoothed = smooth_simple(data, window_size, 1)\n","    smoothed_range = range(int(window_size/2), int(window_size/2) + len(smoothed))\n","    ax1.set_xlim(0, len(data))\n","    ax1.plot(data, linestyle='-', linewidth=1.5)\n","    # ax1.set_title('SMA Smoothed')\n","    ax2.plot(smoothed_range, smoothed, linestyle='-', linewidth=1.5)\n","    # ax2.set_title('Stanford ASAP Smoothed')\n","    axes = [ax1, ax2]\n","    for i in range(2):\n","        axes[i].get_xaxis().set_visible(False)\n","        axes[i].text(0.02, 0.8, \"%s\" %(method_names[i]),\n","            verticalalignment='center', horizontalalignment='left',\n","            transform=axes[i].transAxes, fontsize=25)\n","\n","    fig.set_size_inches(16, 12)\n","    plt.tight_layout(w_pad=1)\n","    plt.title(plot_title)\n","    plt.show()\n","\n","    return smoothed_range, smoothed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dBHzgf7giVl"},"outputs":[],"source":["# Plot both SMA and ASAP Smoothed Sentiment Analysis Time Series\n","\n","def plot_asap(model_name, data, window_size, slide_size, do_plot=True, save2file=False):\n","    plt.clf()\n","    plt.figure()\n","    data = SMA(data, slide_size, slide_size)\n","    method_names = [\"SMA Smoothed\", \"ASAP Smoothed\"]\n","    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n","    smoothed = SMA(data, window_size, 1)\n","    smoothed_range = range(int(window_size/2), int(window_size/2) + len(smoothed))\n","    ax1.set_xlim(0, len(data))\n","    ax1.plot(data, linestyle='-', linewidth=1.5)\n","    title_str = f'Stanford ASAP Smoothing of {model_name} (win={window_size}, slide={slide_size}'\n","    ax2.title.set_text(title_str)\n","    ax2.plot(smoothed_range, smoothed, linestyle='-', linewidth=1.5)\n","    axes = [ax1, ax2]\n","    for i in range(2):\n","        axes[i].get_xaxis().set_visible(False)\n","        axes[i].text(0.02, 0.8, \"%s\" %(method_names[i]),\n","            verticalalignment='center', horizontalalignment='left',\n","            transform=axes[i].transAxes, fontsize=25)\n","\n","    fig.set_size_inches(16, 6)\n","    plt.tight_layout(w_pad=1)\n","\n","\n","    if save2file:\n","      # Save Plot to file.\n","      plot_filename = f'plot_sent_asap_{model_name}_{author_str}_{title_str}.png'\n","      # plotpathfilename_str = gen_pathfiletime(plot_filename)\n","      plt.savefig(plot_filename, format='png', dpi=300)\n","      print(f'Plot saved: {plot_filename}');\n","    \n","    plt.show()\n","\n","    return smoothed_range, smoothed;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZVJEv8koajo"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"6-cC2ho8pMh7"},"source":["## **Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9rw5UplpMh-"},"outputs":[],"source":["def norm2negpos1(data_ser):\n","  '''\n","  Given a series of floating number\n","  Return a a list of same values normed between -1.0 and +1.0\n","  '''\n","  # data_np = np.matrix(data_ser)\n","\n","  scaler=MinMaxScaler(feature_range=(-1.0, 1.0))\n","  temp_ser = scaler.fit_transform(np.matrix(data_ser))\n","  \n","  return temp_ser\n","\n","# Test\n","'''\n","temp_np = norm2negpos1(corpus_all_df[['xlnet_sst5']])\n","print(type(temp_np))\n","temp_np.shape\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tjM257dOpMiA"},"outputs":[],"source":["def standardize_ts(data_ser):\n","  '''\n","  Given a series of floating number\n","  Return a a list of same values normed between -1.0 and +1.0\n","  '''\n","  # data_np = np.matrix(data_ser)\n","\n","  std_scaler = StandardScaler()\n","  df_std = std_scaler.fit_transform(np.array(data_ser))\n","  \n","  return df_std\n","\n","# Test\n","'''\n","temp_np = norm2negpos1(corpus_all_df[['xlnet_sst5']])\n","print(type(temp_np))\n","temp_np.shape\n","temp_np\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ylt_kLuEFDrj"},"outputs":[],"source":["# This must be defined AFTER the corpus_sects_df DataFrame is created in the Preprocessing Step below\n","\n","# Raw Plot of Section Sentiments (Adjusted for (x-axis) mid-Section Sentence No and (y-axis) Sentiment weighted by Section length )\n","\n","# corpus_sects_df = pd.DataFrame()  # Create empty early as required by some utility functions\n","\n","def plot_crux_sections(model_names_ls, semantic_type='section', subtitle_str='', label_token_ct=0, title_xpos = 0.8, title_ypos=0.2, sec_y_height=0, save2file=False):\n","  '''\n","  Given a Sections DataFrame, model_name and semantic type,\n","  Return a Plot of the Cruxes\n","  '''\n","\n","  crux_points_dt = {}\n","  model_stand_names_ls = []\n","  section_boundries_ls = []\n","\n","\n","  # print(f'Using model_names: {model_names_ls}')\n","\n","  # sns.lineplot(data=ts_df, x='sent_no_mid', y=amodel_stand, markers=['o'], alpha=0.5, label=amodel_stand); # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment (Bing Lexicon)')\n","\n","\n","  # At Section boundries draw blue vertical lines \n","  section_boundries_ls = list(corpus_sects_df['sent_no_start'])\n","  for i, sent_no in enumerate(section_boundries_ls):\n","    plt.text(sent_no, sec_y_height, f'Sec#{i}', alpha=0.2, rotation=90)\n","    plt.axvline(sent_no, color='blue', alpha=0.1);\n","\n","  # At Chapter boundaries draw red vertical lines\n","  chapter_boundries_ls = list(corpus_chaps_df['sent_no_start'])\n","  for i, sent_no in enumerate(chapter_boundries_ls):\n","    plt.axvline(sent_no, color='navy', alpha=0.1);\n","\n","  # Error check and assign DataFrame associated with each semantic_type\n","  if semantic_type == 'section':\n","    # Get midpoints of each Section\n","    ts_df=corpus_sects_df\n","    midpoints_ls = list(corpus_sects_df['sent_no_mid'])\n","  elif semantic_type == 'chapter':\n","    # Get midpoints of each Chapter\n","    ts_df=corpus_chaps_df\n","    midpoints_ls = list(corpus_chaps_df['sent_no_mid'])\n","  else:\n","    print(f\"ERROR: semantic_type={semantic_type} must be either 'section' or 'chapter'\")\n","    return -1\n","\n","  # How many sentiment time series are we plotting?\n","  if len(model_names_ls) == 1:\n","    \n","    # Plotting only one model\n","    model_name_full = str(model_names_ls[0])\n","    model_name_root = model_name_full.split('_')[0]\n","    print(f'model_name_full: {model_name_full} and model_name_root: {model_name_root}')\n","    if model_name_root in MODELS_LS:\n","      # Plot\n","      print(f'about to sns.lineplot model: ') # {ts_df}')\n","      g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n","      # g._legend.remove()\n","      # print(f'model_name_full={model_name_full}')\n","      # plt.plot(ts_df.sent_no_mid, ts_df[model_name_full], markers=\"o\", alpha=0.5, label=model_name_full)\n","    else:\n","      print(f'ERROR: model_names_ls[0]={model_name_root} is invalid,\\n    must be one of {MODELS_LS}')\n","      return -1\n","\n","    # If plotting only one model, add labels\n","    midpoints_sentiment_ls = list(ts_df[model_name_full])\n","    sect_ct = 0\n","    for x,y in zip(midpoints_ls, midpoints_sentiment_ls): \n","      label_token_int = int(label_token_ct)\n","      if label_token_int < 0:\n","        label = ''\n","      elif label_token_int == 0:\n","        # if arg label_token_ct == 0, just print sent_no\n","        label = f\"#{x}({sect_ct})\"\n","      else:\n","        # if arg label_token_ct > 0, print the first label_token_ct words of sentence at crux point\n","        label = f\"#{x}({sect_ct}) {' '.join(corpus_sents_df.iloc[x-1]['sent_raw'].split()[:label_token_int])}\"; # \\nPolarity: {y:.2f}'\n","\n","      # Save Crux point in crux_points_dt Dictionary if plotting Cruxes for a single/specific Model\n","      crux_full_str = ' '.join(corpus_sents_df.iloc[x]['sent_raw'].split())\n","      crux_points_dt[x] = [y, crux_full_str]\n","\n","      plt.annotate(label,\n","                   (x,y),\n","                   textcoords='offset points',\n","                   xytext=(0,10),\n","                   ha='center',\n","                   rotation=90)\n","      sect_ct += 1\n","\n","    plt.title(f'{CORPUS_FULL} \\n Plot {semantic_type.capitalize()} Sentiment ({model_name_full.capitalize()})\\n{subtitle_str}', x=title_xpos, y=title_ypos);\n","    # Plot\n","    plt.plot(midpoints_ls, midpoints_sentiment_ls, marker=\"o\", ms=6) # , markevery=[0,1])\n","\n","  else:\n","    # If plotting multiple models\n","    model_names_str = 'Multiple Models'\n","    for i, model_name_full in enumerate(model_names_ls):\n","      # Error check and assign correct model names\n","      model_name_root = model_name_full.split('_')[0]\n","      if model_name_root in MODELS_LS:\n","        # Plot\n","        g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n","        # g._legend.remove()\n","        # plt.plot(ts_df.sent_no_mid, ts_df[model_name_full], marker=\"o\", alpha=0.5, label=model_name_full)\n","      else:\n","        print(f'ERROR: model_names_ls[]={model_name_root} is invalid,\\n    must be one of {MODELS_LS}')\n","        return -1\n","\n","      # Plot\n","      g = sns.lineplot(data=ts_df, x='sent_no_mid', y=model_name_full, markers=['o'], alpha=0.5, label=model_name_full) # .set_title(f'{CORPUS_FULL} \\n Plot Section Sentiment and Cruxes (Model: {models_names_ls[0].capitalize()})')\n","      # g._legend.remove()\n","\n","    plt.title(f'{CORPUS_FULL} \\n Plot {semantic_type.capitalize()} Sentiment (Standardized Models)\\n{subtitle_str}', x=title_xpos, y=title_ypos)\n","\n","  # plt.legend(loc='best');\n","\n","  if (save2file == True):\n","    # Save graph to file.\n","    models_names_ls = [x[:2] for x in model_names_ls]\n","    models_names_str = ''.join(models_names_ls)\n","    plot_filename = f'plot_cruxes_{semantic_type}_{models_names_str}_{models_names_str}.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return crux_points_dt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUNMIlJKHyz3"},"outputs":[],"source":["def plot_histogram(model_name='vader', text_unit='sentence', save2file=False):\n","  '''\n","  Given a model, text_unit\n","  Plot a Histogram using the default DataFrame\n","  '''\n","\n","  if text_unit == 'sentence':\n","    ts_df = corpus_sents_df\n","\n","  elif text_unit == 'paragraph':\n","    ts_df = corpus_parags_df\n","\n","  elif text_unit == 'section':\n","    ts_df = corpus_sects_df\n","\n","  elif text_unit == 'chapter':\n","    ts_df = corpus_chaps_df\n","\n","  else:\n","    print(f'ERROR: {text_unit} must be sentence, paragraph or section')\n","\n","  sns.histplot(ts_df[model_name], kde=True).set_title(f'{CORPUS_FULL} \\n Histogram {text_unit.capitalize()} Sentiment (Model {model_name.capitalize()})')\n","  # get_smas(ts_df, model_name=model_name, text_unit=text_unit, win_ls=wins_def_ls)\n","\n","  if (save2file == True):\n","    # Save graph to file.\n","    plot_filename = f'plot_hist_{text_unit}_{model_name}.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGTkfsSWFCeO"},"outputs":[],"source":["# Raw Plot of Section Sentiments (Not scaled by mid-Section Sentence No to match Sentence/Paragraph x-axes)\n","\n","def plot_raw_sections(ts_df='corpus_sents_df', model_name='vader', semantic_type='sentence', save2file=False):\n","  '''\n","  Given a DataFrame, model_name column, semantic_type \n","  Plot the raw sentiment types\n","  Options to save2file\n","  ''' \n","  \n","  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.lineplot(data=ts_df, x='sect_no', y=model_name, alpha=0.5).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n","\n","  if save2file == True:\n","    # Save graph to file.\n","    plot_filename = f'plot_nostand_sects_{model_name}.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return\n","\n","# Test\n","# plot_raw_sections(ts_df=corpus_sects_df, model_name='pattern', semantic_type='section', save2file=False);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FmrtifYoIjOT"},"outputs":[],"source":["# Raw Plot of Section Sentiments (Not scaled by mid-Section Sentence No to match Sentence/Paragraph x-axes)\n","\n","def plot_raw_sentiments(model_name='vader', semantic_type='sentence', save2file=False):\n","  '''\n","  Given a DataFrame, model_name column, semantic_type \n","  Plot the raw sentiment types\n","  Options to save2file\n","  ''' \n","  \n","  if semantic_type == 'sentence':\n","    ts_df = corpus_sents_df\n","    x_units = 'sent_no'\n","  elif semantic_type == 'paragraph':\n","    ts_df = corpus_parags_df\n","    x_units = 'parag_no'\n","  elif (semantic_type == 'section') | (semantic_type == 'section_stand'):\n","    ts_df = corpus_sects_df\n","    x_units = 'sect_no'\n","  elif (semantic_type == 'chapter') | (semantic_type == 'chapter_stand'):\n","    ts_df = corpus_chaps_df\n","    x_units = 'chap_no'\n","    \n","  else:\n","    print(f'ERROR: {semantic_type} must be sentence, paragraph or section')\n","\n","\n","  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.lineplot(data=ts_df, x=x_units, y=model_name, alpha=0.5, label=model_name).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n","  \n","  plt.legend(loc='best')\n","\n","  if save2file == True:\n","    # Save graph to file.\n","    plot_filename = f'plot_raw_sentiments_{semantic_type}_{model_name}.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return\n","\n","# Test\n","# plot_raw_sections(ts_df=corpus_sects_df, model_name='pattern', semantic_type='section', save2file=False);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iB8ibstaeiVd"},"outputs":[],"source":["# TODO: must plot in order to save, cannot save without first plotting\n","\n","def get_lowess(ts_df='corpus_parags_df', models_ls=MODELS_LS, text_unit='paragraph', plot_subtitle='', alabel='', afrac=1./10, ait=5, alpha=0.5, do_plot=True, save2file=False):\n","  '''\n","  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n","  Return a DataFrame with LOWESS values\n","  If 'plot=True', also output plot\n","  '''\n","\n","  # global corpus_all_df\n","\n","  lowess_df = pd.DataFrame()\n","\n","  # Step 1: Calculate LOWESS smoothed values\n","  for i,acol in enumerate(models_ls):\n","    sm_x, sm_y = sm_lowess(endog=ts_df[acol].values, exog=ts_df.index.values, frac=afrac, it=ait, return_sorted = True).T\n","    col_new = f'{acol}_lowess'\n","    lowess_df[col_new] = pd.Series(sm_y)\n","    # Optionally plot LOWESS for all models\n","    if do_plot:\n","      if alabel == '':\n","        alabel == acol\n","      plt.plot(sm_x, sm_y, label=alabel, alpha=alpha, linewidth=2)\n","\n","  lowess_df['median'] = lowess_df.median(axis=1) # sm_y # corpus_all_df[df_cols_ls].median(axis=1)\n","  \n","  # Step 2: Optionally plot LOWESS for median\n","  if do_plot:\n","    # sm_x, sm_y = sm_lowess(endog=lowess_df.median, exog=lowess_df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n","    # plt.plot(sm_x, sm_y, label='median', alpha=0.9, linewidth=2, color='black')\n","    \n","    frac_str = str(round(100*afrac))\n","    plt.title(f'{CORPUS_FULL} \\n {plot_subtitle} {text_unit} Standardized Sentiment Smoothed with LOWESS (frac={frac_str})')\n","    plt.legend(title='Sentiment Model')\n","\n","  # Step 3: Optionally save to file\n","  if save2file:\n","    # Save Plot to file.\n","    plot_filename = f'plot_{text_unit}_lowess_{plot_subtitle.split()[0].lower()}_{author_str}_{title_str}.png'\n","    # plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plot_filename, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","\n","  return lowess_df\n","\n","# Test\n","'''\n","new_lowess_col = f'{sa_model}_lowess'\n","my_frac = 1./10\n","my_frac_per = round(100*my_frac)\n","new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n","corpus_all_df[new_lowess_col] = plot_lowess(corpus_all_df, [sa_model], afrac=my_frac)\n","corpus_all_df.head()\n","''';"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cANCC2iz6nwo"},"outputs":[],"source":["def get_sent2dets(sent_no):\n","  '''\n","  Given a Sentence Number\n","  Return the corresponding Paragraph, Section and Chapter Numbers that contain it\n","  '''\n","\n","  # Get Paragraph No containing given Sentence No\n","  sent_parag_no = int(corpus_sents_df[corpus_sents_df['sent_no']==sent_no]['parag_no'])\n","\n","  # Get Section No containing given Sentence No.\n","  corpus_sects_ls = list(corpus_sects_df['sect_no'])\n","  for asect_no in corpus_sects_ls:\n","    if (int(corpus_sects_df[corpus_sects_df['sect_no'] == asect_no]['sent_no_start']) > sent_no):\n","      break\n","    sent_sect_no = asect_no\n","    # print(f'asect={asect_no}')\n","\n","  # Get Chapter No containing given Sentence No.\n","  corpus_chaps_ls = list(corpus_chaps_df['chap_no'])\n","  for achap_no in corpus_chaps_ls:\n","    if (int(corpus_chaps_df[corpus_chaps_df['chap_no'] == achap_no]['sent_no_start']) > sent_no):\n","      break\n","    sent_chap_no = achap_no\n","    # print(f'achap={achap_no}')\n","\n","\n","  return sent_parag_no, sent_sect_no, sent_chap_no\n","\n","# Test\n","# sent_parag_no, sent_sect_no, sent_chap_no = get_sent2dets(1408)\n","# print(f'sent_parag_no={sent_parag_no}\\nsent_sect_no={sent_sect_no}\\nsent_chap_no={sent_chap_no}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6liffwhYtSw"},"outputs":[],"source":["# get_sentnocontext_report(the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sijR4OknJive"},"outputs":[],"source":["def get_sentnocontext(sent_no=1, n_sideparags=1, sent_highlight=True):\n","  '''\n","  Given a sentence number in the Corpus\n","  Return the containing paragraph and n-paragraphs on either side\n","  (e.g. if n=2, return 2+1+2=5 paragraphs)\n","  '''\n","\n","  parag_target_no = int(corpus_sents_df[corpus_sents_df['sent_no'] == sent_no]['parag_no'])\n","  # print(f'parag_target_no = {parag_target_no} and type: {type(parag_target_no)}')\n","\n","  if n_sideparags == 0:\n","    parags_context_ls = list(corpus_parags_df[corpus_parags_df['parag_no'] == parag_target_no]['parag_raw'])\n","\n","  else:\n","    parag_start = parag_target_no - n_sideparags\n","    parag_end = parag_target_no + n_sideparags + 1\n","    parags_context_ls = list(corpus_parags_df.iloc[parag_start:parag_end]['parag_raw'])\n","\n","\n","  if sent_highlight == True:\n","    parag_match_str = str(parags_context_ls[n_sideparags])\n","    # print(f'parag_match_str:\\n  {parag_match_str}')\n","    sent_idx = sent_no\n","    sent_str = (corpus_sents_df[corpus_sents_df['sent_no']==sent_idx]['sent_raw'].values)[0]\n","    sent_str_up = sent_str.upper()\n","    # print(f'sent_str:\\n  {sent_str}')\n","    # parags_context_ls[n_sideparags] \n","    parags_context_ls[n_sideparags] = parag_match_str.replace(sent_str, sent_str_up)\n","\n","  return parags_context_ls\n","\n","# Te\n","# context_highlighted = get_sentnoparags(sent_no=1051, n_sideparags=1)\n","# print(context_highlighted)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zM_I8uDfJztH"},"outputs":[],"source":["def get_sentnocontext_report(the_sent_no=7, the_n_sideparags=1, the_sent_highlight=True):\n","  '''\n","  Wrapper function around  get_sentnocontext()\n","  Prints a nicely formatted context report\n","  '''\n","\n","  context_noparags = the_n_sideparags*2+1\n","\n","  # print('-------------------------------------------------------------')\n","  print(f'The {context_noparags} Paragraph(s) Context around the Sentence #{the_sent_no} Crux Point:')\n","  print('-------------------------------------------------------------')\n","  print(f\"\\nCrux Sentence #{the_sent_no} Raw Text: -------------------------------\\n\\n    {str(corpus_sents_df[corpus_sents_df['sent_no'] == the_sent_no]['sent_raw'].values[0])}\\n\") # iloc[the_sent_no]['sent_raw']}\")\n","\n","  sent_parag_no, sent_sect_no, sent_chap_no = get_sent2dets(the_sent_no)\n","  print(f\"\\nCrux Sentence #{the_sent_no} is Contained in: ---------------------------\\n\\n    Paragraph #{sent_parag_no}\\n      Section #{sent_sect_no}\\n      Chapter #{sent_chap_no}\\n\")\n","\n","  print(f\"\\n{context_noparags} Paragraph(s) Context: ------------------------------\")\n","  context_parags_ls = get_sentnocontext(sent_no=the_sent_no, n_sideparags=the_n_sideparags, sent_highlight=the_sent_highlight)\n","  context_len = len(context_parags_ls)\n","  context_mid = context_len//2\n","  for i, aparag in enumerate(context_parags_ls):\n","    if i==context_mid:\n","      # print(f'\\n>>> Paragraph #{i}: <<< Crux Point Sentence CAPITALIZED within this Paragraph\\n\\n    {aparag}')\n","      print(f'\\n<*> {aparag}')\n","    else:\n","      # print(f'\\n    Paragraph #{i}:\\n\\n    {aparag}')\n","      print(f'\\n    {aparag}')\n","\n","  return\n","\n","# Test\n","# get_sentnocontext_report(sent_no=1051, n_sideparags=1, sent_highlight=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y04GiohGypNX"},"outputs":[],"source":["def get_section_timeseries(sect_no):\n","  '''\n","  Given a Section No in the current Corpus\n","  Return the start,mid and ending Sent No for this Section as well as the Sentiment Time Series between the start/end Sentence for this Section\n","  '''\n","  \n","  section_count = corpus_sects_df.shape[0]\n","\n","  # Compute the start, mid and end Sentence numbers for the selected Section\n","  if Select_Section_No >= section_count:\n","    print(f'ERROR: You picked Section #{Select_Section_No}.\\n  Section for this Corpus must be between 0 and {section_count-1}')\n","    return -1\n","\n","  else:\n","\n","    # Get the starting and middle Sentence No of this Section\n","    sect_sent_start = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No]['sent_no_start'].values)\n","    # sect_sent_mid = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No]['sent_no_mid'].values)\n","\n","    # Calculate last Sentence No of this Section\n","    if Select_Section_No == (section_count-1):   \n","      print(f'You selected the last Section of this Corpus')\n","      sect_sent_end = corpus_sents_df.shape[0] - 1\n","    else:\n","      sect_sent_end = int(corpus_sects_df[corpus_sects_df['sect_no'] == Select_Section_No+1]['sent_no_start'].values) # - 1\n","      \n","    print(f'Section #{sect_no}:----------')\n","    print(f'\\nsect_sent_start: {sect_sent_start}')\n","    # print(f'sect_sent_mid: {sect_sent_mid}')\n","    print(f'sect_sent_end: {sect_sent_end}')\n","\n","\n","  # Comput the start, and end Paragraph numbers for the selected Section\n","  sect_parag_start = int(corpus_sents_df[corpus_sents_df['sent_no'] == sect_sent_start]['parag_no'].values)\n","  sect_parag_end = int(corpus_sents_df[corpus_sents_df['sent_no'] == sect_sent_end]['parag_no'].values)\n","\n","  print(f'\\nsect_parag_start: {sect_parag_start}')\n","  print(f'sect_parag_end: {sect_parag_end}')\n","\n","\n","  # Extract and Return both a Sentence and Paragraph DataFrame for this Section \n","\n","  section_sents_df = corpus_sents_df.iloc[sect_sent_start:sect_sent_end]\n","\n","  section_parags_df = corpus_parags_df.iloc[sect_parag_start:sect_parag_end]\n","\n","\n","  return section_sents_df, section_parags_df\n","\n","# Test\n","\n","# section_sents_df, section_parags_df = get_section_timeseries(Select_Section_No)\n","\n","# section_sents_df.head()\n","\n","# print(f'\\nsection_sents_df.shape: {section_sents_df.shape}')\n","# print(f'section_parags_df.shape: {section_parags_df.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Et56z3-Jr1E"},"outputs":[],"source":["\"\"\"\n","\n","\n","def get_crux_points(col_series, semantic_type='sentence', win_lowess=5, do_plot=True, save2file=False):\n","  '''\n","  Given a DataFrame and a Time Series Column within it and a LOWESS window\n","  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n","  '''\n","\n","  crux_ls = []\n","\n","  if semantic_type == 'sentence':\n","    ts_df = corpus_sents_df\n","    x_units = 'sent_no'\n","  elif semantic_type == 'paragraph':\n","    ts_df = corpus_parags_df\n","    x_units = 'parag_no'\n","  elif (semantic_type == 'section') | (semantic_type == 'section_stand'):\n","    ts_df = corpus_sects_df\n","    x_units = 'sect_no'\n","  elif (semantic_type == 'chapter') | (semantic_type == 'chapter_stand'):\n","    ts_df = corpus_chaps_df\n","    x_units = 'chap_no'\n","    \n","  else:\n","    print(f'ERROR: {semantic_type} must be sentence, paragraph or section')\n","\n","\n","\n","  series_len = ts_df.shape[0]\n","\n","  series_no_min = ts_df[x_units].min()\n","  seires_no_max = ts_df[x_units].max()\n","\n","  sm_x = ts_df.index.values\n","  sm_y = ts_df[col_series].values\n","\n","  half_win = int((win_lowess/100)*series_len)\n","\n","  # Find peaks(max).\n","  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n","  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n","  peak_indexes = peak_indexes[0]\n","\n","  peak_x_ls = list(peak_indexes)\n","  peak_y_ls = list(sm_y[peak_indexes])\n","\n","  # Find valleys(min).\n","  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n","  valley_indexes = signal.find_peaks(-sm_y, distance=half_win) # np.less, order=half_win, mode='clip')\n","  valley_indexes = valley_indexes[0]\n","  \n","  valley_x_ls = list(valley_indexes)\n","  valley_y_ls = list(sm_y[valley_indexes])\n","\n","  # Save all peaks/valleys as list of (x,y) coordinate tuples\n","  print(f'type peak_x_ls is: {type(peak_x_ls)}')\n","  x_all_ls = peak_x_ls + valley_x_ls\n","  y_all_ls = peak_y_ls + valley_y_ls\n","  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n","\n","  print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n","\n","\n","  if do_plot == True:\n","    # Plot main graph.\n","    (fig, ax) = plt.subplots()\n","    ax.plot(sm_x, sm_y)\n","\n","    win_half = 0 # 2500\n","\n","    # Plot peaks.\n","    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n","    ax.scatter(peak_x_ls, peak_y_ls)\n","    for i, txt in enumerate(list(peak_x_ls)):\n","        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n","\n","    # Plot valleys.\n","    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n","    ax.scatter(valley_x_ls, valley_y_ls)\n","    for i, txt in enumerate(list(valley_x_ls)):\n","        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n","\n","    # for i, txt in enumerate(list(valley_x_ls)):\n","    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n","    # plt.plot(x, y, 'bo')\n","    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n","    # adjust_text(texts)\n","\n","    # Confidence Interval (Min/Max Range)\n","    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n","\n","    plt.title(f'{CORPUS_FULL}\\nRaw Sentence Sentiments with selected Section #{Select_Section_No}')\n","    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n","\n","    # locs, labels = xticks()  # Get the current locations and labels.\n","    plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n","\n","    plt.ylabel(f'Sentiment Value')\n","    plt.legend(loc='best');\n","  \n","  if save2file == True:\n","    # Save graph to file.\n","    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n","    plt.legend(loc='best')\n","    plt.savefig('argrelextrema.png')\n","\n","  return crux_coord_ls\n","\n","\n","  # if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.lineplot(data=ts_df, x=x_units, y=model_name, alpha=0.5, label=model_name).set_title(f'{CORPUS_FULL} \\n Plot {semantic_type} Sentiment (Raw {model_name.capitalize()})')\n","  \n","  plt.legend(loc='best')\n","\n","  if save2file == True:\n","    # Save graph to file.\n","    plot_filename = f'plot_raw_sentiments_{semantic_type}_{model_name}.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return\n","\n","\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgFMgdQ3X33F"},"outputs":[],"source":["\"\"\"\n","def get_lowess_cruxes(ts_df, col_series, text_type='sentence', win_lowess=5, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n","  '''\n","  Given a DataFrame and a Time Series Column within it and a LOWESS window\n","  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n","  '''\n","\n","  crux_ls = []\n","\n","  series_len = ts_df.shape[0]\n","\n","  sent_no_min = ts_df.sent_no.min()\n","  sent_no_max = ts_df.sent_no.max()\n","  # print(f'sent_no_min {sent_no_min}')\n","\n","  sm_x = ts_df.index.values\n","  sm_y = ts_df[col_series].values\n","\n","  half_win = int((win_lowess/100)*series_len)\n","\n","  # Find peaks(max).\n","  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n","  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n","  # peak_indexes = peak_indexes + sent_no_min\n","  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n","  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n","  # peak_indexes_np = peak_indexes_np + sent_no_min\n","  peak_indexes = peak_indexes[0]\n","\n","  peak_x_ls = list(peak_indexes)\n","  peak_y_ls = list(sm_y[peak_indexes])\n","\n","  # Find valleys(min).\n","  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n","  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n","  valley_indexes = valley_indexes[0]\n","  \n","  valley_x_ls = list(valley_indexes)\n","  valley_y_ls = list(sm_y[valley_indexes])\n","\n","  # Save all peaks/valleys as list of (x,y) coordinate tuples\n","  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n","  x_all_ls = peak_x_ls + valley_x_ls\n","  # readjust starting Sentence No to start with first sentence in segement window\n","  x_all_ls = [x+sent_no_min for x in x_all_ls]\n","  y_all_ls = peak_y_ls + valley_y_ls\n","  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n","\n","  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n","\n","\n","  if do_plot == True:\n","    # Plot main graph.\n","    (fig, ax) = plt.subplots()\n","    ax.plot(sm_x, sm_y)\n","\n","    if text_type == 'sentence':\n","      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n","      for i, aparag in enumerate(paragraph_boundries_ls):\n","        if i%5 == 0:\n","          # Plot every 5th paragraph\n","          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n","          plt.text(sent_no, sec_y_height, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n","          plt.axvline(sent_no, color='blue', alpha=0.1)\n","    elif text_type == 'paragraph':\n","      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n","      for i, aparag_no in enumerate(paragraph_boundries_ls):\n","        if i%5 == 0:\n","          # Plot every 5th paragraph\n","          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n","          plt.text(aparag_no, sec_y_height, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n","          plt.axvline(aparag_no, color='blue', alpha=0.1)    \n","    else:\n","      print(f\"ERROR: text_type is {text_type} but must be either 'sentence' or 'paragarph'\")\n","\n","    win_half = 0 # 2500\n","\n","    # Plot peaks.\n","    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n","\n","    # readjust starting Sentence No to start with first sentence in segement window\n","    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n","    ax.scatter(peak_x_ls, peak_y_ls)\n","    for i, txt in enumerate(list(peak_x_ls)):\n","        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n","\n","    # Plot valleys.\n","    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n","    # readjust starting Sentence No to start with first sentence in segement window\n","    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n","    ax.scatter(valley_x_ls, valley_y_ls)\n","    for i, txt in enumerate(list(valley_x_ls)):\n","        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n","\n","    # for i, txt in enumerate(list(valley_x_ls)):\n","    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n","    # plt.plot(x, y, 'bo')\n","    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n","    # adjust_text(texts)\n","\n","    # Confidence Interval (Min/Max Range)\n","    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n","\n","    plt.title(f'{CORPUS_FULL} Raw Sentence Crux Detection in Section #{Select_Section_No}\\nLOWESS Smoothed {subtitle_str} and SciPy find_peaks')\n","    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n","\n","    # locs, labels = xticks()  # Get the current locations and labels.\n","    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n","\n","    plt.ylabel(f'Sentiment Value')\n","    plt.legend(loc='best');\n","  \n","  if save2file == True:\n","    # Save graph to file.\n","    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n","    plt.legend(loc='best')\n","    plt.savefig('argrelextrema.png')\n","\n","  return crux_coord_ls\n","  \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FAPVrHeyeX7T"},"outputs":[],"source":["\"\"\"\n","def get_crux_points(ts_df, col_series, text_type='sentence', win_per=5, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n","  '''\n","  Given a DataFrame and a Time Series Column within it and a LOWESS window\n","  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n","  '''\n","\n","  crux_ls = []\n","\n","  series_len = ts_df.shape[0]\n","\n","  sent_no_min = ts_df.sent_no.min()\n","  sent_no_max = ts_df.sent_no.max()\n","  # print(f'sent_no_min {sent_no_min}')\n","\n","  sm_x = ts_df.index.values\n","  sm_y = ts_df[col_series].values\n","\n","  half_win = int((win_per/100)*series_len)\n","\n","  # Find peaks(max).\n","  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n","  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n","  # peak_indexes = peak_indexes + sent_no_min\n","  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n","  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n","  # peak_indexes_np = peak_indexes_np + sent_no_min\n","  peak_indexes = peak_indexes[0]\n","\n","  peak_x_ls = list(peak_indexes)\n","  peak_y_ls = list(sm_y[peak_indexes])\n","\n","  # Find valleys(min).\n","  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n","  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n","  valley_indexes = valley_indexes[0]\n","  \n","  valley_x_ls = list(valley_indexes)\n","  valley_y_ls = list(sm_y[valley_indexes])\n","\n","  # Save all peaks/valleys as list of (x,y) coordinate tuples\n","  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n","  x_all_ls = peak_x_ls + valley_x_ls\n","  # readjust starting Sentence No to start with first sentence in segement window\n","  x_all_ls = [x+sent_no_min for x in x_all_ls]\n","  y_all_ls = peak_y_ls + valley_y_ls\n","  crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n","\n","  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n","\n","\n","  if do_plot == True:\n","    # Plot main graph.\n","    (fig, ax) = plt.subplots()\n","    ax.plot(sm_x, sm_y)\n","\n","    if text_type == 'sentence':\n","      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n","      for i, aparag in enumerate(paragraph_boundries_ls):\n","        if i%5 == 0:\n","          # Plot every 5th paragraph\n","          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n","          plt.text(sent_no, sec_y_height, f'Paragraph #{aparag}', alpha=0.2, rotation=90)\n","          plt.axvline(sent_no, color='blue', alpha=0.1)\n","    elif text_type == 'paragraph':\n","      paragraph_boundries_ls = list(section_sents_df['parag_no'].unique())\n","      for i, aparag_no in enumerate(paragraph_boundries_ls):\n","        if i%5 == 0:\n","          # Plot every 5th paragraph\n","          sent_no = section_sents_df[section_sents_df['parag_no'] == aparag]['sent_no'].min()\n","          plt.text(aparag_no, sec_y_height, f'Paragraph #{aparag_no}', alpha=0.2, rotation=90)\n","          plt.axvline(aparag_no, color='blue', alpha=0.1)    \n","    else:\n","      print(f\"ERROR: text_type is {text_type} but must be either 'sentence' or 'paragarph'\")\n","\n","    win_half = 0 # 2500\n","\n","    # Plot peaks.\n","    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n","\n","    # readjust starting Sentence No to start with first sentence in segement window\n","    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n","    ax.scatter(peak_x_ls, peak_y_ls)\n","    for i, txt in enumerate(list(peak_x_ls)):\n","        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n","\n","    # Plot valleys.\n","    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n","    # readjust starting Sentence No to start with first sentence in segement window\n","    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n","    ax.scatter(valley_x_ls, valley_y_ls)\n","    for i, txt in enumerate(list(valley_x_ls)):\n","        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n","\n","    # for i, txt in enumerate(list(valley_x_ls)):\n","    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n","    # plt.plot(x, y, 'bo')\n","    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n","    # adjust_text(texts)\n","\n","    # Confidence Interval (Min/Max Range)\n","    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n","\n","    plt.title(f'{CORPUS_FULL} Raw Sentence Crux Detection in Section #{Select_Section_No}\\nLOWESS Smoothed {subtitle_str} and SciPy find_peaks')\n","    plt.xlabel(f'Sentence No within selected Section #{Select_Section_No}')\n","\n","    # locs, labels = xticks()  # Get the current locations and labels.\n","    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n","\n","    plt.ylabel(f'Sentiment Value')\n","    plt.legend(loc='best');\n","  \n","  if save2file == True:\n","    # Save graph to file.\n","    plt.title(f'{BOOK_TITLE_FULL} \\n LOWESS Smoothed Median Sentiment Curve with Crux Points via SciPy.argrelextrema')\n","    plt.legend(loc='best')\n","    plt.savefig('argrelextrema.png')\n","\n","  return crux_coord_ls\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yv376c5_bfrg"},"outputs":[],"source":["def crux_sortsents(crux_ls, atop_n=3, get_peaks=True, sort_key='sentiment_val'):\n","  '''\n","  Given a list of tuples (sent_no, sentiment value), atop_n cruxes to retrieve and bool flag get_peaks\n","  Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n","  '''\n","  # print(f'Entered crux_sortsents with crux_ls={crux_ls}\\natop_n={atop_n}')\n","\n","  crux_sorted_ls = []\n","  crux_sorted_topn_ls = []\n","  crux_new_ls = []\n","\n","  # Sort by either sent_no or sentiment value\n","  if sort_key == 'sent_no':\n","    crux_sorted_ls = sorted(crux_ls, key=lambda tup: (tup[0]))\n","  else:\n","    crux_sorted_ls = sorted(crux_ls, key=lambda tup: (tup[1]), reverse=get_peaks)\n","\n","  # Trim the n_top cruxes if more cruxes than requested, else return all found cruxes\n","  if (len(crux_sorted_ls) >= atop_n):\n","    crux_sorted_topn_ls = crux_sorted_ls[:atop_n]\n","  else:\n","    crux_sorted_topn_ls = crux_sorted_ls\n","\n","  # Retrieve the Sentence raw text for each Crux and add as Tuple(sent_no, sentiment_val, raw_text) to return List\n","  for asent_no, asentiment_val in crux_sorted_topn_ls:\n","    asent_raw = str(corpus_sents_df[corpus_sents_df['sent_no'] == asent_no]['sent_raw'].values[0])\n","    crux_new_ls.append((int(asent_no), float(f'{asentiment_val:.3f}'), str(asent_raw),)) # Append a Tuple to return List\n","\n","  return crux_new_ls\n","\n","# Test\n","# crux_n_top_ls = crux_sortsents(section_crux_ls, atop_n=3, get_peaks=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFDjK1o8gnQ0"},"outputs":[],"source":["def crux_sortsents_report(crux_ls, library_type='baseline', top_n=3, get_peaks=True, sort_by='sentiment_val', n_sideparags=1, sentence_highlight=True):\n","  '''\n","  Wrapper function to produce report based upon 'crux_sortsents() described as:\n","    Given a list of tuples (sent_no, sentiment value), top_n cruxes to retrieve and bool flag get_peaks\n","    Return a sorted list of peaks/valleys (sentiment_value, sent_no, sent_raw) from greatest down for top_n items\n","\n","    # get_sentnocontext_report\n","  '''\n","\n","  if get_peaks == True:\n","    crux_label = 'Peak'\n","  else:\n","    crux_label = 'Valley'\n","\n","  # Filter and keep only the desired crux type in List crux_subset_ls\n","  crux_subset_ls = []\n","  for acrux_tup in crux_ls:\n","    crux_type, crux_x_coord, crux_y_coord = acrux_tup\n","    if crux_type.lower() == crux_label.lower():\n","      crux_subset_ls.append((crux_x_coord,crux_y_coord)) # Append a Tuple to List\n","\n","  flag_2few_cruxes = False\n","\n","  # Check to see if asked for more Cruxes than were found \n","  top_n_found = len(crux_subset_ls)\n","  if top_n_found < top_n:\n","    flag_2few_cruxes = True\n","    print(f'\\n\\nWARNING: You asked for {top_n} {crux_label}s\\n         but there only {top_n_found} were found above.\\n')\n","    print(f'             Displaying as many {crux_label}s as possible,')\n","    print(f'             to retrieve more, go back to the previous code cells and re-run with wider Crux Window.\\n\\n')\n","\n","\n","  # Get Sentence no and raw text for appropriate Crux subset\n","  # print(f'Calling crux_n_top_ls with crux_subset_ls={crux_subset_ls}\\ntop_n={top_n}\\nget_peaks={get_peaks}')\n","  crux_n_top_ls = crux_sortsents(crux_ls=crux_subset_ls, atop_n=top_n, get_peaks=get_peaks, sort_key=sort_by)\n","  # print(f'Returning crux_n_top_ls = {crux_n_top_ls}')\n","\n","  # Print appropriate header\n","  print('------------------------------')\n","  # print(f'library_type: {library_type}')\n","  if library_type in ['baselines','sentimentr','syuzhetr','transformers']:\n","    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n","      print(f'Library: {library_type.capitalize()} ALL Top {top_n} {crux_label}s Found\\n')\n","    else:\n","      print(f'Library #{library_type.capitalize()} ONLY Top {top_n_found} {crux_label}s Found\\n')\n","  else:\n","    if (sort_by != 'sent_no') & (flag_2few_cruxes == False):\n","      print(f'Section #{Select_Section_No} ALL Top {top_n} {crux_label}s Found\\n')\n","    else:\n","      print(f'Section #{Select_Section_No} ONLY Top {top_n_found} {crux_label}s Found\\n')\n","\n","  # Print summary of subset Cruxes\n","  for i,crux_sent_tup in enumerate(crux_n_top_ls):\n","    # crux_type, crux_x_coord, crux_y_coord = crux_sent_tup\n","    crux_x_coord, crux_y_coord, sent_txt = crux_sent_tup\n","    print(f'   {crux_label} #{i} at Sentence #{crux_x_coord} with Sentiment Value {crux_y_coord}')\n","  # print('------------------------------\\n')\n","  # print('Sent_No  Sentiment   Sentence (Raw Text)\\n')\n","  \n","  # Print details of each Crux in subset\n","  for sent_no, sent_pol, sent_txt in crux_n_top_ls: \n","    sent_no = int(sent_no)\n","    print('\\n\\n-------------------------------------------------------------')\n","    print(f'Sentence #{sent_no}   Sentiment: {sent_pol:.3f}\\n') #     {sent_txt}\\n')\n","    # print('------------------------------')\n","    get_sentnocontext_report(the_sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n","    # get_sentnocontext(sent_no=sent_no, the_n_sideparags=n_sideparags, the_sent_highlight=sentence_highlight)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVQGritEnYSy"},"outputs":[],"source":["library_type='syuzhetr'\n","if library_type in ['baseline','sentimentr','syuzhetr','transformers']:\n","  print(\"It is IN\")\n","else:\n","  print(\"BOO\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJw2WDlwHH5y"},"outputs":[],"source":["# For the selected Section, create an expanded Paragraph DataFrame to match the number of Sentences in the Section\n","\n","def expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df', model_name='vader_lnorm_medianiqr'):\n","  '''\n","  Given a Corpus Paragraph DataFrame and a longer Sentence DataFrame that cover the same Section of a Corpus\n","  Return an expanded version of the Paragraph DataFrame of equal length to the Sentence DataFrame so they can be plotted/compared along the same x-axis\n","  '''\n","\n","  parag_sentiment_expanded_ls = []\n","  parags_midpoint_ls = []\n","  sent_sum = 0\n","  parag_start = section_parags_df.parag_no.min()\n","  print(f'parag_start: {parag_start}')\n","  parag_end = section_parags_df.parag_no.max() + 1 # shape[0] + 3\n","  print(f'parag_end: {parag_end}')\n","  parags_range_ls = list(range(parag_start, parag_end, 1))\n","  print(f'parags_range_ls: {parags_range_ls}')\n","  for i, aparag_no in enumerate(parags_range_ls):\n","    aparag_sentiment_fl = float(corpus_parags_df[corpus_parags_df['parag_no']==aparag_no][model_name])\n","    sent_ct = len(corpus_sents_df[corpus_sents_df.parag_no == aparag_no])\n","    parag_midpoint_int = int(sent_ct//2 + sent_sum)\n","    parags_midpoint_ls.append(parag_midpoint_int)\n","    for asent in range(sent_ct):\n","      parag_sentiment_expanded_ls.append(aparag_sentiment_fl)\n","    sent_sum += sent_ct\n","    print(f'#{i}: Paragraph #{aparag_no} has {sent_ct} Sentences and Avg Sentiment: {aparag_sentiment_fl:.3f}')\n","\n","  print(f'\\nSentence Total: {sent_sum} vs Original section_sents_df: {section_sents_df.shape[0]}')\n","  print(f'  Paragraph Sentiment length: {len(parag_sentiment_expanded_ls)}')\n","\n","  # section_sents_parags_df = section_sents_df.copy()\n","  \n","  # section_sents_parags_df.head(1);\n","\n","  # corpus_sents_df['']\n","\n","  return parag_sentiment_expanded_ls, parags_midpoint_ls\n","\n","# Test\n","# section_sents_df['vader_lnorm_medianiqr_parag'] = expand_parags2sents(parags_df='corpus_parags_df', sents_df='corpus_sents_df')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXRq54NQwI7D"},"outputs":[],"source":["def get_crux_points(ts_df, col_series, text_type='sentence', win_per=5, sec_y_height=0, subtitle_str=' ', do_plot=True, save2file=False):\n","  '''\n","  Given a DataFrame and a Time Series Column within it and a LOWESS window\n","  Return a list of Min/Max Crux Point (x,y) coordinate tuples for that Column Time Series\n","  '''\n","\n","  # print('entered get_crux_points')\n","  crux_ls = []\n","\n","  series_len = ts_df.shape[0]\n","  # print(f'series_len = {series_len}')\n","\n","  sent_no_min = ts_df.sent_no.min()\n","  sent_no_max = ts_df.sent_no.max()\n","  # print(f'sent_no_min {sent_no_min}')\n","\n","  sm_x = ts_df.index.values\n","  sm_y = ts_df[col_series].values.flatten()\n","\n","  half_win = int((win_per/100)*series_len)\n","  # print(f'half_win = {half_win}')\n","  # print(f'sm_y type = {type(sm_y)}')\n","\n","  # Find peaks(max).\n","  # peak_indexes = signal.argrelextrema(sm_y, np.greater, order=half_win, mode='wrap') argrelextrema will not detect flat peaks\n","  peak_indexes = signal.find_peaks(sm_y, distance=half_win) # np.greater, order=half_win, mode='wrap')\n","  # peak_indexes = peak_indexes + sent_no_min\n","  # print(f'peak_indexes[0]: {peak_indexes_np[0]}')\n","  # print(f'peak_indexes type: {type(peak_indexes_np[0])}')\n","  # peak_indexes_np = peak_indexes_np + sent_no_min\n","  # print(f'peak_indexes type = {type(peak_indexes)}')\n","  peak_indexes = peak_indexes[0]\n","\n","  peak_x_ls = list(peak_indexes)\n","  peak_x_adj_ls = [x+sent_no_min for x in peak_x_ls]\n","\n","  peak_y_ls = list(sm_y[peak_indexes])\n","\n","  peak_label_ls = ['peak'] * len(peak_y_ls)\n","  peak_coord_ls = tuple(zip(peak_label_ls, peak_x_adj_ls, peak_y_ls))\n","\n","  # peak_y_all_ls = peak_y_ls + valley_y_ls\n","  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n","\n","  # Find valleys(min).\n","  # valley_indexes = signal.argrelextrema(sm_y, np.less, order=half_win, mode='clip')\n","  valley_indexes = signal.find_peaks(-sm_y, distance=half_win)\n","  valley_indexes = valley_indexes[0]\n","  \n","  valley_x_ls = list(valley_indexes)\n","  valley_x_adj_ls = [x+sent_no_min for x in valley_x_ls]\n","\n","  valley_y_ls = list(sm_y[valley_indexes])\n","\n","  valley_label_ls = ['valley'] * len(valley_y_ls)\n","  valley_coord_ls = tuple(zip(valley_label_ls, valley_x_adj_ls, valley_y_ls))\n","\n","  # Combine Peaks and Valley Coordinates into List of Tuples(label, x_coord, y_coord)\n","  crux_coord_ls = peak_coord_ls + valley_coord_ls\n","\n","  # Save all peaks/valleys as list of (x,y) coordinate tuples\n","  # print(f'type peak_x_ls is: {type(peak_x_ls)}')\n","  #  x_all_ls = peak_x_ls + valley_x_ls\n","  # readjust starting Sentence No to start with first sentence in segement window\n","  #  x_all_ls = [x+sent_no_min for x in x_all_ls]\n","  #  y_all_ls = peak_y_ls + valley_y_ls\n","  # crux_coord_ls = tuple(zip(x_all_ls, y_all_ls)) \n","\n","  # print(f'Original Series length={series_len} vs LOWESS Series length={len(x_all_ls)}')\n","\n","\n","  if do_plot == True:\n","    # Plot main graph.\n","    (fig, ax) = plt.subplots()\n","    ax.plot(sm_x, sm_y)\n","\n","    section_sent_no_boundries_ls = list(corpus_sects_df['sent_no_start'])\n","    section_no_ls = list(corpus_sects_df['sect_no'])\n","    for i, asect_no in enumerate(section_sent_no_boundries_ls):\n","      # Plot vertical lines for section boundries\n","      plt.text(asect_no, sec_y_height, f'Section #{section_no_ls[i]}', alpha=0.2, rotation=90)\n","      plt.axvline(asect_no, color='blue', alpha=0.1)    \n","\n","\n","    win_half = 0 # 2500\n","\n","    # Plot peaks.\n","    # ax.plot(peak_x + win_half, peak_y, marker='o', linestyle='none', color='green', label=\"Peaks\")\n","\n","    # readjust starting Sentence No to start with first sentence in segement window\n","    peak_x_ls = [x+sent_no_min for x in peak_x_ls]\n","    ax.scatter(peak_x_ls, peak_y_ls)\n","    for i, txt in enumerate(list(peak_x_ls)):\n","        ax.annotate(f'  Sent #{txt}', (peak_x_ls[i], peak_y_ls[i]), rotation=90, annotation_clip=True)\n","\n","    # Plot valleys.\n","    # ax.plot(valley_x + win_half, valley_y, marker='o', linestyle='none', color='red', label=\"Valleys\")\n","    # readjust starting Sentence No to start with first sentence in segement window\n","    valley_x_ls = [x+sent_no_min for x in valley_x_ls]\n","    ax.scatter(valley_x_ls, valley_y_ls)\n","    for i, txt in enumerate(list(valley_x_ls)):\n","        ax.annotate(f'Sent #{txt}', (valley_x_ls[i], valley_y_ls[i]), rotation=270, annotation_clip=True) # xytext=(valley_x_ls[i], valley_y_ls[i]-4))\n","\n","    # for i, txt in enumerate(list(valley_x_ls)):\n","    #     ax.annotate(f'\\n\\n\\nSent No.\\n   {txt}', (valley_x_ls[i], valley_y_ls[i]))\n","    # plt.plot(x, y, 'bo')\n","    # texts = [plt.text(valley_x_ls[i], valley_y_ls[i], 'Sent No.\\n   %s' %valley_x_ls[i], ha='right', va='top') for i in range(len(valley_x_ls))]\n","    # adjust_text(texts)\n","\n","    # Confidence Interval (Min/Max Range)\n","    # plt.fill_between(sentiment_lowess_df['x_value'], sentiment_lowess_df['min'], sentiment_lowess_df['max'], alpha=.3, color='lightskyblue')\n","\n","    plt.title(f'{CORPUS_FULL} SMA Smoothed Sentence Sentiment Arcs Crux Detection\\n{subtitle_str} Models: {col_series}')\n","    plt.xlabel(f'Sentence No') # within selected Section #{Select_Section_No}')\n","\n","    # locs, labels = xticks()  # Get the current locations and labels.\n","    # plt.xticks(np.arange(sent_no_min, sent_no_max, step=10))  # Set label locations.\n","\n","    plt.ylabel(f'Sentiment Value')\n","    plt.legend(loc='best');\n","  \n","  if save2file == True:\n","    # Save graph to file.\n","    plt.title(f'{BOOK_TITLE_FULL} \\n SMA Smoothed Sentence Sentiment Arcs Crux Points')\n","    # plt.legend(loc='best')\n","    plt.savefig(f\"{CORPUS_FILENAME.split('.')[0]}_find_peaks.png\")\n","\n","  return crux_coord_ls;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNdJQGtghS_X"},"outputs":[],"source":["def get_standardscaler(series_name, values_ser):\n","  '''\n","  Given a Series of values\n","  Return a list of StandardSclar transformations on that input Series\n","  '''\n","\n","  scaler = StandardScaler()  \n","\n","  # Convert to np.array\n","  values_np = np.array(values_ser)\n","  \n","  values_flat_np = values_np.reshape((len(values_np), 1))\n","\n","  scaler = scaler.fit(values_flat_np)\n","  print(f'Model: {series_name}\\n       Mean: {scaler.mean_}, StandardDeviation: {np.sqrt(scaler.var_)}') # % (scaler.mean_, np.sqrt(scaler.var_)))\n","  values_flat_xform_np = scaler.transform(values_flat_np)\n","\n","  return values_flat_xform_np.flatten().tolist()\n","\n","# Test\n","# stdscaler_series_ls = get_standardscaler('vader_lnorm_medianiqr_roll100', corpus_sents_df['vader_lnorm_medianiqr_roll100'])\n","# corpus_sents_df['vader_roll100_stdscaler'] = pd.Series(stdscaler_series_ls)\n"]},{"cell_type":"markdown","metadata":{"id":"gY7L8hz0ocUi"},"source":["# **Preprocess and Review Corpus Text (Auto)**"]},{"cell_type":"markdown","metadata":{"id":"jQYjye-v9XMY"},"source":["### **Get Corpus by Sections, Chapters, Paragraphs and Sentences**"]},{"cell_type":"markdown","metadata":{"id":"Ll0GJs3u9dEq"},"source":["#### **Get Sections**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwiZTA-r6i-y"},"outputs":[],"source":["corpus_sects_ls, corpus_str_raw = corpus2sects(CORPUS_FILENAME)\n","\n","print('\\n\\nAFTER ----------')\n","print(f'len(corpus_sects_ls): {len(corpus_sects_ls)}')\n","print(\"\\n\\n-----\")\n","print(f'corpus_sects_ls[0]:\\n\\n    {corpus_sects_ls[0]}')\n","print(\"\\n\\n-----\")\n","print(f'corpus_sects_ls[1]:\\n\\n    {corpus_sects_ls[1]}')\n","print(\"\\n\\n-----\")\n","print(f'corpus_sects_ls[2]:\\n\\n    {corpus_sects_ls[2]}')\n","print(\"\\n\\n-----\")\n","print(f'corpus_sects_ls[-2]:\\n\\n    {corpus_sects_ls[-2]}')\n","print(\"\\n\\n-----\")\n","print(f'corpus_sects_ls[-1]:\\n\\n    {corpus_sects_ls[-1]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3ub-PM48554"},"outputs":[],"source":["len(corpus_str_raw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2l6eIDV6i8O"},"outputs":[],"source":["# Verify no CHAPTER headings remain\n","for i,aline in enumerate(corpus_sects_ls):\n","  if aline.strip().startswith('CHAPTER '):\n","    print(f'CHAPTER aline: {aline}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPfffj2i6i5y"},"outputs":[],"source":["print(f'len(corpus_sects_ls): {len(corpus_sects_ls)}')\n","print(corpus_sects_ls[0])"]},{"cell_type":"markdown","metadata":{"id":"JgL9-4Z59gND"},"source":["#### **Get Chapters**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TtV6MuLK9Osr"},"outputs":[],"source":["!ls -altr *.txt\n","!head -n 10 $corpus_filename "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDhm8w9g9Op2"},"outputs":[],"source":["corpus_chaps_ls, corpus_str_raw = corpus2chaps(corpus_filename)\n","\n","print('\\n\\nAFTER ----------')\n","print(f'len(corpus_chaps_ls): {len(corpus_chaps_ls)}')\n","print(\"\\n\\n-----\")\n","print(f'corpus_chaps_ls[0]:\\n\\n    {corpus_chaps_ls[0]}')\n","print(\"\\n\\n-----\")\n","print(f'corpus_chaps_ls[1]:\\n\\n    {corpus_chaps_ls[1]}')\n","print(\"\\n\\n-----\")\n","print(f'corpus_chaps_ls[2]:\\n\\n    {corpus_chaps_ls[2]}')\n","\"\"\"\n","print(\"\\n\\n-----\")\n","print(f'corpus_chaps_ls[-2]:\\n\\n    {corpus_chaps_ls[-2]}')\n","print(\"\\n\\n-----\")\n","print(f'corpus_chaps_ls[-1]:\\n\\n    {corpus_chaps_ls[-1]}')\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wueHJDqE-Ca7"},"outputs":[],"source":["# Verify Chapter and Section Counts\n","\n","print(f'CHAPTER Count: len(corpus_chaps_ls): {len(corpus_chaps_ls)}')\n","print(f'SECTION Count: len(corpus_sects_ls): {len(corpus_sects_ls)}')"]},{"cell_type":"markdown","metadata":{"id":"1N9PUl0V9orH"},"source":["#### **Get Paragraphs**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yz8tvxAc9OnA"},"outputs":[],"source":["# Read corpus into a single string then split into paragraphs\n","\n","corpus_parags_ls, corpus_raw_str = corpus2parags(CORPUS_FILENAME)\n","print(f'F ound #{len(corpus_parags_ls)} paragraphs\\n')\n","\n","print('\\nThe first 10 Paragraphs of the Corpus:')\n","print('-----------------------------------\\n')\n","corpus_parags_ls[:10]\n","\n","print('\\nThe last 10 Paragraphs of the Corpus:')\n","print('-----------------------------------\\n')\n","corpus_parags_ls[-10:]\n","print('\\n')\n","\n","n_shortest = 10\n","print(f'The {n_shortest} shortest Paragraphs in the Corpus are:')\n","print('--------------------------------------------')\n","temp_ls = sorted(corpus_parags_ls, key=lambda x: (len(x), x))\n","for i, asent in enumerate(temp_ls[:n_shortest]):\n","  print(f'Shortest Paragraph #{i}: {asent}')"]},{"cell_type":"markdown","metadata":{"id":"n1oOpMwK9qqj"},"source":["#### **Get Sentences**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JdbvVHsp9OiG"},"outputs":[],"source":["corpus_sents_ls = parag2sents(corpus_parags_ls)\n","\n","print(f'Found {len(corpus_sents_ls)} Sentences in Corpus\\n')\n","\n","print(f'    First List Object in Sentence List {corpus_sents_ls[0]}\\n')\n","\n","print(f'    Last List Object in Sentence List {corpus_sents_ls[-1]}\\n');\n","\n","print(f\"List Object format: ['sent_no', 'parag_no', 'sent_raw']\\n\")"]},{"cell_type":"markdown","metadata":{"id":"6JdQ3aZu_OGm"},"source":["### **Create DataFrames**"]},{"cell_type":"markdown","metadata":{"id":"sCGQN51f_qjB"},"source":["**Create Sentence DataFrame: [corpus_sents_df]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eR8pxTKs9Oe3"},"outputs":[],"source":["# Create Corpus Sentence DataFrame\n","\n","corpus_sents_df = pd.DataFrame(corpus_sents_ls)\n","corpus_sents_df.columns = ['sent_no', 'parag_no', 'sent_raw']\n","corpus_sents_df['sent_raw'] = corpus_sents_df['sent_raw'].astype('string')\n","# Double check to drop any rows where raw Sentence is NaN or empty string ''\n","corpus_sents_df.dropna(subset=['sent_raw'], inplace=True)\n","\n","\n","print(f'First 10 Sentences of {CORPUS_FULL}')\n","corpus_sents_df.head(10)\n","corpus_sents_df.info()"]},{"cell_type":"markdown","metadata":{"id":"qgezNV5b_hNJ"},"source":["**Create Paragraph DataFrame: [corpus_parags_df]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dSDvcM1_Tky"},"outputs":[],"source":["# Create Corpus Paragraph DataFrame\n","\n","parag_no_ls = []\n","parag_raw_ls = []\n","\n","corpus_parags_df = pd.DataFrame()\n","\n","for i, aparag in enumerate(corpus_parags_ls):\n","  parag_no_ls.append(i)\n","  parag_raw_ls.append(aparag)\n","\n","corpus_parags_df = pd.DataFrame(\n","    {'parag_no': parag_no_ls,\n","     'parag_raw': parag_raw_ls,\n","    })\n","\n","# Double check to drop any rows where raw Paragraph is NaN or empty string ''\n","corpus_parags_df.dropna(subset=['parag_raw'], inplace=True)\n","\n","# Test \n","print(f'First 10 Paragraphs of {CORPUS_FULL}')\n","corpus_parags_df.head(10)\n","corpus_parags_df.info()"]},{"cell_type":"markdown","metadata":{"id":"r2vVN-Nu_tcZ"},"source":["**Create Section DataFrame: [corpus_sects_df]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lHKDiPxICu4"},"outputs":[],"source":["MIN_SECT_LEN=25"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F2j6r-8n_Tev"},"outputs":[],"source":["# Create Corpus Section DataFrame\n","\n","sect_no_ls = []\n","sect_raw_ls = []\n","\n","# corpus_sects_df = pd.DataFrame()\n","\n","# Filter out all the CHAPTER [\\d]{1,2} lines\n","corpus_sects_noheaders_ls = []\n","pattern = r'CHAPTER [\\d]{1,2}[^\\n]*'\n","for asect in corpus_sects_ls:\n","  if re.search(pattern, asect) == None:\n","    corpus_sects_noheaders_ls.append(asect)\n","corpus_sects_ls = corpus_sects_noheaders_ls\n","\n","\n","for i, asect in enumerate(corpus_sects_ls):\n","  sect_no_ls.append(i)\n","  sect_raw_ls.append(asect)\n","\n","\n","corpus_sects_df = pd.DataFrame(\n","    {'sect_no': sect_no_ls,\n","     'sect_raw': sect_raw_ls,\n","    })\n","\n","\n","# Calculate the sentence number at the mid-point of each Section\n","\n","sect_mid_sentno_ls = []\n","sect_start_sentno_ls = []\n","sect_sentno_base = 0\n","for i, sect_text in enumerate(corpus_sects_df.sect_raw):\n","  if len(sect_text) > MIN_SECT_LEN:\n","    # Create list of Sentences by sent_tokenizing Section raw text string\n","    sect_sents_ls = sent_tokenize(sect_text)\n","\n","    # Calc and save the sent_no that begins each Section\n","    sect_first_sent = sect_sents_ls[0].strip()[:30]  # Match on the first 20 chars\n","\n","    # Fix to remove leading/trailing parenthesis that are being interpreted by Python\n","    # sect_first_sent = sect_first_sent.replace('(','').replace(')','')\n","    sect_first_sent = sect_first_sent.strip('()[]')\n","\n","    # Find Sentence No for the starting Sentence of each Section\n","    print(f'For Section #{i} seeking first sentence: {sect_first_sent}')\n","    sect_start_sentno = list(corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(sect_first_sent, regex=False)]['sent_no'])[0] # Problems with Sentences beginning/endings with parenthenses\n","    sect_start_sentno_ls.append(int(sect_start_sentno))\n","\n","    # Find the Sentence No for the middle Sentence of each Section\n","    sect_sents_len = len(sect_sents_ls)\n","    sect_mid_sentno = int(sect_sents_len/2 + sect_sentno_base)\n","    # print(f'Section #{i}: {len(sect_sents_ls)} Sentences, midpoint: {sect_mid_sentno}, cumulative midpoint: {sect_mid_sentno}')\n","    sect_mid_sentno_ls.append(sect_mid_sentno)\n","    sect_sentno_base += sect_sents_len\n","\n","corpus_sects_df['sent_no_start'] = pd.Series(sect_start_sentno_ls)\n","corpus_sects_df['sent_no_mid'] = pd.Series(sect_mid_sentno_ls)\n","\n","# Test \n","print(f'First 2 Sections of {CORPUS_FULL}')\n","# corpus_sects_df.head(2)\n","corpus_sects_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pPkn1d9ZDqqf"},"outputs":[],"source":["corpus_sects_df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-CkRGqhBesW"},"outputs":[],"source":["# Test\n","\n","corpus_sents_df[corpus_sents_df['sent_raw'].str.contains('No going to the Lighthouse')]['sent_no']"]},{"cell_type":"markdown","metadata":{"id":"dg6-jzBG_wKV"},"source":["**Create Chapter DataFrame: [corpus_chaps_df]**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6Nb3k6k9Obz"},"outputs":[],"source":["# Create Corpus Chapter DataFrame\n","\n","chap_no_ls = []\n","chap_raw_ls = []\n","\n","# corpus_chaps_df = pd.DataFrame()\n","\n","for i, achap in enumerate(corpus_chaps_ls):\n","  chap_no_ls.append(i)\n","  chap_raw_ls.append(achap)\n","\n","\n","corpus_chaps_df = pd.DataFrame(\n","    {'chap_no': chap_no_ls,\n","     'chap_raw': chap_raw_ls,\n","    })\n","\n","\n","# Calculate the sentence number at the mid-point of each Chapter\n","\n","chap_mid_sentnos_ls = []\n","chap_start_sentnos_ls = []\n","chap_sentno_base = 0\n","for i, chap_text in enumerate(corpus_chaps_df.chap_raw):\n","  if len(chap_text) > MIN_CHAP_LEN:\n","    chap_sents_ls = sent_tokenize(chap_text)\n","    # Calc and save the sent_no that begins each Chapter\n","    chap_first_sent = chap_sents_ls[0].strip()\n","    # print(f'Searching for first sentence: {chap_first_sent}')\n","    chap_start_sentnos_ls.append(int(corpus_sents_df[corpus_sents_df['sent_raw'].str.contains(chap_first_sent, regex=False)]['sent_no']))\n","    # Calc and save the sent_no in the middle of each Chapter\n","    chap_sents_len = len(chap_sents_ls)\n","    chap_mid_sentno = int(chap_sents_len/2) + chap_sentno_base\n","    # print(f'Chapter #{i}: {len(chap_sents_ls)} Sentences, midpoint: {chap_mid_sentno}, cumulative midpoint: {chap_mid_sentno}')\n","    chap_mid_sentnos_ls.append(chap_mid_sentno)\n","    chap_sentno_base += chap_sents_len\n","\n","corpus_chaps_df['sent_no_start'] = pd.Series(chap_start_sentnos_ls)\n","corpus_chaps_df['sent_no_mid'] = pd.Series(chap_mid_sentnos_ls)\n","\n","# Test \n","print(f'First 2 Chapters of {CORPUS_FULL}')\n","# corpus_chaps_df.head(2)\n","corpus_chaps_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5Q2wLFgVr-3"},"outputs":[],"source":["print(corpus_sents_df.iloc[237]['sent_raw'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQgLAiWEocU4"},"outputs":[],"source":["# TODO: More General Cleanup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XvwS0j75ocU4"},"outputs":[],"source":["# TODO: Normalize Paragraphs by Lengths (Smart Aggregate/Split)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BH65q_PIefyg"},"outputs":[],"source":["corpus_sects_df.columns"]},{"cell_type":"markdown","metadata":{"id":"QRxiZatBiW-U"},"source":["# **Preprocess and Review Corpus Text (Auto)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E47j6KGxiW-V"},"outputs":[],"source":["# Read corpus into a single string then split into paragraphs\n","\n","corpus_parags_raw_ls = read_corpus_parags(CORPUS_FILENAME)\n","print(f'We found #{len(corpus_parags_raw_ls)} lines\\n')\n","\n","print('\\nThe first 10 lines of the Corpus:')\n","print('-----------------------------------\\n')\n","corpus_parags_raw_ls[:10]\n","\n","print('\\nThe last 10 lines of the Corpus:')\n","print('-----------------------------------\\n')\n","corpus_parags_raw_ls[-10:]\n","print('\\n')\n","\n","n_shortest = 10\n","print(f'The {n_shortest} Sentences in the Corpus are:')\n","print('--------------------------------------------')\n","temp_ls = sorted(corpus_parags_raw_ls, key=lambda x: (len(x), x))\n","for i, asent in enumerate(temp_ls[:n_shortest]):\n","  print(f'Shortest #{i}: {asent}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4a0aZ9IiW-V"},"outputs":[],"source":["# Tokenize Paragraphs into Sentences\n","\n","'''\n","sent_no = 0\n","# sent_base = 0\n","corpus_sents_row_ls = []\n","for parag_no,aparag in enumerate(corpus_parags_raw_ls):\n","  sents_ls = sent_tokenize(aparag)\n","  # Delete (whitespace only) sentences\n","  sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n","  # print(f'Corpus Sentences -(whitespace only) Count: {len(sents_ls)}')\n","  # Delete (punctuation only) sentences\n","  sents_ls = [x for x in sents_ls if len((re.sub(r'[^\\w\\s]','',x)).strip()) > MIN_SENT_LEN]\n","  # print(f'Corpus Sentences -(punctuation only) Count: {len(sents_ls)}')\n","  # sent_no = sent_base\n","  for s,asent in enumerate(sents_ls):\n","    corpus_sents_row_ls.append([sent_no, parag_no, asent])\n","    sent_no += 1\n","  # sent_base = sent_no \n","\n","\n","print(f'{len(corpus_sents_row_ls)}')\n","\n","print(f'First row {corpus_sents_row_ls[0]}')\n","print('\\n')\n","print(f'Last row {corpus_sents_row_ls[-1]}')\n","'''\n","\n","corpus_sents_row_ls = parag2sents(corpus_parags_raw_ls)\n","print(f'{len(corpus_sents_row_ls)}')\n","\n","print(f'First row {corpus_sents_row_ls[0]}')\n","print('\\n')\n","print(f'Last row {corpus_sents_row_ls[-1]}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zus6CXywiW-W"},"outputs":[],"source":["# Create Corpus Sentence DataFrame\n","\n","corpus_sents_df = pd.DataFrame(corpus_sents_row_ls)\n","corpus_sents_df.columns = ['sent_no', 'parag_no', 'sent_raw']\n","corpus_sents_df['sent_raw'] = corpus_sents_df['sent_raw'].astype('string')\n","# Double check to drop any rows where raw Sentence is NaN or empty string ''\n","corpus_sents_df.dropna(subset=['sent_raw'], inplace=True)\n","\n","\n","print(f'First 10 Sentences of {CORPUS_FULL}')\n","corpus_sents_df.head(10)\n","corpus_sents_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3yjAh3qsEHV"},"outputs":[],"source":["# Create Corpus Paragraph DataFrame\n","\n","parag_no_ls = []\n","parag_raw_ls = []\n","\n","corpus_parags_df = pd.DataFrame()\n","\n","for i, aparag in enumerate(corpus_parags_raw_ls):\n","  parag_no_ls.append(i)\n","  parag_raw_ls.append(aparag)\n","\n","corpus_parags_df = pd.DataFrame(\n","    {'parag_no': parag_no_ls,\n","     'parag_raw': parag_raw_ls,\n","    })\n","\n","# Test \n","print(f'First 10 Paragraphs of {CORPUS_FULL}')\n","corpus_parags_df.head(10)\n","corpus_parags_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8bvn2Gn6mpm"},"outputs":[],"source":["# Calculate (win_(x)1per) 1% of Corpus length for smallest (odd-valued) rolling window\n","\n","# Sentences\n","corpus_sents_len = corpus_sents_df.shape[0]\n","\n","win_raw_s1per = int(corpus_sents_len * 0.01)\n","# print(f'1% Rolling Window: {win_raw_s1per}')\n","\n","if win_raw_s1per % 2:\n","  win_s1per = win_raw_s1per\n","else:\n","  win_s1per = win_raw_s1per + 1\n","\n","# Paragraphs\n","\n","corpus_parags_len = corpus_parags_df.shape[0]\n","\n","win_raw_p1per = int(corpus_parags_len * 0.01)\n","# print(f'1% Rolling Window: {win_raw_1per}')\n","\n","if win_raw_p1per % 2:\n","  win_p1per = win_raw_p1per\n","else:\n","  win_p1per = win_raw_p1per + 1\n","\n","print(f'Sentence 1 Percent window: {win_s1per}')\n","print(f'Paragraph 1 Percent window: {win_p1per}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vl9saM8riW-W"},"outputs":[],"source":["# TODO: More General Cleanup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7pa_mnj-iW-X"},"outputs":[],"source":["# TODO: Normalize Paragraphs by Lengths (Smart Aggregate/Split)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybl62GgKiW-Y"},"outputs":[],"source":["# Calculate some char/token metrics and do some EDA on them\n","\n","corpus_sents_df['char_len'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x))\n","corpus_sents_df['token_len'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x.split())) \n","\n","corpus_parags_df['char_len'] = corpus_parags_df['parag_raw'].apply(lambda x: len(x))\n","corpus_parags_df['token_len'] = corpus_parags_df['parag_raw'].apply(lambda x: len(x.split())) \n","\n","# corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FudLbMpViW-Y"},"outputs":[],"source":["# Default cleaned raw text\n","\n","# Sentences\n","# Let's take a look at the updated text\n","corpus_sents_df['sent_clean'] = corpus_sents_df['sent_raw'].apply(lambda x: text_clean(x))\n","# Ensure to drop all Sentences with NaN or '' Raw Text\n","corpus_sents_df.replace(\"\", np.nan, regex=True, inplace=True)\n","corpus_sents_df.dropna(how='any', axis=0, subset=['sent_raw'], inplace=True)\n","\n","print('\\nCompare Raw and Cleaned Sentences:')\n","print('--------------------------------------')\n","corpus_sents_df\n","\n","# Paragraphs\n","# Let's take a look at the updated text\n","corpus_parags_df['parag_clean'] = corpus_parags_df['parag_raw'].apply(lambda x: text_clean(x))\n","# Ensure to drop all Sentences with NaN or '' Raw Text\n","corpus_parags_df.replace(\"\", np.nan, regex=True, inplace=True)\n","corpus_parags_df.dropna(how='any', axis=0, subset=['parag_raw'], inplace=True)\n","\n","print('\\nCompare Raw and Cleaned Paragraphs:')\n","print('--------------------------------------')\n","corpus_parags_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mf99apfwKPAO"},"outputs":[],"source":["corpus_sents_df.shape\n","print('\\n')\n","corpus_parags_df.shape"]},{"cell_type":"markdown","metadata":{"id":"jHscLkclSYqN"},"source":["##**Save Preprocess Corpus DataFrames**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEzo8eltSvWS"},"outputs":[],"source":["# Save Preprocessed Corpus Sentences DataFrame\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Sentences\n","corpus_sents_filename = f'corpus_sents_clean_{author_str}_{title_str}_{datetime_now}.csv'\n","print(f'Saving to file: {corpus_sents_filename}')\n","\n","corpus_sents_df.to_csv(corpus_sents_filename)\n","\n","# Paragraphs\n","corpus_parags_filename = f'corpus_parags_clean_{author_str}_{title_str}_{datetime_now}.csv'\n","print(f'Saving to file: {corpus_parags_filename}')\n","\n","corpus_parags_df.to_csv(corpus_parags_filename)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rO1QwvSkiWY-"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_MkqrxAiWFc"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"Bsm8awD4AZ8O"},"source":["# (ARCHIVED - BEGINNING) Configuration (Manual)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ln4X_j7WhJkX"},"outputs":[],"source":["# Verify subdirectory change\n","\n","!pwd\n","!ls *.txt\n","\n","# TODO: Intelligently automate the filling of form based upon directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZXC_Ld-hJkY"},"outputs":[],"source":["CORPUS_TITLE = 'Machines Like Me' #@param {type:\"string\"}\n","CORPUS_AUTHOR = \"Ian McEwan\" #@param {type:\"string\"}\n","CORPUS_FILENAME = \"mlm_final_hand.txt\" #@param {type:\"string\"}\n","CORPUS_SUBDIR = \"./research/2021/sa_book_code/books_sa/imcewan_machineslikeme\" #@param {type:\"string\"}\n","\n","CORPUS_FULL = f'{CORPUS_TITLE} by: {CORPUS_AUTHOR}'\n","\n","PLOT_OUTPUT = \"None\" #@param [\"None\", \"Major\", \"All\"]\n","\n","FILE_OUTPUT = \"None\" #@param [\"None\", \"Major\", \"All\"]\n","\n","gdrive_subdir = CORPUS_SUBDIR\n","corpus_filename = ''\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","\n","print(f'\\nWorking Corpus Datafile: {CORPUS_SUBDIR}')\n","print(f'\\nFull Corpus Title/Author: {CORPUS_FULL}')\n","\n","# Verify contents of Corpus File is Correctly Formatted\n","#   \n","# TODO: ./utils/verify_format.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEkBbqwWhVMR"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mW3GOU2ehVIn"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0klW_KrehVFR"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"9ukLCuZFm1bu"},"source":["# Setup Google gDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G64etjAUOOSm"},"outputs":[],"source":["# Connect to Google gDrive\n","\n","from google.colab import drive, files\n","drive.mount('/gdrive')\n","%cd /gdrive/MyDrive/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0fvFZq-eFaw"},"outputs":[],"source":["# Select the Corpus subdirectory on your Google gDrive\n","\n","gdrive_subdir = \"./research/2021/sa_book_code/books_sa/imcewan_machineslikeme\" #@param {type:\"string\"}\n","CORPUS_SUBDIR = gdrive_subdir\n","%cd $gdrive_subdir\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfilGg6Mkxnd"},"outputs":[],"source":["# Verify subdirectory change\n","\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"08KJ7GDrd2jo"},"outputs":[],"source":["!ls *.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cmEPO0sSEhMZ"},"outputs":[],"source":["# Verify contents of Corpus File is Correctly Formatted\n","#   \n","# TODO: ./utils/verify_format.py"]},{"cell_type":"markdown","metadata":{"id":"u932nJxdh0Ac"},"source":["# Configuration (Auto)"]},{"cell_type":"markdown","metadata":{"id":"3_a9eQyBiiTG"},"source":["**Global Configuration Constants**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iT2MyyjpihFj"},"outputs":[],"source":["# Minimum lengths for Sentences and Paragraphs\n","#   (Shorter Sents/Parags will be deleted)\n","\n","MIN_PARAG_LEN = 2\n","MIN_SENT_LEN = 2"]},{"cell_type":"markdown","metadata":{"id":"MOPa6HH-OjZp"},"source":["**Install Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEjSzsusOOJ-"},"outputs":[],"source":["# INSTALL LIBRARIES\n","\n","!pip install sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYF51bPRSFjC"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"cmtqmvu6OlR9"},"source":["**Import Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7bf4lfgwMEz"},"outputs":[],"source":["import os\n","import sys\n","import io\n","import glob\n","import contextlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOmyq4h7OOFi"},"outputs":[],"source":["# IMPORT LIBRARIES\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OslLdEsvOuFU"},"outputs":[],"source":["import re\n","import string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YelenXz5BcmE"},"outputs":[],"source":["import collections\n","from collections import OrderedDict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Suximbjnw8D"},"outputs":[],"source":["# Import libraries for logging\n","\n","import logging\n","from datetime import datetime\n","import time                     # (TODO: check no dependencies and delete)\n","from time import gmtime, strftime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPZmScjVDYyw"},"outputs":[],"source":["import nltk\n","\n","# Download for sentence tokenization\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')\n","\n","# Download for nltk/VADER sentiment analysis\n","nltk.download('vader_lexicon')\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMl2mfF8Haw8"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wcZfSOuBlW7"},"outputs":[],"source":["from scipy import interpolate\n","from scipy.interpolate import CubicSpline\n","from scipy import signal\n","from scipy.signal import argrelextrema\n","import scipy.stats\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CY3UyvYjAvDN"},"outputs":[],"source":["from statsmodels.nonparametric.smoothers_lowess import lowess as sm_lowess"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-ENcltMSK6n"},"outputs":[],"source":["import transformers"]},{"cell_type":"markdown","metadata":{"id":"Kwl0MBDyOwtX"},"source":["**Configure Jupyter Notebook**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vIIjSbyeP2fg"},"outputs":[],"source":["# Configure Jupyter\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = [16, 8]\n","plt.rcParams['figure.dpi'] = 100\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from ipywidgets import widgets, interactive\n","\n","# Configure Google Colab\n","\n","%load_ext google.colab.data_table"]},{"cell_type":"markdown","metadata":{"id":"4dLkfn4KFmDf"},"source":["**Configuration Details Snapshot**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FNPovQBFZky"},"outputs":[],"source":["# Snap Shot of Time, Machine, Data and Library/Version Blueprint\n","# TODO:"]},{"cell_type":"markdown","metadata":{"id":"XKV1uMBEO8TR"},"source":["# Upload Plain Text Corpus (Interactive)\n","\n","NOTE: Paragraphs separated by '\\n\\n'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzjBszTTXX0n"},"outputs":[],"source":["# Create a working subdirectory\n","\n","CORPUS_SUBDIR = './hfmodels/'\n","\n","!mkdir $CORPUS_SUBDIR\n","%cd $CORPUS_SUBDIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AgSXEpDkDKrY"},"outputs":[],"source":["try:\n","  CORPUS_FIELNAME\n","  pass\n","except:\n","  uploaded_fileinfo = files.upload()\n","  CORPUS_FILENAME = list(uploaded_fileinfo.keys())[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AmrQVMpk00JK"},"outputs":[],"source":["# If old version of file exists (e.g. endswith '(n).txt'), delete and rename the newly uploaded version\n","# TODO:\n","\n","# if CORPUS_FILENAME.endswith('\\).txt'):\n","#   print('At least one previous verion of this file already existed in this upload directory')\n","\n","# !rm $CORPUS_FILENAME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trnEswiaW6WA"},"outputs":[],"source":["!pwd\n","!ls -al $CORPUS_FILENAME"]},{"cell_type":"markdown","metadata":{"id":"FU3aHagiRjqR"},"source":["# (ARCHIVED - END) Clean, Preprocess, Convert and Review Corpus (Auto)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HP3WLEv_g5aq"},"outputs":[],"source":["CORPUS_TITLE = \"Machines Like Me\" #@param {type:\"string\"}\n","CORPUS_AUTHOR = \"Ian McEwan\" #@param {type:\"string\"}\n","CORPUS_FILENAME = \"mlm_final_hand.txt\" #@param {type:\"string\"}\n","\n","CORPUS_FULL = f'{CORPUS_TITLE} by: {CORPUS_AUTHOR}'\n","\n","print(f'\\nWorking Corpus Datafile: {CORPUS_SUBDIR}')\n","print(f'\\nFull Corpus Title/Author: {CORPUS_FULL}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NK1cGWw_5eqn"},"outputs":[],"source":["# Read corpus into a single string then split into paragraphs\n","\n","# with open(uploaded_filename, \"r\", encoding='utf-8', errors='ignore') as infp:\n","\n","'''\n","# Uploading a corpus file overrides the earlier corpus form assignments\n","try:\n","    uploaded_filename\n","except NameError:\n","  pass\n","else:\n","  CORPUS_FILENAME = uploaded_filename\n","'''\n","\n","with open(CORPUS_FILENAME, \"r\", encoding=CORPUS_ENCODING) as infp:\n","  corpus_raw_str = infp.read()\n","\n","corpus_parags_raw_ls = corpus_raw_str.split('\\n\\n')\n","\n","# Strip excess whitespace and drop empty lines\n","corpus_parags_raw_ls = [x.strip() for x in corpus_parags_raw_ls if len(x.strip()) > MIN_PARAG_LEN]\n","\n","print(f'We found #{len(corpus_parags_raw_ls)} lines\\n')\n","\n","print('\\nThe first 10 lines of the Corpus:')\n","print('-----------------------------------\\n')\n","corpus_parags_raw_ls[:10]\n","\n","print('\\nThe last 10 lines of the Corpus:')\n","print('-----------------------------------\\n')\n","corpus_parags_raw_ls[-10:]\n","print('\\n')\n","print(sorted(corpus_parags_raw_ls, key=lambda x: (len(x), x)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CB2MgbbRSdlm"},"outputs":[],"source":["# Create Paragraph DataFrame\n","\n","parag_no_ls = []\n","parag_raw_ls = []\n","for i,aparag in enumerate(corpus_parags_raw_ls):\n","  parag_no_ls.append(i)\n","  parag_raw_ls.append(aparag)\n","\n","corpus_parags_df = pd.DataFrame({'parag_no':parag_no_ls, 'parag_raw':parag_raw_ls})\n","corpus_parags_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lv8Vluu55enB"},"outputs":[],"source":["# Tokenize into Sentences\n","\n","sent_no = 0\n","# sent_base = 0\n","corpus_sents_row_ls = []\n","for parag_no,aparag in enumerate(corpus_parags_raw_ls):\n","  sents_ls = sent_tokenize(aparag)\n","  sents_ls = [x.strip() for x in sents_ls if len(x.strip()) > MIN_SENT_LEN]\n","  # sent_no = sent_base\n","  for s,asent in enumerate(sents_ls):\n","    corpus_sents_row_ls.append([sent_no, parag_no, asent])\n","    sent_no += 1\n","  # sent_base = sent_no \n","\n","\n","print(f'{len(corpus_sents_row_ls)}')\n","\n","print(f'First row {corpus_sents_row_ls[0]}')\n","print('\\n')\n","print(f'Last row {corpus_sents_row_ls[-1]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zq4tmIjtI5WG"},"outputs":[],"source":["# Create Corpus DataFrame\n","\n","corpus_sents_df = pd.DataFrame(corpus_sents_row_ls)\n","corpus_sents_df.columns = ['sent_no', 'parag_no', 'sent_raw']\n","corpus_sents_df['sent_raw'] = corpus_sents_df['sent_raw'].astype('string')\n","print(f'First 10 Sentences of {CORPUS_FULL}')\n","corpus_sents_df.head(10)\n","corpus_sents_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNKoxe3kRV71"},"outputs":[],"source":["# TODO: More General Cleanup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRUmgyLvRM1v"},"outputs":[],"source":["# TODO: Normalize Paragraphs by Lengths (Smart Aggregate/Split)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqfhER7ylECP"},"outputs":[],"source":["# Generate full path and timestamp for new filepath/filename\n","\n","def gen_pathfiletime(file_str, subdir_str=''):\n","\n","  # Genreate compressed author and title substrings\n","  author_raw_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","  title_raw_str = ''.join(CORPUS_TITLE.split()).lower()\n","\n","  # Generate current/unique datetime string\n","  datetime_str = str(datetime.now().strftime('%Y%m%d%H%M%S'))\n","\n","  # Built fullpath+filename string\n","  file_base, file_ext = file_str.split('.')\n","\n","  author_str = re.sub('[^A-Za-z0-9]+', '', author_raw_str)\n","  title_str = re.sub('[^A-Za-z0-9]+', '', title_raw_str)\n","\n","  full_filepath_str = f'{subdir_str}{file_base}_{author_str}_{title_str}_{datetime_str}.{file_ext}'\n","\n","  # print(f'Returning from gen_savepath() with full_filepath={full_filepath}')\n","\n","  return full_filepath_str\n","\n","# Test\n","# pathfilename_str = gen_pathfiletime('hist_paraglen.png')\n","# print(pathfilename_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8ZxHMuRe2wF"},"outputs":[],"source":["# Calculate some char/token metrics and do some EDA on them\n","\n","corpus_sents_df['char_len'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x))\n","corpus_sents_df['token_len'] = corpus_sents_df['sent_raw'].apply(lambda x: len(x.split())) \n","# corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sy8VaWVqTbNj"},"outputs":[],"source":["PLOT_OUTPUT='All'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7au9Zy2kR0NK"},"outputs":[],"source":["# Default clean Sentence raw text\n","\n","#This function converts to lower-case, removes square bracket, removes numbers and punctuation\n"," \n","def text_clean(text):\n","    text = text.lower()\n","    text = re.sub('\\[.*?\\]', ' ', text)\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    text = re.sub('[\\n]', ' ', text)  # Replace newline with space\n","    return text\n","\n","# Let's take a look at the updated text\n","corpus_sents_df['sent_clean'] = corpus_sents_df['sent_raw'].apply(lambda x: text_clean(x))\n","corpus_parags_df['parag_clean'] = corpus_parags_df['parag_raw'].apply(lambda x: text_clean(x))\n","\n","\n","if (PLOT_OUTPUT == 'All'):\n","  corpus_sents_df.head(2)\n","  corpus_sents_df.info()\n","  corpus_parags_df.head(2)\n","  corpus_parags_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNxW9rmosKaS"},"outputs":[],"source":["# Verify saved under newest filename\n","\n","def get_recentfile(file_type='csv'):\n","  '''\n","  Given a file extension type,\n","  Return the most recently created file of that type \n","  in the current directory\n","  '''\n","  file_pattern = \"./*.\" + file_type\n","  print(f'file_pattern: {file_pattern}')\n","  list_of_files = glob.glob(file_pattern) # * means all if need specific format then *.csv\n","  latest_file = max(list_of_files, key=os.path.getmtime)\n","\n","  return latest_file\n","\n","# Test\n","\n","# get_recentfile('txt')"]},{"cell_type":"markdown","metadata":{"id":"nUk9J3yXvLaH"},"source":["### **Immediately Save Long-Running Robertlg15-Siebert Values**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggNzOxKynZWG"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# Save Sentence DataFrame\n","sats_filename_str = f'sentiment_sents_{sa_model}_{author_str}_{title_str}_{datetime_now}.csv'\n","print(f'Saving Sentences to: {sats_filename_str}')\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","\n","# Save Paragraph DataFrame\n","sats_filename_str = f'sentiment_parags_{sa_model}_{author_str}_{title_str}_{datetime_now}.csv'\n","print(f'Saving Paragraphs to: {sats_filename_str}')\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fio-H1DyKom9"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"oUpbQ_fDL4nB"},"source":["# **Transformer Sentiment Analysis Models**"]},{"cell_type":"markdown","metadata":{"id":"GEfus5Rc_umE"},"source":["### Select Interactive Model and Epochs\n","\n","1) Models\n","* bert-base-uncased\n","* xlnet-base-cased\n","* \n","* BertForSentimentClassification\n","* AlbertForSentimentClassification\n","* DistilBertForSentimentClassification\n","\n","2) Hyperparameters (Finetuning)\n","* Epochs (1,2,3,4,6,8,10,15,20)\n","* Batch\n","\n","2) Datasets\n","* SST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MiwmZW4Bq8N"},"outputs":[],"source":["# !pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zN9H3wXEBtPu"},"outputs":[],"source":["# import sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4UHc2zzqIsO"},"outputs":[],"source":["corpus_sents_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"p8sQtSUjAbOl"},"source":["## **(2) Distill BERT Default Huggingface Sentiment Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EcAZbIWs2Awl"},"outputs":[],"source":["from transformers import AutoTokenizer,AutoModelForSequenceClassification\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B98xH2hF2A0M"},"outputs":[],"source":["# https://github.com/brianadit24/SentimentAnalysiswithBERT_HF/blob/main/Sentiment_Analysis_with_BERT.ipynb\n","\n","\n","from transformers import pipeline\n","\n","senti_pipeline = pipeline(\"sentiment-analysis\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KhIPOW72Ats"},"outputs":[],"source":["\n","\n","senti_pipeline(\"I am extremely happy to share this video with all of you\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snA6owtY2App"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"SOHsdO5fAgIF"},"source":["### **Sentiment Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q0-pOM3vAgIG"},"outputs":[],"source":["# Setup for default Huggingface Sentiment Analysis  siebert/sentiment-roberta-large-english\n","\n","sa_model = 'distillbertsst'\n","\n","from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n","\n","# Sentiment analysis pipeline\n","pipeline('sentiment-analysis')\n","\n","hf_sadef_clf = pipeline('sentiment-analysis')\n","hf_sadef_clf('Such a nice weather outside !')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6uXU_og8LOEb"},"outputs":[],"source":["hf_sadef_clf('Fuck you asshole!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poMR5iNTAgIH"},"outputs":[],"source":["# Prepare Text from DataFrame\n","\n","sents_pred_df = corpus_sents_df.copy()\n","sents_pred_texts = sents_pred_df['sent_raw'].astype('str').tolist() # Want to catch NaN, .dropna().astype('str').tolist()\n","\n","# Create class for data preparation\n","class SimpleDataset:\n","    def __init__(self, tokenized_texts):\n","        self.tokenized_texts = tokenized_texts\n","    \n","    def __len__(self):\n","        return len(self.tokenized_texts[\"input_ids\"])\n","    \n","    def __getitem__(self, idx):\n","        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","        \n","# Tokenize texts and create prediction data set\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","sents_tokenized_texts = tokenizer(sents_pred_texts,truncation=True,padding=True)\n","sents_pred_dataset = SimpleDataset(sents_tokenized_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7U4Kg4DAgII"},"outputs":[],"source":["# Run predictions\n","\n","sents_predictions = trainer.predict(sents_pred_dataset)\n","# sents_predictions = trainer.predict(sents_tokenized_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"en8-8FGVD8Qo"},"outputs":[],"source":["# polprob2sentiment(temp_sentiment_df(temp_sentiment_df.iloc[0]['label'], temp_sentiment_df.iloc[0]['score']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQnIzZUg3YRA"},"outputs":[],"source":["temp_sentiment_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O3vFgP2g3KAv"},"outputs":[],"source":["temp_sentiment_df['polarity'] = ['NEGATIVE' if x.strip()=='1 star' else 'POSITIVE' for x in temp_sentiment_df.label]\n","temp_sentiment_df['polarity_sign'] = [-1.0 if x.strip()=='1 star' else +1.0 for x in temp_sentiment_df.label]\n","temp_sentiment_df['distillbertsst'] = temp_sentiment_df['score']*temp_sentiment_df['polarity_sign']\n","temp_sentiment_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwPIpF7b7MED"},"outputs":[],"source":["corpus_sents_df['distillbertsst'] = temp_sentiment_df['distillbertsst']\n","corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2DLzxtyAgIJ"},"outputs":[],"source":["# Transform predictions to labels\n","\n","sa_model = 'distillbertsst'\n","\n","sents_preds = sents_predictions.predictions.argmax(-1)\n","sents_labels = pd.Series(sents_preds).map(model.config.id2label)\n","sents_scores = (np.exp(sents_predictions[0])/np.exp(sents_predictions[0]).sum(-1,keepdims=True)).max(1)\n","\n","# Create DataFrame with texts, predictions, labels, and scores\n","\n","temp_sentiment_df = pd.DataFrame(list(zip(sents_pred_texts,sents_preds,sents_labels,sents_scores)), columns=['text','pred','label','score'])\n","# temp_sentiment_df.head()\n","\n","# Convert label (Neg/Pos) and score (Prob) to a +/-Sentiment Float Value\n","\n","corpus_sents_df[sa_model] = temp_sentiment_df.apply(lambda x: polprob2sentiment(x.polarity,x.score), axis=1)\n","corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPUHuMQz6Nay"},"outputs":[],"source":["polprob2sentiment('{o',0.889)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wp3a9mTXAgIJ"},"outputs":[],"source":["# Verify the head and tail are complete and correct\n","\n","corpus_parags_df.iloc[:3]\n","corpus_parags_df.iloc[-3:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eI87gx0VAgIK"},"outputs":[],"source":["# Aggregate Sentence Sentiments to populate Paragraph Sentiment DataFrame\n","\n","sa_model = 'distillbertsst'\n","# def sentiment_sents2parags(ts_df, model_name='roberta_lg15'):\n","parags_sentiment_ls = sentiment_sents2parags(corpus_sents_df, sa_model)\n","corpus_parags_df[sa_model] = pd.Series(parags_sentiment_ls)\n","corpus_parags_df.head(2)\n","corpus_parags_df.tail(2)"]},{"cell_type":"markdown","metadata":{"id":"RIN60RDoAgIK"},"source":["### **Histogram Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jrq23QtdAgIM"},"outputs":[],"source":["# Debug\n","PLOT_OUTPUT = 'Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbkwvaQNTWn_"},"outputs":[],"source":["sa_model = 'distillbertsst'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qZnZ_62AgIM"},"outputs":[],"source":["# Create histogram of Sentence Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_sents_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_sents_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0ODZGa1AgIN"},"outputs":[],"source":["# Create histogram of Paragraph Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_parags_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_parags_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"markdown","metadata":{"id":"i5z9OeYPAgIN"},"source":["### **Mean SMA Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UkqPYYBnAgIO"},"outputs":[],"source":["# SMA % Sentiment of Sentence Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_sents_df, sa_model, text_unit='sentence', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"It5UzlG_AgIO"},"outputs":[],"source":["# SMA % Sentiment of Paragraph Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_parags_df, sa_model, text_unit='paragraph', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5K2n933AgIP"},"outputs":[],"source":["# Compare Sentence and Paragraph\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y='roberta_lg15_mean_roll100', data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (10% SMA)')\n","  sns.lineplot(x=corpus_parags_df.index, y='roberta_lg15_mean_roll100', data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (10% SMA)')\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYMHdYslAgIQ"},"outputs":[],"source":["# corpus_sents_df.drop(columns=['y_scaled'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"IIa66ZYOAgIQ"},"source":["### **Stanford ASAP Plot**\n"]},{"cell_type":"markdown","metadata":{"id":"QvK7RFY0AgIR"},"source":["**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n","\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n","* http://futuredata.stanford.edu/asap/ \n","* https://www.datadoghq.com/blog/auto-smoother-asap/"]},{"cell_type":"markdown","metadata":{"id":"DUSv_avRAgIS"},"source":["**Save the following plots to gDrive files?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MSqGQ2FvAgIT"},"outputs":[],"source":["Save_to_File = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U64G7cWoAgIU","scrolled":false},"outputs":[],"source":["# Sentence SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_sents_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, do_plot=True, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JT-em9kVAgIW"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"siLQ4onZAgIX"},"outputs":[],"source":["# Paragraph SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_parags_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLlNTZSSAgIY"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"markdown","metadata":{"id":"yLafSuGcAgIZ"},"source":["### **LOWESS Plots**"]},{"cell_type":"markdown","metadata":{"id":"t5f2VLEvAgIZ"},"source":["**Sentence Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2Y6eUwpAgIZ"},"outputs":[],"source":["# Debug\n","\n","PLOT_OUTPUT='Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tEaBYNuuAgIa"},"outputs":[],"source":["# Plot Sentence Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./4, 1./6, 1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_sents_df[new_lowess_mean_col] = corpus_sents_df[cols_lowess].mean(axis=1)\n","\n","corpus_sents_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"w7_O3-zkAgIc"},"source":["**Paragraph Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxtV3-rjAgIc"},"outputs":[],"source":["# Plot Paragraph Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_parags_df[new_lowess_col] = plot_lowess(corpus_parags_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_parags_df[new_lowess_mean_col] = corpus_parags_df[cols_lowess].mean(axis=1)\n","\n","corpus_parags_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"5xrTpF85AgIc"},"source":["**Compare Sentence and Paragraph Median LOWESS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DEiaD_nJAgId"},"outputs":[],"source":["# Compare Sentence and Paragraph LOWESS means\n","\n","# Calculate the Sentence and Paragraph LOWESS means and plot\n","\n","# Get all the calculated LOWESS columns in a list \n","cols_sents_lowess = matching_cols = get_cols_regex(corpus_sents_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","cols_parags_lowess = matching_cols = get_cols_regex(corpus_parags_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","\n","# Compute the mean for all the LOWESS columns\n","col_mean_lowess = f'{sa_model}_mean_lowess'\n","corpus_sents_df[col_mean_lowess] = corpus_sents_df[cols_sents_lowess].mean(axis=1)\n","corpus_parags_df[col_mean_lowess] = corpus_parags_df[cols_parags_lowess].mean(axis=1)\n","\n","# Plot the Sentence and Paragraph LOWESS means\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y=col_mean_lowess, data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (8-12 LOWESS)')\n","  sns.lineplot(x=corpus_parags_df.index, y=col_mean_lowess, data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (8-12 LOWESS)')\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"q2BHR7SgAgId"},"source":["### **Save Newly Computed Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9udtdYXAgId"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","sats_filename_str = f'sentiment_sents_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Sentence Sentiments with LOWESS in file: {sats_filename_str}')\n","\n","sats_filename_str = f'sentiment_parags_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Paragraph Sentiments with LOWESS in file: {sats_filename_str}')\n"]},{"cell_type":"markdown","metadata":{"id":"vEJKBjquUxVn"},"source":["## **RoBERTa Large English Tuned on 15 SA Dataset**\n","* **siebert/sentiment-roberta-large-english**\n","\n","This model is a fine-tuned checkpoint of RoBERTa-large (Liu et al. 2019). It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment. The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance generalization across different types of texts (reviews, tweets, etc.). Consequently, it outperforms models trained on only one type of text (e.g., movie reviews from the popular SST-2 benchmark) when used on new data as shown below.\n","\n","Jon Chun\n","20 Jun 2021\n","\n","Reference:\n","\n","* https://huggingface.co/siebert/sentiment-roberta-large-english\n","\n","* https://huggingface.co/siebert/sentiment-roberta-large-english"]},{"cell_type":"markdown","metadata":{"id":"1w4_XguFZBUL"},"source":["### **Sentiment Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wC0q6Bxp3or"},"outputs":[],"source":["# Setup for RoBERTa Large English 15datasets: siebert/sentiment-roberta-large-english\n","\n","sa_model = 'robertalg15'\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n","\n","# Create class for data preparation\n","class SimpleDataset:\n","    def __init__(self, tokenized_texts):\n","        self.tokenized_texts = tokenized_texts\n","    \n","    def __len__(self):\n","        return len(self.tokenized_texts[\"input_ids\"])\n","    \n","    def __getitem__(self, idx):\n","        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","\n","\n","# Set model\n","sa_model = 'robertalg15'\n","model_name = \"siebert/sentiment-roberta-large-english\"\n","\n","# Load tokenizer and model, create trainer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","trainer = Trainer(model=model);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhVAyqOYsL5m"},"outputs":[],"source":["# Prepare Text from DataFrame\n","\n","sents_pred_df = corpus_sents_df.copy()\n","sents_pred_texts = sents_pred_df['sent_raw'].astype('str').tolist() # Want to catch NaN, .dropna().astype('str').tolist()\n","\n","# Tokenize texts and create prediction data set\n","\n","sents_tokenized_texts = tokenizer(sents_pred_texts,truncation=True,padding=True)\n","sents_pred_dataset = SimpleDataset(sents_tokenized_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCfVrVPSIDte"},"outputs":[],"source":["# temp_sentiment_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhLC6PQWIj9K"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nMJbKaZJMw8"},"outputs":[],"source":["# temp_sentiment_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8p64vwpFIwEe"},"outputs":[],"source":["# temp_ser = temp_sentiment_df.apply(lambda x: polprob2sentiment(str(x.label), float(x.score)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JrBnIoJAs6PG"},"outputs":[],"source":["# Transform predictions to labels\n","\n","sents_preds = sents_predictions.predictions.argmax(-1)\n","sents_labels = pd.Series(sents_preds).map(model.config.id2label)\n","sents_scores = (np.exp(sents_predictions[0])/np.exp(sents_predictions[0]).sum(-1,keepdims=True)).max(1)\n","\n","# Create DataFrame with texts, predictions, labels, and scores\n","\n","temp_sentiment_df = pd.DataFrame(list(zip(sents_pred_texts,sents_preds,sents_labels,sents_scores)), columns=['text','pred','label','score'])\n","# temp_sentiment_df.head()\n","\n","# Convert label (Neg/Pos) and score (Prob) to a +/-Sentiment Float Value\n","\n","corpus_sents_df[sa_model] = temp_sentiment_df.apply(lambda x: polprob2sentiment(x.label,x.score), axis=1)\n","corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Df2X7uMPvSBS"},"outputs":[],"source":["# Verify the head and tail are complete and correct\n","\n","corpus_parags_df.iloc[:3]\n","corpus_parags_df.iloc[-3:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXQBoUvPz-Dx"},"outputs":[],"source":["# Aggregate Sentence Sentiments to populate Paragraph Sentiment DataFrame\n","\n","# def sentiment_sents2parags(ts_df, model_name='roberta_lg15'):\n","parags_sentiment_ls = sentiment_sents2parags(corpus_sents_df, sa_model)\n","corpus_parags_df[sa_model] = pd.Series(parags_sentiment_ls)\n","corpus_parags_df.head(2)\n","corpus_parags_df.tail(2)"]},{"cell_type":"markdown","metadata":{"id":"HKi5gSSxac_z"},"source":["### **Histogram Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZjdojOGPK_N"},"outputs":[],"source":["# Debug\n","PLOT_OUTPUT = 'Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxH6paNhac_0"},"outputs":[],"source":["# Create histogram of Sentence Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_sents_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_sents_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ooMDmialac_1"},"outputs":[],"source":["# Create histogram of Paragraph Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_parags_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_parags_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"markdown","metadata":{"id":"EwZtQu9Rac_2"},"source":["### **Mean SMA Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0EEEQ3qk2W3"},"outputs":[],"source":["# SMA % Sentiment of Sentence Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_sents_df, sa_model, text_unit='sentence', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HAVnkk9kjRa"},"outputs":[],"source":["# SMA % Sentiment of Paragraph Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_parags_df, sa_model, text_unit='paragraph', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNJt59ZyWVjZ"},"outputs":[],"source":["# Compare Sentence and Paragraph\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y='roberta_lg15_mean_roll100', data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (10% SMA)')\n","  sns.lineplot(x=corpus_parags_df.index, y='roberta_lg15_mean_roll100', data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (10% SMA)')\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0gw2iJ-dac_3"},"outputs":[],"source":["# corpus_sents_df.drop(columns=['y_scaled'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"L3uJpOvsac_4"},"source":["### **Stanford ASAP Plot**\n"]},{"cell_type":"markdown","metadata":{"id":"b1Qn2aELac_4"},"source":["**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n","\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n","* http://futuredata.stanford.edu/asap/ \n","* https://www.datadoghq.com/blog/auto-smoother-asap/"]},{"cell_type":"markdown","metadata":{"id":"TXFZ_kds6eoX"},"source":["**Save the following plots to gDrive files?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jF6lGCsZ6eoZ"},"outputs":[],"source":["Save_to_File = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPqItfGOk3kg","scrolled":false},"outputs":[],"source":["# Sentence SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_sents_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, do_plot=True, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2z02p7HqfBK"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6cL3UNeMUHn"},"outputs":[],"source":["# Paragraph SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_parags_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QoMzpkfvMUEo"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"markdown","metadata":{"id":"ROMQjaZWac_7"},"source":["### **LOWESS Plots**"]},{"cell_type":"markdown","metadata":{"id":"jowij_p4ac_8"},"source":["**Sentence Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0b7FSPqy-088"},"outputs":[],"source":["# Debug\n","\n","PLOT_OUTPUT='Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYFjPKspX36I"},"outputs":[],"source":["# Plot Sentence Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./4, 1./6, 1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_sents_df[new_lowess_mean_col] = corpus_sents_df[cols_lowess].mean(axis=1)\n","\n","corpus_sents_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5l8YSvB-vsU"},"outputs":[],"source":["corpus_parags_df.columns"]},{"cell_type":"markdown","metadata":{"id":"lfEXZV4H9fS3"},"source":["**Paragraph Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"joc8Mx699fBm"},"outputs":[],"source":["# Plot Paragraph Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_parags_df[new_lowess_col] = plot_lowess(corpus_parags_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_parags_df[new_lowess_mean_col] = corpus_parags_df[cols_lowess].mean(axis=1)\n","\n","corpus_parags_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"yglVp2Fuac_8"},"source":["**Compare Sentence and Paragraph Median LOWESS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZabW5e3wW-t_"},"outputs":[],"source":["# Compare Sentence and Paragraph LOWESS means\n","\n","# Calculate the Sentence and Paragraph LOWESS means and plot\n","\n","# Get all the calculated LOWESS columns in a list \n","cols_sents_lowess = matching_cols = get_cols_regex(corpus_sents_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","cols_parags_lowess = matching_cols = get_cols_regex(corpus_parags_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","\n","# Compute the mean for all the LOWESS columns\n","col_mean_lowess = f'{sa_model}_mean_lowess'\n","corpus_sents_df[col_mean_lowess] = corpus_sents_df[cols_sents_lowess].mean(axis=1)\n","corpus_parags_df[col_mean_lowess] = corpus_parags_df[cols_parags_lowess].mean(axis=1)\n","\n","# Plot the Sentence and Paragraph LOWESS means\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y=col_mean_lowess, data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (8-12 LOWESS)')\n","  sns.lineplot(x=corpus_parags_df.index, y=col_mean_lowess, data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (8-12 LOWESS)')\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"G_qX167ERYse"},"source":["### **Save Newly Computed Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Smg08trrRYXi"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","sats_filename_str = f'sentiment_sents_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Sentence Sentiments with LOWESS in file: {sats_filename_str}')\n","\n","sats_filename_str = f'sentiment_parags_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Paragraph Sentiments with LOWESS in file: {sats_filename_str}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2xhJ-IjNEii"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRRtd0VPNFaO"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"4iAMKo6cNFvh"},"source":["## **(4) BERT NLP Town**"]},{"cell_type":"markdown","metadata":{"id":"74vLULfLNFvi"},"source":["### **Sentiment Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvPUBvqKQLpa"},"outputs":[],"source":["from transformers import AutoTokenizer,AutoModelForSequenceClassification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJf0eIk-QLpd"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDW2ToklQLpb"},"outputs":[],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7X-t-qZQLpe"},"outputs":[],"source":["import sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zd-OrQ06QLpf"},"outputs":[],"source":["# Instantiate model\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"]},{"cell_type":"markdown","metadata":{"id":"PFJEI54t9N9G"},"source":["#### **Sentences**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aQsbTOwQLpg"},"outputs":[],"source":["tokens = tokenizer.encode(\"I hated mas Yoza, he absolutely the worst mentor\", return_tensors='pt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ovx6Sjn7QLph"},"outputs":[],"source":["# Predict Tokens\n","tokens = tokenizer.encode(\"It wasn't the worst i've seen, in fact, it was the opposite\", return_tensors='pt')\n","# tokens[0]\n","# tokenizer.decode(tokens[0])\n","result = model(tokens)\n","result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jScyh0b0QLpi"},"outputs":[],"source":["predict_sentiment = int(torch.argmax(result.logits))+1\n","predict_sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1kphOsuQLpj"},"outputs":[],"source":["def nlptown_sentiment_score(text):\n","  '''\n","  Given a text string (sentence or paragraph)\n","  Return a floating point sentiment value\n","  '''\n","\n","  # tokens = tokenizer.encode(text, return_tensors='pt')\n","  # result = model(tokens)\n","  # sentiment_int = int(torch.argmax(result.logits))+1\n","  # sentiment_fl = sentiment_int + result.logits[sentiment_int-1]\n","  # return sentiment_fl\n","\n","  tokens = tokenizer.encode(text, return_tensors='pt')\n","  result = model(tokens)\n","  type(result)\n","  prob_ls = list(result.logits)[0].tolist()\n","  # print(f'prob_ls: {prob_ls}')\n","  # prob_ls_sum = sum(prob_ls)\n","  prob_ls_sum = sum(map(abs, prob_ls))\n","  prob_norm_ls = [abs(i/prob_ls_sum) for i in prob_ls]\n","  # prob_ls_min = min(prob_ls)\n","  # prob_ls_max = max(prob_ls)\n","  # prob_norm_ls = [(x-prob_ls_min)/(prob_ls_max-prob_ls_min) for x in prob_ls]\n","  # print(f'prob_norm_ls {prob_norm_ls}')\n","  prob_int = int(torch.argmax(result.logits))\n","  # print(f'prob_int {prob_int}')\n","  prob_frac = abs(float(prob_norm_ls[prob_int]))\n","  # print(f'prob_frac {prob_frac}')\n","  \n","  return prob_int + prob_frac # int(torch.argmax(result.logits))+1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUjsHtvIQLpk"},"outputs":[],"source":["nlptown_sentiment_score('i love the smell of beautiful flowers, the make me happy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_HZOv7pQLpl"},"outputs":[],"source":["%time\n","\n","# NOTE: 10m Long-running process\n","\n","# Calculate Sentence Sentiment Scores using the NLPTown BERT fine-grained, fine-tuned, multi-lingual model\n","\n","# https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n","\n","# This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in \n","#    six languages: English, Dutch, German, French, Spanish and Italian. \n","#    It predicts the sentiment of the review as a number of stars (between 1 and 5).\n","\n","corpus_sents_df['nlptown'] = corpus_sents_df['sent_raw'].astype('str').apply(lambda x: nlptown_sentiment_score(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3Bq5s1YQLpl"},"outputs":[],"source":["# Verify\n","\n","corpus_sents_df.iloc[:3]"]},{"cell_type":"markdown","metadata":{"id":"CX3qCPuP9GAl"},"source":["#### **Paragraphs**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4R5yRgTVQLpm"},"outputs":[],"source":["def trim_maxtokens(astr, token_max):\n","  '''\n","  Given an input string of tokens and a maximum token limit\n","  Return a string at token limit by dropping frequent words w/o sentiment value first, them random\n","  '''\n","  deadwords_ls = ['the','of','to','and','a','in','is','it','that','was','for','on','are','with','as','be','at','one','have','this','from','or','had','by']\n","\n","  astr_ls = astr.split()\n","  deadword_idx = 0\n","  while deadword_idx < len(deadwords_ls):\n","    del_word = deadwords_ls[deadword_idx]\n","    astr_ls = [aword for aword in astr_ls if aword != del_word]\n","    deadword_idx += 1\n","\n","  if len(astr_ls) > token_max:\n","    print('too long')\n","    # Start removing longest words first, random words, shortest words, POS, Capitalized?\n","    while len(astr_ls) > token_max:\n","      random_list_element = random.choice(astr_ls)\n","      astr_ls.remove(random_list_element)\n","\n","  astr_condensed = ' '.join(astr_ls)\n","\n","  return astr_condensed\n","\n","# Test\n","# sentences_condensed = trim_maxtokens('Hello big boy! What the heck are you doing here?', 6)\n","# sentences_condensed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pnx3rTjUQLpm"},"outputs":[],"source":["# Calculate Paragraph Sentiment Scores using the NLPTown BERT fine-grained, fine-tuned, multi-lingual model\n","\n","# NOTE: Long-running\n","\n","# https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n","\n","# This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in \n","#    six languages: English, Dutch, German, French, Spanish and Italian. \n","#    It predicts the sentiment of the review as a number of stars (between 1 and 5).\n","\n","corpus_parags_df['nlptown'] = corpus_parags_df['parag_raw'].astype('str').apply(lambda x: nlptown_sentiment_score(trim_maxtokens(x,510)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ymvdb0mpQLpo"},"outputs":[],"source":["# Verify\n","\n","corpus_parags_df[:2]"]},{"cell_type":"markdown","metadata":{"id":"Mxbh4h96QLpo"},"source":["# **Save Newest Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCmyhhA8QLpo"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","# sats_filename_str = f'sentiment_sents_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","sats_filename_str = f'../sum_sentiments_sents_transformers_{author_str}_{title_str}.csv' # _{datetime_now}.csv'\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Sentence Sentiments with NLPTown added to file: {sats_filename_str}')\n","\n","# sats_filename_str = f'sentiment_parags_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","sats_filename_str = f'../sum_sentiments_parags_transformers_{author_str}_{title_str}.csv' # _{datetime_now}.csv'\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Paragraph Sentiments with NLPTown added to file: {sats_filename_str}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vk-Ws6SbQLBx"},"outputs":[],"source":["corpus_sents_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFr7ZVCLQKqy"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OU_zzCywNFvj"},"outputs":[],"source":["# Setup for RoBERTa Large English 15datasets: siebert/sentiment-roberta-large-english\n","\n","sa_model = 'robertalg15'\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n","\n","# Create class for data preparation\n","class SimpleDataset:\n","    def __init__(self, tokenized_texts):\n","        self.tokenized_texts = tokenized_texts\n","    \n","    def __len__(self):\n","        return len(self.tokenized_texts[\"input_ids\"])\n","    \n","    def __getitem__(self, idx):\n","        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","\n","\n","# Set model\n","sa_model = 'robertalg15'\n","model_name = \"siebert/sentiment-roberta-large-english\"\n","\n","# Load tokenizer and model, create trainer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","trainer = Trainer(model=model);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l6XZY5VHNFvk"},"outputs":[],"source":["# Prepare Text from DataFrame\n","\n","sents_pred_df = corpus_sents_df.copy()\n","sents_pred_texts = sents_pred_df['sent_raw'].astype('str').tolist() # Want to catch NaN, .dropna().astype('str').tolist()\n","\n","# Tokenize texts and create prediction data set\n","\n","sents_tokenized_texts = tokenizer(sents_pred_texts,truncation=True,padding=True)\n","sents_pred_dataset = SimpleDataset(sents_tokenized_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hk9Dh4BWNFvm"},"outputs":[],"source":["# Run predictions\n","\n","sents_predictions = trainer.predict(sents_pred_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nLH8gAiNFvm"},"outputs":[],"source":["temp_sentiment_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSfTuAS7NFvn"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IM5c0xIGNFvn"},"outputs":[],"source":["temp_sentiment_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1pKCQKFNFvo"},"outputs":[],"source":["temp_ser = temp_sentiment_df.apply(lambda x: polprob2sentiment(str(x.label), float(x.score)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywfXoIwqNFvp"},"outputs":[],"source":["# Transform predictions to labels\n","\n","sents_preds = sents_predictions.predictions.argmax(-1)\n","sents_labels = pd.Series(sents_preds).map(model.config.id2label)\n","sents_scores = (np.exp(sents_predictions[0])/np.exp(sents_predictions[0]).sum(-1,keepdims=True)).max(1)\n","\n","# Create DataFrame with texts, predictions, labels, and scores\n","\n","temp_sentiment_df = pd.DataFrame(list(zip(sents_pred_texts,sents_preds,sents_labels,sents_scores)), columns=['text','pred','label','score'])\n","# temp_sentiment_df.head()\n","\n","# Convert label (Neg/Pos) and score (Prob) to a +/-Sentiment Float Value\n","\n","corpus_sents_df[sa_model] = temp_sentiment_df.apply(lambda x: polprob2sentiment(x.label,x.score), axis=1)\n","corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFjrn2J6NFvq"},"outputs":[],"source":["# Verify the head and tail are complete and correct\n","\n","corpus_parags_df.iloc[:3]\n","corpus_parags_df.iloc[-3:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGgpP0KYNFvq"},"outputs":[],"source":["# Aggregate Sentence Sentiments to populate Paragraph Sentiment DataFrame\n","\n","# def sentiment_sents2parags(ts_df, model_name='roberta_lg15'):\n","parags_sentiment_ls = sentiment_sents2parags(corpus_sents_df, sa_model)\n","corpus_parags_df[sa_model] = pd.Series(parags_sentiment_ls)\n","corpus_parags_df.head(2)\n","corpus_parags_df.tail(2)"]},{"cell_type":"markdown","metadata":{"id":"HhPNBrDoNFvr"},"source":["### **Histogram Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jU0-yWoXNFvr"},"outputs":[],"source":["# Debug\n","PLOT_OUTPUT = 'Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3QD9XvANFvr"},"outputs":[],"source":["# Create histogram of Sentence Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_sents_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_sents_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsWp1t2bNFvr"},"outputs":[],"source":["# Create histogram of Paragraph Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_parags_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_parags_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"markdown","metadata":{"id":"kHf9t7N5NFvs"},"source":["### **Mean SMA Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFFPvIvRNFvs"},"outputs":[],"source":["# SMA % Sentiment of Sentence Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_sents_df, sa_model, text_unit='sentence', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ho5_8BV-NFvt"},"outputs":[],"source":["# SMA % Sentiment of Paragraph Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_parags_df, sa_model, text_unit='paragraph', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C2tCPrcyNFvt"},"outputs":[],"source":["# Compare Sentence and Paragraph\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y='roberta_lg15_mean_roll100', data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (10% SMA)')\n","  sns.lineplot(x=corpus_parags_df.index, y='roberta_lg15_mean_roll100', data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (10% SMA)')\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"acT1j_ZfNFvv"},"outputs":[],"source":["# corpus_sents_df.drop(columns=['y_scaled'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"zBL3d0ZxNFvv"},"source":["### **Stanford ASAP Plot**\n"]},{"cell_type":"markdown","metadata":{"id":"KZBrX0_5NFvv"},"source":["**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n","\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n","* http://futuredata.stanford.edu/asap/ \n","* https://www.datadoghq.com/blog/auto-smoother-asap/"]},{"cell_type":"markdown","metadata":{"id":"jbFCPug9NFvv"},"source":["**Save the following plots to gDrive files?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mM0G3V91NFvw"},"outputs":[],"source":["Save_to_File = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ggs6UlXNFvw","scrolled":false},"outputs":[],"source":["# Sentence SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_sents_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, do_plot=True, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxeIk8FLNFvx"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hP3dRx3NFvx"},"outputs":[],"source":["# Paragraph SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_parags_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNLyo-1qNFvy"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"markdown","metadata":{"id":"Oyk0VMxdNFvy"},"source":["### **LOWESS Plots**"]},{"cell_type":"markdown","metadata":{"id":"-y27FB32NFvy"},"source":["**Sentence Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ra0vhowaNFvz"},"outputs":[],"source":["# Debug\n","\n","PLOT_OUTPUT='Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mctur1wQNFvz"},"outputs":[],"source":["# Plot Sentence Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./4, 1./6, 1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_sents_df[new_lowess_mean_col] = corpus_sents_df[cols_lowess].mean(axis=1)\n","\n","corpus_sents_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6tzGEjYNFv0"},"outputs":[],"source":["corpus_parags_df.columns"]},{"cell_type":"markdown","metadata":{"id":"nvDMvRlLNFv1"},"source":["**Paragraph Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDsNRy-CNFv1"},"outputs":[],"source":["# Plot Paragraph Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_parags_df[new_lowess_col] = plot_lowess(corpus_parags_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_parags_df[new_lowess_mean_col] = corpus_parags_df[cols_lowess].mean(axis=1)\n","\n","corpus_parags_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"S4VpcpjiNFv1"},"source":["**Compare Sentence and Paragraph Median LOWESS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bcPhQeHNFv2"},"outputs":[],"source":["# Compare Sentence and Paragraph LOWESS means\n","\n","# Calculate the Sentence and Paragraph LOWESS means and plot\n","\n","# Get all the calculated LOWESS columns in a list \n","cols_sents_lowess = matching_cols = get_cols_regex(corpus_sents_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","cols_parags_lowess = matching_cols = get_cols_regex(corpus_parags_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","\n","# Compute the mean for all the LOWESS columns\n","col_mean_lowess = f'{sa_model}_mean_lowess'\n","corpus_sents_df[col_mean_lowess] = corpus_sents_df[cols_sents_lowess].mean(axis=1)\n","corpus_parags_df[col_mean_lowess] = corpus_parags_df[cols_parags_lowess].mean(axis=1)\n","\n","# Plot the Sentence and Paragraph LOWESS means\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y=col_mean_lowess, data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (8-12 LOWESS)')\n","  sns.lineplot(x=corpus_parags_df.index, y=col_mean_lowess, data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (8-12 LOWESS)')\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"dEtMIy9pNFv2"},"source":["### **Save Newly Computed Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlCkHSjbNFv3"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","sats_filename_str = f'sentiment_sents_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Sentence Sentiments with LOWESS in file: {sats_filename_str}')\n","\n","sats_filename_str = f'sentiment_parags_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Paragraph Sentiments with LOWESS in file: {sats_filename_str}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MMgtdY8FNFJc"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KnPZY3J8Naje"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"UeEQ0mCVNa6a"},"source":["## **ALBERT Sentiment Analysis**"]},{"cell_type":"markdown","metadata":{"id":"JZQ5GwtxNa6c"},"source":["### **Sentiment Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58Flq0XPNa6d"},"outputs":[],"source":["# Setup for RoBERTa Large English 15datasets: siebert/sentiment-roberta-large-english\n","\n","sa_model = 'robertalg15'\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n","\n","# Create class for data preparation\n","class SimpleDataset:\n","    def __init__(self, tokenized_texts):\n","        self.tokenized_texts = tokenized_texts\n","    \n","    def __len__(self):\n","        return len(self.tokenized_texts[\"input_ids\"])\n","    \n","    def __getitem__(self, idx):\n","        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","\n","\n","# Set model\n","sa_model = 'robertalg15'\n","model_name = \"siebert/sentiment-roberta-large-english\"\n","\n","# Load tokenizer and model, create trainer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","trainer = Trainer(model=model);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-Z21_jhNa6f"},"outputs":[],"source":["# Prepare Text from DataFrame\n","\n","sents_pred_df = corpus_sents_df.copy()\n","sents_pred_texts = sents_pred_df['sent_raw'].astype('str').tolist() # Want to catch NaN, .dropna().astype('str').tolist()\n","\n","# Tokenize texts and create prediction data set\n","\n","sents_tokenized_texts = tokenizer(sents_pred_texts,truncation=True,padding=True)\n","sents_pred_dataset = SimpleDataset(sents_tokenized_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"netYf4AkNa6h"},"outputs":[],"source":["# Run predictions\n","\n","sents_predictions = trainer.predict(sents_pred_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"viApVTuJNa6j"},"outputs":[],"source":["temp_sentiment_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWXsCDoXNa6k"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bStxm1nLNa6m"},"outputs":[],"source":["temp_sentiment_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFVFHl-KNa6m"},"outputs":[],"source":["temp_ser = temp_sentiment_df.apply(lambda x: polprob2sentiment(str(x.label), float(x.score)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YENmxunXNa6n"},"outputs":[],"source":["# Transform predictions to labels\n","\n","sents_preds = sents_predictions.predictions.argmax(-1)\n","sents_labels = pd.Series(sents_preds).map(model.config.id2label)\n","sents_scores = (np.exp(sents_predictions[0])/np.exp(sents_predictions[0]).sum(-1,keepdims=True)).max(1)\n","\n","# Create DataFrame with texts, predictions, labels, and scores\n","\n","temp_sentiment_df = pd.DataFrame(list(zip(sents_pred_texts,sents_preds,sents_labels,sents_scores)), columns=['text','pred','label','score'])\n","# temp_sentiment_df.head()\n","\n","# Convert label (Neg/Pos) and score (Prob) to a +/-Sentiment Float Value\n","\n","corpus_sents_df[sa_model] = temp_sentiment_df.apply(lambda x: polprob2sentiment(x.label,x.score), axis=1)\n","corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pdl1Ur8JNa6o"},"outputs":[],"source":["# Verify the head and tail are complete and correct\n","\n","corpus_parags_df.iloc[:3]\n","corpus_parags_df.iloc[-3:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Imfkbq-xNa6p"},"outputs":[],"source":["# Aggregate Sentence Sentiments to populate Paragraph Sentiment DataFrame\n","\n","# def sentiment_sents2parags(ts_df, model_name='roberta_lg15'):\n","parags_sentiment_ls = sentiment_sents2parags(corpus_sents_df, sa_model)\n","corpus_parags_df[sa_model] = pd.Series(parags_sentiment_ls)\n","corpus_parags_df.head(2)\n","corpus_parags_df.tail(2)"]},{"cell_type":"markdown","metadata":{"id":"wIG6jJ4VNa6q"},"source":["### **Histogram Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qm4jxLoeNa6q"},"outputs":[],"source":["# Debug\n","PLOT_OUTPUT = 'Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYFVnekpNa6q"},"outputs":[],"source":["# Create histogram of Sentence Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_sents_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_sents_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMS1B8ObNa6s"},"outputs":[],"source":["# Create histogram of Paragraph Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_parags_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_parags_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"markdown","metadata":{"id":"4244TFWbNa6t"},"source":["### **Mean SMA Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4r6LnAIxNa6t"},"outputs":[],"source":["# SMA % Sentiment of Sentence Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_sents_df, sa_model, text_unit='sentence', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zBOI5KNGNa6t"},"outputs":[],"source":["# SMA % Sentiment of Paragraph Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_parags_df, sa_model, text_unit='paragraph', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQey5n4INa6u"},"outputs":[],"source":["# Compare Sentence and Paragraph\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y='roberta_lg15_mean_roll100', data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (10% SMA)')\n","  sns.lineplot(x=corpus_parags_df.index, y='roberta_lg15_mean_roll100', data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (10% SMA)')\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A2cjhQxGNa6u"},"outputs":[],"source":["# corpus_sents_df.drop(columns=['y_scaled'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"p22P8FT_Na6u"},"source":["### **Stanford ASAP Plot**\n"]},{"cell_type":"markdown","metadata":{"id":"ijSTqg_XNa6u"},"source":["**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n","\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n","* http://futuredata.stanford.edu/asap/ \n","* https://www.datadoghq.com/blog/auto-smoother-asap/"]},{"cell_type":"markdown","metadata":{"id":"4De6Gxp7Na6v"},"source":["**Save the following plots to gDrive files?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x77vnmaSNa6w"},"outputs":[],"source":["Save_to_File = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjChRndrNa6w","scrolled":false},"outputs":[],"source":["# Sentence SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_sents_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, do_plot=True, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZSxF8eNNa6w"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDRkTg7gNa6x"},"outputs":[],"source":["# Paragraph SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_parags_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-E72rgYNa6x"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"markdown","metadata":{"id":"BQt7Dn7ENa6y"},"source":["### **LOWESS Plots**"]},{"cell_type":"markdown","metadata":{"id":"N7b4SDIONa6z"},"source":["**Sentence Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZpAo3TwNa6z"},"outputs":[],"source":["# Debug\n","\n","PLOT_OUTPUT='Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-CW9Q9xSNa60"},"outputs":[],"source":["# Plot Sentence Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./4, 1./6, 1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_sents_df[new_lowess_mean_col] = corpus_sents_df[cols_lowess].mean(axis=1)\n","\n","corpus_sents_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LN8ufUfeNa60"},"outputs":[],"source":["corpus_parags_df.columns"]},{"cell_type":"markdown","metadata":{"id":"vPFxcyoJNa61"},"source":["**Paragraph Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkBsYQsfNa61"},"outputs":[],"source":["# Plot Paragraph Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_parags_df[new_lowess_col] = plot_lowess(corpus_parags_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_parags_df[new_lowess_mean_col] = corpus_parags_df[cols_lowess].mean(axis=1)\n","\n","corpus_parags_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"SZayIiqrNa61"},"source":["**Compare Sentence and Paragraph Median LOWESS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mh40xm4KNa62"},"outputs":[],"source":["# Compare Sentence and Paragraph LOWESS means\n","\n","# Calculate the Sentence and Paragraph LOWESS means and plot\n","\n","# Get all the calculated LOWESS columns in a list \n","cols_sents_lowess = matching_cols = get_cols_regex(corpus_sents_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","cols_parags_lowess = matching_cols = get_cols_regex(corpus_parags_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","\n","# Compute the mean for all the LOWESS columns\n","col_mean_lowess = f'{sa_model}_mean_lowess'\n","corpus_sents_df[col_mean_lowess] = corpus_sents_df[cols_sents_lowess].mean(axis=1)\n","corpus_parags_df[col_mean_lowess] = corpus_parags_df[cols_parags_lowess].mean(axis=1)\n","\n","# Plot the Sentence and Paragraph LOWESS means\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y=col_mean_lowess, data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (8-12 LOWESS)')\n","  sns.lineplot(x=corpus_parags_df.index, y=col_mean_lowess, data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (8-12 LOWESS)')\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"UHu-7wPfNa62"},"source":["### **Save Newly Computed Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JkjU6LCNa62"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","sats_filename_str = f'sentiment_sents_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Sentence Sentiments with LOWESS in file: {sats_filename_str}')\n","\n","sats_filename_str = f'sentiment_parags_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Paragraph Sentiments with LOWESS in file: {sats_filename_str}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXF8YwQqNaf_"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"OxjBSH1oQ9ax"},"source":["## **XLNet Sentiment Analysis**"]},{"cell_type":"markdown","metadata":{"id":"mszalXKRQ9ay"},"source":["### **Sentiment Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h35RYbkHQ9a1"},"outputs":[],"source":["# Setup for RoBERTa Large English 15datasets: siebert/sentiment-roberta-large-english\n","\n","sa_model = 'robertalg15'\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n","\n","# Create class for data preparation\n","class SimpleDataset:\n","    def __init__(self, tokenized_texts):\n","        self.tokenized_texts = tokenized_texts\n","    \n","    def __len__(self):\n","        return len(self.tokenized_texts[\"input_ids\"])\n","    \n","    def __getitem__(self, idx):\n","        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","\n","\n","# Set model\n","sa_model = 'robertalg15'\n","model_name = \"siebert/sentiment-roberta-large-english\"\n","\n","# Load tokenizer and model, create trainer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","trainer = Trainer(model=model);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XoSsMpMmQ9a2"},"outputs":[],"source":["# Prepare Text from DataFrame\n","\n","sents_pred_df = corpus_sents_df.copy()\n","sents_pred_texts = sents_pred_df['sent_raw'].astype('str').tolist() # Want to catch NaN, .dropna().astype('str').tolist()\n","\n","# Tokenize texts and create prediction data set\n","\n","sents_tokenized_texts = tokenizer(sents_pred_texts,truncation=True,padding=True)\n","sents_pred_dataset = SimpleDataset(sents_tokenized_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUvy0rzWQ9a2"},"outputs":[],"source":["# Run predictions\n","\n","sents_predictions = trainer.predict(sents_pred_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vuzDU5mqQ9a3"},"outputs":[],"source":["temp_sentiment_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfVUwkITQ9a5"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vRpke_SQ9a5"},"outputs":[],"source":["temp_sentiment_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9_Rv7qmQ9a6"},"outputs":[],"source":["temp_ser = temp_sentiment_df.apply(lambda x: polprob2sentiment(str(x.label), float(x.score)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MwJSAjjdQ9a7"},"outputs":[],"source":["# Transform predictions to labels\n","\n","sents_preds = sents_predictions.predictions.argmax(-1)\n","sents_labels = pd.Series(sents_preds).map(model.config.id2label)\n","sents_scores = (np.exp(sents_predictions[0])/np.exp(sents_predictions[0]).sum(-1,keepdims=True)).max(1)\n","\n","# Create DataFrame with texts, predictions, labels, and scores\n","\n","temp_sentiment_df = pd.DataFrame(list(zip(sents_pred_texts,sents_preds,sents_labels,sents_scores)), columns=['text','pred','label','score'])\n","# temp_sentiment_df.head()\n","\n","# Convert label (Neg/Pos) and score (Prob) to a +/-Sentiment Float Value\n","\n","corpus_sents_df[sa_model] = temp_sentiment_df.apply(lambda x: polprob2sentiment(x.label,x.score), axis=1)\n","corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1WGW4LC_Q9a8"},"outputs":[],"source":["# Verify the head and tail are complete and correct\n","\n","corpus_parags_df.iloc[:3]\n","corpus_parags_df.iloc[-3:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PA0xFgU9Q9a9"},"outputs":[],"source":["# Aggregate Sentence Sentiments to populate Paragraph Sentiment DataFrame\n","\n","# def sentiment_sents2parags(ts_df, model_name='roberta_lg15'):\n","parags_sentiment_ls = sentiment_sents2parags(corpus_sents_df, sa_model)\n","corpus_parags_df[sa_model] = pd.Series(parags_sentiment_ls)\n","corpus_parags_df.head(2)\n","corpus_parags_df.tail(2)"]},{"cell_type":"markdown","metadata":{"id":"X-Ptl7MJQ9a-"},"source":["### **Histogram Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Liy0Pr98Q9a_"},"outputs":[],"source":["# Debug\n","PLOT_OUTPUT = 'Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FWMTh5XQ9bA"},"outputs":[],"source":["# Create histogram of Sentence Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_sents_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_sents_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mggAs_xSQ9bA"},"outputs":[],"source":["# Create histogram of Paragraph Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_parags_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_parags_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"markdown","metadata":{"id":"EnHQinLPQ9bB"},"source":["### **Mean SMA Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePIvhT_lQ9bB"},"outputs":[],"source":["# SMA % Sentiment of Sentence Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_sents_df, sa_model, text_unit='sentence', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOk_JJv3Q9bB"},"outputs":[],"source":["# SMA % Sentiment of Paragraph Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_parags_df, sa_model, text_unit='paragraph', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxn5vzroQ9bD"},"outputs":[],"source":["# Compare Sentence and Paragraph\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y='roberta_lg15_mean_roll100', data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (10% SMA)')\n","  sns.lineplot(x=corpus_parags_df.index, y='roberta_lg15_mean_roll100', data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (10% SMA)')\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oa5u7yklQ9bE"},"outputs":[],"source":["# corpus_sents_df.drop(columns=['y_scaled'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"10FKuTrHQ9bE"},"source":["### **Stanford ASAP Plot**\n"]},{"cell_type":"markdown","metadata":{"id":"NEFUAOU_Q9bF"},"source":["**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n","\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n","* http://futuredata.stanford.edu/asap/ \n","* https://www.datadoghq.com/blog/auto-smoother-asap/"]},{"cell_type":"markdown","metadata":{"id":"6YXsGbMsQ9bF"},"source":["**Save the following plots to gDrive files?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8vTdgl58Q9bF"},"outputs":[],"source":["Save_to_File = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H64J92HCQ9bG","scrolled":false},"outputs":[],"source":["# Sentence SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_sents_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, do_plot=True, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ES44pQZ6Q9bG"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTNOn-IGQ9bH"},"outputs":[],"source":["# Paragraph SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_parags_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3sOYWz_Q9bH"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"markdown","metadata":{"id":"qOJc7UqMQ9bI"},"source":["### **LOWESS Plots**"]},{"cell_type":"markdown","metadata":{"id":"I9d6EERFQ9bI"},"source":["**Sentence Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xG4fLAKyQ9bI"},"outputs":[],"source":["# Debug\n","\n","PLOT_OUTPUT='Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQLJsfWvQ9bJ"},"outputs":[],"source":["# Plot Sentence Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./4, 1./6, 1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_sents_df[new_lowess_mean_col] = corpus_sents_df[cols_lowess].mean(axis=1)\n","\n","corpus_sents_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yy2MKu5aQ9bJ"},"outputs":[],"source":["corpus_parags_df.columns"]},{"cell_type":"markdown","metadata":{"id":"ekWb2IiZQ9bK"},"source":["**Paragraph Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DP7KIObOQ9bL"},"outputs":[],"source":["# Plot Paragraph Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_parags_df[new_lowess_col] = plot_lowess(corpus_parags_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_parags_df[new_lowess_mean_col] = corpus_parags_df[cols_lowess].mean(axis=1)\n","\n","corpus_parags_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"IwT0P-csQ9bL"},"source":["**Compare Sentence and Paragraph Median LOWESS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMxtNdhBQ9bL"},"outputs":[],"source":["# Compare Sentence and Paragraph LOWESS means\n","\n","# Calculate the Sentence and Paragraph LOWESS means and plot\n","\n","# Get all the calculated LOWESS columns in a list \n","cols_sents_lowess = matching_cols = get_cols_regex(corpus_sents_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","cols_parags_lowess = matching_cols = get_cols_regex(corpus_parags_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","\n","# Compute the mean for all the LOWESS columns\n","col_mean_lowess = f'{sa_model}_mean_lowess'\n","corpus_sents_df[col_mean_lowess] = corpus_sents_df[cols_sents_lowess].mean(axis=1)\n","corpus_parags_df[col_mean_lowess] = corpus_parags_df[cols_parags_lowess].mean(axis=1)\n","\n","# Plot the Sentence and Paragraph LOWESS means\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y=col_mean_lowess, data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (8-12 LOWESS)')\n","  sns.lineplot(x=corpus_parags_df.index, y=col_mean_lowess, data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (8-12 LOWESS)')\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"66X4VsqOQ9bL"},"source":["### **Save Newly Computed Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iK_SPZwMQ9bM"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","sats_filename_str = f'sentiment_sents_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Sentence Sentiments with LOWESS in file: {sats_filename_str}')\n","\n","sats_filename_str = f'sentiment_parags_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Paragraph Sentiments with LOWESS in file: {sats_filename_str}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MX6U1-_uRJOZ"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"aoJKjcAuRWLp"},"source":["## **T5 Sentiment Analysis**\n","\n","* https://huggingface.co/mrm8488/t5-base-finetuned-imdb-sentiment"]},{"cell_type":"markdown","metadata":{"id":"ZAeo69QSRWLr"},"source":["### **Sentiment Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJ2XuDXIrC2R"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelWithLMHead\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")\n","\n","model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")\n","\n","def get_sentiment(text):\n","  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n","\n","  output = model.generate(input_ids=input_ids,\n","               max_length=2)\n","\n","  dec = [tokenizer.decode(ids) for ids in output]\n","  label = dec[0]\n","\n","  return dec\n","\n","return_str = get_sentiment(\"I like a lot that film\")\n","print(return_str[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-iTTG3gVrCyH"},"outputs":[],"source":["return_str = get_sentiment(\"I like a lot that film\")\n","type(return_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbExszvyrCt6"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_MwbgBn8rCpI"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KliEkOwcRWLr"},"outputs":[],"source":["# Setup for RoBERTa Large English 15datasets: siebert/sentiment-roberta-large-english\n","\n","sa_model = 'robertalg15'\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n","\n","# Create class for data preparation\n","class SimpleDataset:\n","    def __init__(self, tokenized_texts):\n","        self.tokenized_texts = tokenized_texts\n","    \n","    def __len__(self):\n","        return len(self.tokenized_texts[\"input_ids\"])\n","    \n","    def __getitem__(self, idx):\n","        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","\n","\n","# Set model\n","sa_model = 'robertalg15'\n","model_name = \"siebert/sentiment-roberta-large-english\"\n","\n","# Load tokenizer and model, create trainer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","trainer = Trainer(model=model);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1ufU6ZTRWLs"},"outputs":[],"source":["# Prepare Text from DataFrame\n","\n","sents_pred_df = corpus_sents_df.copy()\n","sents_pred_texts = sents_pred_df['sent_raw'].astype('str').tolist() # Want to catch NaN, .dropna().astype('str').tolist()\n","\n","# Tokenize texts and create prediction data set\n","\n","sents_tokenized_texts = tokenizer(sents_pred_texts,truncation=True,padding=True)\n","sents_pred_dataset = SimpleDataset(sents_tokenized_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5USFL47QRWLt"},"outputs":[],"source":["# Run predictions\n","\n","sents_predictions = trainer.predict(sents_pred_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbwBYV6vRWLv"},"outputs":[],"source":["temp_sentiment_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRBnCcceRWLw"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Que5aNVFRWLx"},"outputs":[],"source":["temp_sentiment_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l1AFc9CXRWLy"},"outputs":[],"source":["temp_ser = temp_sentiment_df.apply(lambda x: polprob2sentiment(str(x.label), float(x.score)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqgTZuzMRWLy"},"outputs":[],"source":["# Transform predictions to labels\n","\n","sents_preds = sents_predictions.predictions.argmax(-1)\n","sents_labels = pd.Series(sents_preds).map(model.config.id2label)\n","sents_scores = (np.exp(sents_predictions[0])/np.exp(sents_predictions[0]).sum(-1,keepdims=True)).max(1)\n","\n","# Create DataFrame with texts, predictions, labels, and scores\n","\n","temp_sentiment_df = pd.DataFrame(list(zip(sents_pred_texts,sents_preds,sents_labels,sents_scores)), columns=['text','pred','label','score'])\n","# temp_sentiment_df.head()\n","\n","# Convert label (Neg/Pos) and score (Prob) to a +/-Sentiment Float Value\n","\n","corpus_sents_df[sa_model] = temp_sentiment_df.apply(lambda x: polprob2sentiment(x.label,x.score), axis=1)\n","corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_A6gl1hRWL0"},"outputs":[],"source":["# Verify the head and tail are complete and correct\n","\n","corpus_parags_df.iloc[:3]\n","corpus_parags_df.iloc[-3:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iwF318Y_RWL2"},"outputs":[],"source":["# Aggregate Sentence Sentiments to populate Paragraph Sentiment DataFrame\n","\n","# def sentiment_sents2parags(ts_df, model_name='roberta_lg15'):\n","parags_sentiment_ls = sentiment_sents2parags(corpus_sents_df, sa_model)\n","corpus_parags_df[sa_model] = pd.Series(parags_sentiment_ls)\n","corpus_parags_df.head(2)\n","corpus_parags_df.tail(2)"]},{"cell_type":"markdown","metadata":{"id":"cFhxaDsRRWL3"},"source":["### **Histogram Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90XZDN3GRWL4"},"outputs":[],"source":["# Debug\n","PLOT_OUTPUT = 'Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Gg9x7rGRWL4"},"outputs":[],"source":["# Create histogram of Sentence Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_sents_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_sents_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBNMJiZ8RWL5"},"outputs":[],"source":["# Create histogram of Paragraph Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_parags_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_parags_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"markdown","metadata":{"id":"h2_kqauIRWL5"},"source":["### **Mean SMA Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoW9Zv6FRWL5"},"outputs":[],"source":["# SMA % Sentiment of Sentence Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_sents_df, sa_model, text_unit='sentence', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2o0zGaR7RWL7"},"outputs":[],"source":["# SMA % Sentiment of Paragraph Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_parags_df, sa_model, text_unit='paragraph', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2dw7jXoRWL8"},"outputs":[],"source":["# Compare Sentence and Paragraph\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y='roberta_lg15_mean_roll100', data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (10% SMA)')\n","  sns.lineplot(x=corpus_parags_df.index, y='roberta_lg15_mean_roll100', data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (10% SMA)')\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ROfgmVsNRWL8"},"outputs":[],"source":["# corpus_sents_df.drop(columns=['y_scaled'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"spzcqVNERWL8"},"source":["### **Stanford ASAP Plot**\n"]},{"cell_type":"markdown","metadata":{"id":"k7oiRTaYRWL9"},"source":["**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n","\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n","* http://futuredata.stanford.edu/asap/ \n","* https://www.datadoghq.com/blog/auto-smoother-asap/"]},{"cell_type":"markdown","metadata":{"id":"ZfwBzSEbRWL9"},"source":["**Save the following plots to gDrive files?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7vah6E8LRWL9"},"outputs":[],"source":["Save_to_File = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WmD_CtPDRWL-","scrolled":false},"outputs":[],"source":["# Sentence SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_sents_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, do_plot=True, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgpK8wUORWL_"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"POjuRYxGRWL_"},"outputs":[],"source":["# Paragraph SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_parags_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjgO6zjrRWMA"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"markdown","metadata":{"id":"_7gQPdzIRWMA"},"source":["### **LOWESS Plots**"]},{"cell_type":"markdown","metadata":{"id":"dKD-ThJURWMA"},"source":["**Sentence Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W50MIw2wRWMB"},"outputs":[],"source":["# Debug\n","\n","PLOT_OUTPUT='Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SWaT3q-RWMD"},"outputs":[],"source":["# Plot Sentence Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./4, 1./6, 1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_sents_df[new_lowess_mean_col] = corpus_sents_df[cols_lowess].mean(axis=1)\n","\n","corpus_sents_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQy7c5FHRWMD"},"outputs":[],"source":["corpus_parags_df.columns"]},{"cell_type":"markdown","metadata":{"id":"hg10ApzERWME"},"source":["**Paragraph Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCdoFtx0RWME"},"outputs":[],"source":["# Plot Paragraph Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_parags_df[new_lowess_col] = plot_lowess(corpus_parags_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_parags_df[new_lowess_mean_col] = corpus_parags_df[cols_lowess].mean(axis=1)\n","\n","corpus_parags_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"Yy2ZyG3YRWMF"},"source":["**Compare Sentence and Paragraph Median LOWESS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1nP-NBiRWMF"},"outputs":[],"source":["# Compare Sentence and Paragraph LOWESS means\n","\n","# Calculate the Sentence and Paragraph LOWESS means and plot\n","\n","# Get all the calculated LOWESS columns in a list \n","cols_sents_lowess = matching_cols = get_cols_regex(corpus_sents_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","cols_parags_lowess = matching_cols = get_cols_regex(corpus_parags_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","\n","# Compute the mean for all the LOWESS columns\n","col_mean_lowess = f'{sa_model}_mean_lowess'\n","corpus_sents_df[col_mean_lowess] = corpus_sents_df[cols_sents_lowess].mean(axis=1)\n","corpus_parags_df[col_mean_lowess] = corpus_parags_df[cols_parags_lowess].mean(axis=1)\n","\n","# Plot the Sentence and Paragraph LOWESS means\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y=col_mean_lowess, data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (8-12 LOWESS)')\n","  sns.lineplot(x=corpus_parags_df.index, y=col_mean_lowess, data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (8-12 LOWESS)')\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"IbUmQijwRWMF"},"source":["### **Save Newly Computed Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWnpOqmnRWMG"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","sats_filename_str = f'sentiment_sents_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Sentence Sentiments with LOWESS in file: {sats_filename_str}')\n","\n","sats_filename_str = f'sentiment_parags_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Paragraph Sentiments with LOWESS in file: {sats_filename_str}')\n"]},{"cell_type":"markdown","metadata":{"id":"feCf3yBERJrj"},"source":["## **BERT Google Apps Sentiment Analysis**"]},{"cell_type":"markdown","metadata":{"id":"dXJM14P4RJrk"},"source":["### **Sentiment Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzfeqjkyRJrl"},"outputs":[],"source":["# Setup for RoBERTa Large English 15datasets: siebert/sentiment-roberta-large-english\n","\n","sa_model = 'robertalg15'\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n","\n","# Create class for data preparation\n","class SimpleDataset:\n","    def __init__(self, tokenized_texts):\n","        self.tokenized_texts = tokenized_texts\n","    \n","    def __len__(self):\n","        return len(self.tokenized_texts[\"input_ids\"])\n","    \n","    def __getitem__(self, idx):\n","        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","\n","\n","# Set model\n","sa_model = 'robertalg15'\n","model_name = \"siebert/sentiment-roberta-large-english\"\n","\n","# Load tokenizer and model, create trainer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","trainer = Trainer(model=model);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TXNZiHauRJrm"},"outputs":[],"source":["# Prepare Text from DataFrame\n","\n","sents_pred_df = corpus_sents_df.copy()\n","sents_pred_texts = sents_pred_df['sent_raw'].astype('str').tolist() # Want to catch NaN, .dropna().astype('str').tolist()\n","\n","# Tokenize texts and create prediction data set\n","\n","sents_tokenized_texts = tokenizer(sents_pred_texts,truncation=True,padding=True)\n","sents_pred_dataset = SimpleDataset(sents_tokenized_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hd_ze_0ERJro"},"outputs":[],"source":["# Run predictions\n","\n","sents_predictions = trainer.predict(sents_pred_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9prbj3-rRJro"},"outputs":[],"source":["temp_sentiment_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDiXF8LARJrp"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgS2nmV4RJrr"},"outputs":[],"source":["temp_sentiment_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kkutdE3cRJrr"},"outputs":[],"source":["temp_ser = temp_sentiment_df.apply(lambda x: polprob2sentiment(str(x.label), float(x.score)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdlRqBZkRJrs"},"outputs":[],"source":["# Transform predictions to labels\n","\n","sents_preds = sents_predictions.predictions.argmax(-1)\n","sents_labels = pd.Series(sents_preds).map(model.config.id2label)\n","sents_scores = (np.exp(sents_predictions[0])/np.exp(sents_predictions[0]).sum(-1,keepdims=True)).max(1)\n","\n","# Create DataFrame with texts, predictions, labels, and scores\n","\n","temp_sentiment_df = pd.DataFrame(list(zip(sents_pred_texts,sents_preds,sents_labels,sents_scores)), columns=['text','pred','label','score'])\n","# temp_sentiment_df.head()\n","\n","# Convert label (Neg/Pos) and score (Prob) to a +/-Sentiment Float Value\n","\n","corpus_sents_df[sa_model] = temp_sentiment_df.apply(lambda x: polprob2sentiment(x.label,x.score), axis=1)\n","corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNy3KzwGRJrt"},"outputs":[],"source":["# Verify the head and tail are complete and correct\n","\n","corpus_parags_df.iloc[:3]\n","corpus_parags_df.iloc[-3:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsXovoBfRJrt"},"outputs":[],"source":["# Aggregate Sentence Sentiments to populate Paragraph Sentiment DataFrame\n","\n","# def sentiment_sents2parags(ts_df, model_name='roberta_lg15'):\n","parags_sentiment_ls = sentiment_sents2parags(corpus_sents_df, sa_model)\n","corpus_parags_df[sa_model] = pd.Series(parags_sentiment_ls)\n","corpus_parags_df.head(2)\n","corpus_parags_df.tail(2)"]},{"cell_type":"markdown","metadata":{"id":"fvVk3s4nRJru"},"source":["### **Histogram Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjyARacrRJru"},"outputs":[],"source":["# Debug\n","PLOT_OUTPUT = 'Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlCPyYMVRJrv"},"outputs":[],"source":["# Create histogram of Sentence Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_sents_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_sents_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDsqbmDpRJrv"},"outputs":[],"source":["# Create histogram of Paragraph Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_parags_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_parags_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"markdown","metadata":{"id":"U239UeoZRJrw"},"source":["### **Mean SMA Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHptflBMRJrw"},"outputs":[],"source":["# SMA % Sentiment of Sentence Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_sents_df, sa_model, text_unit='sentence', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zg_y5ex7RJrw"},"outputs":[],"source":["# SMA % Sentiment of Paragraph Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_parags_df, sa_model, text_unit='paragraph', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUYyb6rHRJrx"},"outputs":[],"source":["# Compare Sentence and Paragraph\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y='roberta_lg15_mean_roll100', data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (10% SMA)')\n","  sns.lineplot(x=corpus_parags_df.index, y='roberta_lg15_mean_roll100', data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (10% SMA)')\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EAqMP5AhRJrz"},"outputs":[],"source":["# corpus_sents_df.drop(columns=['y_scaled'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"YUMnsVU2RJrz"},"source":["### **Stanford ASAP Plot**\n"]},{"cell_type":"markdown","metadata":{"id":"7MyF-y0gRJr0"},"source":["**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n","\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n","* http://futuredata.stanford.edu/asap/ \n","* https://www.datadoghq.com/blog/auto-smoother-asap/"]},{"cell_type":"markdown","metadata":{"id":"8mvHzeDhRJr0"},"source":["**Save the following plots to gDrive files?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUJwp3Q4RJr0"},"outputs":[],"source":["Save_to_File = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HoYxa3qdRJr0","scrolled":false},"outputs":[],"source":["# Sentence SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_sents_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, do_plot=True, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKprDQwwRJr1"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WzmfkCvXRJr2"},"outputs":[],"source":["# Paragraph SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_parags_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RaT_uXpvRJr3"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"markdown","metadata":{"id":"MKIy4DXNRJr3"},"source":["### **LOWESS Plots**"]},{"cell_type":"markdown","metadata":{"id":"WQ1ayrEzRJr4"},"source":["**Sentence Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z909MsRgRJr4"},"outputs":[],"source":["# Debug\n","\n","PLOT_OUTPUT='Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RH-HpUqnRJr4"},"outputs":[],"source":["# Plot Sentence Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./4, 1./6, 1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_sents_df[new_lowess_mean_col] = corpus_sents_df[cols_lowess].mean(axis=1)\n","\n","corpus_sents_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yg-pY4x5RJr5"},"outputs":[],"source":["corpus_parags_df.columns"]},{"cell_type":"markdown","metadata":{"id":"qu6hy3gKRJr6"},"source":["**Paragraph Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q06ESbtmRJr6"},"outputs":[],"source":["# Plot Paragraph Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_parags_df[new_lowess_col] = plot_lowess(corpus_parags_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_parags_df[new_lowess_mean_col] = corpus_parags_df[cols_lowess].mean(axis=1)\n","\n","corpus_parags_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"AbMkRZmpRJr7"},"source":["**Compare Sentence and Paragraph Median LOWESS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ni785YjSRJr7"},"outputs":[],"source":["# Compare Sentence and Paragraph LOWESS means\n","\n","# Calculate the Sentence and Paragraph LOWESS means and plot\n","\n","# Get all the calculated LOWESS columns in a list \n","cols_sents_lowess = matching_cols = get_cols_regex(corpus_sents_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","cols_parags_lowess = matching_cols = get_cols_regex(corpus_parags_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","\n","# Compute the mean for all the LOWESS columns\n","col_mean_lowess = f'{sa_model}_mean_lowess'\n","corpus_sents_df[col_mean_lowess] = corpus_sents_df[cols_sents_lowess].mean(axis=1)\n","corpus_parags_df[col_mean_lowess] = corpus_parags_df[cols_parags_lowess].mean(axis=1)\n","\n","# Plot the Sentence and Paragraph LOWESS means\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y=col_mean_lowess, data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (8-12 LOWESS)')\n","  sns.lineplot(x=corpus_parags_df.index, y=col_mean_lowess, data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (8-12 LOWESS)')\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"uxP8H1hDRJr7"},"source":["### **Save Newly Computed Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lr3n9dexRJr8"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","sats_filename_str = f'sentiment_sents_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Sentence Sentiments with LOWESS in file: {sats_filename_str}')\n","\n","sats_filename_str = f'sentiment_parags_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Paragraph Sentiments with LOWESS in file: {sats_filename_str}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xxpHu0f8RJKc"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNeIXTWCRJHY"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"HqORymYeROnW"},"source":["## **BERT Google IMDB Sentiment Analysis**"]},{"cell_type":"markdown","metadata":{"id":"WxQZt2hsROnZ"},"source":["### **Sentiment Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0J3seTuROna"},"outputs":[],"source":["# Setup for RoBERTa Large English 15datasets: siebert/sentiment-roberta-large-english\n","\n","sa_model = 'robertalg15'\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n","\n","# Create class for data preparation\n","class SimpleDataset:\n","    def __init__(self, tokenized_texts):\n","        self.tokenized_texts = tokenized_texts\n","    \n","    def __len__(self):\n","        return len(self.tokenized_texts[\"input_ids\"])\n","    \n","    def __getitem__(self, idx):\n","        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","\n","\n","# Set model\n","sa_model = 'robertalg15'\n","model_name = \"siebert/sentiment-roberta-large-english\"\n","\n","# Load tokenizer and model, create trainer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","trainer = Trainer(model=model);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TF4W_lzmROnc"},"outputs":[],"source":["# Prepare Text from DataFrame\n","\n","sents_pred_df = corpus_sents_df.copy()\n","sents_pred_texts = sents_pred_df['sent_raw'].astype('str').tolist() # Want to catch NaN, .dropna().astype('str').tolist()\n","\n","# Tokenize texts and create prediction data set\n","\n","sents_tokenized_texts = tokenizer(sents_pred_texts,truncation=True,padding=True)\n","sents_pred_dataset = SimpleDataset(sents_tokenized_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZGoACS3lROpE"},"outputs":[],"source":["# Run predictions\n","\n","sents_predictions = trainer.predict(sents_pred_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h65yynKvROpF"},"outputs":[],"source":["temp_sentiment_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8VkrtnxfROpF"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQ2vG6g5ROpF"},"outputs":[],"source":["temp_sentiment_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QK8suLH1ROpG"},"outputs":[],"source":["temp_ser = temp_sentiment_df.apply(lambda x: polprob2sentiment(str(x.label), float(x.score)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Znjx36feROpH"},"outputs":[],"source":["# Transform predictions to labels\n","\n","sents_preds = sents_predictions.predictions.argmax(-1)\n","sents_labels = pd.Series(sents_preds).map(model.config.id2label)\n","sents_scores = (np.exp(sents_predictions[0])/np.exp(sents_predictions[0]).sum(-1,keepdims=True)).max(1)\n","\n","# Create DataFrame with texts, predictions, labels, and scores\n","\n","temp_sentiment_df = pd.DataFrame(list(zip(sents_pred_texts,sents_preds,sents_labels,sents_scores)), columns=['text','pred','label','score'])\n","# temp_sentiment_df.head()\n","\n","# Convert label (Neg/Pos) and score (Prob) to a +/-Sentiment Float Value\n","\n","corpus_sents_df[sa_model] = temp_sentiment_df.apply(lambda x: polprob2sentiment(x.label,x.score), axis=1)\n","corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbZPjQCJROpI"},"outputs":[],"source":["# Verify the head and tail are complete and correct\n","\n","corpus_parags_df.iloc[:3]\n","corpus_parags_df.iloc[-3:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nj4ZD3gYROpM"},"outputs":[],"source":["# Aggregate Sentence Sentiments to populate Paragraph Sentiment DataFrame\n","\n","# def sentiment_sents2parags(ts_df, model_name='roberta_lg15'):\n","parags_sentiment_ls = sentiment_sents2parags(corpus_sents_df, sa_model)\n","corpus_parags_df[sa_model] = pd.Series(parags_sentiment_ls)\n","corpus_parags_df.head(2)\n","corpus_parags_df.tail(2)"]},{"cell_type":"markdown","metadata":{"id":"aMHwrv3CROpM"},"source":["### **Histogram Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03yyPX6sROpN"},"outputs":[],"source":["# Debug\n","PLOT_OUTPUT = 'Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L927dA7qROpN"},"outputs":[],"source":["# Create histogram of Sentence Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_sents_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_sents_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FyMGRJJlROpO"},"outputs":[],"source":["# Create histogram of Paragraph Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_parags_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_parags_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"markdown","metadata":{"id":"amhXN4zKROpP"},"source":["### **Mean SMA Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"COxXv2xrROpP"},"outputs":[],"source":["# SMA % Sentiment of Sentence Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_sents_df, sa_model, text_unit='sentence', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jus_amc9ROpR"},"outputs":[],"source":["# SMA % Sentiment of Paragraph Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_parags_df, sa_model, text_unit='paragraph', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXVF-DGpROpR"},"outputs":[],"source":["# Compare Sentence and Paragraph\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y='roberta_lg15_mean_roll100', data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (10% SMA)')\n","  sns.lineplot(x=corpus_parags_df.index, y='roberta_lg15_mean_roll100', data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (10% SMA)')\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3kxnve3eROpS"},"outputs":[],"source":["# corpus_sents_df.drop(columns=['y_scaled'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"1V0Rh5QjROpS"},"source":["### **Stanford ASAP Plot**\n"]},{"cell_type":"markdown","metadata":{"id":"xInJXZoaROpS"},"source":["**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n","\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n","* http://futuredata.stanford.edu/asap/ \n","* https://www.datadoghq.com/blog/auto-smoother-asap/"]},{"cell_type":"markdown","metadata":{"id":"EZUtmdfkROpT"},"source":["**Save the following plots to gDrive files?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wd2eNWiSROpT"},"outputs":[],"source":["Save_to_File = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJe0gWo9ROpU","scrolled":false},"outputs":[],"source":["# Sentence SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_sents_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, do_plot=True, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKlEwYWdROpV"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhDm3W8IROpV"},"outputs":[],"source":["# Paragraph SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_parags_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2maBkp9ROpV"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"markdown","metadata":{"id":"67lf-IWPROpW"},"source":["### **LOWESS Plots**"]},{"cell_type":"markdown","metadata":{"id":"uacviTZyROpW"},"source":["**Sentence Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JWZyz94eROpW"},"outputs":[],"source":["# Debug\n","\n","PLOT_OUTPUT='Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SExXRVyxROpX"},"outputs":[],"source":["# Plot Sentence Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./4, 1./6, 1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_sents_df[new_lowess_mean_col] = corpus_sents_df[cols_lowess].mean(axis=1)\n","\n","corpus_sents_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLNxpTJNROpY"},"outputs":[],"source":["corpus_parags_df.columns"]},{"cell_type":"markdown","metadata":{"id":"6F1qcCveROpY"},"source":["**Paragraph Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZ8hLUtzROpZ"},"outputs":[],"source":["# Plot Paragraph Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_parags_df[new_lowess_col] = plot_lowess(corpus_parags_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_parags_df[new_lowess_mean_col] = corpus_parags_df[cols_lowess].mean(axis=1)\n","\n","corpus_parags_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"4xdGdo1cROpZ"},"source":["**Compare Sentence and Paragraph Median LOWESS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHryEhSpROpZ"},"outputs":[],"source":["# Compare Sentence and Paragraph LOWESS means\n","\n","# Calculate the Sentence and Paragraph LOWESS means and plot\n","\n","# Get all the calculated LOWESS columns in a list \n","cols_sents_lowess = matching_cols = get_cols_regex(corpus_sents_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","cols_parags_lowess = matching_cols = get_cols_regex(corpus_parags_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","\n","# Compute the mean for all the LOWESS columns\n","col_mean_lowess = f'{sa_model}_mean_lowess'\n","corpus_sents_df[col_mean_lowess] = corpus_sents_df[cols_sents_lowess].mean(axis=1)\n","corpus_parags_df[col_mean_lowess] = corpus_parags_df[cols_parags_lowess].mean(axis=1)\n","\n","# Plot the Sentence and Paragraph LOWESS means\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y=col_mean_lowess, data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (8-12 LOWESS)')\n","  sns.lineplot(x=corpus_parags_df.index, y=col_mean_lowess, data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (8-12 LOWESS)')\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"V5h1medsROpb"},"source":["### **Save Newly Computed Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBXbYYFmROpb"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","sats_filename_str = f'sentiment_sents_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Sentence Sentiments with LOWESS in file: {sats_filename_str}')\n","\n","sats_filename_str = f'sentiment_parags_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Paragraph Sentiments with LOWESS in file: {sats_filename_str}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8DrjUhCsRJDc"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"35HpYvehNn8y"},"source":["## **(1) RoBERTa Large 15 Datasets **"]},{"cell_type":"markdown","metadata":{"id":"Re_PwdP3Nn81"},"source":["### **Sentiment Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNMijdQT8bqZ"},"outputs":[],"source":["from transformers import pipeline\n","sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n","print(sentiment_analysis(\"I love this!\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7EkLDenG89nv"},"outputs":[],"source":["sentiment_analysis(\"section\")[0]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RUyhHlgDtOV"},"outputs":[],"source":["test_str = 'I adore the lovely wonderful greatness of everything good'\n","\n","polprob2sentiment(sentiment_analysis(test_str)[0]['label'],sentiment_analysis(test_str)[0]['score']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bet68BIV8e3k"},"outputs":[],"source":["def wrapper_polprob2sentiment(text_str):\n","  '''\n","  Given a text string\n","  Get the RoBERTa Large 15 Dataset sentiment score -1.0 to 1.0\n","  '''\n","  roberta_score = sentiment_analysis(test_str)\n","  pol_str = roberta_score[0]['label']\n","  score_fl = roberta_score[0]['score']\n","  roberta_adj_score = polprob2sentiment(pol_str, score_fl)\n","\n","  return roberta_adj_score\n","\n","# Test\n","test_fl = wrapper_polprob2sentiment('I hate your guts you bastard!') # sentiment_analysis('section')[0]['label'],sentiment_analysis('section')[0]['score']))\n","print(f'test_fl: {test_fl}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p-hf4IluEDHt"},"outputs":[],"source":["corpus_sents_df['robertalg15_wrap'] = corpus_sents_df['sent_raw'].str.apply(lambda x: wrapper_polprob2sentiment(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kclngx8D8eyS"},"outputs":[],"source":["corpus_sents_df['robertalg15'] = corpus_sents_df['sent_raw'].apply(lambda x: polprob2sentiment(sentiment_analysis(x)[0]['label'],sentiment_analysis(x)[0]['score']))\n","                                                                   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p-YPgklc9eQx"},"outputs":[],"source":["corpus_sents_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s016rps09eM7"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nT18eSCs9eJ4"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lP1SF7zFNn82"},"outputs":[],"source":["# Setup for RoBERTa Large English 15datasets: siebert/sentiment-roberta-large-english\n","\n","sa_model = 'robertalg15'\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n","\n","# Create class for data preparation\n","class SimpleDataset:\n","    def __init__(self, tokenized_texts):\n","        self.tokenized_texts = tokenized_texts\n","    \n","    def __len__(self):\n","        return len(self.tokenized_texts[\"input_ids\"])\n","    \n","    def __getitem__(self, idx):\n","        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n","\n","\n","# Set model\n","sa_model = 'robertalg15'\n","model_name = \"siebert/sentiment-roberta-large-english\"\n","\n","# Load tokenizer and model, create trainer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","trainer = Trainer(model=model);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAdux2btNn83"},"outputs":[],"source":["# Prepare Text from DataFrame\n","\n","sents_pred_df = corpus_sents_df.copy()\n","sents_pred_texts = sents_pred_df['sent_raw'].astype('str').tolist() # Want to catch NaN, .dropna().astype('str').tolist()\n","\n","# Tokenize texts and create prediction data set\n","\n","sents_tokenized_texts = tokenizer(sents_pred_texts,truncation=True,padding=True)\n","sents_pred_dataset = SimpleDataset(sents_tokenized_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSr8CuP4Nn85"},"outputs":[],"source":["# Run predictions\n","\n","sents_predictions = trainer.predict(sents_pred_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgmKn-rfNn86"},"outputs":[],"source":["# temp_sentiment_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbhIkYAGNn87"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZsDd_xxNn88"},"outputs":[],"source":["# temp_sentiment_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ud0j6NcXNn89"},"outputs":[],"source":["# temp_ser = temp_sentiment_df.apply(lambda x: polprob2sentiment(str(x.label), float(x.score)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ST8yLi3hNn8-"},"outputs":[],"source":["# Transform predictions to labels\n","\n","sents_preds = sents_predictions.predictions.argmax(-1)\n","sents_labels = pd.Series(sents_preds).map(model.config.id2label)\n","sents_scores = (np.exp(sents_predictions[0])/np.exp(sents_predictions[0]).sum(-1,keepdims=True)).max(1)\n","\n","# Create DataFrame with texts, predictions, labels, and scores\n","\n","temp_sentiment_df = pd.DataFrame(list(zip(sents_pred_texts,sents_preds,sents_labels,sents_scores)), columns=['text','pred','label','score'])\n","# temp_sentiment_df.head()\n","\n","# Convert label (Neg/Pos) and score (Prob) to a +/-Sentiment Float Value\n","\n","corpus_sents_df[sa_model] = temp_sentiment_df.apply(lambda x: polprob2sentiment(x.label,x.score), axis=1)\n","corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6dud6Tj-Nn8_"},"outputs":[],"source":["# Verify the head and tail are complete and correct\n","\n","corpus_parags_df.iloc[:3]\n","corpus_parags_df.iloc[-3:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VU1CPZrSNn9B"},"outputs":[],"source":["# Aggregate Sentence Sentiments to populate Paragraph Sentiment DataFrame\n","\n","# def sentiment_sents2parags(ts_df, model_name='roberta_lg15'):\n","parags_sentiment_ls = sentiment_sents2parags(corpus_sents_df, sa_model)\n","corpus_parags_df[sa_model] = pd.Series(parags_sentiment_ls)\n","corpus_parags_df.head(2)\n","corpus_parags_df.tail(2)"]},{"cell_type":"markdown","metadata":{"id":"tugQJFEHNn9C"},"source":["### **Histogram Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KpsCNWLfNn9C"},"outputs":[],"source":["# Debug\n","PLOT_OUTPUT = 'Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJew1txTNn9D"},"outputs":[],"source":["# Create histogram of Sentence Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_sents_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_sents_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmiiKMIPNn9E"},"outputs":[],"source":["# Create histogram of Paragraph Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_parags_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_parags_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"markdown","metadata":{"id":"DIKumKU-Nn9E"},"source":["### **Mean SMA Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qf8HMOUNn9F"},"outputs":[],"source":["# SMA % Sentiment of Sentence Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_sents_df, sa_model, text_unit='sentence', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YTe93_ymNn9F"},"outputs":[],"source":["# SMA % Sentiment of Paragraph Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_parags_df, sa_model, text_unit='paragraph', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i1MJ6mVeNn9G"},"outputs":[],"source":["# Compare Sentence and Paragraph\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y='roberta_lg15_mean_roll100', data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (10% SMA)')\n","  sns.lineplot(x=corpus_parags_df.index, y='roberta_lg15_mean_roll100', data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (10% SMA)')\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUtjDsRLNn9G"},"outputs":[],"source":["# corpus_sents_df.drop(columns=['y_scaled'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"NRxkgYcMNn9H"},"source":["### **Stanford ASAP Plot**\n"]},{"cell_type":"markdown","metadata":{"id":"_CcHoNWxNn9H"},"source":["**Stanford ASAP: Automatic Smoothing for Attention Prioritization in Time Series**\n","\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP.ipynb (Python)\n","* https://github.com/stanford-futuredata/ASAP/blob/master/ASAP-simple.js\n","* http://futuredata.stanford.edu/asap/ \n","* https://www.datadoghq.com/blog/auto-smoother-asap/"]},{"cell_type":"markdown","metadata":{"id":"O3haqsixNn9H"},"source":["**Save the following plots to gDrive files?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Db16kr6-Nn9H"},"outputs":[],"source":["Save_to_File = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNa52QNlNn9J","scrolled":false},"outputs":[],"source":["# Sentence SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_sents_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, do_plot=True, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgcKZJ1uNn9J"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNcI31klNn9K"},"outputs":[],"source":["# Paragraph SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_parags_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tuv0w3m6Nn9L"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"markdown","metadata":{"id":"Zb7mJUJTNn9L"},"source":["### **LOWESS Plots**"]},{"cell_type":"markdown","metadata":{"id":"-p9ATxK6Nn9M"},"source":["**Sentence Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fty_CmZJNn9M"},"outputs":[],"source":["# Debug\n","\n","PLOT_OUTPUT='Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvcfEL6NNn9M"},"outputs":[],"source":["# Plot Sentence Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./4, 1./6, 1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_sents_df[new_lowess_mean_col] = corpus_sents_df[cols_lowess].mean(axis=1)\n","\n","corpus_sents_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ljf6wJMRNn9N"},"outputs":[],"source":["corpus_parags_df.columns"]},{"cell_type":"markdown","metadata":{"id":"9b_OEEsTNn9N"},"source":["**Paragraph Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GFL_k2MVNn9O"},"outputs":[],"source":["# Plot Paragraph Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [0.25, 0.2, 0.15, 0.1, 0.075, 0.05] # [1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_parags_df[new_lowess_col] = plot_lowess(corpus_parags_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_parags_df[new_lowess_mean_col] = corpus_parags_df[cols_lowess].mean(axis=1)\n","\n","corpus_parags_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"El5bfW4QNn9P"},"source":["**Compare Sentence and Paragraph Median LOWESS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BI03Cx9Nn9P"},"outputs":[],"source":["# Compare Sentence and Paragraph LOWESS means\n","\n","# Calculate the Sentence and Paragraph LOWESS means and plot\n","\n","# Get all the calculated LOWESS columns in a list \n","cols_sents_lowess = matching_cols = get_cols_regex(corpus_sents_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","cols_parags_lowess = matching_cols = get_cols_regex(corpus_parags_df, find_regex = '_lowess', ignore_regex = '_mean_', strict_match=False)\n","\n","# Compute the mean for all the LOWESS columns\n","col_mean_lowess = f'{sa_model}_mean_lowess'\n","corpus_sents_df[col_mean_lowess] = corpus_sents_df[cols_sents_lowess].mean(axis=1)\n","corpus_parags_df[col_mean_lowess] = corpus_parags_df[cols_parags_lowess].mean(axis=1)\n","\n","# Plot the Sentence and Paragraph LOWESS means\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y=col_mean_lowess, data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (8-12 LOWESS)')\n","  sns.lineplot(x=corpus_parags_df.index, y=col_mean_lowess, data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (8-12 LOWESS)')\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"7XAJnUH3Nn9Q"},"source":["### **Save Newly Computed Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"se7zvTW1Nn9Q"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","sats_filename_str = f'sentiment_sents_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Sentence Sentiments with LOWESS in file: {sats_filename_str}')\n","\n","sats_filename_str = f'sentiment_parags_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Paragraph Sentiments with LOWESS in file: {sats_filename_str}')\n"]},{"cell_type":"markdown","metadata":{"id":"PuU7i_xBKsmO"},"source":["## (3) BERT, ALBERT DistBERT, SST (barissayil/SentimentAnalysis)\n","\n","* Fine-tune BERT, ALBERT, DistBERT, SST, etc\n","* https://github.com/barissayil/SentimentAnalysis.git"]},{"cell_type":"markdown","metadata":{"id":"2OyEZMKHAfbC"},"source":["### **Barissayil SentimentAnalysis Setup and Configuration**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6A4hdGUKnd-"},"outputs":[],"source":["!pip install numpy pandas torch transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Y5tuPQXD40D"},"outputs":[],"source":["# If already installed\n","\n","# !rm -rf ./SentimentAnalysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tjFtAMYaKnbR"},"outputs":[],"source":["!git clone https://github.com/barissayil/SentimentAnalysis.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T96KPfAAKnYr"},"outputs":[],"source":["%cd SentimentAnalysis/\n","!pwd\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kq5tTIfnLkq-"},"outputs":[],"source":["%time\n","\n","# Interactive Demo \n","!python analyze.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mq7LLUGGdN4F"},"outputs":[],"source":["# !python train.py --model_name_or_path bert-base-uncased --output_dir my_model --num_eps 2\n","\n","# !python train.py --model_name_or_path albert-base-v2 --output_dir my_model --num_eps 2\n","\n","!python train.py --model_name_or_path albert-base-v2 --output_dir my_model --num_eps 2\n","sa_model = 'bertsst'\n","\n","# !python train.py --model_name_or_path bert-base-uncased --output_dir my_model --num_eps 2\n","\n","# !python train.py --model_name_or_path xlnet-base-cased --output_dir my_model --num_eps 2\n","# ValueError: This transformer model is not supported yet\n","\n","# !python train.py --model_name_or_path t5-base-finetuned-imdb-sentiment --output_dir my_model --num_eps 2\n","# df['sentiment'] = df['review'].apply(lambda x: sentiment_score(x[:512])) #Limited to 512 Token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vLTXE7budvzL"},"outputs":[],"source":["!ls my_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Po2fJsQhe11X"},"outputs":[],"source":["# Import libraries\n","import torch\n","from transformers import AutoTokenizer, AutoConfig\n","from modeling import BertForSentimentClassification, AlbertForSentimentClassification, DistilBertForSentimentClassification\n","# from arguments import args\n","\n","from transformers import BertTokenizer as tokenizer\n","\n","# Setup model and path\n","sa_model = 'bertsst'\n","model_path = 'barissayil/bert-sentiment-analysis-sst'\n","# sa_model = model_path.split('/')[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHiYADipd1Dn"},"outputs":[],"source":["# https://github.com/barissayil/SentimentAnalysis/blob/master/server.py\n","\n","def bari_classify_sentiment(sentence):\n","\twith torch.no_grad():\n","\t\ttokens = tokenizer.tokenize(text=sentence)\n","\t\ttokens = ['[CLS]'] + tokens + ['[SEP]']\n","\t\ttokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\t\tseq = torch.tensor(tokens_ids)\n","\t\tseq = seq.unsqueeze(0)\n","\t\tattn_mask = (seq != 0).long()\n","\t\tlogit = model(seq, attn_mask)\n","\t\tprob = torch.sigmoid(logit.unsqueeze(-1))\n","\t\tprob = prob.item()\n","\t\tsoft_prob = prob > 0.5\n","\t\tif soft_prob == 1:\n","\t\t\treturn 'Positive', int(prob*100)\n","\t\telse:\n","\t\t\treturn 'Negative', int(100-prob*100)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIDQHuLlFjVq"},"outputs":[],"source":["# ALBERT\n","\n","config = AutoConfig.from_pretrained(model_path)\n","config.model_type"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtsSyMb7fE-9"},"outputs":[],"source":["#Configuration for the desired transformer model\n","\n","config = AutoConfig.from_pretrained(model_path)\n","\n","print('Please wait while the analyser is being prepared.')\n","\n","#Create the model with the desired transformer model\n","if config.model_type == 'bert':\n","  model = BertForSentimentClassification.from_pretrained(model_path)\n","elif config.model_type == 'albert':\n","  model = AlbertForSentimentClassification.from_pretrained(model_path)\n","elif config.model_type == 'distilbert':\n","  model = DistilBertForSentimentClassification.from_pretrained(model_path)\n","else:\n","  raise ValueError('This transformer model is not supported yet.')\n","\n","# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# model = model.to(device)\n","\n","model.eval()\n","\n","#Initialize the tokenizer for the desired transformer model\n","tokenizer = AutoTokenizer.from_pretrained(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eahEYJXiehly"},"outputs":[],"source":["# Test\n","\n","text = \"I don't really care for these. They are old, smelly and too expensive.\"\n","sentiment, probability = bari_classify_sentiment(text)\n","print(f'Sentiment: {sentiment}')\n","print(f'Probability: {probability}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWP1bqZIcfoU"},"outputs":[],"source":["# Test classifier working against Annie Swafford Edge Case Tests\n","\n","swafford_test = [\n","    (\"I haven't been sad in a long time.\", \"POSITIVE\"),\n","    (\"I am extremely happy today.\", \"POSITIVE\"),\n","    (\"It's a good day.\", \"POSITIVE\"),\n","    (\"But suddenly I'm only a little bit happy.\", \"POSITIVE\"),\n","    (\"Then I'm not happy at all.\", \"NEGATIVE\"),\n","    (\"In fact, I am now the least happy person on the planet.\", \"NEGATIVE\"),\n","    (\"There is no happiness left in me.\", \"NEGATIVE\"),\n","    (\"Wait, it's returned!\", \"NEUTRAL\"),\n","    (\"I don't feel so bad after all!\", \"POSITIVE\") ]\n","\n","print('Testing some edge case sentences from Anne Swafford')\n","print('---------------------------------------------------\\n')\n","right_ct = 0\n","wrong_ct = 0\n","for i, alinetest in enumerate(swafford_test):\n","  asent, apolarity = alinetest\n","  asent_pol, asent_prob = bari_classify_sentiment(asent)\n","  print(f\"[SENTENCE]:\\n     {asent}\")\n","  print(f\"[POLARITY]: {asent_pol}\")\n","  print(f\"[PROBABILITY]: {asent_prob}\\n\\n\")\n","  # sents_pol_ls.append(asent_pol)\n","  # sents_prob_ls.append(asent_prob)\n","\n","  if (asent_pol.lower() == apolarity.lower()):\n","    print(f'            CORRECT! (answer is {apolarity})\\n')\n","    right_ct += 1\n","  else:\n","    print(f'            WRONG! (predicated {asent_pol} but answer is {apolarity})\\n')\n","    wrong_ct += 1\n","\n","total_ct = right_ct + wrong_ct\n","print(f'OVERALL PERFORMANCE = {right_ct/total_ct:.2f} ({right_ct} correct out of {total_ct} total sentences)')"]},{"cell_type":"markdown","metadata":{"id":"1kiBfbzBBamW"},"source":["### **Calculate Sentence Sentiment Values**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mj02Ee2mTL4"},"outputs":[],"source":["# Create a Corpus list of all Raw Sentence text\n","\n","sents_ls = corpus_sents_df['sent_raw'].to_list()\n","type(sents_ls)\n","sents_ls[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wan85I2eopci"},"outputs":[],"source":["sents_pol_ls = []\n","# sents_pol_ls = sents_pol_ls.cuda()\n","sents_prob_ls = []\n","# sents_prob_ls = sents_prob_ls.cuda()\n","\n","output_pols = True\n","for i,asent in enumerate(sents_ls):\n","  # asent = asent.cuda()\n","  asent_pol, asent_prob = bari_classify_sentiment(asent)\n","  if output_pols:\n","    print(f\"[SENTENCE]:\\n     {asent}\")\n","    print(f\"[POLARITY]: {asent_pol}\")\n","    print(f\"[PROBABILITY]: {asent_prob}\\n\\n\")\n","  sents_pol_ls.append(asent_pol)\n","  sents_prob_ls.append(asent_prob)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSO4P40-shi9"},"outputs":[],"source":["# Debug\n","\n","# corpus_sents_df.drop(columns=['bertsst_barissayil_raw'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"71YQcTVbnuLJ"},"outputs":[],"source":["sa_model = 'bertsst'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w39TGpzer_ag"},"outputs":[],"source":["col_pol = f'{sa_model}_pol'\n","col_prob = f'{sa_model}_prob'\n","\n","# corpus_sents_df['bertsst_barissayil_pol'] = pd.Series(sent_pol_ls)\n","# corpus_sents_df['bertsst_barissayil_prob'] = pd.Series([(lambda x: x/100.0)(x) for x in sent_prob_ls])\n","\n","corpus_sents_df[col_pol] = pd.Series(sents_pol_ls)\n","corpus_sents_df[col_prob] = pd.Series([(lambda x: x/100.0)(x) for x in sents_prob_ls])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OP9n9usdr5yg"},"outputs":[],"source":["corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTlVJzeQx-9W"},"outputs":[],"source":["\"\"\"\n","def polprob2sentiment(pol_str, prob_fl):\n","  '''\n","  Given a Polarity string (Negative or Positive) and a Probability float (0.0-1.0)\n","  Return a Sentiment float value (-1.0 to 1.0)\n","  '''\n","  sign_fl = 1.0\n","  if pol_str.lower().startswith('neg'):\n","    # print(f'pol_str: {pol_str} is Negative')\n","    sign_fl = -1.0\n","  elif pol_str.lower().startswith('pos'):\n","    # print(f'pol_str: {pol_str} is Positive')\n","    pass\n","  else:\n","    print(f'ERROR: pol_str: {pol_str} is neither Negative nor Positive')\n","    sign_fl = 0.0\n","\n","  return sign_fl * prob_fl\n","\n","# Test\n","polprob2sentiment('Positive', 0.91)\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5r9ausPHwvyt"},"outputs":[],"source":["# Calculate single Sentiment Value from both Polarity (Pos/Neg) and Probability(0.0 to 1.0)\n","\n","corpus_sents_df[sa_model] = corpus_sents_df.apply(lambda x: polprob2sentiment(x[col_pol], x[col_prob]), axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qmy7MODAfS-b"},"outputs":[],"source":["corpus_sents_df[sa_model].std()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJKNomU_wVpV"},"outputs":[],"source":["corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dvKWbmkEu7pH"},"outputs":[],"source":["# PASS?\n","\n","# Scale values within range\n","\n","# from sklearn.preprocessing import MinMaxScaler\n","\"\"\"\n","scaler = MinMaxScaler(feature_range=(-1.0,1.0)) \n","\n","col_norm = f'{sa_model}_norm'\n","corpus_sents_df[col_norm] = scaler.fit_transform(np.array(corpus_sents_df[sa_model]).reshape(-1, 1)) # .to_numpy())\n","\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2R_d8pJG05GC"},"outputs":[],"source":["corpus_sents_df.describe()"]},{"cell_type":"markdown","metadata":{"id":"Ay6BBvIo2qeE"},"source":["### **Calculate Paragraph Sentiment Values**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHPDBvQL2w6_"},"outputs":[],"source":["# Create a Corpus list of all Raw Paragraph text\n","\n","parags_ls = corpus_parags_df['parag_raw'].to_list()\n","type(parags_ls)\n","parags_ls[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIEx09YZgiqO"},"outputs":[],"source":["next(model.parameters()).is_cuda"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wC_HG-CRhTIA"},"outputs":[],"source":["parags_pol_ls = []\n","parags_prob_ls = []\n","output_pols = True\n","\n","aparag_pol = 'Negative'\n","aparag_prob = 0.\n","# aparag_pol.cuda()\n","# aparag_prob.cuda()\n","for i,aparag in enumerate(parags_ls):\n","  aparag_pol, aparag_prob = bari_classify_sentiment(trim_maxtokens(aparag,510))\n","  if output_pols:\n","    print(f\"[SENTENCE]:\\n     {aparag}\")\n","    print(f\"[POLARITY]: {aparag_pol}\")\n","    print(f\"[PROBABILITY]: {aparag_prob}\\n\\n\")\n","  sents_pol_ls.append(aparag_pol)\n","  sents_prob_ls.append(aparag_prob)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yrDh2tI2qeF"},"outputs":[],"source":["\"\"\"\n","parags_pol_ls = []\n","# parags_pol_ls.cuda()\n","parags_prob_ls = []\n","# parags_prob_ls.cuda()\n","output_pols = True\n","for i,aparag in enumerate(parags_ls):\n","  # aparag.to(device='cuda')\n","  aparag_pol, aparag_prob = (bari_classify_sentiment(aparag)).cuda()\n","  if output_pols:\n","    print(f\"[PARAGRAPH]:\\n     {aparag}\")\n","    print(f\"[POLARITY]: {aparag_pol}\")\n","    print(f\"[PROBABILITY]: {aparag_prob}\\n\\n\")\n","  parags_pol_ls.append(aparag_pol)\n","  parags_prob_ls.append(aparag_prob)\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGPjBq6K2qeG"},"outputs":[],"source":["# Debug\n","\n","# corpus_sents_df.drop(columns=['bertsst_barissayil_raw'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amTPPPrd2qeH"},"outputs":[],"source":["col_pol = f'{sa_model}_pol'\n","col_prob = f'{sa_model}_prob'\n","\n","# corpus_sents_df['bertsst_barissayil_pol'] = pd.Series(sent_pol_ls)\n","# corpus_sents_df['bertsst_barissayil_prob'] = pd.Series([(lambda x: x/100.0)(x) for x in sent_prob_ls])\n","\n","corpus_parags_df[col_pol] = pd.Series(parags_pol_ls)\n","corpus_parags_df[col_prob] = pd.Series([(lambda x: x/100.0)(x) for x in parags_prob_ls])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_0kbyg92qeI"},"outputs":[],"source":["corpus_parags_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kf1Dk8gE2qeI"},"outputs":[],"source":["def polprob2sentiment(pol_str, prob_fl):\n","  '''\n","  Given a Polarity string (Negative or Positive) and a Probability float (0.0-1.0)\n","  Return a Sentiment float value (-1.0 to 1.0)\n","  '''\n","  sign_fl = 1.0\n","  if pol_str.lower().startswith('neg'):\n","    # print(f'pol_str: {pol_str} is Negative')\n","    sign_fl = -1.0\n","  elif pol_str.lower().startswith('pos'):\n","    # print(f'pol_str: {pol_str} is Positive')\n","    pass\n","  else:\n","    print(f'ERROR: pol_str: {pol_str} is neither Negative nor Positive')\n","    sign_fl = 0.0\n","\n","  return sign_fl * prob_fl\n","\n","# Test\n","polprob2sentiment('Positive', 0.91)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6bPSV7g2qeJ"},"outputs":[],"source":["# Calculate single Sentiment Value from both Polarity (Pos/Neg) and Probability(0.0 to 1.0)\n","\n","corpus_parags_df[sa_model] = corpus_parags_df.apply(lambda x: polprob2sentiment(x[col_pol], x[col_prob]), axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"191An7uu2qeJ"},"outputs":[],"source":["# Scale values within range\n","\n","# from sklearn.preprocessing import MinMaxScaler\n","\"\"\"\n","scaler = MinMaxScaler(feature_range=(-1.0,1.0)) \n","\n","col_norm = f'{sa_model}_norm'\n","corpus_parags_df[col_norm] = scaler.fit_transform(np.array(corpus_parags_df[sa_model]).reshape(-1, 1)) # .to_numpy())\n","\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBdxWynY2qeL"},"outputs":[],"source":["corpus_sents_df.describe()"]},{"cell_type":"markdown","metadata":{"id":"f0P7xlhUBkwr"},"source":["### **Histogram Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27PcVyu6fULz"},"outputs":[],"source":["PLOT_OUTPUT='All'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxvxkbUGzkfy"},"outputs":[],"source":["# Create histogram of Sentence Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_sents_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_sents_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBIf_CUJ3hFe"},"outputs":[],"source":["# Create histogram of Paragraph Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_parags_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_parags_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Lv_kGlez2-z"},"outputs":[],"source":["corpus_sents_df.info()"]},{"cell_type":"markdown","metadata":{"id":"-f_uCxvQBq2b"},"source":["### **Mean SMA Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rKgSCky3xx8"},"outputs":[],"source":["# SMA % Sentiment of Sentence Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_sents_df, sa_model, text_unit='sentence', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tRLr3g523xu_"},"outputs":[],"source":["# SMA % Sentiment of Paragraph Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_parags_df, sa_model, text_unit='paragraph', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qTn0x1m31nV"},"outputs":[],"source":["# Compare Sentence and Paragraph\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y=sa_model, data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (10% SMA)')\n","  sns.lineplot(x=corpus_parags_df.index, y=sa_model, data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (10% SMA)')\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfwc6TQ_31jg"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxldUqCD4AV_"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAQFKq-qIL6Z"},"outputs":[],"source":["# Calculate (win_5per) 5% of Corpus length for smallest (odd-valued) rolling window\n","\n","corpus_sents_len = corpus_sents_df.shape[0]\n","\n","win_raw_5per = int(corpus_sents_len * 0.05)\n","# print(f'5% Rolling Window: {win_raw_5per}')\n","\n","if win_raw_5per % 2:\n","  win_5per = win_raw_5per\n","else:\n","  win_5per = win_raw_5per + 1\n","\n","# Test\n","print(f'win_5per: {win_5per}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1PVws-d2N7Z"},"outputs":[],"source":["# Plot SMA for given Sentiment Time Series and Window Sizes\n","\n","# Standard set of SMA window percentages to calculate Median baseline\n","#   across all models\n","WINS_SMA_STD_LS = [2,4,6,8,10]\n","\n","def plot_smas(model_name, wins_ls, do_plot=True, save2file=False):\n","  '''\n","  Given the name of a Sentiment Time Series and a list of window sizes in %,\n","  Return a plot of Simple Moving Averages\n","  Assumptions:\n","  * sentiment time series is Series/Col Name in the main corpus_sents_df\n","  * CORPUS_\n","  '''\n","  # global corpus_sents_df\n","\n","  win_1per = int(corpus_sents_df.shape[0] * 0.01)\n","\n","  # Mandatory SMA windows sizes to compute standard SMA medians\n","  wins_req_ls = [2,4,6,8,10]\n","  wins_all_ls = list(set(wins_ls) | set(wins_req_ls))\n","\n","  roll_temp_df = pd.DataFrame()\n","\n","  # print(f'The length is: {len(corpus_sents_df[model_name])}')\n","\n","  for i,win_per in enumerate(wins_all_ls):\n","    # print(f'{i} is win_per = {win_per}')\n","    win_sents = int(win_per * win_1per)\n","    # print(f'win_sents: {win_sents}')\n","    # col_name = f'{model_name}_roll{str(win_per)}'\n","    roll_temp_df[win_per] = corpus_sents_df[model_name].rolling(win_sents, center=True).mean()\n","    # Plot SMA for this win_per\n","    if (win_per in wins_ls):\n","      plot_label = f'{win_per}%'\n","      sns.lineplot(data=roll_temp_df, x=roll_temp_df.index, y=win_per, alpha=0.3, label=plot_label)\n","\n","  plt.legend(title='Window Size')\n","  plt.title(f'{CORPUS_FULL} \\n Mean Simple Moving Average (5% = {win_5per} sentences) Plots of Sentence Sentiment (Model: {sa_model})')\n","\n","  # TODO: Add widget to select which models to include\n","\n","  # Standard Window sizes for calculating median of SMA at various window sizes\n","  # Global: wins_sma_std_ls = [2,4,6,8,10]\n","  for awin_std in WINS_SMA_STD_LS:\n","    awin_col = f'{model_name}_roll{awin_std}'\n","    corpus_sents_df[awin_col] = roll_temp_df[awin_std]\n","  sa_median_col = f'{model_name}_roll_med'\n","  roll_temp_df[sa_median_col] = roll_temp_df[WINS_SMA_STD_LS].median(axis=1)\n","  corpus_sents_df[sa_median_col] = roll_temp_df[sa_median_col]\n","  \n","  if do_plot:\n","    sns.lineplot(data=roll_temp_df, x=roll_temp_df.index, y=sa_median_col, color='black', label='Median')\n","\n","  if (save2file):\n","    # Save Plot to file.\n","    plot_filename = f'plot_sent_sa_smamedians_{model_name}.png'\n","    plotpathfilename_str = gen_pathfiletime(plot_filename)\n","    plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","    print(f'Plot saved: {plot_filename}');\n","\n","  return roll_temp_df\n","\n","# Test\n","model_name = 'albertbv2'\n","temp_roll_df = plot_smas(model_name, [5,10,20], True);\n","\n","# If you choose to add the individual standard rolling mean columns\n","for awin in WINS_SMA_STD_LS:\n","  acol_name = f'{model_name}_roll{awin}'  \n","  corpus_sents_df[acol_name] = temp_roll_df[awin]\n","\n","corpus_sents_df.head(2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdFHoQwlzEK6"},"outputs":[],"source":["corpus_sents_df.drop(columns=['albertbv2_sma_med','albertbv2_pol','albertbv2_porb','albertbv2_prob'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aldCIne6nCB6"},"outputs":[],"source":["corpus_sents_df.head()"]},{"cell_type":"markdown","metadata":{"id":"QfuGKhZYB1F1"},"source":["### **Stanford ASAP Plot**\n"]},{"cell_type":"markdown","metadata":{"id":"52OsWb5X4Miq"},"source":["**Save the following plots to gDrive files?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5i60SjO4Mir"},"outputs":[],"source":["Save_to_File = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGVhqRPo4Mis","scrolled":false},"outputs":[],"source":["# Sentence SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_sents_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, do_plot=True, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGiydN0T4Mit"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIprkPrH4Mi3"},"outputs":[],"source":["# Paragraph SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_parags_df.sa_model)\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0lrfsuv4Mi3"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTPhrW1W4Mi3"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2qsLpSwPCA0"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgXNgzU74Mi4"},"outputs":[],"source":["# Taxi\n","# raw_data = load_csv('Taxi.csv')\n","# window_size, slide_size = smooth_ASAP(raw_data, resolution=1000)\n","# window_size, slide_size = smooth_ASAP(raw_data, resolution=1000)\n","'''\n","window_size, slide_size = smooth_simple(list(corpus_sentiments_df['median']), resolution=1000)\n","print(\"Window Size: \", window_size)\n","plot_title = f'{BOOK_TITLE_FULL} \\n Median Sentiment Smoothed with ASAP from Stanford (res=1000)'\n","plot(list(corpus_sentiments_df['median']), window_size, slide_size, plot_title)\n","'''\n","print(f'win_5per: {win_5per}')\n","window_size, slide_size = smooth_simple(list(corpus_sents_df[sa_model]), resolution=win_5per)\n","print(\"Window Size: \", window_size)\n","plot_title = f'{CORPUS_FULL} \\n {sa_model} Sentiment Smoothed with Stanford ASAP (res=1000)'\n","asap_x, asap_y = plot(list(corpus_sents_df[sa_model]), window_size, slide_size, plot_title);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01vXnR8z4Mi4"},"outputs":[],"source":["orig_x_len = corpus_sents_df.shape[0]\n","\n","asap_x_len = len(asap_x)\n","asap_y_len = len(asap_y)\n","\n","print(f'The original Sentiment Time Series has {orig_x_len} x-values')\n","print(f'The ASAP Smoothed Time Series reduced this to {asap_x_len} x-values')\n","\n","asap_ratio = orig_x_len/asap_x_len\n","print(f'The ASAP compression ratio is {asap_ratio}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1D8mnTWm4Mi4"},"outputs":[],"source":["new_col = f'{sa_model}_asap_{afrac}'\n","corpus_sents_df[]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqESs7Cl4MOD"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HR40a7Lr4MK7"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMaR9FEz4MHx"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLFb9ZitVV3E"},"outputs":[],"source":["# ASAP Simple (Brute Force)\n","def moving_average(data, _range):\n","    ret = np.cumsum(data, dtype=float)\n","    ret[_range:] = ret[_range:] - ret[:-_range]\n","    return ret[_range - 1:] / _range\n","\n","def SMA(data, _range, slide):\n","    ret = moving_average(data, _range)[::slide]\n","    return list(ret)\n","\n","def kurtosis(values):\n","    return scipy.stats.kurtosis(values)\n","\n","def roughness(vals):\n","    return np.std(np.diff(vals))\n","\n","def smooth_simple(data, max_window=5, resolution=None):\n","    data = np.array(data)\n","    # Preaggregate according to resolution\n","    window_size = 1\n","    slide_size = 1\n","    if resolution:\n","        slide_size = int(len(data) / resolution)\n","        if slide_size > 1:\n","            data = SMA(data, slide_size, slide_size)\n","    orig_kurt   = kurtosis(data)\n","    min_obj     = roughness(data)\n","    for w in range(2, int(len(data) / max_window + 1)):\n","        smoothed = SMA(data, w, 1)\n","        if kurtosis(smoothed) >= orig_kurt:\n","            r = roughness(smoothed)\n","            if r < min_obj:\n","                min_obj = r\n","                window_size = w\n","    return window_size, slide_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHdCbShgVVrR"},"outputs":[],"source":["# Plot time series before and after smoothing\n","\n","def plot(data, window_size, slide_size, plot_title):\n","    plt.clf()\n","    plt.figure()\n","    data = SMA(data, slide_size, slide_size)\n","    method_names = [\"SMA Smoothed\", \"ASAP Smoothed\"]\n","    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n","    # smoothed = SMA(data, window_size, 1)\n","    smoothed = smooth_simple(data, window_size, 1)\n","    smoothed_range = range(int(window_size/2), int(window_size/2) + len(smoothed))\n","    ax1.set_xlim(0, len(data))\n","    ax1.plot(data, linestyle='-', linewidth=1.5)\n","    # ax1.set_title('SMA Smoothed')\n","    ax2.plot(smoothed_range, smoothed, linestyle='-', linewidth=1.5)\n","    # ax2.set_title('Stanford ASAP Smoothed')\n","    axes = [ax1, ax2]\n","    for i in range(2):\n","        axes[i].get_xaxis().set_visible(False)\n","        axes[i].text(0.02, 0.8, \"%s\" %(method_names[i]),\n","            verticalalignment='center', horizontalalignment='left',\n","            transform=axes[i].transAxes, fontsize=25)\n","\n","    fig.set_size_inches(16, 12)\n","    plt.tight_layout(w_pad=1)\n","    plt.title(plot_title)\n","    plt.show()\n","\n","    return smoothed_range, smoothed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWIpeDY51-dj"},"outputs":[],"source":["# Plot SMA for given Sentiment Time Series and Window Sizes\n","\n","# Standard set of SMA window percentages to calculate Median baseline\n","#   across all models\n","WINS_SMA_STD_LS = [2,4,6,8,10]\n","\n","def plot_smas(sentiment_ts, wins_ls, do_plot=True, save2file=False):\n","  '''\n","  Given the name of a Sentiment Time Series and a list of window sizes in %,\n","  Return a plot of Simple Moving Averages\n","  Assumptions:\n","  * sentiment time series is Series/Col Name in the main corpus_sents_df\n","  * CORPUS_\n","  '''\n","  global corpus_sents_df\n","\n","  win_1per = int(corpus_sents_df.shape[0] * 0.01)\n","\n","  # Mandatory SMA windows sizes to compute standard SMA medians\n","  wins_req_ls = [2,4,6,8,10]\n","  wins_all_ls = list(set(wins_ls) | set(wins_req_ls))\n","\n","  roll_temp_df = pd.DataFrame()\n","\n","  # print(f'The length is: {len(corpus_sents_df[sentiment_ts])}')\n","\n","  for i,win_per in enumerate(wins_all_ls):\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dg7tcoxK1xDK"},"outputs":[],"source":["# Plot time series before and after smoothing\n","\n","def plot_asap(data, window_size, slide_size, plot_title):\n","    plt.clf()\n","    plt.figure()\n","    data = SMA(data, slide_size, slide_size)\n","    method_names = [\"Original\", \"ASAP Smoothed\"]\n","    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n","    smoothed = SMA(data, window_size, 1)\n","    smoothed_range = range(int(window_size/2), int(window_size/2) + len(smoothed))\n","    ax1.set_xlim(0, len(data))\n","    ax1.plot(data, linestyle='-', linewidth=1.5)\n","    ax2.plot(smoothed_range, smoothed, linestyle='-', linewidth=1.5)\n","    axes = [ax1, ax2]\n","    for i in range(2):\n","        axes[i].get_xaxis().set_visible(False)\n","        axes[i].text(0.02, 0.8, \"%s\" %(method_names[i]),\n","            verticalalignment='center', horizontalalignment='left',\n","            transform=axes[i].transAxes, fontsize=25)\n","\n","    fig.set_size_inches(16, 12)\n","    plt.tight_layout(w_pad=1)\n","    plt.title(plot_title)\n","    plt.show()\n","\n","    return smoothed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpSktA4gWV3J"},"outputs":[],"source":["corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDov94xIlE9w"},"outputs":[],"source":["import scipy.stats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"710ghW5qVVoF"},"outputs":[],"source":["# Taxi\n","# raw_data = load_csv('Taxi.csv')\n","# window_size, slide_size = smooth_ASAP(raw_data, resolution=1000)\n","# window_size, slide_size = smooth_ASAP(raw_data, resolution=1000)\n","'''\n","window_size, slide_size = smooth_simple(list(corpus_sentiments_df['median']), resolution=1000)\n","print(\"Window Size: \", window_size)\n","plot_title = f'{BOOK_TITLE_FULL} \\n Median Sentiment Smoothed with ASAP from Stanford (res=1000)'\n","plot(list(corpus_sentiments_df['median']), window_size, slide_size, plot_title)\n","'''\n","print(f'win_5per: {win_5per}')\n","window_size, slide_size = smooth_simple(list(corpus_sents_df[sa_model]), resolution=win_5per)\n","print(\"Window Size: \", window_size)\n","plot_title = f'{CORPUS_FULL} \\n {sa_model} Sentiment Smoothed with Stanford ASAP (res=1000)'\n","asap_x, asap_y = plot(list(corpus_sents_df[sa_model]), window_size, slide_size, plot_title);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JT2c_8UY4LNZ"},"outputs":[],"source":["orig_x_len = corpus_sents_df.shape[0]\n","\n","asap_x_len = len(asap_x)\n","asap_y_len = len(asap_y)\n","\n","print(f'The original Sentiment Time Series has {orig_x_len} x-values')\n","print(f'The ASAP Smoothed Time Series reduced this to {asap_x_len} x-values')\n","\n","asap_ratio = orig_x_len/asap_x_len\n","print(f'The ASAP compression ratio is {asap_ratio}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DuHbx_y66oBi"},"outputs":[],"source":["new_col = f'{sa_model}_asap_{afrac}'\n","corpus_sents_df[]"]},{"cell_type":"markdown","metadata":{"id":"4ibfZf9yB6Gx"},"source":["### **LOWESS Plots**"]},{"cell_type":"markdown","metadata":{"id":"Np5M9myY4lQ9"},"source":["##### **Sentence Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c72o9Zij4lQ-"},"outputs":[],"source":["# corpus_sents_df.drop(columns=['albertbv2_sma_med'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7w1gXdld4lQ_"},"outputs":[],"source":["# Debug\n","\n","PLOT_OUTPUT='Major'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLnoxZaJ4lQ_"},"outputs":[],"source":["# Plot Sentence Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_sents_df[new_lowess_mean_col] = corpus_sents_df[cols_lowess].mean(axis=1)\n","\n","corpus_sents_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"qMvzIH9r4lRA"},"source":["##### **Paragraph Sentiment LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0tC1FWg4lRA"},"outputs":[],"source":["# Plot Paragraph Sentiment LOWESS for various frac's\n","\n","lowess_frac_ls = [1./8, 1./10, 1./12, 1./14, 1./16]\n","cols_lowess = []\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  aplot = True \n","else:\n","  aplot = False\n","\n","for afrac in lowess_frac_ls:\n","  print(f'Calculating LOWESS for frac = {afrac}...')\n","  afrac_per_str = str(round(100*afrac))\n","  new_lowess_col = f'{sa_model}_frac{afrac_per_str}_lowess'\n","  cols_lowess.append(new_lowess_col)\n","  corpus_parags_df[new_lowess_col] = plot_lowess(corpus_parags_df, [sa_model], do_plot=aplot, afrac=afrac)\n","\n","new_lowess_mean_col = f'{sa_model}_mean_lowess'\n","print(f'new_lowess_mean_cols: {new_lowess_mean_col}')\n","print(f'cols_lowess: {cols_lowess}')\n","corpus_parags_df[new_lowess_mean_col] = corpus_parags_df[cols_lowess].mean(axis=1)\n","\n","corpus_parags_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2i8ohle4lRA"},"outputs":[],"source":["corpus_sents_df.columns\n","corpus_parags_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owvzkHt24lRA"},"outputs":[],"source":["corpus_sents_df.roberta_lg15_mean_lowess[3000:3005]"]},{"cell_type":"markdown","metadata":{"id":"RkLSi5fC4lRA"},"source":["##### **Compare Sentence and Paragraph Median LOWESS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a868Spr64lRA"},"outputs":[],"source":["corpus_sents_df.roberta_lg15_mean_lowess"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZa28fUA4lRA"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qs060l3q4lRB"},"outputs":[],"source":["corpus_sents_df['roberta_lg15_frac10_lowess'][:100]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJ1AtOa94lRC"},"outputs":[],"source":["corpus_sents_df['roberta_lg15_mean_lowess'] = corpus_sents_df[['roberta_lg15_frac8_lowess', 'roberta_lg15_frac10_lowess', 'roberta_lg15_frac12_lowess']].mean(axis=1)\n","corpus_parags_df['roberta_lg15_mean_lowess'] = corpus_parags_df[['roberta_lg15_frac8_lowess', 'roberta_lg15_frac10_lowess', 'roberta_lg15_frac12_lowess']].mean(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0zFrE4D4lRC"},"outputs":[],"source":["corpus_sents_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqCfEIqP4lRC"},"outputs":[],"source":["# Compare Sentence and Paragraph\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y='roberta_lg15_mean_lowess', data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (8-12 LOWESS)')\n","  sns.lineplot(x=corpus_parags_df.index, y='roberta_lg15_mean_lowess', data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (8-12 LOWESS)')\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sb7UNFRG4lRD"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mm5UIKtn7uWR"},"outputs":[],"source":["corpus_sents_df.to_csv('')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGStdyOL7uTm"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppT069Ew7uRc"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xuvsF1Li7uPr"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3nmKP5M7uLC"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"xbuowCI_PM_f"},"source":["### Single Raw Sentiment LOWESS Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AmGKcdz-r5d6"},"outputs":[],"source":["# corpus_sents_df.drop(columns=['albertbv2_sma_med'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjZW5AyKpzhg"},"outputs":[],"source":["def plot_lowess(df, df_cols_ls, aplot=True, afrac=1./10, ait=5):\n","  '''\n","  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n","  Return a DataFrame with LOWESS values\n","  If 'plot=True', also output plot\n","  '''\n","\n","  global corpus_sents_df\n","\n","  lowess_df = pd.DataFrame()\n","\n","  for i,acol in enumerate(df_cols_ls):\n","    sm_x, sm_y = sm_lowess(endog=corpus_sents_df[acol].values, exog=corpus_sents_df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n","    col_new = f'{acol}_lowess'\n","    lowess_df[col_new] = pd.Series(sm_x)\n","    if aplot:\n","      plt.plot(sm_x, sm_y, label=acol, alpha=0.5, linewidth=2)\n","\n","      frac_str = str(round(100*afrac))\n","      plt.title(f'{CORPUS_FULL} \\n LOWESS (frac={frac_str} Sentence Sentiment (Model: {sa_model})')\n","      plt.legend(title='Sentiment Series')\n","\n","  return lowess_df\n","\n","# Test\n","new_lowess_col = f'{sa_model}_lowess'\n","my_frac = 1./10\n","my_frac_per = round(100*my_frac)\n","new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n","corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], afrac=my_frac)\n","corpus_sents_df.head()"]},{"cell_type":"markdown","metadata":{"id":"0C5HWzBdPQqH"},"source":["### Multiple SMA LOWESS Plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2UPM_RMBPTM"},"outputs":[],"source":["corpus_sents_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oaC7oUrPBkpp"},"outputs":[],"source":["# Test\n","df_test = plot_lowess('ALBERT Base v2', ['albertbv2_roll_med', 'albertbv2_roll2','albertbv2_roll4','albertbv2_roll6','albertbv2_roll8','albertbv2_roll10'], afrac=my_frac)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlHwP9FRCWQ0"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZQ3F2CkCWMC"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y4e8z9koCWF5"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JLdvU9s9B5rn"},"outputs":[],"source":["%time\n","\n","plt.title(f'{CORPUS_FULL} \\n {sa_model} with statsmodels LOWESS (frac=0.1/iter=5)')\n","\n","sm_x, sm_y = sm_lowess(endog=corpus_sents_df['median'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n","                       it=5, return_sorted = True).T\n","plt.plot(sm_x, sm_y, label='Median', color='black', linewidth=3)\n","\n","sm_x, sm_y = sm_lowess(endog=corpus_sents_df['vader_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n","                       it=5, return_sorted = True).T\n","sns.lineplot(sm_x, sm_y, label='VADER', color='royalblue', alpha=0.5)\n","\n","sm_x, sm_y = sm_lowess(endog=corpus_sents_df['jockers_rinker_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n","                       it=5, return_sorted = True).T\n","sns.lineplot(sm_x, sm_y, label='Jockers-Rinker', color='red', alpha=0.5)\n","\n","sm_x, sm_y = sm_lowess(endog=corpus_sents_df['syuzhet_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n","                       it=5, return_sorted = True).T\n","sns.lineplot(sm_x, sm_y, label='Syuzhet', color='tomato', alpha=0.5)\n","\n","sm_x, sm_y = sm_lowess(endog=corpus_sents_df['huliu_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n","                       it=5, return_sorted = True).T\n","sns.lineplot(sm_x, sm_y, label='HuLiu', color='teal', alpha=0.5)\n","\n","sm_x, sm_y = sm_lowess(endog=corpus_sents_df['textblob_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n","                       it=5, return_sorted = True).T\n","sns.lineplot(sm_x, sm_y, label='TextBlob', color='lime', alpha=0.5)\n","\n","sm_x, sm_y = sm_lowess(endog=corpus_sents_df['sentiword_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n","                       it=5, return_sorted = True).T\n","sns.lineplot(sm_x, sm_y, label='SentiWord', color='goldenrod', alpha=0.5)\n","\n","sm_x, sm_y = sm_lowess(endog=corpus_sents_df['senticnet_roll500'].values, exog=corpus_sentiments_df.index.values,  frac=1./10., \n","                       it=5, return_sorted = True).T\n","sns.lineplot(sm_x, sm_y, label='SenticNet', color='forestgreen', alpha=0.5)\n","\n","# y_upper = corpus_sentiments_df['norm_score'].max() + 0.01\n","# y_lower = corpus_sentiments_df['norm_score'].min() - 0.01\n","# y_range = y_upper - y_lower + 0.02\n","# plt.ylim([0.71, 0.74])\n","# plt.ylim([y_lower, y_upper])\n","# plt.plot(x, y, 'k.');"]},{"cell_type":"markdown","metadata":{"id":"bcV6rVapCCp3"},"source":["### **Save Newest Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YzX7l3BDp0O"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","sats_filename_str = f'sentiment_sents_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Sentence Sentiments with LOWESS in file: {sats_filename_str}')\n","\n","sats_filename_str = f'sentiment_parags_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Paragraph Sentiments with LOWESS in file: {sats_filename_str}')\n"]},{"cell_type":"markdown","metadata":{"id":"siW0rK5lStfk"},"source":["## **NLPTown BERT Multilingual 5 Language/5 fine-grained SA Reviews**\n","\n","* (brianadit24/SentimentAnalysiswithBERT_HF)\n","  \n","* https://github.com/brianadit24/SentimentAnalysiswithBERT_HF/blob/main/Sentiment_Analysis_with_BERT.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbH5Ur2cSskZ"},"outputs":[],"source":["from transformers import AutoTokenizer,AutoModelForSequenceClassification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fsisAdVT-s2"},"outputs":[],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2Iqs84AHDQl"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPC-Am2fUB7F"},"outputs":[],"source":["import sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWRHoJztGzi7"},"outputs":[],"source":["# Instantiate model\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUgl1UBdTCMP"},"outputs":[],"source":["tokens = tokenizer.encode(\"I hated mas Yoza, he absolutely the worst mentor\", return_tensors='pt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHPA3wwlUEOE"},"outputs":[],"source":["# Predict Tokens\n","tokens = tokenizer.encode(\"It wasn't the worst i've seen, in fact, it was the opposite\", return_tensors='pt')\n","# tokens[0]\n","# tokenizer.decode(tokens[0])\n","result = model(tokens)\n","result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EmMMm-McUoJr"},"outputs":[],"source":["predict_sentiment = int(torch.argmax(result.logits))+1\n","predict_sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bygorxpCbPOZ"},"outputs":[],"source":["def nlptown_sentiment_score(text):\n","  '''\n","  Given a text string (sentence or paragraph)\n","  Return a floating point sentiment value\n","  '''\n","\n","  # tokens = tokenizer.encode(text, return_tensors='pt')\n","  # result = model(tokens)\n","  # sentiment_int = int(torch.argmax(result.logits))+1\n","  # sentiment_fl = sentiment_int + result.logits[sentiment_int-1]\n","  # return sentiment_fl\n","\n","  tokens = tokenizer.encode(text, return_tensors='pt')\n","  result = model(tokens)\n","  type(result)\n","  prob_ls = list(result.logits)[0].tolist()\n","  # print(f'prob_ls: {prob_ls}')\n","  # prob_ls_sum = sum(prob_ls)\n","  prob_ls_sum = sum(map(abs, prob_ls))\n","  prob_norm_ls = [abs(i/prob_ls_sum) for i in prob_ls]\n","  # prob_ls_min = min(prob_ls)\n","  # prob_ls_max = max(prob_ls)\n","  # prob_norm_ls = [(x-prob_ls_min)/(prob_ls_max-prob_ls_min) for x in prob_ls]\n","  # print(f'prob_norm_ls {prob_norm_ls}')\n","  prob_int = int(torch.argmax(result.logits))\n","  # print(f'prob_int {prob_int}')\n","  prob_frac = abs(float(prob_norm_ls[prob_int]))\n","  # print(f'prob_frac {prob_frac}')\n","  \n","  return prob_int + prob_frac # int(torch.argmax(result.logits))+1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g3FEU96APHAa"},"outputs":[],"source":["nlptown_sentiment_score('i love the smell of beautiful flowers, the make me happy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Urtu1VfRMd2n"},"outputs":[],"source":["%time\n","\n","# NOTE: Long-running process\n","\n","# Calculate Sentence Sentiment Scores using the NLPTown BERT fine-grained, fine-tuned, multi-lingual model\n","\n","# https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n","\n","# This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in \n","#    six languages: English, Dutch, German, French, Spanish and Italian. \n","#    It predicts the sentiment of the review as a number of stars (between 1 and 5).\n","\n","corpus_sents_df['nlptown'] = corpus_sents_df['sent_raw'].astype('str').apply(lambda x: nlptown_sentiment_score(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ltIKANVN-O2"},"outputs":[],"source":["# Verify\n","\n","corpus_sents_df.iloc[:3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F7g7LERFXczP"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLIIgy29SOBo"},"outputs":[],"source":["def trim_maxtokens(astr, token_max):\n","  '''\n","  Given an input string of tokens and a maximum token limit\n","  Return a string at token limit by dropping frequent words w/o sentiment value first, them random\n","  '''\n","  deadwords_ls = ['the','of','to','and','a','in','is','it','that','was','for','on','are','with','as','be','at','one','have','this','from','or','had','by']\n","\n","  astr_ls = astr.split()\n","  deadword_idx = 0\n","  while deadword_idx < len(deadwords_ls):\n","    del_word = deadwords_ls[deadword_idx]\n","    astr_ls = [aword for aword in astr_ls if aword != del_word]\n","    deadword_idx += 1\n","\n","  if len(astr_ls) > token_max:\n","    print('too long')\n","    # Start removing longest words first, random words, shortest words, POS, Capitalized?\n","    while len(astr_ls) > token_max:\n","      random_list_element = random.choice(astr_ls)\n","      astr_ls.remove(random_list_element)\n","\n","  astr_condensed = ' '.join(astr_ls)\n","\n","  return astr_condensed\n","\n","# Test\n","# sentences_condensed = trim_maxtokens('Hello big boy! What the heck are you doing here?', 6)\n","# sentences_condensed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l1bfe9Ppck6Z"},"outputs":[],"source":["# Calculate Paragraph Sentiment Scores using the NLPTown BERT fine-grained, fine-tuned, multi-lingual model\n","\n","# https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n","\n","# This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in \n","#    six languages: English, Dutch, German, French, Spanish and Italian. \n","#    It predicts the sentiment of the review as a number of stars (between 1 and 5).\n","\n","corpus_parags_df['nlptown'] = corpus_parags_df['parag_raw'].astype('str').apply(lambda x: nlptown_sentiment_score(trim_maxtokens(x,510)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"suz27SWIc_8d"},"outputs":[],"source":["# Verify\n","\n","corpus_parags_df[:2]"]},{"cell_type":"markdown","metadata":{"id":"lCOWoxMdRHzj"},"source":["### **Save Newest Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2opuG-CUG8Ez"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","sats_filename_str = f'sentiment_sents_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_sents_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Sentence Sentiments with LOWESS in file: {sats_filename_str}')\n","\n","sats_filename_str = f'sentiment_parags_lowess_{author_str}_{title_str}_{datetime_now}.csv'\n","corpus_parags_df.to_csv(sats_filename_str, index=False)\n","print(f'Saved Paragraph Sentiments with LOWESS in file: {sats_filename_str}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tTmvHhBG8Bn"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"59NwSoBbL7of"},"source":["## Calculate Sentiment Values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeDjvu9tKvJF"},"outputs":[],"source":["# Score Sentences\n","\n","def sentiment_score(text):\n","    tokens = tokenizer.encode(text, return_tensors='pt')\n","    result = model(tokens)\n","    return int(torch.argmax(result.logits))+1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byV4etHZKi8j"},"outputs":[],"source":["sentiment_score(corpus_sents_df.iloc[0]['sent_raw'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVL7ncadKi5m"},"outputs":[],"source":["%time\n","\n","# Calculate Sentiment Scores for every Sentence in Corpus\n","\n","corpus_sents_df[sa_model] = corpus_sents_df['sent_raw'].apply(lambda x: sentiment_score(x[:512])) #Limited to 512 Token"]},{"cell_type":"markdown","metadata":{"id":"GOSDhB0yL5nZ"},"source":["## **Histogram Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zUiMC21COfwI"},"outputs":[],"source":["PLOT_OUTPUT='All'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Udx_fGc1OfwL"},"outputs":[],"source":["# Create histogram of Sentence Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_sents_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_sents_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QU_NM2LBOfwO"},"outputs":[],"source":["# Create histogram of Paragraph Polarities\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  sns.histplot(data=corpus_parags_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Paragraph Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = 'hist_parags_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUOXY-xtOfwQ"},"outputs":[],"source":["corpus_sents_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2G6rXQ7kKi3U"},"outputs":[],"source":["# Create histogram of Sentence Polarities\n","\n","sns.histplot(data=corpus_sents_df[sa_model], kde=False).set_title(f'{CORPUS_FULL} \\n Histogram of Sentence Sentiment Values (Model: {sa_model})');\n","\n","if (PLOT_OUTPUT == 'All'):\n","  # Save graph to file.\n","  plot_filename = f'hist_sent_{sa_model}.png'\n","  plotpathfilename_str = gen_pathfiletime(plot_filename)\n","  plt.savefig(plotpathfilename_str, format='png', dpi=300)\n","  print(f'Plot saved: {plot_filename}');"]},{"cell_type":"markdown","metadata":{"id":"LjVttED6MfqD"},"source":["## **Mean SMA Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zkfs4NPfO8Ce"},"outputs":[],"source":["# SMA % Sentiment of Sentence Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_sents_df, sa_model, text_unit='sentence', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2V8QiA9RO8Ck"},"outputs":[],"source":["# SMA % Sentiment of Paragraph Sentiments\n","\n","# def get_smas(model_name, ts_df, win_ls=[5,10], do_plot=True, save2file=False):\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  get_smas(corpus_parags_df, sa_model, text_unit='paragraph', win_ls=[5,10,20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qKm-xLSO8Co"},"outputs":[],"source":["# Compare Sentence and Paragraph\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  fig, axs = plt.subplots(nrows=2)\n","  sns.lineplot(x=corpus_sents_df.index, y=sa_model, data=corpus_sents_df, ax=axs[0]).set_title(f'{CORPUS_FULL}\\n Sentence Sentiment (10% SMA)')\n","  sns.lineplot(x=corpus_parags_df.index, y=sa_model, data=corpus_parags_df, ax=axs[1]).set_title(f'{CORPUS_FULL}\\n Paragraph Sentiment (10% SMA)')\n","  fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQMnG6XFO-Rq"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ClA7n0CXO7No"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOg6aUXEMv5_"},"outputs":[],"source":["# Calculate (win_5per) 5% of Corpus length for smallest (odd-valued) rolling window\n","\n","corpus_sents_len = corpus_sents_df.shape[0]\n","\n","win_raw_5per = int(corpus_sents_len * 0.05)\n","# print(f'5% Rolling Window: {win_raw_5per}')\n","\n","if win_raw_5per % 2:\n","  win_5per = win_raw_5per\n","else:\n","  win_5per = win_raw_5per + 1\n","\n","# Test\n","print(f'win_5per: {win_5per}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PbZeU84LMHPF"},"outputs":[],"source":["# Test\n","# model_name = 'albertbv2'\n","temp_roll_df = plot_smas(sa_model, [5,10,20], True);\n","\n","# If you choose to add the individual standard rolling mean columns\n","for awin in WINS_SMA_STD_LS:\n","  acol_name = f'{model_name}_roll{awin}'  \n","  corpus_sents_df[acol_name] = temp_roll_df[awin]\n","\n","corpus_sents_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"OP4TJHppNC87"},"source":["## **Stanford ASAP Plot**"]},{"cell_type":"markdown","metadata":{"id":"WVf5I0wIPEs1"},"source":["**Save the following plots to gDrive files?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMOwuQsrPEs4"},"outputs":[],"source":["Save_to_File = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywUDiS3gPEs6","scrolled":false},"outputs":[],"source":["# Sentence SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_sents_df[sa_model])\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, do_plot=True, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GAPwou8CPEs9"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kr-YxLoEPEtB"},"outputs":[],"source":["# Paragraph SMA/ASAP Plots with roberta_lg15\n","\n","# raw_data = load_csv('Taxi.csv')\n","raw_data = list(corpus_parags_df.sa_model)\n","window_size, slide_size = smooth_ASAP(raw_data, 5, resolution=1000)      # 20210621 Fixed JChun\n","# window_size, slide_size = smooth_simple(raw_data, resolution=1000)      # 20210621 Fixed JChun\n","print(f'Window Size: {window_size} and Slide_Size: {slide_size}')\n","asap_x, asap_y = plot_asap(sa_model,raw_data, window_size, slide_size, save2file=Save_to_File);\n","\n","if (PLOT_OUTPUT == 'All') | (PLOT_OUTPUT == 'Major'):\n","  pass\n","  # Save figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ou1jUqiBPEtD"},"outputs":[],"source":["# Check the automatically computed ASAP values for test SA TS\n","\n","print(f'Based upon the hyperparamter resolution = 1000:')\n","print(f'  ASAP [window_size]: {window_size} [slide_size]: {slide_size}')\n","print(f'--------------------')\n","print(f'Original Length = {len(raw_data)} vs ASAP Length: {len(asap_x)}')"]},{"cell_type":"markdown","metadata":{"id":"cYoXWL6bNFK2"},"source":["## **LOWESS Plots**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGQJnTk4PLgM"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"gjNL7F7vPHyr"},"source":["### Single Raw Sentiment LOWESS Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqQotF8BOGIS"},"outputs":[],"source":["def plot_lowess(df, df_cols_ls, aplot=True, afrac=1./10, ait=5):\n","  '''\n","  Given a DataFrame, list of column to plot, LOWESS params fraction and iterations,\n","  Return a DataFrame with LOWESS values\n","  If 'plot=True', also output plot\n","  '''\n","  lowess_df = pd.DataFrame()\n","\n","  for i,acol in enumerate(df_cols_ls):\n","    sm_x, sm_y = sm_lowess(endog=df[acol].values, exog=df.index.values,  frac=afrac, it=ait, return_sorted = True).T\n","    col_new = f'{acol}_lowess'\n","    lowess_df[col_new] = pd.Series(sm_x)\n","    if aplot:\n","      plt.plot(sm_x, sm_y, label=sa_model, alpha=0.5, linewidth=2)\n","\n","  return lowess_df\n","\n","# Test\n","new_lowess_col = f'{sa_model}_lowess'\n","my_frac = 1./10\n","my_frac_per = round(100*my_frac)\n","new_lowess_col = f'{sa_model}_lowess_{my_frac_per}'\n","corpus_sents_df[new_lowess_col] = plot_lowess(corpus_sents_df, [sa_model], afrac=my_frac)\n","corpus_sents_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3rapZeMOr14"},"outputs":[],"source":["corpus_sents_df.info()"]},{"cell_type":"markdown","metadata":{"id":"FvsXVDXAPCeY"},"source":["### Multiple SMA LOWESS PLots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ob7owqVcOr17"},"outputs":[],"source":["# Test\n","# df_test = plot_lowess('BERT Multi-Lingual Uncased Sentiment by NLPTown', ['bertmlus_nlptown_roll_med','bertmlus_nlptown_roll2','bertmlus_nlptown_roll4','bertmlus_nlptown_roll6','bertmlus_nlptown_roll8','bertmlus_nlptown_roll10'], afrac=my_frac)\n","\n","df_test = plot_lowess('BERT Multi-Lingual Uncased Sentiment by NLPTown', ['bertmlus_nlptown_roll_med','bertmlus_nlptown_roll10'], afrac=my_frac)"]},{"cell_type":"markdown","metadata":{"id":"VIjZ9dR2OV-w"},"source":["### **Save Newly Computed Sentiment Time Series**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBUz2Z9OOV-1"},"outputs":[],"source":["# Save all the calculated Sentiment Values\n","\n","author_str = ''.join(CORPUS_AUTHOR.split()).lower()\n","title_str = ''.join(CORPUS_TITLE.split()).lower()\n","datetime_now = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n","\n","sats_filename_str = f'sentiment_{author_str}_{title_str}_{sa_model}_{datetime_now}.csv'\n","# print(sats_filename_str)\n","\n","# sentiment_raw_pathfilename_str = gen_pathfiletime(sats_filename_str)\n","\n","print(f'Saving all Sentiment Series to file:\\n\\n  {sats_filename_str}\\n')\n","for i,col_name in enumerate(corpus_sents_df.columns):\n","  print(f'Column #{i}: {col_name}')\n","corpus_sents_df.to_csv(sats_filename_str, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAB6hVnCNHIp"},"outputs":[],"source":["# Test\n","df_test = plot_lowess('ALBERT Base v2', ['albertbv2_roll_med','albertbv2_roll2','albertbv2_roll4','albertbv2_roll6','albertbv2_roll8','albertbv2_roll10'], afrac=my_frac)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiF70c8HNCkh"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zpiy3swCUO31"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZO8gETawUO0i"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-BjruyaUOxp"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tRPsKawjUOu6"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7mIfsw9UOsO"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"MBaPOJZjNtlj"},"source":["# (3) RoBERTa: BERT, XLNet and RoBERTa (1tangerine1day/Sentiment-Analysis)\n","\n","* https://github.com/1tangerine1day/Sentiment-Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EI7O07w5sMVi"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TLr6IWKavDXc"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"w02bInCnvD0q"},"source":["# **Pysentimiento ES/EN Sentiment/Emotions**\n","\n","* https://github.com/pysentimiento/pysentimiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHgTqYjfvMN4"},"outputs":[],"source":["!pip install pysentimiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wpz_iaZ0vDTF"},"outputs":[],"source":["from pysentimiento import SentimentAnalyzer\n","analyzer = SentimentAnalyzer(lang=\"en\")\n","\n","analyzer.predict(\"Qué gran jugador es Messi\")\n","# returns SentimentOutput(output=POS, probas={POS: 0.998, NEG: 0.002, NEU: 0.000})\n","analyzer.predict(\"Esto es pésimo\")\n","# returns SentimentOutput(output=NEG, probas={NEG: 0.999, POS: 0.001, NEU: 0.000})\n","analyzer.predict(\"Qué es esto?\")\n","# returns SentimentOutput(output=NEU, probas={NEU: 0.993, NEG: 0.005, POS: 0.002})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwhwfcylvVC9"},"outputs":[],"source":["# Emotion Analysis in English\n","\n","from pysentimiento import EmotionAnalyzer\n","emotion_analyzer = EmotionAnalyzer(lang=\"en\")\n","\n","analyzer.predict(\"The the fuck are you looking at\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hDGbOrEvsMRd"},"outputs":[],"source":["from pysentimiento import EmotionAnalyzer\n","emotion_analyzer = EmotionAnalyzer(lang=\"en\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RD1QzzbrLQYi"},"outputs":[],"source":["emotion_analyzer.predict('I am so fucking tired of this shit!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHCOwLW3LQRQ"},"outputs":[],"source":["analyzer.predict(\"Qué es esto?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJmuuWxiL4s2"},"outputs":[],"source":["analyzer.predict(\"Qué es esto?\").probas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srordGRaL4lS"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AkgKCa-XNTAo"},"outputs":[],"source":["def pysentimiento_sentiment_score(text_str):\n","  '''\n","  Given a text string (sentence or paragraph)\n","  Return a floating point sentiment value\n","  '''\n","\n","  sentiment_adj = 0\n","\n","  sentiment_cls = analyzer.predict(text_str).output\n","  # print(f'sentiment_cls={sentiment_cls}')\n","  sentiment_val = analyzer.predict(text_str).probas[sentiment_cls]\n","  # print(f'sentiment_val={sentiment_val}')\n","\n","  if sentiment_cls == 'NEU':\n","    sentiment_adj = sentiment_val + 2\n","  elif sentiment_cls == 'NEG':\n","    sentiment_adj = 2*(1-sentiment_val)\n","  elif sentiment_cls == 'POS':\n","    sentiment_adj = 2*sentiment_val + 3\n","  else:\n","    print(f'ERROR: sentiment_cls={sentiment_cls} but must be NEU, NEG or POS')\n","\n","  return sentiment_adj  # , sentiment_cls, sentiment_val\n","\n","\n","# Test\n","test_sent = 'I am so happy and joyful for enjoying this lovely day'\n","# test_sent = 'This is so very bad and awful I think everything will be ruined'\n","test_sent = 'That is it'\n","# sentiment_adj, sentiment_cls, sentiment_val = pysentimiento_sentiment_score(test_sent)\n","sentiment_adj = pysentimiento_sentiment_score(test_sent)\n","print(f'SentimentAdj: {sentiment_adj}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2EeREWiFNTAo"},"outputs":[],"source":["nlptown_sentiment_score('i love the smell of beautiful flowers, the make me happy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DvnXAH-iT5Kb"},"outputs":[],"source":["corpus_sents_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DwvwK_9zNTAp"},"outputs":[],"source":["%time\n","\n","# NOTE: 10m Long-running process\n","\n","# Calculate Sentence Sentiment Scores using the NLPTown BERT fine-grained, fine-tuned, multi-lingual model\n","\n","# https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n","\n","# This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in \n","#    six languages: English, Dutch, German, French, Spanish and Italian. \n","#    It predicts the sentiment of the review as a number of stars (between 1 and 5).\n","\n","# corpus_sents_df['pysentimiento']\n","corpus_sents_pysenti1000 = corpus_sents_df.iloc[0:1001]['sent_raw'].astype('str').apply(lambda x: pysentimiento_sentiment_score(trim_maxtokens(x,225)))\n","corpus_sents_pysenti2000 = corpus_sents_df.iloc[1001:2001]['sent_raw'].astype('str').apply(lambda x: pysentimiento_sentiment_score(trim_maxtokens(x,225)))\n","corpus_sents_pysenti3000 = corpus_sents_df.iloc[2001:3001]['sent_raw'].astype('str').apply(lambda x: pysentimiento_sentiment_score(trim_maxtokens(x,225)))\n","corpus_sents_pysenti4000 = corpus_sents_df.iloc[3001:4001]['sent_raw'].astype('str').apply(lambda x: pysentimiento_sentiment_score(trim_maxtokens(x,225)))\n","corpus_sents_pysenti5000 = corpus_sents_df.iloc[4001:5001]['sent_raw'].astype('str').apply(lambda x: pysentimiento_sentiment_score(trim_maxtokens(x,225)))\n","corpus_sents_pysenti6000 = corpus_sents_df.iloc[5001:6001]['sent_raw'].astype('str').apply(lambda x: pysentimiento_sentiment_score(trim_maxtokens(x,225)))\n","corpus_sents_pysenti7000 = corpus_sents_df.iloc[6001:7001]['sent_raw'].astype('str').apply(lambda x: pysentimiento_sentiment_score(trim_maxtokens(x,225)))\n","corpus_sents_pysenti8000 = corpus_sents_df.iloc[7001:]['sent_raw'].astype('str').apply(lambda x: pysentimiento_sentiment_score(trim_maxtokens(x,225)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHZjCy88V7VE"},"outputs":[],"source":["corpus_sents_df['pysentimiento'] = pd.Series()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YRRPxJalQ4Vl"},"outputs":[],"source":["corpus_sentd_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j35YMZ2GQ4QL"},"outputs":[],"source":["corpus_parags_df['pysentimiento'] = corpus_parags_df.iloc[:1000]['parag_raw'].astype('str').apply(lambda x: pysentimiento_sentiment_score(trim_maxtokens(x,510)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kytzW8OUL4fb"},"outputs":[],"source":["corpus_parags_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"96w1Lu93tHoJ"},"source":["## **(3) RoBERTa+XLM Twitter8l198m**\n","\n","* https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZnwtOJwtlhS"},"outputs":[],"source":["from transformers import pipeline\n","model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n","sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\n","sentiment_task(\"T'estimo!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zT8XCBs4ttiF"},"outputs":[],"source":["sentiment_task(\"The sky is blue\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZKZCr-KttDt"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8b6aLodOtVQH"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","from transformers import TFAutoModelForSequenceClassification\n","from transformers import AutoTokenizer, AutoConfig\n","import numpy as np\n","from scipy.special import softmax\n","\n","# Preprocess text (username and link placeholders)\n","def preprocess(text):\n","    new_text = []\n","    for t in text.split(\" \"):\n","        t = '@user' if t.startswith('@') and len(t) > 1 else t\n","        t = 'http' if t.startswith('http') else t\n","        new_text.append(t)\n","    return \" \".join(new_text)\n","\n","MODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","config = AutoConfig.from_pretrained(MODEL)\n","\n","# PT\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n","model.save_pretrained(MODEL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PEZbPFkdsMNl"},"outputs":[],"source":["text = \"Good night 😊\"\n","text = preprocess(text)\n","encoded_input = tokenizer(text, return_tensors='pt')\n","output = model(**encoded_input)\n","scores = output[0][0].detach().numpy()\n","scores = softmax(scores)\n","\n","# # TF\n","# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n","# model.save_pretrained(MODEL)\n","\n","# text = \"Good night 😊\"\n","# encoded_input = tokenizer(text, return_tensors='tf')\n","# output = model(encoded_input)\n","# scores = output[0][0].numpy()\n","# scores = softmax(scores)\n","\n","# Print labels and scores\n","ranking = np.argsort(scores)\n","ranking = ranking[::-1]\n","for i in range(scores.shape[0]):\n","    l = config.id2label[ranking[i]]\n","    s = scores[ranking[i]]\n","    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"]},{"cell_type":"markdown","metadata":{"id":"nKNhxDFysNlp"},"source":["## **(3-Start?) RoBERTa Twitter58m**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9M9087cNz69"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","from transformers import TFAutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","import numpy as np\n","from scipy.special import softmax\n","import csv\n","import urllib.request\n","\n","# Preprocess text (username and link placeholders)\n","def preprocess(text):\n","    new_text = []\n","\n","\n","    for t in text.split(\" \"):\n","        t = '@user' if t.startswith('@') and len(t) > 1 else t\n","        t = 'http' if t.startswith('http') else t\n","        new_text.append(t)\n","    return \" \".join(new_text)\n","\n","# Tasks:\n","# emoji, emotion, hate, irony, offensive, sentiment\n","# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n","\n","task='sentiment'\n","MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","\n","# download label mapping\n","labels=[]\n","mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n","with urllib.request.urlopen(mapping_link) as f:\n","    html = f.read().decode('utf-8').split(\"\\n\")\n","    csvreader = csv.reader(html, delimiter='\\t')\n","labels = [row[1] for row in csvreader if len(row) > 1]\n","\n","# PT\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n","model.save_pretrained(MODEL)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E30AaSBPsmef"},"outputs":[],"source":["text = \"Good night 😊\"\n","text = \"That\"\n","text = preprocess(text)\n","encoded_input = tokenizer(text, return_tensors='pt')\n","output = model(**encoded_input)\n","scores = output[0][0].detach().numpy()\n","scores = softmax(scores)\n","\n","# # TF\n","# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n","# model.save_pretrained(MODEL)\n","\n","# text = \"Good night 😊\"\n","# encoded_input = tokenizer(text, return_tensors='tf')\n","# output = model(encoded_input)\n","# scores = output[0][0].numpy()\n","# scores = softmax(scores)\n","\n","ranking = np.argsort(scores)\n","ranking = ranking[::-1]\n","for i in range(scores.shape[0]):\n","    l = labels[ranking[i]]\n","    s = scores[ranking[i]]\n","    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P--EtYI3smZW"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xcCdDleMNz3b"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"8dPWPoYxUPQL"},"source":["# T5 Base IMDB (sentencepiece)\n","\n","*  mrm8488/t5-base-finetuned-imdb-sentiment\n","\n","* https://huggingface.co/mrm8488/t5-base-finetuned-imdb-sentiment\n","\n","* Requires Sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAGMZsbdo1re"},"outputs":[],"source":["!pip install transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aindN65OoyxB"},"outputs":[],"source":["import transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGixvCKIpHU2"},"outputs":[],"source":["%whos DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjTQIdPsocD-"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelWithLMHead\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")\n","model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-imdb-sentiment\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHDmEB7co54Z"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7d7mMEYEo50D"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jSyzv-SXNz0r"},"outputs":[],"source":["def get_sentiment(text):\n","  input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n","\n","  output = model.generate(input_ids=input_ids,\n","               max_length=2)\n","\n","  dec = [tokenizer.decode(ids) for ids in output]\n","  label = dec[0]\n","  return label\n","\n","get_sentiment(\"I'm not sure who I like more\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XTKs79IDNzxe"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQG7_cn8NzuV"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYAqRwkRNzrf"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvVlrEIVNzo4"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uosXOGf3Nzlq"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XpVZAlFNzi6"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1-OvHKzNzgL"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"nsHxC7Xlg20p"},"source":["# Install and Import Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hTaZjH1gzNw"},"outputs":[],"source":["from transformers import AutoTokenizer,AutoModelForSequenceClassification\n","import torch\n","import requests\n","from bs4 import BeautifulSoup\n","import re\n"]},{"cell_type":"markdown","metadata":{"id":"s-amBdNQgq-q"},"source":["# Instantiate Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAHi14bZgvFr"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"]},{"cell_type":"markdown","metadata":{"id":"anDJKprJg8iQ"},"source":["# Encode and Calculate Sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwD0s3fJgvDf"},"outputs":[],"source":["tokens = tokenizer.encode(\"I hated mas Yoza, he absolutely the worst mentor\", return_tensors='pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORVUmGgHgvBt"},"outputs":[],"source":["tokens[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"POWu51WAgu7M"},"outputs":[],"source":["tokenizer.decode(tokens[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jzjn7kphClC"},"outputs":[],"source":["# Predict Tokens\n","result = model(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P54i49GWhEYk"},"outputs":[],"source":["result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJtfKYgWhE2r"},"outputs":[],"source":["predict_sentiment = int(torch.argmax(result.logits))+1\n","predict_sentiment"]},{"cell_type":"markdown","metadata":{"id":"ZRmoxocehJdL"},"source":["# Collect Reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJeQCo3dhHGQ"},"outputs":[],"source":["\n","r = requests.get('https://yelp.com/biz/8milepi-detroit-style-pizza-san-francisco-2')\n","soup = BeautifulSoup(r.text, 'html.parser')\n","regex = re.compile('.*comment.*')\n","results = soup.find_all('p', {'class':regex})\n","reviews = [result.text for result in results]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVPzroTDhK3d"},"outputs":[],"source":["reviews"]},{"cell_type":"markdown","metadata":{"id":"3w8w6IJ0hQc8"},"source":["# Load Reviews into DataFrame and Score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2nbYQRwhNfF"},"outputs":[],"source":["df = pd.DataFrame(np.array(reviews), columns=['review'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pnTp5kz8hSFk"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNxrKRqKhS67"},"outputs":[],"source":["def sentiment_score(review):\n","    tokens = tokenizer.encode(review, return_tensors='pt')\n","    result = model(tokens)\n","    return int(torch.argmax(result.logits))+1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8H4cobtjhfQ0"},"outputs":[],"source":["result.logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dHQ6xJLChU8O"},"outputs":[],"source":["torch.argmax(result.logits)\n","sentiment_score(df.review.iloc[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVZ2Q8jDhWj8"},"outputs":[],"source":["df['sentiment'] = df['review'].apply(lambda x: sentiment_score(x[:512])) #Limited to 512 Token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qmAT1SVhX97"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOhoA_jOhYJq"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["ZnKuJjHQhPqN","OXnCgU3UAhxz","egqtbEOICSGG","MJbeiU1R_z5y","ReDQHLzGrQG8","G3anSxJbAIuM","8opUJxLkunNu","qEOydcZpum2h","LEC93OI0um9e","cPtgYODJ4EIc","ilhcBc9JAOqv","neXSDXd1L5Gd","14X-LOG3AMcM","AkRjCJFHuWMu","QINAbw-_rQAf","ICiq1NpK7S_R","0eEqeDMnoenp","Bsm8awD4AZ8O","9ukLCuZFm1bu","u932nJxdh0Ac","XKV1uMBEO8TR","FU3aHagiRjqR"],"machine_shape":"hm","name":"master_sentimentarcs_part2_transformers_iota.ipynb","toc_visible":true,"provenance":[{"file_id":"1DhZ3kRFo35Vm4Lsfn-07zhj2wi4_jSFa","timestamp":1646263303755}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}