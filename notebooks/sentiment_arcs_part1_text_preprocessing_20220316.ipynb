{"cells":[{"cell_type":"markdown","metadata":{"id":"3i0Fg4SYB7g0"},"source":["# **SentimentArcs (Part 1): Text Preprocessing**\n","\n","```\n","Jon Chun\n","12 Jun 2021: Started\n","04 Mar 2022: Last Update\n","```\n","\n","Welcome! \n","\n","SentimentArcs is a methodlogy and software framework for analyzing narrative in text. Virtually all long text contains narrative elements...(TODO: Insert excerpts from Paper Abstract/Intro Sections here)\n","\n","***\n","\n","* **SentimentArcs: Cloning the Github repository to your gDrive**\n","\n","If this is the first time using SentimentArcs, you will need to copy the software from our Github.com repository (github repo). The default recommended gDrive path is ./gdrive/MyDrive/research/sentiment_arcs/'. \n","\n","The first time you run this notebook and connect your Google gDrive, it will allow to to specify the path to your SentimentArcs subdirectory. If it does not exists, this notebook will copy/clone the SentimentArcs github repository code to your gDrive at the path you specify.\n","\n","\n","***\n","\n","* **NovelText: A Reference Corpus of 24 Diverse Novel**\n","\n","Sentiment Arcs comes with a carefully curated reference corpus of Novels to illustrate the unique diachronic sentiment analysis characteristic of long form fictional narrativeas. This corpus of 24 diverse novels also provides a baseline for exploring and comparing new novels with sentiment analysis using SentimentArcs.\n","\n","***\n","\n","* **Preparing New Novels: Formatting and adding to subdirectory**\n","\n","To analyze new novels with SentimentArcs, the body of the text should consist of plain text organized in to blocks separated by two newlines which visually look like a single blank line between blocks. These blocks are usually paragraphs but can also include title headers, separate lines of dialog or quotes. Please reference any of the 24 novels in the NovelText corpus for examples of this expected format.\n","\n","Once the new novel is correctly formatted as a plain text file, it should follow this standard file naming convention:\n","\n","[first letter of first name]+[full lastname]_[abbreviated book title].txt\n","\n","Examples:\n","\n","* fdouglass_narrativelifeofaslave\n","* fscottfitzgerald_thegreatgatsby.txt\n","* vwoolf_mrsdalloway.txt\n","* homer-ewilson_odyssey.txt (trans. E.Wilson)\n","* mproust-mtreharne_3guermantesway.txt (Book 3, trans. M.Treharne)\n","* staugustine_confessions9end.txt (Upto and incl Book 9)\n","\n","Note the optional author suffix (-translator) and optional title suffix (-selected chapters/books)\n","\n","***\n","\n","* **Adding New Novels: Add file to subdirectory and Update this Notebook**\n","\n","Once you have a cleaned and text file named according the standard rule above, you must move that file to the subdirectory of all input novels and update the global variable in this notebook that defines which novels to analyze.\n","\n","First, copy your cleaned text file to the subdirectory containing all novels read by this notebook. This subdir is defined by the program variable 'subdir_novels' with the default value './in1_novels/'\n","\n","Second, update the program variable 'novels_dt'. This is a Dictionary data structure that following the pattern below:\n","```\n","novels_dt = {\n","  'cdickens_achristmascarol':['A Christmas Carol by Charles Dickens ',1843,1399],\n","```\n","Where the first string (the dictionary key) must match the filename root without the '.txt' suffix (e.g. cdickens_achristmascarol). The Dictionary value after the ':' is a list of three elements:\n","\n","* A nicely formatted string of the form '(title) by (full first and last name of author)' that should be a human friendly string used to label plots and saved files.\n","\n","* The (publication year) and the (sentence count). Both are optional, but should have placeholder string '0' if unknown. These are intended for future reference and analytics.\n","\n","* Your future self will thank you if you insert new novels into the 'novels_dt' in alphabetic order for faster and more accurate reference.\n","\n","***\n","\n","* **How to Execute SentimentArcs Notebooks:**\n","\n","This is a Jupyter Notebook created to run on Google's free Colab service using only a browers and your exiting Google email account. We chose Google Colab because it is relatively, fast, free, easy to use and makes collaboration as simple as web browsing.\n","\n","A few reminders about using Jupyter Notebooks general and SentimentArcs in particular:\n","\n","* All cells must be run ***in order*** as later code cells often depend upon the output of earlier code cells\n","\n","* ***Cells that take more time to execute*** (> 1 min) usually begin with *%%time* which outputs the *total execution time* of the last run.  This timing output is deleted and recalculated each time the code cell is executed.\n","\n","* **[OPTIONAL]** at the top of a cell indicates you *may* change a setting in that cell to customize behavior.\n","\n","* **[CUSTOMIZE]** at the top of a cell indicates you *must* change a setting in that cell.\n","\n","* **[RESTART REQUIRED]** at the top of a cell indicates you *may* see a *[RESTART REQUIRED] button* at the end of the output. *If you see this button, you must select [Runtime]->[Restart Runtime] from the top menubar.\n","\n","* **[INPUT REQUIRED]** at the top of a cell indicates you will be required to take some action for execution to proceed, usually by clicking a button or entering the response to a prompt.\n","\n","All cells with a top comment prefixed with # [OPTIONAL]: indicates that you can change a setting to customize behavior, the prefix [CUSTOMIZE] indicates you MUST set/change a setting\n","\n","* SentimentArcs divides workflow into a series of chronological Jupyter Notebooks that must be run in order. Here is an overview of the workflow:\n","\n","***\n","\n","**SentimentArcs Notebooks Workflow**\n","1. Notebook #1: Preprocess Text\n","2. Notebook #2: Compute Sentiment Values (Simple Models/CPUs)\n","3. Notebook #3: Compute Sentiment Values (Complex Models/GPUs)\n","4. Notebook #4: Combine all Sentiment Values, perform Time Series analysis, and extract Crux points and surrounding text\n","\n","If you are unfamilar with setting up and using Google Colab or Jupyter Notebooks, here are a series of resources to quickly bring you up to speed. If you are using SentimentArcs with the Cambridge University Press Elements textbook, there are also a series of videos by Prof Elkins and Chun stepping you through these notebooks.\n","\n","***\n","\n","**Additional Resources and Tutorials**\n","\n","\n","**Google Colab and Jupyter Resources:**\n","\n","* Coming...\n","* [IPython, Python Data Science Handbook by Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/01.00-ipython-beyond-normal-python.html) \n","\n","**Cambridge University Press Videos:**\n","\n","* Coming...\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vgvTrI7bevn2"},"source":["# **[STEP 1] Manual Configuration/Setup**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qkcsI681TaDM"},"source":["## (Popups) Connect Google gDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2bfkqjgMiw7T","outputId":"64309c5b-f578-4d02-de3e-2f25b20d36b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to attach your Google gDrive to this Colab Jupyter Notebook\n"]}],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive', force_remount=True)\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"markdown","metadata":{"id":"XVWagkv16GKQ"},"source":["## (3 Inputs) Define Directory Tree"]},{"cell_type":"code","source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","# #@markdown **Instructions**\n","\n","# #@markdown Set Directory and Corpus names:\n","# #@markdown <li> Set <b>Path_to_SentimentArcs</b> to the project root in your **GDrive folder**\n","# #@markdown <li> Set <b>Corpus_Genre</b> = [novels, finance, social_media]\n","# #@markdown <li> <b>Corpus_Type</b> = [reference_corpus, new_corpus]\n","# #@markdown <li> <b>Corpus_Number</b> = [1-20] (id nunmber if a new_corpus)\n","\n","# #@markdown <hr>\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/cdh/sentiment_arcs/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","\n","#@markdown Set this to the project root in your <b>GDrive folder</b>\n","#@markdown <br> (e.g. /<wbr><b>gdrive/MyDrive/research/sentiment_arcs/</b>)\n","\n","#@markdown <hr>\n","\n","#@markdown **Which type of texts are you cleaning?** \\\n","\n","Corpus_Genre = \"novels\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","\n","Corpus_Type = \"new\" #@param [\"new\", \"reference\"]\n","\n","\n","Corpus_Number = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n","\n","\n","#@markdown Put in the corresponding Subdirectory under **./text_raw**:\n","#@markdown <li> All Texts as clean <b>plaintext *.txt</b> files \n","#@markdown <li> A <b>YAML Configuration File</b> describing each Texts\n","\n","#@markdown Please verify the required textfiles and YAML file exist in the correct subdirectories before continuing.\n","\n","print('Current Working Directory:')\n","%cd $Path_to_SentimentArcs\n","\n","print('\\n')\n","\n","if Corpus_Type == 'reference':\n","  SUBDIR_TEXT_RAW_CORPUS = f'text_raw_{Corpus_Genre}_ref'\n","else:\n","  SUBDIR_TEXT_RAW_CORPUS = f'text_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","\n","\n","PATH_TEXT_RAW_CORPUS = f'./text_raw/{SUBDIR_TEXT_RAW_CORPUS}'\n","\n","\n","print(f'SUBDIR_TEXT_RAW_CORPUS:\\n  [{SUBDIR_TEXT_RAW_CORPUS}]')\n","print(f'PATH_TEXT_RAW_CORPUS:\\n  [{PATH_TEXT_RAW_CORPUS}]')"],"metadata":{"id":"kskWCX1KyrV_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P00BhwLVyL8X"},"source":["# **[STEP 2] Automatic Configuration/Setup**"]},{"cell_type":"code","source":["# Add PATH for ./utils subdirectory\n","\n","import sys\n","import os\n","\n","!python --version\n","\n","print('\\n')\n","\n","PATH_UTILS = f'{Path_to_SentimentArcs}/utils'\n","PATH_UTILS\n","\n","sys.path.append(PATH_UTILS)\n","\n","print('Contents of Subdirectory [./sentiment_arcs/utils/]\\n')\n","!ls $PATH_UTILS\n","\n","# More Specific than PATH for searching libraries\n","# !echo $PYTHONPATH"],"metadata":{"id":"uRx8mIVxUyXM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Review Global Variables and set the first few\n","\n","import global_vars as global_vars\n","\n","global_vars.SUBDIR_SENTIMENTARCS = Path_to_SentimentArcs\n","global_vars.Corpus_Genre = Corpus_Genre\n","global_vars.Corpus_Type = Corpus_Type\n","global_vars.Corpus_Number = Corpus_Number\n","\n","global_vars.SUBDIR_TEXT_RAW_CORPUS = SUBDIR_TEXT_RAW_CORPUS\n","global_vars.PATH_TEXT_RAW_CORPUS = PATH_TEXT_RAW_CORPUS\n","\n","dir(global_vars)"],"metadata":{"id":"RBtWnOBxiw8H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Custom Libraries & Define Globals"],"metadata":{"id":"CBoEHX9Z9imD"}},{"cell_type":"code","source":["# Import SentimentArcs Utilities to define Directory Structure\n","#   based the Selected Corpus Genre, Type and Number\n","\n","!pwd \n","print('\\n')\n","\n","# from utils import sa_config # .sentiment_arcs_utils\n","from utils import sa_config\n","\n","print('Objects in sa_config()')\n","print(dir(sa_config))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","sa_config.get_subdirs(Corpus_Genre, Corpus_Type, Corpus_Number, 'none')\n"],"metadata":{"id":"VtaPyy4VSohJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Call SentimentArcs Utility to define Global Variables\n","\n","sa_config.set_globals()\n","\n","# Verify sample global var set\n","print(f'MIN_PARAG_LEN: {global_vars.MIN_PARAG_LEN}')\n","print(f'STOPWORDS_ADD_EN: {global_vars.STOPWORDS_ADD_EN}')\n","print(f'TEST_WORDS_LS: {global_vars.TEST_WORDS_LS}')\n","print(f'SLANG_DT: {global_vars.SLANG_DT}')"],"metadata":{"id":"Tx8j_3Y6qQna"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mGoFJmeFkTxk"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kK8zKENjsyig"},"outputs":[],"source":["# Configure Jupyter\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"Ns5NwArZmush"},"source":["## Read YAML Configuration for Corpus and Models "]},{"cell_type":"code","source":["# from utils import sa_config # .sentiment_arcs_utils\n","\n","import yaml\n","\n","from utils import read_yaml\n","\n","print('Objects in read_yaml()')\n","print(dir(read_yaml))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","read_yaml.read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number)\n","\n","print('SentimentArcs Model Ensemble ------------------------------\\n')\n","model_titles_ls = global_vars.models_titles_dt.keys()\n","print('\\n'.join(model_titles_ls))\n","\n","\n","print('\\n\\nCorpus Texts ------------------------------\\n')\n","corpus_titles_ls = global_vars.corpus_titles_dt.keys()\n","print('\\n'.join(corpus_titles_ls))\n","\n","\n","print(f'\\n\\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\\n')\n","print(f'\\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\\n')\n","print('\\n')\n"],"metadata":{"id":"mUveIcUOzYav"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5GqEXyRkPjj"},"source":["## Install Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tz5jGrDYi9Qe"},"outputs":[],"source":["# Library to Read R datafiles from within Python programs\n","\n","!pip install pyreadr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFXzmfPQouNR"},"outputs":[],"source":["# Powerful Industry-Grade NLP Library\n","\n","!pip install -U spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sD_ZVbywJ4_e"},"outputs":[],"source":["# NLP Library to Simply Cleaning Text\n","\n","!pip install texthero"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T94fr2ymLFgV"},"outputs":[],"source":["# Advanced Sentence Boundry Detection Pythn Library\n","#   for splitting raw text into grammatical sentences\n","#   (can be difficult due to common motifs like Mr., ..., ?!?, etc)\n","\n","!pip install pysbd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3ev2lK-MU9E"},"outputs":[],"source":["# Python Library to expand contractions to aid in Sentiment Analysis\n","#   (e.g. aren't -> are not, can't -> can not)\n","\n","!pip install contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kScSfHO1Q8Y-"},"outputs":[],"source":["# Library for dealing with Emoticons (punctuation) and Emojis (icons)\n","\n","!pip install emot"]},{"cell_type":"markdown","metadata":{"id":"ajD8hCbzkStO"},"source":["## Load Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCRgJK2ri9Nx"},"outputs":[],"source":["# Core Python Libraries\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","\n","import re\n","import string\n","from datetime import datetime\n","import os\n","import sys\n","import glob\n","import json\n","from pathlib import Path\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pify1umf6A8K"},"outputs":[],"source":["# More advanced Sentence Tokenizier Object from PySBD\n","from pysbd.utils import PySBDFactory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEPQ67KrCO6f"},"outputs":[],"source":["# Simplier Sentence Tokenizer Object from NLTK\n","import nltk \n","from nltk.tokenize import sent_tokenize\n","\n","# Download required NLTK tokenizer data\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRYua8r7MP07"},"outputs":[],"source":["# Instantiate and Import Text Cleaning Ojects into Global Variable space\n","import texthero as hero\n","from texthero import preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7PBsG4WRMvrN"},"outputs":[],"source":["# Expand contractions (e.g. can't -> can not)\n","import contractions\n","\n","# Translate emoticons :0 and emoji icons to text\n","import emot \n","emot_obj = emot.core.emot() \n","\n","from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n","\n","# Test\n","text = \"I love python ☮ 🙂 ❤ :-) :-( :-)))\" \n","emot_obj.emoticons(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPFS4MEm6MyF"},"outputs":[],"source":["# Import spaCy, language model and setup minimal pipeline\n","\n","import spacy\n","\n","nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n","# nlp.max_length = 1027203\n","nlp.max_length = 2054406\n","nlp.add_pipe(nlp.create_pipe('sentencizer')) # https://stackoverflow.com/questions/51372724/how-to-speed-up-spacy-lemmatization\n","\n","# Test some edge cases, try to find examples that break spaCy\n","doc= nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n","print('\\n')\n","print(\"Token Attributes: \\n\", \"token.text, token.pos_, token.tag_, token.dep_, token.lemma_\")\n","for token in doc:\n","    # Print the text and the predicted part-of-speech tag\n","    print(\"{:<12}{:<12}{:<12}{:<12}{:<12}\".format(token.text, token.pos_, token.tag_, token.dep_, token.lemma_))\n","\n","print('\\nAnother Test:\\n')\n","doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n","\n","for token in doc:\n","    print(\"{:<12}{:<30}{:<12}\".format(token.text, token.lemma, token.lemma_))"]},{"cell_type":"markdown","metadata":{"id":"umZfB0YCqajW"},"source":["## Define/Customize Stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8_qLJBbtoOA"},"outputs":[],"source":["# Define Globals\n","\"\"\"\n","# Main data structure: Dictionary (key=text_name) of DataFrames (cols: text_raw, text_clean)\n","corpus_texts_dt = {}\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/get_globals.py'\n","\n","SLANG_DT.keys()\n","\"\"\";"]},{"cell_type":"code","source":["global_vars.SLANG_DT.keys()"],"metadata":{"id":"DWFx7-P-Ai5D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dir(global_vars)"],"metadata":{"id":"zqrk5TEuAzzq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%whos"],"metadata":{"id":"wdTV7rVOAsNO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify English Stopword List\n","\n","stopwords_spacy_en_ls = nlp.Defaults.stop_words\n","\n","','.join([x for x in stopwords_spacy_en_ls])\n","\n","stopwords_en_ls = stopwords_spacy_en_ls\n","\n","print(f'\\n\\nThere are {len(stopwords_spacy_en_ls)} default English Stopwords from spaCy\\n')"],"metadata":{"id":"j1Lp4GLndZhY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_438Act0jds"},"outputs":[],"source":["# Customize Default SpaCy English Stopword List\n","\n","print(f'\\n\\nThere are {len(stopwords_spacy_en_ls)} default English Stopwords from spaCy\\n')\n","\n","# [CUSTOMIZE] Stopwords to ADD or DELETE from default spaCy English stopword list\n","LOCAL_STOPWORDS_DEL_EN = set(global_vars.STOPWORDS_DEL_EN).union(set(['a','an','the','but','yet']))\n","print(f'    Deleting these stopwords: {LOCAL_STOPWORDS_DEL_EN}')\n","LOCAL_STOPWORDS_ADD_EN = set(global_vars.STOPWORDS_ADD_EN).union(set(['a','an','the','but','yet']))\n","print(f'    Adding these stopwords: {LOCAL_STOPWORDS_ADD_EN}\\n')\n","\n","stopwords_en_ls = list(set(stopwords_spacy_en_ls).difference(set(LOCAL_STOPWORDS_DEL_EN)).union(set(LOCAL_STOPWORDS_ADD_EN)))\n","print(f'Final Count: {len(stopwords_en_ls)} Stopwords')"]},{"cell_type":"markdown","metadata":{"id":"EA1yTaY_9Qod"},"source":["## Setup Matplotlib Style"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmf7SFb7PnUi"},"outputs":[],"source":["# Configure Matplotlib\n","\n","# View available styles\n","# plt.style.available\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/config_matplotlib.py'\n","\n","config_matplotlib()\n","\n","print('Matplotlib Configuration ------------------------------')\n","print('\\n  (Uncomment to view)')\n","# plt.rcParams.keys()\n","print('\\n  Edit ./utils/config_matplotlib.py to change')"]},{"cell_type":"markdown","metadata":{"id":"7dPPrZwyIIze"},"source":["## Setup Seaborn Style"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hM3oRY-UOmzX"},"outputs":[],"source":["# Configure Seaborn\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/config_seaborn.py'\n","\n","config_seaborn()\n","\n","print('Seaborn Configuration ------------------------------\\n')\n","# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')"]},{"cell_type":"markdown","metadata":{"id":"xBpIUgstnE62"},"source":["## **Utility Functions**"]},{"cell_type":"markdown","metadata":{"id":"JXG_G6um4ijG"},"source":["### Generate Convenient Data Lists"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TO08GFoGlP3y"},"outputs":[],"source":["# Derive List of Texts in Corpus a)keys and b)full author and titles\n","\n","print('Dictionary: corpus_titles_dt')\n","global_vars.corpus_titles_dt\n","print('\\n')\n","\n","corpus_texts_ls = list(global_vars.corpus_titles_dt.keys())\n","print(f'\\nCorpus Texts:')\n","for akey in corpus_texts_ls:\n","  print(f'  {akey}')\n","print('\\n')\n","\n","print(f'\\nNatural Corpus Titles:')\n","corpus_titles_ls = [x[0] for x in list(global_vars.corpus_titles_dt.values())]\n","for akey in corpus_titles_ls:\n","  print(f'  {akey}')\n"]},{"cell_type":"code","source":["# Get Model Families of Ensemble\n","\n","from utils.get_model_families import get_ensemble_model_famalies\n","\n","global_vars.models_ensemble_dt = get_ensemble_model_famalies(global_vars.models_titles_dt)\n","\n","print('\\nTest: Lexicon Family of Models:')\n","global_vars.models_ensemble_dt['lexicon']"],"metadata":{"id":"rQNlQr4_Ckb1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pjQBAoLjOzDO"},"source":["### Text Cleaning "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qpchjKtfy2H4"},"outputs":[],"source":["# [VERIFY]: Texthero preprocessing pipeline\n","\n","hero.preprocessing.get_default_pipeline()\n","\n","\n","\n","# Create Default and Custom Stemming TextHero pipeline\n","\n","# Create a custom cleaning pipeline\n","def_pipeline = [preprocessing.fillna\n","                , preprocessing.lowercase\n","                , preprocessing.remove_digits\n","                , preprocessing.remove_punctuation\n","                , preprocessing.remove_diacritics\n","                # , preprocessing.remove_stopwords\n","                , preprocessing.remove_whitespace]\n","\n","# Create a custom cleaning pipeline\n","stem_pipeline = [preprocessing.fillna\n","                , preprocessing.lowercase\n","                , preprocessing.remove_digits\n","                , preprocessing.remove_punctuation\n","                , preprocessing.remove_diacritics\n","                , preprocessing.remove_stopwords\n","                , preprocessing.remove_whitespace\n","                , preprocessing.stem]\n","                   \n","# Test: pass the custom_pipeline to the pipeline argument\n","# df['clean_title'] = hero.clean(df['title'], pipeline = custom_pipeline)df.head()"]},{"cell_type":"code","source":["# Test Text Cleaning Functions\n","# NOTE: These functions rely on big imports made in this main notebook (e.g. NLTK, SpaCy, TextHero)\n","#       therefore we execute/define them inline with %run rather than as modules with better separation \n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/text_cleaners.py'\n","\n","test_suite_ls = ['text2lemmas',\n","                 'text_str2sents',\n","                 'textfile2df',\n","                 'emojis2text',\n","                 'all_emos2text',\n","                 'expand_slang',\n","                 'clean_text',\n","                 'lemma_pipe'\n","                 ]\n","\n","# test_suite_ls = []\n","\n","# Test: text2lemmas()\n","if 'text2lemmas' in test_suite_ls:\n","  text2lemmas('I am going to start studying more often and working harder.', lowercase=True, remove_stopwords=False)\n","  print('\\n')\n","\n","# Test: text_str2sents()\n","if 'text_str2sents' in test_suite_ls:\n","  text_str2sents('Hello. You are a great dude! WTF?\\n\\n You are a goat. What is a goat?!? A big lazy GOAT... No way-', pysbd_only=False) # !?! Dr. and Mrs. Elipses...', pysbd_only=True)\n","  print('\\n')\n","\n","# Test: textfile2df()\n","if 'textfile2df' in test_suite_ls:\n","  # ???\n","  print('\\n')\n","\n","# Test: emojis2text()\n","if 'emojis2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling of making a sale 😎, The feeling of actually ;) fulfilling orders 😒\"\n","  test_str = emojis2text(test_str)\n","  print(f'test_str: [{test_str}]')\n","  print('\\n')\n","\n","# Test: all_emos2text()\n","if 'all_emos2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling :o of making a sale 😎, The feeling :( of actually ;) fulfilling orders 😒\"\n","  all_emos2text(test_str)\n","  print('\\n')\n","\n","# Test: expand_slang():\n","if 'expand_slang' in test_suite_ls:\n","  expand_slang('idk LOL you suck!')\n","  print('\\n')\n","\n","# Test: clean_text()\n","if 'clean_text' in test_suite_ls:\n","  test_df = pd.DataFrame({'text_dirty':['The RAin in SPain','WTF?!?! Do you KnoW...']})\n","  clean_text(test_df, 'text_dirty', text_type='formal')\n","  print('\\n')\n","\n","# Test: lemma_pipe()\n","if 'lemma_pipe' in test_suite_ls:\n","  print('\\nTest #1:\\n')\n","  test_ls = ['I am running late for a meetings with all the many people.',\n","            'What time is it when you fall down running away from a growing problem?',\n","            \"You've got to be kidding me - you're joking right?\"]\n","  lemma_pipe(test_ls)\n","  print('\\nTest #2:\\n')\n","  texts = pd.Series([\"I won't go and you can't make me.\", \"Billy is running really quickly and with great haste.\", \"Eating freshly caught seafood.\"])\n","  for doc in nlp.pipe(texts):\n","    print([tok.lemma_ for tok in doc])\n","  print('\\nTest #3:\\n')\n","  lemma_pipe(texts)\n"],"metadata":{"id":"J2DLzn1TJ12C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WlfujTpEKhCp"},"source":["### File Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOX-fpiuApL4"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","# Method #1: Preferred isolation using module directories\n","# TODO: list individual methods() used to min polluting global namespace\n","from utils.file_utils import get_fullpath, textfile2df, write_dict_dfs, read_dict_dfs\n","\n","# Method #2: Run code in-line by executing file\n","# %run -i './utils/file_utils.py'\n","\n","# TODO: Not used? Delete?\n","# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)"]},{"cell_type":"markdown","metadata":{"id":"Ilz5X9AEbP8r"},"source":["# **[STEP 2] Read in Corpus and Clean**"]},{"cell_type":"markdown","metadata":{"id":"1a-1wyeiGt4Z"},"source":["## Create List of Raw Textfiles"]},{"cell_type":"code","source":["# Current key Directories\n","\n","print('Current Subdirectory:')\n","!pwd\n","print('\\n')\n","\n","print(f'SentimentArcs root Subdirectory: [{global_vars.SUBDIR_SENTIMENTARCS}]\\n')\n","\n","path_text_raw = './' + '/'.join(global_vars.SUBDIR_TEXT_RAW.split('/')[1:-1])\n","print(f'path_text_raw: [{path_text_raw}]\\n')\n","# SUBDIR_TEXT_RAW = path_text_raw + '/'\n","print(f'Full Path to Corpus text_raw: [{global_vars.SUBDIR_SENTIMENTARCS}/text_raw/{global_vars.SUBDIR_TEXT_RAW_CORPUS}]')"],"metadata":{"id":"cHXNIMtPfZ1j"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXN4pdZReL4Y"},"outputs":[],"source":["# Get a list of all the Textfile filename roots in Subdir text_raw\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n","\n","print(f'Corpus_Genre: {global_vars.Corpus_Genre}')\n","print(f'Corpus_Type: {global_vars.Corpus_Type}\\n')\n","\n","# Build path to Corpus Subdir\n","# TODO: Temp fix until print(f'Original: {SUBDIR_TEXT_RAW}\\n')\n","# path_text_raw = './' + '/'.join(SUBDIR_TEXT_RAW.split('/')[1:-1]) + '/' + SUBDIR_TEXT_RAW_CORPUS\n","path_text_raw = './text_raw/' + global_vars.SUBDIR_TEXT_RAW_CORPUS\n","# print(f'Corpus Subdir: {path_text_raw}')\n","\n","# Create a List (preprocessed_ls) of all preprocessed text files\n","try:\n","  # texts_raw_ls = glob.glob(f'{SUBDIR_TEXT_RAW}*.txt')\n","  texts_raw_root_ls = glob.glob(f'{path_text_raw}/*.txt')\n","  texts_raw_root_ls = [x.split('/')[-1] for x in texts_raw_root_ls]\n","  texts_raw_root_ls = [x.split('.')[0] for x in texts_raw_root_ls]\n","except IndexError:\n","  raise RuntimeError('No *.txt files found')\n","\n","# print(f'\\ntexts_raw_root_ls:\\n  {texts_raw_root_ls}\\n')\n","\n","print('Texts found in Corpus Subdirectory:')\n","print('-----------------------------------')\n","\n","text_ct = 0\n","for afile_root in texts_raw_root_ls:\n","  # file_root = file_fullpath.split('/')[-1].split('.')[0]\n","  text_ct += 1\n","  print(f'{afile_root}: ') # {corpus_titles_dt[afile_root]}')\n","\n","print(f'\\nThe Corpus has [{text_ct}] Texts found in the Subdirectory:')\n","print(f'---------------------------------------------------\\n  {global_vars.SUBDIR_TEXT_RAW}')"]},{"cell_type":"markdown","metadata":{"id":"KkXBipRrGoCQ"},"source":["## Read and Segment into Sentences"]},{"cell_type":"code","source":["from utils.text_cleaners import textfile2df"],"metadata":{"id":"5DjFuHwzY_5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %%time\n","# %xmode Verbose\n","# %debug\n","\n","%run -i './utils/text_cleaners.py'\n","\n","\n","# Read all Corpus Textfiles and Segment each into Sentences\n","\n","# NOTE: 3m30s Entire Corpus of 25 \n","#       7m30s Ref Corpus 32 Novels\n","#       7m24s Ref Corpus 32 Novels\n","#       1m00s New Corpus1 2 Novels\n","#      ~1m30s New Corpus2 3 Novels\n","\n","# Read all novel files into a Dictionary of DataFrames\n","#   Dict.keys() are novel names\n","#   Dict.values() are DataFrames with one row per Sentence\n","\n","# Continue here ONLY if last cell completed WITHOUT ERROR\n","\n","# anovel_df = pd.DataFrame()\n","# import pandas as pd\n","\n","# %run -i './utils/text_cleaners.py'\n","\n","global_vars.corpus_titles_ls = list(global_vars.corpus_texts_dt.keys())\n","for i, file_root in enumerate(global_vars.corpus_titles_ls):\n","  file_fullpath = f'{global_vars.SUBDIR_TEXT_RAW}{file_root}.txt'\n","  # print(f'Processing Novel #{i}: {file_fullpath}') # {file_root}')\n","  # fullpath_str = novels_subdir + asubdir + '/' + asubdir + '.txt'\n","  # print(f\"  Size: {os.path.getsize(file_fullpath)}\")\n","\n","  global_vars.corpus_texts_dt[file_root] = textfile2df(file_fullpath)\n","  \n","# corpus_dt.keys()\n","\n","# Verify First Text is Segmented into text_raw Sentences\n","print('\\n\\n')\n","global_vars.corpus_texts_dt[global_vars.corpus_titles_ls[0]].head()\n"],"metadata":{"id":"F8ONziXzc1jK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(global_vars.corpus_texts_dt.keys())"],"metadata":{"id":"aawiaPkyeLf8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eq50LyAMHKYX"},"outputs":[],"source":["%%time\n","# %xmode Verbose\n","# %debug\n","\n","%run -i './utils/text_cleaners.py'\n","\n","\n","# Read all Corpus Textfiles and Segment each into Sentences\n","\n","# NOTE: 3m30s Entire Corpus of 25 \n","#       7m30s Ref Corpus 32 Novels\n","#       7m24s Ref Corpus 32 Novels\n","#       1m00s New Corpus1 2 Novels\n","#      ~1m30s New Corpus2 3 Novels\n","\n","# Read all novel files into a Dictionary of DataFrames\n","#   Dict.keys() are novel names\n","#   Dict.values() are DataFrames with one row per Sentence\n","\n","# Continue here ONLY if last cell completed WITHOUT ERROR\n","\n","# anovel_df = pd.DataFrame()\n","import pandas as pd\n","\n","# %run -i './utils/text_cleaners.py'\n","\n","for i, file_root in enumerate(corpus_titles_ls):\n","  file_fullpath = f'{global_vars.SUBDIR_TEXT_RAW}{file_root}.txt'\n","  # print(f'Processing Novel #{i}: {file_fullpath}') # {file_root}')\n","  # fullpath_str = novels_subdir + asubdir + '/' + asubdir + '.txt'\n","  # print(f\"  Size: {os.path.getsize(file_fullpath)}\")\n","\n","  global_vars.corpus_texts_dt[file_root] = textfile2df(file_fullpath)\n","  \n","# corpus_dt.keys()\n","\n","# Verify First Text is Segmented into text_raw Sentences\n","print('\\n\\n')\n","global_vars.corpus_texts_dt[global_vars.corpus_titles_ls[0]].head()\n"]},{"cell_type":"markdown","metadata":{"id":"tw-Ll-fdI_yb"},"source":["## Clean Sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPrpL1wyNPva"},"outputs":[],"source":["%%time\n","\n","# NOTE: (no stem) 4m09s (24 Novels)\n","#       (w/ stem) 4m24s (24 Novels)\n","\n","i = 0\n","\n","for key_novel, atext_df in global_vars.corpus_texts_dt.items():\n","\n","  print(f'Processing Novel #{i}: {key_novel}...')\n","\n","  atext_df['text_clean'] = clean_text(atext_df, 'text_raw', text_type='formal')\n","  atext_df['text_clean'] = lemma_pipe(atext_df['text_clean'])\n","  atext_df['text_clean'] = atext_df['text_clean'].astype('string')\n","\n","  # TODO: Fill in all blank 'text_clean' rows with filler semaphore\n","  atext_df.text_clean = atext_df.text_clean.fillna('empty_placeholder')\n","\n","  atext_df.head(2)\n","\n","  print(f'  shape: {atext_df.shape}')\n","\n","  i += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXuwwg2_XgZu"},"outputs":[],"source":["# Verify the first Text in Corpus is cleaned\n","\n","global_vars.corpus_texts_dt[global_vars.corpus_titles_ls[0]].head(20)\n","global_vars.corpus_texts_dt[global_vars.corpus_titles_ls[0]].info()"]},{"cell_type":"markdown","metadata":{"id":"WAjjOEFx7F5J"},"source":["## Save Cleaned Corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_CrH24Dv7YwK"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","print('Currently in SentimentArcs root directory:')\n","!pwd\n","\n","# Verify Subdir to save Cleaned Texts and Texts into..\n","\n","print(f'\\nSaving Clean Texts to Subdir: {global_vars.SUBDIR_TEXT_CLEAN}')\n","print(f'\\nSaving these Texts:\\n  {global_vars.corpus_texts_dt.keys()}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgcmvfbvXqfy"},"outputs":[],"source":["# Save the cleaned Textfiles\n","\n","i = 0\n","for key_novel, anovel_df in global_vars.corpus_texts_dt.items():\n","  anovel_fname = f'{key_novel}.csv'\n","\n","  anovel_fullpath = f'{global_vars.SUBDIR_TEXT_CLEAN}{anovel_fname}'\n","  print(f'Saving Novel #{i} to {anovel_fullpath}')\n","  global_vars.corpus_texts_dt[key_novel].to_csv(anovel_fullpath)\n","  i += 1"]},{"cell_type":"markdown","metadata":{"id":"_348z09gQKe3"},"source":["# **[END OF NOTEBOOK]**"]}],"metadata":{"colab":{"collapsed_sections":["CBoEHX9Z9imD","mGoFJmeFkTxk","Ns5NwArZmush","umZfB0YCqajW","EA1yTaY_9Qod","7dPPrZwyIIze"],"name":"sentiment_arcs_part1_text_preprocessing_20220316.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}