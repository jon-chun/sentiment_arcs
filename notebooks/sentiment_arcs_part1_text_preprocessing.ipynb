{"cells":[{"cell_type":"markdown","metadata":{"id":"3i0Fg4SYB7g0"},"source":["# **SentimentArcs (Part 1): Text Preprocessing**\n","\n","```\n","Jon Chun\n","12 Jun 2021: Started\n","04 Mar 2022: Last Update\n","```\n","\n","Welcome! \n","\n","SentimentArcs is a methodlogy and software framework for analyzing narrative in text. Virtually all long text contains narrative elements...(TODO: Insert excerpts from Paper Abstract/Intro Sections here)\n","\n","***\n","\n","* **SentimentArcs: Cloning the Github repository to your gDrive**\n","\n","If this is the first time using SentimentArcs, you will need to copy the software from our Github.com repository (github repo). The default recommended gDrive path is ./gdrive/MyDrive/research/sentiment_arcs/'. \n","\n","The first time you run this notebook and connect your Google gDrive, it will allow to to specify the path to your SentimentArcs subdirectory. If it does not exists, this notebook will copy/clone the SentimentArcs github repository code to your gDrive at the path you specify.\n","\n","\n","***\n","\n","* **NovelText: A Reference Corpus of 24 Diverse Novel**\n","\n","Sentiment Arcs comes with a carefully curated reference corpus of Novels to illustrate the unique diachronic sentiment analysis characteristic of long form fictional narrativeas. This corpus of 24 diverse novels also provides a baseline for exploring and comparing new novels with sentiment analysis using SentimentArcs.\n","\n","***\n","\n","* **Preparing New Novels: Formatting and adding to subdirectory**\n","\n","To analyze new novels with SentimentArcs, the body of the text should consist of plain text organized in to blocks separated by two newlines which visually look like a single blank line between blocks. These blocks are usually paragraphs but can also include title headers, separate lines of dialog or quotes. Please reference any of the 24 novels in the NovelText corpus for examples of this expected format.\n","\n","Once the new novel is correctly formatted as a plain text file, it should follow this standard file naming convention:\n","\n","[first letter of first name]+[full lastname]_[abbreviated book title].txt\n","\n","Examples:\n","\n","* fdouglass_narrativelifeofaslave\n","* fscottfitzgerald_thegreatgatsby.txt\n","* vwoolf_mrsdalloway.txt\n","* homer-ewilson_odyssey.txt (trans. E.Wilson)\n","* mproust-mtreharne_3guermantesway.txt (Book 3, trans. M.Treharne)\n","* staugustine_confessions9end.txt (Upto and incl Book 9)\n","\n","Note the optional author suffix (-translator) and optional title suffix (-selected chapters/books)\n","\n","***\n","\n","* **Adding New Novels: Add file to subdirectory and Update this Notebook**\n","\n","Once you have a cleaned and text file named according the standard rule above, you must move that file to the subdirectory of all input novels and update the global variable in this notebook that defines which novels to analyze.\n","\n","First, copy your cleaned text file to the subdirectory containing all novels read by this notebook. This subdir is defined by the program variable 'subdir_novels' with the default value './in1_novels/'\n","\n","Second, update the program variable 'novels_dt'. This is a Dictionary data structure that following the pattern below:\n","```\n","novels_dt = {\n","  'cdickens_achristmascarol':['A Christmas Carol by Charles Dickens ',1843,1399],\n","```\n","Where the first string (the dictionary key) must match the filename root without the '.txt' suffix (e.g. cdickens_achristmascarol). The Dictionary value after the ':' is a list of three elements:\n","\n","* A nicely formatted string of the form '(title) by (full first and last name of author)' that should be a human friendly string used to label plots and saved files.\n","\n","* The (publication year) and the (sentence count). Both are optional, but should have placeholder string '0' if unknown. These are intended for future reference and analytics.\n","\n","* Your future self will thank you if you insert new novels into the 'novels_dt' in alphabetic order for faster and more accurate reference.\n","\n","***\n","\n","* **How to Execute SentimentArcs Notebooks:**\n","\n","This is a Jupyter Notebook created to run on Google's free Colab service using only a browers and your exiting Google email account. We chose Google Colab because it is relatively, fast, free, easy to use and makes collaboration as simple as web browsing.\n","\n","A few reminders about using Jupyter Notebooks general and SentimentArcs in particular:\n","\n","* All cells must be run ***in order*** as later code cells often depend upon the output of earlier code cells\n","\n","* ***Cells that take more time to execute*** (\u003e 1 min) usually begin with *%%time* which outputs the *total execution time* of the last run.  This timing output is deleted and recalculated each time the code cell is executed.\n","\n","* **[OPTIONAL]** at the top of a cell indicates you *may* change a setting in that cell to customize behavior.\n","\n","* **[CUSTOMIZE]** at the top of a cell indicates you *must* change a setting in that cell.\n","\n","* **[RESTART REQUIRED]** at the top of a cell indicates you *may* see a *[RESTART REQUIRED] button* at the end of the output. *If you see this button, you must select [Runtime]-\u003e[Restart Runtime] from the top menubar.\n","\n","* **[INPUT REQUIRED]** at the top of a cell indicates you will be required to take some action for execution to proceed, usually by clicking a button or entering the response to a prompt.\n","\n","All cells with a top comment prefixed with # [OPTIONAL]: indicates that you can change a setting to customize behavior, the prefix [CUSTOMIZE] indicates you MUST set/change a setting\n","\n","* SentimentArcs divides workflow into a series of chronological Jupyter Notebooks that must be run in order. Here is an overview of the workflow:\n","\n","***\n","\n","**SentimentArcs Notebooks Workflow**\n","1. Notebook #1: Preprocess Text\n","2. Notebook #2: Compute Sentiment Values (Simple Models/CPUs)\n","3. Notebook #3: Compute Sentiment Values (Complex Models/GPUs)\n","4. Notebook #4: Combine all Sentiment Values, perform Time Series analysis, and extract Crux points and surrounding text\n","\n","If you are unfamilar with setting up and using Google Colab or Jupyter Notebooks, here are a series of resources to quickly bring you up to speed. If you are using SentimentArcs with the Cambridge University Press Elements textbook, there are also a series of videos by Prof Elkins and Chun stepping you through these notebooks.\n","\n","***\n","\n","**Additional Resources and Tutorials**\n","\n","\n","**Google Colab and Jupyter Resources:**\n","\n","* Coming...\n","* [IPython, Python Data Science Handbook by Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/01.00-ipython-beyond-normal-python.html) \n","\n","**Cambridge University Press Videos:**\n","\n","* Coming...\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vgvTrI7bevn2"},"source":["# **[STEP 1] Manual Configuration/Setup**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qkcsI681TaDM"},"source":["## (Popups) Connect Google gDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"2bfkqjgMiw7T"},"outputs":[{"name":"stdout","output_type":"stream","text":["Attempting to attach your Google gDrive to this Colab Jupyter Notebook\n"]}],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive', force_remount=True)\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"markdown","metadata":{"id":"XVWagkv16GKQ"},"source":["## (3 Inputs) Define Directory Tree"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1647421882542,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"kskWCX1KyrV_","outputId":"74758e25-fa49-4962-f884-afadddf9d076"},"outputs":[{"name":"stdout","output_type":"stream","text":["Current Working Directory:\n","/gdrive/MyDrive/cdh/sentiment_arcs\n","\n","\n","SUBDIR_TEXT_RAW_CORPUS:\n","  [text_raw_novels_new_corpus2/]\n","PATH_TEXT_RAW_CORPUS:\n","  [./text_raw/text_raw_novels_new_corpus2/]\n"]}],"source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","# #@markdown **Instructions**\n","\n","# #@markdown Set Directory and Corpus names:\n","# #@markdown \u003cli\u003e Set \u003cb\u003ePath_to_SentimentArcs\u003c/b\u003e to the project root in your **GDrive folder**\n","# #@markdown \u003cli\u003e Set \u003cb\u003eCorpus_Genre\u003c/b\u003e = [novels, finance, social_media]\n","# #@markdown \u003cli\u003e \u003cb\u003eCorpus_Type\u003c/b\u003e = [reference_corpus, new_corpus]\n","# #@markdown \u003cli\u003e \u003cb\u003eCorpus_Number\u003c/b\u003e = [1-20] (id nunmber if a new_corpus)\n","\n","#@markdown \u003chr\u003e\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/cdh/sentiment_arcs/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","\n","#@markdown Set this to the project root in your \u003cb\u003eGDrive folder\u003c/b\u003e\n","#@markdown \u003cbr\u003e (e.g. /\u003cwbr\u003e\u003cb\u003egdrive/MyDrive/research/sentiment_arcs/\u003c/b\u003e)\n","\n","#@markdown \u003chr\u003e\n","\n","#@markdown **Which type of texts are you cleaning?** \\\n","\n","Corpus_Genre = \"novels\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","\n","Corpus_Type = \"new\" #@param [\"new\", \"reference\"]\n","\n","\n","Corpus_Number = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n","\n","\n","#@markdown Put in the corresponding Subdirectory under **./text_raw**:\n","#@markdown \u003cli\u003e All Texts as clean \u003cb\u003eplaintext *.txt\u003c/b\u003e files \n","#@markdown \u003cli\u003e A \u003cb\u003eYAML Configuration File\u003c/b\u003e describing each Texts\n","\n","#@markdown Please verify the required textfiles and YAML file exist in the correct subdirectories before continuing.\n","\n","print('Current Working Directory:')\n","%cd $Path_to_SentimentArcs\n","\n","print('\\n')\n","\n","if Corpus_Type == 'reference':\n","  SUBDIR_TEXT_RAW_CORPUS = f'text_raw_{Corpus_Genre}_ref'\n","else:\n","  SUBDIR_TEXT_RAW_CORPUS = f'text_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","\n","\n","PATH_TEXT_RAW_CORPUS = f'./text_raw/{SUBDIR_TEXT_RAW_CORPUS}'\n","\n","\n","print(f'SUBDIR_TEXT_RAW_CORPUS:\\n  [{SUBDIR_TEXT_RAW_CORPUS}]')\n","print(f'PATH_TEXT_RAW_CORPUS:\\n  [{PATH_TEXT_RAW_CORPUS}]')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":770,"status":"ok","timestamp":1647421883928,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"uRx8mIVxUyXM","outputId":"6ef33e66-796b-49b8-d738-6b6c1e46ad66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.7.12\n","\n","\n","Contents of Subdirectory [./sentiment_arcs/utils/]\n","\n","config_matplotlib.py   global_constants.py    sa_config_bu.py\n","config_seaborn.py      global_vars.py\t      sa_config.py\n","file_utils.py\t       imdb50k_lemmas.csv     sentiment_analysis.py\n","get_fullpath.py        imdb50k_lemmas_df.csv  sentiment_arcs_config.py\n","get_model_families.py  imdb50k_stems.csv      set_globals.py\n","get_sentimentr.R       __init__.py\t      subdir_constants.py\n","get_sentiments.py      __pycache__\t      text_cleaners.py\n","get_subdirs.py\t       read_yaml.py\n"]}],"source":["# Add PATH for ./utils subdirectory\n","\n","import sys\n","import os\n","\n","!python --version\n","\n","print('\\n')\n","\n","PATH_UTILS = f'{Path_to_SentimentArcs}/utils'\n","PATH_UTILS\n","\n","sys.path.append(PATH_UTILS)\n","\n","print('Contents of Subdirectory [./sentiment_arcs/utils/]\\n')\n","!ls $PATH_UTILS\n","\n","# More Specific than PATH for searching libraries\n","# !echo $PYTHONPATH"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":560,"status":"ok","timestamp":1647421884486,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"RBtWnOBxiw8H","outputId":"961f2cf6-62b9-4f47-a3de-ef591c83cc5d"},"outputs":[{"data":{"text/plain":["['Corpus_Genre',\n"," 'Corpus_Number',\n"," 'Corpus_Type',\n"," 'FNAME_SENTIMENT_RAW',\n"," 'MIN_PARAG_LEN',\n"," 'MIN_SENT_LEN',\n"," 'NotebookModels',\n"," 'PATH_TEXT_RAW_CORPUS',\n"," 'SLANG_DT',\n"," 'STOPWORDS_ADD_EN',\n"," 'STOPWORDS_DEL_EN',\n"," 'SUBDIR_DATA',\n"," 'SUBDIR_GRAPHS',\n"," 'SUBDIR_SENTIMENTARCS',\n"," 'SUBDIR_SENTIMENT_CLEAN',\n"," 'SUBDIR_SENTIMENT_RAW',\n"," 'SUBDIR_TEXT_CLEAN',\n"," 'SUBDIR_TEXT_RAW',\n"," 'SUBDIR_TEXT_RAW_CORPUS',\n"," 'SUBDIR_TIMESERIES_CLEAN',\n"," 'SUBDIR_TIMESERIES_RAW',\n"," 'SUBDIR_UTILS',\n"," 'TEST_SENTENCES_LS',\n"," 'TEST_WORDS_LS',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__spec__',\n"," 'corpus_titles_dt',\n"," 'lexicons_dt',\n"," 'model_titles_dt']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Review Global Variables and set the first few\n","\n","import global_vars as global_vars\n","\n","global_vars.SUBDIR_SENTIMENTARCS = Path_to_SentimentArcs\n","global_vars.Corpus_Genre = Corpus_Genre\n","global_vars.Corpus_Type = Corpus_Type\n","global_vars.Corpus_Number = Corpus_Number\n","\n","global_vars.SUBDIR_TEXT_RAW_CORPUS = SUBDIR_TEXT_RAW_CORPUS\n","global_vars.PATH_TEXT_RAW_CORPUS = PATH_TEXT_RAW_CORPUS\n","\n","dir(global_vars)"]},{"cell_type":"markdown","metadata":{"id":"P00BhwLVyL8X"},"source":["# **[STEP 2] Automatic Configuration/Setup**"]},{"cell_type":"markdown","metadata":{"id":"CBoEHX9Z9imD"},"source":["## Custom Libraries \u0026 Define Globals"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":704,"status":"ok","timestamp":1647421885185,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"VtaPyy4VSohJ","outputId":"65025b3b-342d-4344-8be9-e903d4a9fa70"},"outputs":[{"name":"stdout","output_type":"stream","text":["/gdrive/MyDrive/cdh/sentiment_arcs\n","\n","\n","Objects in sa_config()\n","['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'get_subdirs', 'global_vars', 'set_globals']\n","\n","\n","Verify the Directory Structure:\n","\n","-------------------------------\n","\n","           [Corpus Genre]: novels\n","\n","            [Corpus Type]: new\n","\n","\n","    [FNAME_SENTIMENT_RAW]: [NONE]\n","\n","\n","\n","\n","INPUTS:\n","-------------------------------\n","\n","   [SUBDIR_SENTIMENTARCS]: /gdrive/MyDrive/cdh/sentiment_arcs\n","\n","\n","STEP 1: Clean Text\n","--------------------\n","\n","        [SUBDIR_TEXT_RAW]: ./text_raw/text_raw_novels_new_corpus2/\n","\n","      [SUBDIR_TEXT_CLEAN]: ./text_clean/text_clean_novels_new_corpus2/\n","\n","\n","STEP 2: Get Sentiments\n","--------------------\n","\n","   [SUBDIR_SENTIMENT_RAW]: ./sentiment_raw/sentiment_raw_novels_new_corpus2/\n","\n"," [SUBDIR_SENTIMENT_CLEAN]: ./sentiment_clean/sentiemnt_clean_novels_new_corpus2/\n","\n","\n","STEP 3: Smooth Time Series and Get Crux Points\n","--------------------\n","\n","  [SUBDIR_TIMESERIES_RAW]: ./sentiment_raw/sentiment_raw_novels_new_corpus2/\n","\n","[SUBDIR_TIMESERIES_CLEAN]: ./sentiment_clean/sentiemnt_clean_novels_new_corpus2/\n","\n","\n","\n","OUTPUTS:\n","-------------------------------\n","\n","          [SUBDIR_GRAPHS]: ./graphs/graphs_novels/\n","\n","            [SUBDIR_DATA]: ./data/data_novels\n","\n","           [SUBDIR_UTILS]: ./utils/\n","\n"]}],"source":["# Import SentimentArcs Utilities to define Directory Structure\n","#   based the Selected Corpus Genre, Type and Number\n","\n","!pwd \n","print('\\n')\n","\n","# from utils import sa_config # .sentiment_arcs_utils\n","from utils import sa_config\n","\n","print('Objects in sa_config()')\n","print(dir(sa_config))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","sa_config.get_subdirs(Corpus_Genre, Corpus_Type, Corpus_Number, 'none')\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1647421885185,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"Tx8j_3Y6qQna","outputId":"cc0531b3-0170-4bd0-c9ed-8e6da8548b12"},"outputs":[{"name":"stdout","output_type":"stream","text":["MIN_PARAG_LEN: 10\n","STOPWORDS_ADD_EN: ['a', 'the', 'an']\n","TEST_WORDS_LS: ['Love', 'Hate', 'bizarre', 'strange', 'furious', 'elated', 'curious', 'beserk', 'gambaro']\n","SLANG_DT: {'$': ' dollar ', '€': ' euro ', '4ao': 'for adults only', 'a.m': 'before midday', 'a3': 'anytime anywhere anyplace', 'aamof': 'as a matter of fact', 'acct': 'account', 'adih': 'another day in hell', 'afaic': 'as far as i am concerned', 'afaict': 'as far as i can tell', 'afaik': 'as far as i know', 'afair': 'as far as i remember', 'afk': 'away from keyboard', 'app': 'application', 'approx': 'approximately', 'apps': 'applications', 'asap': 'as soon as possible', 'asl': 'age, sex, location', 'atk': 'at the keyboard', 'ave.': 'avenue', 'aymm': 'are you my mother', 'ayor': 'at your own risk', 'b\u0026b': 'bed and breakfast', 'b+b': 'bed and breakfast', 'b.c': 'before christ', 'b2b': 'business to business', 'b2c': 'business to customer', 'b4': 'before', 'b4n': 'bye for now', 'b@u': 'back at you', 'bae': 'before anyone else', 'bak': 'back at keyboard', 'bbbg': 'bye bye be good', 'bbc': 'british broadcasting corporation', 'bbias': 'be back in a second', 'bbl': 'be back later', 'bbs': 'be back soon', 'be4': 'before', 'bfn': 'bye for now', 'blvd': 'boulevard', 'bout': 'about', 'brb': 'be right back', 'bros': 'brothers', 'brt': 'be right there', 'bsaaw': 'big smile and a wink', 'btw': 'by the way', 'bwl': 'bursting with laughter', 'c/o': 'care of', 'cet': 'central european time', 'cf': 'compare', 'cia': 'central intelligence agency', 'csl': 'can not stop laughing', 'cu': 'see you', 'cul8r': 'see you later', 'cv': 'curriculum vitae', 'cwot': 'complete waste of time', 'cya': 'see you', 'cyt': 'see you tomorrow', 'dae': 'does anyone else', 'dbmib': 'do not bother me i am busy', 'diy': 'do it yourself', 'dm': 'direct message', 'dwh': 'during work hours', 'e123': 'easy as one two three', 'eet': 'eastern european time', 'eg': 'example', 'embm': 'early morning business meeting', 'encl': 'enclosed', 'encl.': 'enclosed', 'etc': 'and so on', 'faq': 'frequently asked questions', 'fawc': 'for anyone who cares', 'fb': 'facebook', 'fc': 'fingers crossed', 'fig': 'figure', 'fimh': 'forever in my heart', 'ft.': 'feet', 'ft': 'featuring', 'ftl': 'for the loss', 'ftw': 'for the win', 'fwiw': 'for what it is worth', 'fyi': 'for your information', 'g9': 'genius', 'gahoy': 'get a hold of yourself', 'gal': 'get a life', 'gcse': 'general certificate of secondary education', 'gfn': 'gone for now', 'gg': 'good game', 'gl': 'good luck', 'glhf': 'good luck have fun', 'gmt': 'greenwich mean time', 'gmta': 'great minds think alike', 'gn': 'good night', 'g.o.a.t': 'greatest of all time', 'goat': 'greatest of all time', 'goi': 'get over it', 'gps': 'global positioning system', 'gr8': 'great', 'gratz': 'congratulations', 'gyal': 'girl', 'h\u0026c': 'hot and cold', 'hp': 'horsepower', 'hr': 'hour', 'hrh': 'his royal highness', 'ht': 'height', 'ibrb': 'i will be right back', 'ic': 'i see', 'icq': 'i seek you', 'icymi': 'in case you missed it', 'idc': 'i do not care', 'idgadf': 'i do not give a damn fuck', 'idgaf': 'i do not give a fuck', 'idk': 'i do not know', 'ie': 'that is', 'i.e': 'that is', 'ifyp': 'i feel your pain', 'IG': 'instagram', 'iirc': 'if i remember correctly', 'ilu': 'i love you', 'ily': 'i love you', 'imho': 'in my humble opinion', 'imo': 'in my opinion', 'imu': 'i miss you', 'iow': 'in other words', 'irl': 'in real life', 'j4f': 'just for fun', 'jic': 'just in case', 'jk': 'just kidding', 'jsyk': 'just so you know', 'l8r': 'later', 'lb': 'pound', 'lbs': 'pounds', 'ldr': 'long distance relationship', 'lmao': 'laugh my ass off', 'lmfao': 'laugh my fucking ass off', 'lol': 'laughing out loud', 'ltd': 'limited', 'ltns': 'long time no see', 'm8': 'mate', 'mf': 'motherfucker', 'mfs': 'motherfuckers', 'mfw': 'my face when', 'mofo': 'motherfucker', 'mph': 'miles per hour', 'mr': 'mister', 'mrw': 'my reaction when', 'ms': 'miss', 'mte': 'my thoughts exactly', 'nagi': 'not a good idea', 'nbc': 'national broadcasting company', 'nbd': 'not big deal', 'nfs': 'not for sale', 'ngl': 'not going to lie', 'nhs': 'national health service', 'nrn': 'no reply necessary', 'nsfl': 'not safe for life', 'nsfw': 'not safe for work', 'nth': 'nice to have', 'nvr': 'never', 'nyc': 'new york city', 'oc': 'original content', 'og': 'original', 'ohp': 'overhead projector', 'oic': 'oh i see', 'omdb': 'over my dead body', 'omg': 'oh my god', 'omw': 'on my way', 'p.a': 'per annum', 'p.m': 'after midday', 'pm': 'prime minister', 'poc': 'people of color', 'pov': 'point of view', 'pp': 'pages', 'ppl': 'people', 'prw': 'parents are watching', 'ps': 'postscript', 'pt': 'point', 'ptb': 'please text back', 'pto': 'please turn over', 'qpsa': 'what happens', 'ratchet': 'rude', 'rbtl': 'read between the lines', 'rlrt': 'real life retweet', 'rofl': 'rolling on the floor laughing', 'roflol': 'rolling on the floor laughing out loud', 'rotflmao': 'rolling on the floor laughing my ass off', 'rt': 'retweet', 'ruok': 'are you ok', 'sfw': 'safe for work', 'sk8': 'skate', 'smh': 'shake my head', 'sq': 'square', 'srsly': 'seriously', 'ssdd': 'same stuff different day', 'tbh': 'to be honest', 'tbs': 'tablespooful', 'tbsp': 'tablespooful', 'tfw': 'that feeling when', 'thks': 'thank you', 'tho': 'though', 'thx': 'thank you', 'tia': 'thanks in advance', 'til': 'today i learned', 'tl;dr': 'too long i did not read', 'tldr': 'too long i did not read', 'tmb': 'tweet me back', 'tntl': 'trying not to laugh', 'ttyl': 'talk to you later', 'u': 'you', 'u2': 'you too', 'u4e': 'yours for ever', 'utc': 'coordinated universal time', 'w/': 'with', 'w/o': 'without', 'w8': 'wait', 'wassup': 'what is up', 'wb': 'welcome back', 'wtf': 'what the fuck', 'wtg': 'way to go', 'wtpa': 'where the party at', 'wuf': 'where are you from', 'wuzup': 'what is up', 'wywh': 'wish you were here', 'yd': 'yard', 'ygtr': 'you got that right', 'ynk': 'you never know', 'zzz': 'sleeping bored and tired'}\n"]}],"source":["# Call SentimentArcs Utility to define Global Variables\n","\n","sa_config.set_globals()\n","\n","# Verify sample global var set\n","print(f'MIN_PARAG_LEN: {global_vars.MIN_PARAG_LEN}')\n","print(f'STOPWORDS_ADD_EN: {global_vars.STOPWORDS_ADD_EN}')\n","print(f'TEST_WORDS_LS: {global_vars.TEST_WORDS_LS}')\n","print(f'SLANG_DT: {global_vars.SLANG_DT}')"]},{"cell_type":"markdown","metadata":{"id":"mGoFJmeFkTxk"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1647421885185,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"kK8zKENjsyig"},"outputs":[],"source":["# Configure Jupyter\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"Ns5NwArZmush"},"source":["## Read YAML Configuration for Corpus and Models "]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":431,"status":"ok","timestamp":1647421885614,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"MKLqjedT7CDF"},"outputs":[{"name":"stdout","output_type":"stream","text":["config_matplotlib.py   global_constants.py    sa_config_bu.py\n","config_seaborn.py      global_vars.py\t      sa_config.py\n","file_utils.py\t       imdb50k_lemmas.csv     sentiment_analysis.py\n","get_fullpath.py        imdb50k_lemmas_df.csv  sentiment_arcs_config.py\n","get_model_families.py  imdb50k_stems.csv      set_globals.py\n","get_sentimentr.R       __init__.py\t      subdir_constants.py\n","get_sentiments.py      __pycache__\t      text_cleaners.py\n","get_subdirs.py\t       read_yaml.py\n"]}],"source":["!ls utils"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1647421885614,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"PLnMEb4e-cOE","outputId":"bc3c883e-1761-4f74-a76a-ef694eb3fdb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["import global_vars\n","\n","import yaml\n","\n","def read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number):\n","  '''\n","  Given a Corpus_Genre (e.g. novels), Corpus_Type (new or reference) and Corpus_Number (for new)\n","  Read and return the long-form titles for both Models and Corpus Texts\n","  '''\n","\n","  if Corpus_Type == 'new':\n","    path_info_yaml = f'text_raw/text_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}'\n","    file_yaml = f'text_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_info.yaml'\n","  elif Corpus_Type == 'reference':\n","    path_info_yaml = f'text_raw/text_raw_{Corpus_Genre}_{Corpus_Type}'\n","    file_yaml = f'text_raw_{Corpus_Genre}_{Corpus_Type}_info.yaml'\n","  else:\n","    print(f'ERROR: Illegal value for Corpus_Type = {Corpus_Type}')\n","    return\n","\n","  # Read Models Ensemble YAML Config Files \n","  with open(\"./config/models_ref_info.yaml\", \"r\") as stream:\n","    try:\n","      global_vars.models_titles_dt = yaml.safe_load(stream)\n","    except yaml.YAMLError as exc:\n","      print(exc)\n","\n","  # Read Corpus Texts YAML Config File\n","  print(f'YAML Directory: {path_info_yaml}')\n","  print(f'YAML File: {file_yaml}')\n","  with open(f\"{path_info_yaml}/{file_yaml}\", \"r\") as stream:\n","    try:\n","      global_vars.corpus_titles_dt = yaml.safe_load(stream)\n","    except yaml.YAMLError as exc:\n","      print(exc)\n","\n","  return"]}],"source":["!head -n 40 ./utils/read_yaml.py"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1647421935579,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"Oc2HAp5J_7xU","outputId":"b94c915a-810a-4fe1-fb8a-1a662826e158"},"outputs":[{"name":"stdout","output_type":"stream","text":["Variable                 Type             Data/Info\n","---------------------------------------------------\n","Corpus_Genre             str              novels\n","Corpus_Number            int              2\n","Corpus_Type              str              new\n","IN_COLAB                 bool             True\n","Image                    type             \u003cclass 'IPython.core.display.Image'\u003e\n","InteractiveShell         MetaHasTraits    \u003cclass 'IPython.core.inte\u003c...\u003eeshell.InteractiveShell'\u003e\n","PATH_TEXT_RAW_CORPUS     str              ./text_raw/text_raw_novels_new_corpus2/\n","PATH_UTILS               str              /gdrive/MyDrive/cdh/sentiment_arcs//utils\n","Path_to_SentimentArcs    str              /gdrive/MyDrive/cdh/sentiment_arcs/\n","SUBDIR_TEXT_RAW_CORPUS   str              text_raw_novels_new_corpus2/\n","display                  function         \u003cfunction display at 0x7f41da133290\u003e\n","drive                    module           \u003cmodule 'google.colab.dri\u003c...\u003es/google/colab/drive.py'\u003e\n","global_vars              module           \u003cmodule 'global_vars' fro\u003c...\u003es//utils/global_vars.py'\u003e\n","interactive              MetaHasTraits    \u003cclass 'ipywidgets.widget\u003c...\u003einteraction.interactive'\u003e\n","logging                  module           \u003cmodule 'logging' from '/\u003c...\u003e3.7/logging/__init__.py'\u003e\n","os                       module           \u003cmodule 'os' from '/usr/lib/python3.7/os.py'\u003e\n","sa_config                module           \u003cmodule 'utils.sa_config'\u003c...\u003earcs/utils/sa_config.py'\u003e\n","sys                      module           \u003cmodule 'sys' (built-in)\u003e\n","warnings                 module           \u003cmodule 'warnings' from '\u003c...\u003eb/python3.7/warnings.py'\u003e\n","widgets                  module           \u003cmodule 'ipywidgets.widge\u003c...\u003eets/widgets/__init__.py'\u003e\n"]}],"source":["%whos"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1197,"status":"ok","timestamp":1647421949946,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"mUveIcUOzYav","outputId":"f5004513-cced-4440-b10d-2b353ccbfd85"},"outputs":[{"name":"stdout","output_type":"stream","text":["Objects in read_yaml()\n","['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'global_vars', 'read_corpus_yaml', 'yaml']\n","\n","\n","YAML Directory: text_raw/text_raw_novels_new_corpus2\n","YAML File: text_raw_novels_new_corpus2_info.yaml\n","SentimentArcs Model Ensemble ------------------------------\n","\n","AutoGluon_Text\n","BERT_2IMDB\n","BERT_Dual_Coding\n","BERT_Multilingual\n","BERT_Yelp\n","CNN_DNN\n","Distilled_BERT\n","FLAML_AutoML\n","Fully_Connected_Network\n","HyperOpt_CNN_Flair_AutoML\n","LSTM_DNN\n","Logistic_Regression\n","Logistic_Regression_CV\n","Multilingual_CNN_Stanza_AutoML\n","Multinomial_Naive_Bayes\n","Pattern\n","Random_Forest\n","RoBERTa_Large_15DB\n","RoBERTa_XML_8Language\n","SentimentR_JockersRinker\n","SentimentR_Jockers\n","SentimentR_Bing\n","SentimentR_NRC\n","SentimentR_SentiWord\n","SentimentR_SenticNet\n","SentimentR_LMcD\n","SentimentR_SentimentR\n","PySentimentR_JockersRinker\n","PySentimentR_Huliu\n","PySentimentR_NRC\n","PySentimentR_SentiWord\n","PySentimentR_SenticNet\n","PySentimentR_LMcD\n","SyuzhetR_AFINN\n","SyuzhetR_Bing\n","SyuzhetR_NRC\n","SyuzhetR_SyuzhetR\n","T5_IMDB\n","TextBlob\n","VADER\n","AFINN\n","XGBoost\n","\n","\n","Corpus Texts ------------------------------\n","\n","tmorrison_songofsolomon\n","cliu_threebodyproblem\n","sking_doctorsleep\n","\n","\n","There are 42 Models in the SentimentArcs Ensemble above.\n","\n","\n","There are 3 Texts in the Corpus above.\n","\n","\n","\n"]}],"source":["# from utils import sa_config # .sentiment_arcs_utils\n","\n","import yaml\n","\n","from utils import read_yaml\n","\n","print('Objects in read_yaml()')\n","print(dir(read_yaml))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","read_yaml.read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number)\n","\n","print('SentimentArcs Model Ensemble ------------------------------\\n')\n","model_titles_ls = global_vars.models_titles_dt.keys()\n","print('\\n'.join(model_titles_ls))\n","\n","\n","print('\\n\\nCorpus Texts ------------------------------\\n')\n","corpus_titles_ls = global_vars.corpus_titles_dt.keys()\n","print('\\n'.join(corpus_titles_ls))\n","\n","\n","print(f'\\n\\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\\n')\n","print(f'\\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\\n')\n","print('\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"o5GqEXyRkPjj"},"source":["## Install Libraries"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4865,"status":"ok","timestamp":1647421970975,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"Tz5jGrDYi9Qe","outputId":"ec25d543-085f-41ce-8e6f-7e3624fc6028"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyreadr\n","  Downloading pyreadr-0.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (361 kB)\n","\u001b[K     |████████████████████████████████| 361 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: pandas\u003e=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyreadr) (1.3.5)\n","Requirement already satisfied: pytz\u003e=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas\u003e=1.2.0-\u003epyreadr) (2018.9)\n","Requirement already satisfied: numpy\u003e=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas\u003e=1.2.0-\u003epyreadr) (1.21.5)\n","Requirement already satisfied: python-dateutil\u003e=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas\u003e=1.2.0-\u003epyreadr) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003e=2.7.3-\u003epandas\u003e=1.2.0-\u003epyreadr) (1.15.0)\n","Installing collected packages: pyreadr\n","Successfully installed pyreadr-0.4.4\n"]}],"source":["# Library to Read R datafiles from within Python programs\n","\n","!pip install pyreadr"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19109,"status":"ok","timestamp":1647421990080,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"WFXzmfPQouNR","outputId":"e2484100-452d-4a0e-f96f-c70d9aff29fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Collecting spacy\n","  Downloading spacy-3.2.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n","\u001b[K     |████████████████████████████████| 6.0 MB 5.3 MB/s \n","\u001b[?25hCollecting pathy\u003e=0.3.5\n","  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n","Collecting catalogue\u003c2.1.0,\u003e=2.0.6\n","  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n","Collecting srsly\u003c3.0.0,\u003e=2.4.1\n","  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n","\u001b[K     |████████████████████████████████| 451 kB 55.9 MB/s \n","\u001b[?25hCollecting thinc\u003c8.1.0,\u003e=8.0.12\n","  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n","\u001b[K     |████████████████████████████████| 653 kB 60.2 MB/s \n","\u001b[?25hRequirement already satisfied: blis\u003c0.8.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Collecting langcodes\u003c4.0.0,\u003e=3.2.0\n","  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 58.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: typing-extensions\u003c4.0.0.0,\u003e=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n","Collecting pydantic!=1.8,!=1.8.1,\u003c1.9.0,\u003e=1.7.4\n","  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 44.7 MB/s \n","\u001b[?25hRequirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Collecting spacy-legacy\u003c3.1.0,\u003e=3.0.8\n","  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n","Collecting typer\u003c0.5.0,\u003e=0.3.0\n","  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n","Requirement already satisfied: wasabi\u003c1.1.0,\u003e=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n","Collecting spacy-loggers\u003c2.0.0,\u003e=1.0.0\n","  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: numpy\u003e=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue\u003c2.1.0,\u003e=2.0.6-\u003espacy) (3.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003espacy) (3.0.7)\n","Requirement already satisfied: smart-open\u003c6.0.0,\u003e=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy\u003e=0.3.5-\u003espacy) (5.2.1)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy) (2021.10.8)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy) (2.10)\n","Requirement already satisfied: click\u003c9.0.0,\u003e=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer\u003c0.5.0,\u003e=0.3.0-\u003espacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe\u003e=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-\u003espacy) (2.0.1)\n","Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 1.0.0\n","    Uninstalling catalogue-1.0.0:\n","      Successfully uninstalled catalogue-1.0.0\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 1.0.5\n","    Uninstalling srsly-1.0.5:\n","      Successfully uninstalled srsly-1.0.5\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.3 spacy-legacy-3.0.9 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.15 typer-0.4.0\n"]}],"source":["# Powerful Industry-Grade NLP Library\n","\n","!pip install -U spacy"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13002,"status":"ok","timestamp":1647422003073,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"sD_ZVbywJ4_e","outputId":"72a607b2-1640-4fe5-823e-257a36727cf5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting texthero\n","  Downloading texthero-1.1.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: scikit-learn\u003e=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.0.2)\n","Requirement already satisfied: pandas\u003e=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.3.5)\n","Collecting nltk\u003e=3.3\n","  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.21.5)\n","Requirement already satisfied: gensim\u003c4.0,\u003e=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n","Requirement already satisfied: plotly\u003e=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (5.5.0)\n","Collecting spacy\u003c3.0.0\n","  Downloading spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n","\u001b[K     |████████████████████████████████| 10.4 MB 44.9 MB/s \n","\u001b[?25hRequirement already satisfied: wordcloud\u003e=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n","Collecting unidecode\u003e=1.1.1\n","  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 64.6 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib\u003e=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n","Requirement already satisfied: tqdm\u003e=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.63.0)\n","Requirement already satisfied: scipy\u003e=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim\u003c4.0,\u003e=3.6.0-\u003etexthero) (1.4.1)\n","Requirement already satisfied: smart-open\u003e=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim\u003c4.0,\u003e=3.6.0-\u003etexthero) (5.2.1)\n","Requirement already satisfied: six\u003e=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim\u003c4.0,\u003e=3.6.0-\u003etexthero) (1.15.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,\u003e=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=3.1.0-\u003etexthero) (3.0.7)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=3.1.0-\u003etexthero) (0.11.0)\n","Requirement already satisfied: python-dateutil\u003e=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=3.1.0-\u003etexthero) (2.8.2)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=3.1.0-\u003etexthero) (1.3.2)\n","Collecting regex\u003e=2021.8.3\n","  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n","\u001b[K     |████████████████████████████████| 749 kB 48.2 MB/s \n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk\u003e=3.3-\u003etexthero) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk\u003e=3.3-\u003etexthero) (7.1.2)\n","Requirement already satisfied: pytz\u003e=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas\u003e=1.0.2-\u003etexthero) (2018.9)\n","Requirement already satisfied: tenacity\u003e=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly\u003e=4.2.0-\u003etexthero) (8.0.1)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn\u003e=0.22-\u003etexthero) (3.1.0)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0-\u003etexthero) (2.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0-\u003etexthero) (57.4.0)\n","Collecting srsly\u003c1.1.0,\u003e=1.0.2\n","  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n","\u001b[K     |████████████████████████████████| 184 kB 46.9 MB/s \n","\u001b[?25hRequirement already satisfied: plac\u003c1.2.0,\u003e=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0-\u003etexthero) (1.1.3)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0-\u003etexthero) (2.23.0)\n","Requirement already satisfied: blis\u003c0.8.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0-\u003etexthero) (0.4.1)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0-\u003etexthero) (3.0.6)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0-\u003etexthero) (1.0.6)\n","Collecting thinc\u003c7.5.0,\u003e=7.4.1\n","  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 58.4 MB/s \n","\u001b[?25hRequirement already satisfied: wasabi\u003c1.1.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.0.0-\u003etexthero) (0.9.0)\n","Collecting catalogue\u003c1.1.0,\u003e=0.0.7\n","  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n","Requirement already satisfied: importlib-metadata\u003e=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003c3.0.0-\u003etexthero) (4.11.2)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=0.20-\u003ecatalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003c3.0.0-\u003etexthero) (3.10.0.2)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=0.20-\u003ecatalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003c3.0.0-\u003etexthero) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.0.0-\u003etexthero) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.0.0-\u003etexthero) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.0.0-\u003etexthero) (3.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.0.0-\u003etexthero) (2021.10.8)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud\u003e=1.5.0-\u003etexthero) (7.1.2)\n","Installing collected packages: srsly, catalogue, thinc, regex, unidecode, spacy, nltk, texthero\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 2.4.2\n","    Uninstalling srsly-2.4.2:\n","      Successfully uninstalled srsly-2.4.2\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 2.0.6\n","    Uninstalling catalogue-2.0.6:\n","      Successfully uninstalled catalogue-2.0.6\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.0.15\n","    Uninstalling thinc-8.0.15:\n","      Successfully uninstalled thinc-8.0.15\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2019.12.20\n","    Uninstalling regex-2019.12.20:\n","      Successfully uninstalled regex-2019.12.20\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.2.3\n","    Uninstalling spacy-3.2.3:\n","      Successfully uninstalled spacy-3.2.3\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed catalogue-1.0.0 nltk-3.7 regex-2022.3.15 spacy-2.3.7 srsly-1.0.5 texthero-1.1.0 thinc-7.4.5 unidecode-1.3.4\n"]}],"source":["# NLP Library to Simply Cleaning Text\n","\n","!pip install texthero"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3130,"status":"ok","timestamp":1647422016328,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"T94fr2ymLFgV","outputId":"949635b2-6150-4f08-b90b-e3fd4d81fa7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pysbd\n","  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n","\u001b[?25l\r\u001b[K     |████▋                           | 10 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 20 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 30 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71 kB 3.6 MB/s \n","\u001b[?25hInstalling collected packages: pysbd\n","Successfully installed pysbd-0.3.4\n"]}],"source":["# Advanced Sentence Boundry Detection Pythn Library\n","#   for splitting raw text into grammatical sentences\n","#   (can be difficult due to common motifs like Mr., ..., ?!?, etc)\n","\n","!pip install pysbd"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3970,"status":"ok","timestamp":1647422022879,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"E3ev2lK-MU9E","outputId":"5801a09f-e537-4800-ca02-693cf48e80c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting contractions\n","  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n","Collecting textsearch\u003e=0.0.21\n","  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n","Collecting anyascii\n","  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n","\u001b[K     |████████████████████████████████| 284 kB 5.4 MB/s \n","\u001b[?25hCollecting pyahocorasick\n","  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[K     |████████████████████████████████| 106 kB 36.3 MB/s \n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.0 contractions-0.1.68 pyahocorasick-1.4.4 textsearch-0.0.21\n"]}],"source":["# Python Library to expand contractions to aid in Sentiment Analysis\n","#   (e.g. aren't -\u003e are not, can't -\u003e can not)\n","\n","!pip install contractions"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4809,"status":"ok","timestamp":1647422027679,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"kScSfHO1Q8Y-","outputId":"fee64f63-b22b-47a2-ab52-62da3ab576a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting emot\n","  Downloading emot-3.1-py3-none-any.whl (61 kB)\n","\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 40 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 13 kB/s \n","\u001b[?25hInstalling collected packages: emot\n","Successfully installed emot-3.1\n"]}],"source":["# Library for dealing with Emoticons (punctuation) and Emojis (icons)\n","\n","!pip install emot"]},{"cell_type":"markdown","metadata":{"id":"ajD8hCbzkStO"},"source":["## Load Libraries"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":434,"status":"ok","timestamp":1647422057588,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"oCRgJK2ri9Nx"},"outputs":[],"source":["# Core Python Libraries\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","\n","import re\n","import string\n","from datetime import datetime\n","import os\n","import sys\n","import glob\n","import json\n","from pathlib import Path\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":183,"status":"ok","timestamp":1647422059244,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"pify1umf6A8K"},"outputs":[],"source":["# More advanced Sentence Tokenizier Object from PySBD\n","from pysbd.utils import PySBDFactory"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1599,"status":"ok","timestamp":1647422062944,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"EEPQ67KrCO6f","outputId":"63a16d5a-8ca3-434c-d761-51b881bd71cb"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Simplier Sentence Tokenizer Object from NLTK\n","import nltk \n","from nltk.tokenize import sent_tokenize\n","\n","# Download required NLTK tokenizer data\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2462,"status":"ok","timestamp":1647422065401,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"NRYua8r7MP07","outputId":"f9e85f24-bbac-45ef-f1ba-3e33c63d0880"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-03-16 09:14:22,847 : INFO : 'pattern' package not found; tag filters are not available for English\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# Instantiate and Import Text Cleaning Ojects into Global Variable space\n","import texthero as hero\n","from texthero import preprocessing"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":918,"status":"ok","timestamp":1647422066315,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"7PBsG4WRMvrN","outputId":"ea113665-da90-4e10-aeb4-f5c908c92690"},"outputs":[{"data":{"text/plain":["{'flag': True,\n"," 'location': [[20, 23], [24, 27], [28, 33]],\n"," 'mean': ['Happy face smiley',\n","  'Frown, sad, andry or pouting',\n","  'Very very Happy face or smiley'],\n"," 'value': [':-)', ':-(', ':-)))']}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Expand contractions (e.g. can't -\u003e can not)\n","import contractions\n","\n","# Translate emoticons :0 and emoji icons to text\n","import emot \n","emot_obj = emot.core.emot() \n","\n","from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n","\n","# Test\n","text = \"I love python ☮ 🙂 ❤ :-) :-( :-)))\" \n","emot_obj.emoticons(text)"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":877,"status":"ok","timestamp":1647423345440,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"JPFS4MEm6MyF","outputId":"62e30a94-9215-46a4-9c9e-5e891024a25a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Token Attributes: \n"," token.text, token.pos_, token.tag_, token.dep_, token.lemma_\n","Apples                                          Apples      \n","and                                             and         \n","oranges                                         orange      \n","are                                             be          \n","similar                                         similar     \n",".                                               .           \n","Boots                                           Boots       \n","and                                             and         \n","hippos                                          hippo       \n","are         AUX         VBP                     be          \n","n't         PART        RB                      not         \n",".                                               .           \n","\n","Another Test:\n","\n","Apples      9297668116247400838           Apples      \n","and         2283656566040971221           and         \n","oranges     2208928596161743350           orange      \n","are         10382539506755952630          be          \n","similar     18166476740537071113          similar     \n",".           12646065887601541794          .           \n","Boots       18231591219755621867          Boots       \n","and         2283656566040971221           and         \n","hippos      6542994350242320795           hippo       \n","are         10382539506755952630          be          \n","n't         447765159362469301            not         \n",".           12646065887601541794          .           \n"]}],"source":["# Import spaCy, language model and setup minimal pipeline\n","\n","import spacy\n","\n","nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n","# nlp.max_length = 1027203\n","nlp.max_length = 2054406\n","nlp.add_pipe(nlp.create_pipe('sentencizer')) # https://stackoverflow.com/questions/51372724/how-to-speed-up-spacy-lemmatization\n","\n","# Test some edge cases, try to find examples that break spaCy\n","doc= nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n","print('\\n')\n","print(\"Token Attributes: \\n\", \"token.text, token.pos_, token.tag_, token.dep_, token.lemma_\")\n","for token in doc:\n","    # Print the text and the predicted part-of-speech tag\n","    print(\"{:\u003c12}{:\u003c12}{:\u003c12}{:\u003c12}{:\u003c12}\".format(token.text, token.pos_, token.tag_, token.dep_, token.lemma_))\n","\n","print('\\nAnother Test:\\n')\n","doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n","\n","for token in doc:\n","    print(\"{:\u003c12}{:\u003c30}{:\u003c12}\".format(token.text, token.lemma, token.lemma_))"]},{"cell_type":"markdown","metadata":{"id":"umZfB0YCqajW"},"source":["## Define/Customize Stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8_qLJBbtoOA","outputId":"f41f7bc4-d627-49ef-aa96-b5026bd4827c"},"outputs":[{"data":{"text/plain":["dict_keys(['$', '€', '4ao', 'a.m', 'a3', 'aamof', 'acct', 'adih', 'afaic', 'afaict', 'afaik', 'afair', 'afk', 'app', 'approx', 'apps', 'asap', 'asl', 'atk', 'ave.', 'aymm', 'ayor', 'b\u0026b', 'b+b', 'b.c', 'b2b', 'b2c', 'b4', 'b4n', 'b@u', 'bae', 'bak', 'bbbg', 'bbc', 'bbias', 'bbl', 'bbs', 'be4', 'bfn', 'blvd', 'bout', 'brb', 'bros', 'brt', 'bsaaw', 'btw', 'bwl', 'c/o', 'cet', 'cf', 'cia', 'csl', 'cu', 'cul8r', 'cv', 'cwot', 'cya', 'cyt', 'dae', 'dbmib', 'diy', 'dm', 'dwh', 'e123', 'eet', 'eg', 'embm', 'encl', 'encl.', 'etc', 'faq', 'fawc', 'fb', 'fc', 'fig', 'fimh', 'ft.', 'ft', 'ftl', 'ftw', 'fwiw', 'fyi', 'g9', 'gahoy', 'gal', 'gcse', 'gfn', 'gg', 'gl', 'glhf', 'gmt', 'gmta', 'gn', 'g.o.a.t', 'goat', 'goi', 'gps', 'gr8', 'gratz', 'gyal', 'h\u0026c', 'hp', 'hr', 'hrh', 'ht', 'ibrb', 'ic', 'icq', 'icymi', 'idc', 'idgadf', 'idgaf', 'idk', 'ie', 'i.e', 'ifyp', 'IG', 'iirc', 'ilu', 'ily', 'imho', 'imo', 'imu', 'iow', 'irl', 'j4f', 'jic', 'jk', 'jsyk', 'l8r', 'lb', 'lbs', 'ldr', 'lmao', 'lmfao', 'lol', 'ltd', 'ltns', 'm8', 'mf', 'mfs', 'mfw', 'mofo', 'mph', 'mr', 'mrw', 'ms', 'mte', 'nagi', 'nbc', 'nbd', 'nfs', 'ngl', 'nhs', 'nrn', 'nsfl', 'nsfw', 'nth', 'nvr', 'nyc', 'oc', 'og', 'ohp', 'oic', 'omdb', 'omg', 'omw', 'p.a', 'p.m', 'pm', 'poc', 'pov', 'pp', 'ppl', 'prw', 'ps', 'pt', 'ptb', 'pto', 'qpsa', 'ratchet', 'rbtl', 'rlrt', 'rofl', 'roflol', 'rotflmao', 'rt', 'ruok', 'sfw', 'sk8', 'smh', 'sq', 'srsly', 'ssdd', 'tbh', 'tbs', 'tbsp', 'tfw', 'thks', 'tho', 'thx', 'tia', 'til', 'tl;dr', 'tldr', 'tmb', 'tntl', 'ttyl', 'u', 'u2', 'u4e', 'utc', 'w/', 'w/o', 'w8', 'wassup', 'wb', 'wtf', 'wtg', 'wtpa', 'wuf', 'wuzup', 'wywh', 'yd', 'ygtr', 'ynk', 'zzz'])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Define Globals\n","\"\"\"\n","# Main data structure: Dictionary (key=text_name) of DataFrames (cols: text_raw, text_clean)\n","corpus_texts_dt = {}\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/get_globals.py'\n","\n","SLANG_DT.keys()\n","\"\"\";"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1647422109629,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"DWFx7-P-Ai5D","outputId":"d682ffd3-3bd2-4e70-fd9e-59f1696a9b63"},"outputs":[{"data":{"text/plain":["dict_keys(['$', '€', '4ao', 'a.m', 'a3', 'aamof', 'acct', 'adih', 'afaic', 'afaict', 'afaik', 'afair', 'afk', 'app', 'approx', 'apps', 'asap', 'asl', 'atk', 'ave.', 'aymm', 'ayor', 'b\u0026b', 'b+b', 'b.c', 'b2b', 'b2c', 'b4', 'b4n', 'b@u', 'bae', 'bak', 'bbbg', 'bbc', 'bbias', 'bbl', 'bbs', 'be4', 'bfn', 'blvd', 'bout', 'brb', 'bros', 'brt', 'bsaaw', 'btw', 'bwl', 'c/o', 'cet', 'cf', 'cia', 'csl', 'cu', 'cul8r', 'cv', 'cwot', 'cya', 'cyt', 'dae', 'dbmib', 'diy', 'dm', 'dwh', 'e123', 'eet', 'eg', 'embm', 'encl', 'encl.', 'etc', 'faq', 'fawc', 'fb', 'fc', 'fig', 'fimh', 'ft.', 'ft', 'ftl', 'ftw', 'fwiw', 'fyi', 'g9', 'gahoy', 'gal', 'gcse', 'gfn', 'gg', 'gl', 'glhf', 'gmt', 'gmta', 'gn', 'g.o.a.t', 'goat', 'goi', 'gps', 'gr8', 'gratz', 'gyal', 'h\u0026c', 'hp', 'hr', 'hrh', 'ht', 'ibrb', 'ic', 'icq', 'icymi', 'idc', 'idgadf', 'idgaf', 'idk', 'ie', 'i.e', 'ifyp', 'IG', 'iirc', 'ilu', 'ily', 'imho', 'imo', 'imu', 'iow', 'irl', 'j4f', 'jic', 'jk', 'jsyk', 'l8r', 'lb', 'lbs', 'ldr', 'lmao', 'lmfao', 'lol', 'ltd', 'ltns', 'm8', 'mf', 'mfs', 'mfw', 'mofo', 'mph', 'mr', 'mrw', 'ms', 'mte', 'nagi', 'nbc', 'nbd', 'nfs', 'ngl', 'nhs', 'nrn', 'nsfl', 'nsfw', 'nth', 'nvr', 'nyc', 'oc', 'og', 'ohp', 'oic', 'omdb', 'omg', 'omw', 'p.a', 'p.m', 'pm', 'poc', 'pov', 'pp', 'ppl', 'prw', 'ps', 'pt', 'ptb', 'pto', 'qpsa', 'ratchet', 'rbtl', 'rlrt', 'rofl', 'roflol', 'rotflmao', 'rt', 'ruok', 'sfw', 'sk8', 'smh', 'sq', 'srsly', 'ssdd', 'tbh', 'tbs', 'tbsp', 'tfw', 'thks', 'tho', 'thx', 'tia', 'til', 'tl;dr', 'tldr', 'tmb', 'tntl', 'ttyl', 'u', 'u2', 'u4e', 'utc', 'w/', 'w/o', 'w8', 'wassup', 'wb', 'wtf', 'wtg', 'wtpa', 'wuf', 'wuzup', 'wywh', 'yd', 'ygtr', 'ynk', 'zzz'])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["global_vars.SLANG_DT.keys()"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1647422165284,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"zqrk5TEuAzzq","outputId":"e748bcaf-845d-4dff-d618-b272350b536b"},"outputs":[{"data":{"text/plain":["['Corpus_Genre',\n"," 'Corpus_Number',\n"," 'Corpus_Type',\n"," 'FNAME_SENTIMENT_RAW',\n"," 'MIN_PARAG_LEN',\n"," 'MIN_SENT_LEN',\n"," 'NotebookModels',\n"," 'PATH_TEXT_RAW_CORPUS',\n"," 'SLANG_DT',\n"," 'STOPWORDS_ADD_EN',\n"," 'STOPWORDS_DEL_EN',\n"," 'SUBDIR_DATA',\n"," 'SUBDIR_GRAPHS',\n"," 'SUBDIR_SENTIMENTARCS',\n"," 'SUBDIR_SENTIMENT_CLEAN',\n"," 'SUBDIR_SENTIMENT_RAW',\n"," 'SUBDIR_TEXT_CLEAN',\n"," 'SUBDIR_TEXT_RAW',\n"," 'SUBDIR_TEXT_RAW_CORPUS',\n"," 'SUBDIR_TIMESERIES_CLEAN',\n"," 'SUBDIR_TIMESERIES_RAW',\n"," 'SUBDIR_UTILS',\n"," 'TEST_SENTENCES_LS',\n"," 'TEST_WORDS_LS',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__spec__',\n"," 'corpus_titles_dt',\n"," 'lexicons_dt',\n"," 'model_titles_dt',\n"," 'models_titles_dt']"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["dir(global_vars)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1647422132492,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"wdTV7rVOAsNO","outputId":"a6829fcd-4dc4-4e05-ca6f-64b8741b8341"},"outputs":[{"name":"stdout","output_type":"stream","text":["Variable                 Type             Data/Info\n","---------------------------------------------------\n","Corpus_Genre             str              novels\n","Corpus_Number            int              2\n","Corpus_Type              str              new\n","EMOTICONS_EMO            dict             n=221\n","IN_COLAB                 bool             True\n","Image                    type             \u003cclass 'IPython.core.display.Image'\u003e\n","InteractiveShell         MetaHasTraits    \u003cclass 'IPython.core.inte\u003c...\u003eeshell.InteractiveShell'\u003e\n","PATH_TEXT_RAW_CORPUS     str              ./text_raw/text_raw_novels_new_corpus2/\n","PATH_UTILS               str              /gdrive/MyDrive/cdh/sentiment_arcs//utils\n","Path                     type             \u003cclass 'pathlib.Path'\u003e\n","Path_to_SentimentArcs    str              /gdrive/MyDrive/cdh/sentiment_arcs/\n","PySBDFactory             type             \u003cclass 'pysbd.utils.PySBDFactory'\u003e\n","SUBDIR_TEXT_RAW_CORPUS   str              text_raw_novels_new_corpus2/\n","UNICODE_EMOJI            dict             n=3521\n","contractions             module           \u003cmodule 'contractions' fr\u003c...\u003eontractions/__init__.py'\u003e\n","corpus_titles_ls         dict_keys        dict_keys(['tmorrison_son\u003c...\u003em', 'sking_doctorsleep'])\n","datetime                 type             \u003cclass 'datetime.datetime'\u003e\n","deepcopy                 function         \u003cfunction deepcopy at 0x7f41db831830\u003e\n","display                  function         \u003cfunction display at 0x7f41da133290\u003e\n","doc                      Doc              Apples and oranges are si\u003c...\u003e Boots and hippos aren't.\n","drive                    module           \u003cmodule 'google.colab.dri\u003c...\u003es/google/colab/drive.py'\u003e\n","emot                     module           \u003cmodule 'emot' from '/usr\u003c...\u003eckages/emot/__init__.py'\u003e\n","emot_obj                 emot             \u003cemot.core.emot object at 0x7f419fa6a650\u003e\n","glob                     module           \u003cmodule 'glob' from '/usr/lib/python3.7/glob.py'\u003e\n","global_vars              module           \u003cmodule 'global_vars' fro\u003c...\u003es//utils/global_vars.py'\u003e\n","hero                     module           \u003cmodule 'texthero' from '\u003c...\u003ees/texthero/__init__.py'\u003e\n","interactive              MetaHasTraits    \u003cclass 'ipywidgets.widget\u003c...\u003einteraction.interactive'\u003e\n","json                     module           \u003cmodule 'json' from '/usr\u003c...\u003ehon3.7/json/__init__.py'\u003e\n","logging                  module           \u003cmodule 'logging' from '/\u003c...\u003e3.7/logging/__init__.py'\u003e\n","model_titles_ls          dict_keys        dict_keys(['AutoGluon_Tex\u003c...\u003eER', 'AFINN', 'XGBoost'])\n","nlp                      English          \u003cspacy.lang.en.English object at 0x7f419f739350\u003e\n","nltk                     module           \u003cmodule 'nltk' from '/usr\u003c...\u003eckages/nltk/__init__.py'\u003e\n","np                       module           \u003cmodule 'numpy' from '/us\u003c...\u003ekages/numpy/__init__.py'\u003e\n","os                       module           \u003cmodule 'os' from '/usr/lib/python3.7/os.py'\u003e\n","pd                       module           \u003cmodule 'pandas' from '/u\u003c...\u003eages/pandas/__init__.py'\u003e\n","plt                      module           \u003cmodule 'matplotlib.pyplo\u003c...\u003ees/matplotlib/pyplot.py'\u003e\n","preprocessing            module           \u003cmodule 'texthero.preproc\u003c...\u003exthero/preprocessing.py'\u003e\n","re                       module           \u003cmodule 're' from '/usr/lib/python3.7/re.py'\u003e\n","read_yaml                module           \u003cmodule 'utils.read_yaml'\u003c...\u003earcs/utils/read_yaml.py'\u003e\n","sa_config                module           \u003cmodule 'utils.sa_config'\u003c...\u003earcs/utils/sa_config.py'\u003e\n","sent_tokenize            function         \u003cfunction sent_tokenize at 0x7f41a8f93dd0\u003e\n","sns                      module           \u003cmodule 'seaborn' from '/\u003c...\u003eges/seaborn/__init__.py'\u003e\n","spacy                    module           \u003cmodule 'spacy' from '/us\u003c...\u003ekages/spacy/__init__.py'\u003e\n","string                   module           \u003cmodule 'string' from '/u\u003c...\u003elib/python3.7/string.py'\u003e\n","sys                      module           \u003cmodule 'sys' (built-in)\u003e\n","text                     str              I love python ☮ 🙂 ❤ :-) :-( :-)))\n","token                    Token            .\n","warnings                 module           \u003cmodule 'warnings' from '\u003c...\u003eb/python3.7/warnings.py'\u003e\n","widgets                  module           \u003cmodule 'ipywidgets.widge\u003c...\u003eets/widgets/__init__.py'\u003e\n","yaml                     module           \u003cmodule 'yaml' from '/usr\u003c...\u003eckages/yaml/__init__.py'\u003e\n"]}],"source":["%whos"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"executionInfo":{"elapsed":159,"status":"ok","timestamp":1647422189663,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"j1Lp4GLndZhY","outputId":"9ea8e675-e778-47a1-f0ac-ae47d59bd71f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"re,’s,becomes,or,towards,already,thence,before,next,their,noone,namely,none,he,alone,against,fifteen,very,ours,moreover,ourselves,really,nobody,various,for,him,twenty,to,else,into,from,whole,amongst,when,i,was,until,done,had,becoming,six,others,whereafter,only,former,all,hereby,more,whoever,her,any,did,once,otherwise,whereby,elsewhere,therefore,twelve,’d,move,rather,you,often,below,where,whither,his,who,sometimes,go,n’t,further,seemed,during,onto,whether,take,around,give,my,will,full,except,‘re,such,first,which,seem,anyhow,something,of,same,does,though,hence,’m,on,nothing,show,besides,thereafter,least,be,been,someone,due,they,have,with,should,are,n‘t,do,up,every,please,am,third,again,less,nevertheless,sixty,'ll,regarding,whenever,anywhere,while,can,five,‘ve,themselves,thereupon,latterly,among,another,it,per,here,many,ten,’re,well,wherever,upon,now,whereas,mine,what,’ll,sometime,one,still,in,those,must,doing,anything,seeming,himself,yourselves,several,and,neither,get,either,out,just,is,each,thereby,top,enough,'m,part,hereupon,your,perhaps,via,quite,forty,may,cannot,down,were,whence,keep,yours,beforehand,eight,both,latter,so,yet,mostly,everyone,the,across,side,but,bottom,‘ll,everything,'s,'ve,seems,might,too,therein,through,herself,empty,‘s,never,between,even,hereafter,put,than,as,off,would,however,always,itself,few,there,these,because,she,also,used,yourself,own,could,anyway,meanwhile,everywhere,’ve,somehow,me,say,indeed,three,since,unless,'re,ever,'d,‘m,along,how,why,we,together,amount,anyone,become,then,thus,‘d,us,a,much,without,nowhere,above,herein,no,most,call,front,thru,using,has,formerly,not,other,afterwards,myself,nor,four,at,made,its,an,beyond,our,two,about,hundred,toward,whose,back,see,being,became,within,make,whatever,wherein,throughout,hers,although,fifty,whereupon,serious,name,n't,after,that,over,behind,whom,last,this,by,somewhere,ca,almost,if,under,eleven,some,them,nine,beside\""]},"execution_count":28,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n","There are 326 default English Stopwords from spaCy\n","\n"]}],"source":["# Verify English Stopword List\n","\n","stopwords_spacy_en_ls = nlp.Defaults.stop_words\n","\n","','.join([x for x in stopwords_spacy_en_ls])\n","\n","stopwords_en_ls = stopwords_spacy_en_ls\n","\n","print(f'\\n\\nThere are {len(stopwords_spacy_en_ls)} default English Stopwords from spaCy\\n')"]},{"cell_type":"markdown","metadata":{"id":"ZD8Qe-H12p6j"},"source":["## (Optional) Customize Stopword List (add/del)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1647422346190,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"Q_438Act0jds","outputId":"1d0946e8-6d19-43cb-9875-44297e5b0ecd"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","There are 326 default English Stopwords from spaCy\n","\n","    Deleting these stopwords: {'a', 'the', 'yet', 'but', 'jimmy', 'an', 'dean'}\n","    Adding these stopwords: {'yet', 'the', 'an', 'a', 'but'}\n","\n","Final Count: 326 Stopwords\n"]}],"source":["# Customize Default SpaCy English Stopword List\n","\n","print(f'\\n\\nThere are {len(stopwords_spacy_en_ls)} default English Stopwords from spaCy\\n')\n","\n","# [CUSTOMIZE] Stopwords to ADD or DELETE from default spaCy English stopword list\n","LOCAL_STOPWORDS_DEL_EN = set(global_vars.STOPWORDS_DEL_EN).union(set(['a','an','the','but','yet']))\n","print(f'    Deleting these stopwords: {LOCAL_STOPWORDS_DEL_EN}')\n","LOCAL_STOPWORDS_ADD_EN = set(global_vars.STOPWORDS_ADD_EN).union(set(['a','an','the','but','yet']))\n","print(f'    Adding these stopwords: {LOCAL_STOPWORDS_ADD_EN}\\n')\n","\n","stopwords_en_ls = list(set(stopwords_spacy_en_ls).difference(set(LOCAL_STOPWORDS_DEL_EN)).union(set(LOCAL_STOPWORDS_ADD_EN)))\n","print(f'Final Count: {len(stopwords_en_ls)} Stopwords')"]},{"cell_type":"markdown","metadata":{"id":"EA1yTaY_9Qod"},"source":["## Setup Matplotlib Style"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":660,"status":"ok","timestamp":1647422360527,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"xmf7SFb7PnUi","outputId":"3cfebee4-0a10-4cfa-bd9d-b711bee92be7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"," New figure size:  (20, 10)\n","Matplotlib Configuration ------------------------------\n","\n","  (Uncomment to view)\n","\n","  Edit ./utils/config_matplotlib.py to change\n"]}],"source":["# Configure Matplotlib\n","\n","# View available styles\n","# plt.style.available\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/config_matplotlib.py'\n","\n","config_matplotlib()\n","\n","print('Matplotlib Configuration ------------------------------')\n","print('\\n  (Uncomment to view)')\n","# plt.rcParams.keys()\n","print('\\n  Edit ./utils/config_matplotlib.py to change')"]},{"cell_type":"markdown","metadata":{"id":"7dPPrZwyIIze"},"source":["## Setup Seaborn Style"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":298,"status":"ok","timestamp":1647422361408,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"hM3oRY-UOmzX","outputId":"9aac4d28-9159-4f51-d342-b68aad623312"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","Seaborn Configuration ------------------------------\n","\n"]}],"source":["# Configure Seaborn\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/config_seaborn.py'\n","\n","config_seaborn()\n","\n","print('Seaborn Configuration ------------------------------\\n')\n","# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')"]},{"cell_type":"markdown","metadata":{"id":"xBpIUgstnE62"},"source":["## **Utility Functions**"]},{"cell_type":"markdown","metadata":{"id":"JXG_G6um4ijG"},"source":["### Generate Convenient Data Lists"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278,"status":"ok","timestamp":1647422396805,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"TO08GFoGlP3y","outputId":"a0592950-4b74-4256-e05f-3a4a9fb7725b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dictionary: corpus_titles_dt\n"]},{"data":{"text/plain":["{'cliu_threebodyproblem': ['The Three Body Problem by Cixin Liu', 2008, 0],\n"," 'sking_doctorsleep': ['Doctor Sleep by Stphen King', 2013, 0],\n"," 'tmorrison_songofsolomon': ['Song of Solomon by Toni Morrison', 1977, 0]}"]},"execution_count":33,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","Corpus Texts:\n","  tmorrison_songofsolomon\n","  cliu_threebodyproblem\n","  sking_doctorsleep\n","\n","\n","\n","Natural Corpus Titles:\n","  Song of Solomon by Toni Morrison\n","  The Three Body Problem by Cixin Liu\n","  Doctor Sleep by Stphen King\n"]}],"source":["# Derive List of Texts in Corpus a)keys and b)full author and titles\n","\n","print('Dictionary: corpus_titles_dt')\n","global_vars.corpus_titles_dt\n","print('\\n')\n","\n","corpus_texts_ls = list(global_vars.corpus_titles_dt.keys())\n","print(f'\\nCorpus Texts:')\n","for akey in corpus_texts_ls:\n","  print(f'  {akey}')\n","print('\\n')\n","\n","print(f'\\nNatural Corpus Titles:')\n","corpus_titles_ls = [x[0] for x in list(global_vars.corpus_titles_dt.values())]\n","for akey in corpus_titles_ls:\n","  print(f'  {akey}')\n"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":202,"status":"ok","timestamp":1647423115149,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"rQNlQr4_Ckb1","outputId":"ecd4f795-ca00-4652-9d03-09d713f9cb0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","There are 12 Lexicon Models\n","  Lexicon Model #0: sentimentr_sentimentr\n","  Lexicon Model #1: pysentimentr_jockersrinker\n","  Lexicon Model #2: pysentimentr_huliu\n","  Lexicon Model #3: pysentimentr_nrc\n","  Lexicon Model #4: pysentimentr_sentiword\n","  Lexicon Model #5: pysentimentr_senticnet\n","  Lexicon Model #6: pysentimentr_lmcd\n","  Lexicon Model #7: syuzhetr_afinn\n","  Lexicon Model #8: syuzhetr_bing\n","  Lexicon Model #9: syuzhetr_nrc\n","  Lexicon Model #10: syuzhetr_syuzhetr\n","  Lexicon Model #11: afinn\n","\n","There are 9 Heuristic Models\n","  Heuristic Model #0: pattern\n","  Heuristic Model #1: sentimentr_jockersrinker\n","  Heuristic Model #2: sentimentr_jockers\n","  Heuristic Model #3: sentimentr_bing\n","  Heuristic Model #4: sentimentr_nrc\n","  Heuristic Model #5: sentimentr_sentiword\n","  Heuristic Model #6: sentimentr_senticnet\n","  Heuristic Model #7: sentimentr_lmcd\n","  Heuristic Model #8: vader\n","\n","There are 8 Traditional ML Models\n","  Traditional ML Model #0: autogluon\n","  Traditional ML Model #1: flaml\n","  Traditional ML Model #2: logreg\n","  Traditional ML Model #3: logreg_cv\n","  Traditional ML Model #4: multinb\n","  Traditional ML Model #5: rf\n","  Traditional ML Model #6: textblob\n","  Traditional ML Model #7: xgb\n","\n","There are 5 DNN Models\n","  DNN Model #0: cnn\n","  DNN Model #1: fcn\n","  DNN Model #2: flair\n","  DNN Model #3: lstm\n","  DNN Model #4: stanza\n","\n","There are 8 Transformer Models\n","  Transformer Model #0: imdb2way\n","  Transformer Model #1: hinglish\n","  Transformer Model #2: nlptown\n","  Transformer Model #3: yelp\n","  Transformer Model #4: huggingface\n","  Transformer Model #5: roberta15lg\n","  Transformer Model #6: robertaxml8lang\n","  Transformer Model #7: t5imdb50k\n","\n","There are 5 Total Models:\n","  Model # 0: lexicon\n","  Model # 1: heuristic\n","  Model # 2: ml\n","  Model # 3: dnn\n","  Model # 4: transformer\n","\n","There are 5 Total Models (+1 for Ensemble Mean)\n","\n","Test: Lexicon Family of Models:\n"]},{"data":{"text/plain":["['sentimentr_sentimentr',\n"," 'pysentimentr_jockersrinker',\n"," 'pysentimentr_huliu',\n"," 'pysentimentr_nrc',\n"," 'pysentimentr_sentiword',\n"," 'pysentimentr_senticnet',\n"," 'pysentimentr_lmcd',\n"," 'syuzhetr_afinn',\n"," 'syuzhetr_bing',\n"," 'syuzhetr_nrc',\n"," 'syuzhetr_syuzhetr',\n"," 'afinn']"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["# Get Model Families of Ensemble\n","\n","from utils.get_model_families import get_ensemble_model_famalies\n","\n","global_vars.models_ensemble_dt = get_ensemble_model_famalies(global_vars.models_titles_dt)\n","\n","print('\\nTest: Lexicon Family of Models:')\n","global_vars.models_ensemble_dt['lexicon']"]},{"cell_type":"markdown","metadata":{"id":"pjQBAoLjOzDO"},"source":["### Text Cleaning "]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"executionInfo":{"elapsed":384,"status":"error","timestamp":1647425040116,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"J2DLzn1TJ12C","outputId":"31057388-8910-4463-e35d-12dbaa5b20af"},"outputs":[{"ename":"SyntaxError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"/gdrive/MyDrive/cdh/sentiment_arcs/utils/text_cleaners.py\"\u001b[0;36m, line \u001b[0;32m49\u001b[0m\n\u001b[0;31m    global\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]},{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/gdrive/MyDrive/cdh/sentiment_arcs/utils/text_cleaners.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Test: text2lemmas()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'text2lemmas'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_suite_ls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 22\u001b[0;31m   \u001b[0mtext2lemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I am going to start studying more often and working harder.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/gdrive/MyDrive/cdh/sentiment_arcs/utils/text_cleaners.py\u001b[0m in \u001b[0;36mtext2lemmas\u001b[0;34m(comment, lowercase, remove_stopwords)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"]}],"source":["# Test Text Cleaning Functions\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/text_cleaners.py'\n","\n","test_suite_ls = ['text2lemmas',\n","                 'text_str2sents',\n","                 'textfile2df',\n","                 'emojis2text',\n","                 'all_emos2text',\n","                 'expand_slang',\n","                 'clean_text',\n","                 'lemma_pipe'\n","                 ]\n","\n","# test_suite_ls = []\n","\n","# Test: text2lemmas()\n","if 'text2lemmas' in test_suite_ls:\n","  text2lemmas('I am going to start studying more often and working harder.', lowercase=True, remove_stopwords=False)\n","  print('\\n')\n","\n","# Test: text_str2sents()\n","if 'text_str2sents' in test_suite_ls:\n","  text_str2sents('Hello. You are a great dude! WTF?\\n\\n You are a goat. What is a goat?!? A big lazy GOAT... No way-', pysbd_only=False) # !?! Dr. and Mrs. Elipses...', pysbd_only=True)\n","  print('\\n')\n","\n","# Test: textfile2df()\n","if 'textfile2df' in test_suite_ls:\n","  # ???\n","  print('\\n')\n","\n","# Test: emojis2text()\n","if 'emojis2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling of making a sale 😎, The feeling of actually ;) fulfilling orders 😒\"\n","  test_str = emojis2text(test_str)\n","  print(f'test_str: [{test_str}]')\n","  print('\\n')\n","\n","# Test: all_emos2text()\n","if 'all_emos2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling :o of making a sale 😎, The feeling :( of actually ;) fulfilling orders 😒\"\n","  all_emos2text(test_str)\n","  print('\\n')\n","\n","# Test: expand_slang():\n","if 'expand_slang' in test_suite_ls:\n","  expand_slang('idk LOL you suck!')\n","  print('\\n')\n","\n","# Test: clean_text()\n","if 'clean_text' in test_suite_ls:\n","  test_df = pd.DataFrame({'text_dirty':['The RAin in SPain','WTF?!?! Do you KnoW...']})\n","  clean_text(test_df, 'text_dirty', text_type='formal')\n","  print('\\n')\n","\n","# Test: lemma_pipe()\n","if 'lemma_pipe' in test_suite_ls:\n","  print('\\nTest #1:\\n')\n","  test_ls = ['I am running late for a meetings with all the many people.',\n","            'What time is it when you fall down running away from a growing problem?',\n","            \"You've got to be kidding me - you're joking right?\"]\n","  lemma_pipe(test_ls)\n","  print('\\nTest #2:\\n')\n","  texts = pd.Series([\"I won't go and you can't make me.\", \"Billy is running really quickly and with great haste.\", \"Eating freshly caught seafood.\"])\n","  for doc in nlp.pipe(texts):\n","    print([tok.lemma_ for tok in doc])\n","  print('\\nTest #3:\\n')\n","  lemma_pipe(texts)\n"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":168,"status":"ok","timestamp":1647423129788,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"qpchjKtfy2H4","outputId":"5a9c4165-d81f-4b8a-c12a-c7292a110fe4"},"outputs":[{"data":{"text/plain":["[\u003cfunction texthero.preprocessing.fillna\u003e,\n"," \u003cfunction texthero.preprocessing.lowercase\u003e,\n"," \u003cfunction texthero.preprocessing.remove_digits\u003e,\n"," \u003cfunction texthero.preprocessing.remove_punctuation\u003e,\n"," \u003cfunction texthero.preprocessing.remove_diacritics\u003e,\n"," \u003cfunction texthero.preprocessing.remove_stopwords\u003e,\n"," \u003cfunction texthero.preprocessing.remove_whitespace\u003e]"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["# [VERIFY]: Texthero preprocessing pipeline\n","\n","hero.preprocessing.get_default_pipeline()\n","\n","\n","\n","# Create Default and Custom Stemming TextHero pipeline\n","\n","# Create a custom cleaning pipeline\n","def_pipeline = [preprocessing.fillna\n","                , preprocessing.lowercase\n","                , preprocessing.remove_digits\n","                , preprocessing.remove_punctuation\n","                , preprocessing.remove_diacritics\n","                # , preprocessing.remove_stopwords\n","                , preprocessing.remove_whitespace]\n","\n","# Create a custom cleaning pipeline\n","stem_pipeline = [preprocessing.fillna\n","                , preprocessing.lowercase\n","                , preprocessing.remove_digits\n","                , preprocessing.remove_punctuation\n","                , preprocessing.remove_diacritics\n","                , preprocessing.remove_stopwords\n","                , preprocessing.remove_whitespace\n","                , preprocessing.stem]\n","                   \n","# Test: pass the custom_pipeline to the pipeline argument\n","# df['clean_title'] = hero.clean(df['title'], pipeline = custom_pipeline)df.head()"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":409},"executionInfo":{"elapsed":161,"status":"error","timestamp":1647423837462,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"FNiSgwZqElpJ","outputId":"f6d35369-a045-4ebc-ebb1-1a27c46cf286"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n# Test: text2lemmas()\\nif 'text2lemmas' in test_suite_ls:\\n  text2lemmas('I am going to start studying more often and working harder.', lowercase=True, remove_stopwords=False)\\n  print('\\n')\\n\""]},"execution_count":50,"metadata":{},"output_type":"execute_result"},{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/gdrive/MyDrive/cdh/sentiment_arcs/utils/config_seaborn.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Test: text_str2sents()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'text_str2sents'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_suite_ls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 27\u001b[0;31m   \u001b[0mtext_str2sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hello. You are a great dude! WTF?\\n\\n You are a goat. What is a goat?!? A big lazy GOAT... No way-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpysbd_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# !?! Dr. and Mrs. Elipses...', pysbd_only=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/gdrive/MyDrive/cdh/sentiment_arcs/utils/text_cleaners.py\u001b[0m in \u001b[0;36mtext_str2sents\u001b[0;34m(text_str, pysbd_only)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mparags_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 28\u001b[0;31m   \u001b[0msents_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mpysbd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPySBDFactory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"]}],"source":["# Test Text Cleaning Functions\n","\n","from utils.text_cleaners import text2lemmas, text_str2sents, emojis2text, expand_slang, clean_text, lemma_pipe\n","\n","test_suite_ls = ['text2lemmas',\n","                 'text_str2sents',\n","                 'textfile2df',\n","                 'emojis2text',\n","                 'all_emos2text',\n","                 'expand_slang',\n","                 'clean_text',\n","                 'lemma_pipe'\n","                 ]\n","\n","# Comment out this line to active tests above\n","# test_suite_ls = []\n","\n","\"\"\"\n","# Test: text2lemmas()\n","if 'text2lemmas' in test_suite_ls:\n","  text2lemmas('I am going to start studying more often and working harder.', lowercase=True, remove_stopwords=False)\n","  print('\\n')\n","\"\"\"\n","\n","# Test: text_str2sents()\n","if 'text_str2sents' in test_suite_ls:\n","  text_str2sents('Hello. You are a great dude! WTF?\\n\\n You are a goat. What is a goat?!? A big lazy GOAT... No way-', pysbd_only=False) # !?! Dr. and Mrs. Elipses...', pysbd_only=True)\n","  print('\\n')\n","\n","# Test: textfile2df()\n","if 'textfile2df' in test_suite_ls:\n","  # ???\n","  print('\\n')\n","\n","# Test: emojis2text()\n","if 'emojis2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling of making a sale 😎, The feeling of actually ;) fulfilling orders 😒\"\n","  test_str = emojis2text(test_str)\n","  print(f'test_str: [{test_str}]')\n","  print('\\n')\n","\n","# Test: all_emos2text()\n","if 'all_emos2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling :o of making a sale 😎, The feeling :( of actually ;) fulfilling orders 😒\"\n","  all_emos2text(test_str)\n","  print('\\n')\n","\n","# Test: expand_slang():\n","if 'expand_slang' in test_suite_ls:\n","  expand_slang('idk LOL you suck!')\n","  print('\\n')\n","\n","# Test: clean_text()\n","if 'clean_text' in test_suite_ls:\n","  test_df = pd.DataFrame({'text_dirty':['The RAin in SPain','WTF?!?! Do you KnoW...']})\n","  clean_text(test_df, 'text_dirty', text_type='formal')\n","  print('\\n')\n","\"\"\"\n","# Test: lemma_pipe()\n","if 'lemma_pipe' in test_suite_ls:\n","  print('\\nTest #1:\\n')\n","  test_ls = ['I am running late for a meetings with all the many people.',\n","            'What time is it when you fall down running away from a growing problem?',\n","            \"You've got to be kidding me - you're joking right?\"]\n","  lemma_pipe(test_ls)\n","  print('\\nTest #2:\\n')\n","  texts = pd.Series([\"I won't go and you can't make me.\", \"Billy is running really quickly and with great haste.\", \"Eating freshly caught seafood.\"])\n","  for doc in nlp.pipe(texts):\n","    print([tok.lemma_ for tok in doc])\n","  print('\\nTest #3:\\n')\n","  lemma_pipe(texts)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"WlfujTpEKhCp"},"source":["### File Functions"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":376,"status":"ok","timestamp":1647423772270,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"yOX-fpiuApL4"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","from utils.file_utils import *\n","\n","# %run -i './utils/file_utils.py'\n","\n","# TODO: Not used? Delete?\n","# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)"]},{"cell_type":"markdown","metadata":{"id":"Ilz5X9AEbP8r"},"source":["# **[STEP 2] Read in Corpus and Clean**"]},{"cell_type":"markdown","metadata":{"id":"1a-1wyeiGt4Z"},"source":["## Create List of Raw Textfiles"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1647423904245,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"Tcl8BtfGfvgZ","outputId":"c73f36e0-1eab-42d8-a5fc-754c37ebdd0a"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/gdrive/MyDrive/cdh/sentiment_arcs'"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["global_vars.SUBDIR_SENTIMENTARCS"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1647423941399,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"cHXNIMtPfZ1j","outputId":"44df9b84-5e9c-47bd-c731-14eb0e65318b"},"outputs":[{"name":"stdout","output_type":"stream","text":["path_text_raw: ./text_raw/text_raw_novels_new_corpus2\n","\n","Full Path to Corpus text_raw: ./text_raw/text_raw_novels_new_corpus2/\n"]}],"source":["# TODO: Temp fix until print(f'Original: {SUBDIR_TEXT_RAW}\\n')\n","path_text_raw = './' + '/'.join(global_vars.SUBDIR_TEXT_RAW.split('/')[1:-1])\n","print(f'path_text_raw: {path_text_raw}\\n')\n","# SUBDIR_TEXT_RAW = path_text_raw + '/'\n","print(f'Full Path to Corpus text_raw: ./text_raw/{global_vars.SUBDIR_TEXT_RAW_CORPUS}')"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":400,"status":"ok","timestamp":1647423958120,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"eyDzL7zJfYZf","outputId":"04d01fa9-2bc0-4e01-f6e0-19a7690ed0e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/gdrive/MyDrive/cdh/sentiment_arcs\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1647424039821,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"iXN4pdZReL4Y","outputId":"71c10d5a-3bdc-44a2-a28c-2b7af1460e01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Corpus_Genre: novels\n","Corpus_Type: new\n","\n","Corpus Subdir: ./text_raw/text_raw_novels_new_corpus2/\n","\n","texts_raw_root_ls:\n","  ['sking_doctorsleep', 'cliu_threebodyproblem', 'tmorrison_songsolomon']\n","\n","sking_doctorsleep: \n","cliu_threebodyproblem: \n","tmorrison_songsolomon: \n","\n","There are 3 Texts defined in SentmentArcs [corpus_dt] and found in the subdir: [SUBDIR_TEXT_RAW]\n"]}],"source":["# Get a list of all the Textfile filename roots in Subdir text_raw\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n","\n","print(f'Corpus_Genre: {global_vars.Corpus_Genre}')\n","print(f'Corpus_Type: {global_vars.Corpus_Type}\\n')\n","\n","# Build path to Corpus Subdir\n","# TODO: Temp fix until print(f'Original: {SUBDIR_TEXT_RAW}\\n')\n","# path_text_raw = './' + '/'.join(SUBDIR_TEXT_RAW.split('/')[1:-1]) + '/' + SUBDIR_TEXT_RAW_CORPUS\n","path_text_raw = './text_raw/' + global_vars.SUBDIR_TEXT_RAW_CORPUS\n","print(f'Corpus Subdir: {path_text_raw}')\n","\n","# Create a List (preprocessed_ls) of all preprocessed text files\n","try:\n","  # texts_raw_ls = glob.glob(f'{SUBDIR_TEXT_RAW}*.txt')\n","  texts_raw_root_ls = glob.glob(f'{path_text_raw}/*.txt')\n","  texts_raw_root_ls = [x.split('/')[-1] for x in texts_raw_root_ls]\n","  texts_raw_root_ls = [x.split('.')[0] for x in texts_raw_root_ls]\n","except IndexError:\n","  raise RuntimeError('No *.txt files found')\n","\n","print(f'\\ntexts_raw_root_ls:\\n  {texts_raw_root_ls}\\n')\n","\n","text_ct = 0\n","for afile_root in texts_raw_root_ls:\n","  # file_root = file_fullpath.split('/')[-1].split('.')[0]\n","  text_ct += 1\n","  print(f'{afile_root}: ') # {corpus_titles_dt[afile_root]}')\n","\n","print(f'\\nThere are {text_ct} Texts defined in SentmentArcs [corpus_dt] and found in the subdir: [SUBDIR_TEXT_RAW]')"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":891,"status":"ok","timestamp":1647424046837,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"f_gr9Y2ShTDk","outputId":"bbd573e1-b71f-4cb7-b453-3f6c746a10c0"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'./text_raw/text_raw_novels_new_corpus2/'"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["path_text_raw"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1647424048411,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"lKpausmwhXJU","outputId":"af7389a9-d555-46ab-becf-c1721370e06d"},"outputs":[{"name":"stdout","output_type":"stream","text":["cliu_threebodyproblem.txt  text_raw_novels_new_corpus2_info.yaml\n","sking_doctorsleep.txt\t   tmorrison_songsolomon.txt\n"]}],"source":["!ls $path_text_raw"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1647424050094,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"EvqS1TQthfxF","outputId":"52f0a1e3-6f0c-4aac-812d-1862298ff94d"},"outputs":[{"data":{"text/plain":["['./text_raw/text_raw_novels_new_corpus2/sking_doctorsleep.txt',\n"," './text_raw/text_raw_novels_new_corpus2/cliu_threebodyproblem.txt',\n"," './text_raw/text_raw_novels_new_corpus2/tmorrison_songsolomon.txt']"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["glob.glob(f'{path_text_raw}/*.txt')"]},{"cell_type":"markdown","metadata":{"id":"KkXBipRrGoCQ"},"source":["## Read and Segment into Sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ug8XpPsMJfLE"},"outputs":[],"source":["%run -i "]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":222,"status":"error","timestamp":1647424367247,"user":{"displayName":"Jon Chun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14430240466678867548"},"user_tz":240},"id":"eq50LyAMHKYX","outputId":"8bec18e9-7267-48c0-9bf0-81071b3cd253"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-03-16 09:52:47,024 : ERROR : Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","2022-03-16 09:52:47,032 : INFO : \n","Unfortunately, your original traceback can not be constructed.\n","\n"]},{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"\u003cipython-input-64-3dbd978afc86\u003e\", line 1, in \u003cmodule\u003e\n","    get_ipython().run_cell_magic('time', '', '\\n# Read all Corpus Textfiles and Segment each into Sentences\\n\\n# NOTE: 3m30s Entire Corpus of 25 \\n#       7m30s Ref Corpus 32 Novels\\n#       7m24s Ref Corpus 32 Novels\\n#       1m00s New Corpus 2 Novels\\n\\n# Read all novel files into a Dictionary of DataFrames\\n#   Dict.keys() are novel names\\n#   Dict.values() are DataFrames with one row per Sentence\\n\\n# Continue here ONLY if last cell completed WITHOUT ERROR\\n\\n# anovel_df = pd.DataFrame()\\n\\nfor i, file_root in enumerate(corpus_titles_ls):\\n  file_fullpath = f\\'{global_vars.SUBDIR_TEXT_RAW}{file_root}.txt\\'\\n  # print(f\\'Processing Novel #{i}: {file_fullpath}\\') # {file_root}\\')\\n  # fullpath_str = novels_subdir + asubdir + \\'/\\' + asubdir + \\'.txt\\'\\n  # print(f\"  Size: {os.path.getsize(file_fullpath)}\")\\n\\n  corpus_texts_dt[file_root] = textfile2df(file_fullpath)\\n  \\n# corpus_dt.keys()\\n\\n# Verify First Text is Segmented into text_raw Sentences\\nprint(\\'\\\\n\\\\n\\')\\ncorpus_texts_dt[corpus_titles_ls[0]].head()')\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2117, in run_cell_magic\n","    result = fn(magic_arg_s, cell)\n","  File \"\u003cdecorator-gen-53\u003e\", line 2, in time\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\", line 188, in \u003clambda\u003e\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\", line 1193, in time\n","    exec(code, glob, local_ns)\n","  File \"\u003ctimed exec\u003e\", line 23, in \u003cmodule\u003e\n","  File \"/gdrive/MyDrive/cdh/sentiment_arcs/utils/file_utils.py\", line 85, in textfile2df\n","    textfile_df = pd.DataFrame()\n","NameError: name 'pd' is not defined\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'NameError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]},{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}],"source":["%%time\n","\n","# Read all Corpus Textfiles and Segment each into Sentences\n","\n","# NOTE: 3m30s Entire Corpus of 25 \n","#       7m30s Ref Corpus 32 Novels\n","#       7m24s Ref Corpus 32 Novels\n","#       1m00s New Corpus 2 Novels\n","\n","# Read all novel files into a Dictionary of DataFrames\n","#   Dict.keys() are novel names\n","#   Dict.values() are DataFrames with one row per Sentence\n","\n","# Continue here ONLY if last cell completed WITHOUT ERROR\n","\n","# anovel_df = pd.DataFrame()\n","\n","for i, file_root in enumerate(corpus_titles_ls):\n","  file_fullpath = f'{global_vars.SUBDIR_TEXT_RAW}{file_root}.txt'\n","  # print(f'Processing Novel #{i}: {file_fullpath}') # {file_root}')\n","  # fullpath_str = novels_subdir + asubdir + '/' + asubdir + '.txt'\n","  # print(f\"  Size: {os.path.getsize(file_fullpath)}\")\n","\n","  corpus_texts_dt[file_root] = textfile2df(file_fullpath)\n","  \n","# corpus_dt.keys()\n","\n","# Verify First Text is Segmented into text_raw Sentences\n","print('\\n\\n')\n","corpus_texts_dt[corpus_titles_ls[0]].head()\n"]},{"cell_type":"markdown","metadata":{"id":"tw-Ll-fdI_yb"},"source":["## Clean Sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPrpL1wyNPva","outputId":"60b0d57e-4a08-49c0-eb53-911f5dd5eb82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing Novel #0: scollins_thehungergames1...\n","  shape: (9021, 2)\n","Processing Novel #1: cmieville_thecityandthecity...\n","  shape: (10125, 2)\n","CPU times: user 6.92 s, sys: 140 ms, total: 7.07 s\n","Wall time: 7.75 s\n"]}],"source":["%%time\n","\n","# NOTE: (no stem) 4m09s (24 Novels)\n","#       (w/ stem) 4m24s (24 Novels)\n","\n","i = 0\n","\n","for key_novel, atext_df in corpus_texts_dt.items():\n","\n","  print(f'Processing Novel #{i}: {key_novel}...')\n","\n","  atext_df['text_clean'] = clean_text(atext_df, 'text_raw', text_type='formal')\n","  atext_df['text_clean'] = lemma_pipe(atext_df['text_clean'])\n","  atext_df['text_clean'] = atext_df['text_clean'].astype('string')\n","\n","  # TODO: Fill in all blank 'text_clean' rows with filler semaphore\n","  atext_df.text_clean = atext_df.text_clean.fillna('empty_placeholder')\n","\n","  atext_df.head(2)\n","\n","  print(f'  shape: {atext_df.shape}')\n","\n","  i += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":833},"id":"RXuwwg2_XgZu","outputId":"3638076f-0e3e-45b9-8b77-1514658c763a"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-fc3c797f-8bf4-4fee-b0dc-473aad5fa68d\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003etext_raw\u003c/th\u003e\n","      \u003cth\u003etext_clean\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e\"THE TRIBUTES\"\u003c/td\u003e\n","      \u003ctd\u003ethe tribute\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eWhen I wake up, the other side of the bed is c...\u003c/td\u003e\n","      \u003ctd\u003ewhen i wake up the other side of the bed be cold\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003eMy fingers stretch out, seeking Prims warmth b...\u003c/td\u003e\n","      \u003ctd\u003emy finger stretch out seek prims warmth but fi...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003eShe must have had bad dreams and climbed in wi...\u003c/td\u003e\n","      \u003ctd\u003eshe must have have bad dream and climb in with...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003eOf course, she did.\u003c/td\u003e\n","      \u003ctd\u003eof course she do\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e5\u003c/th\u003e\n","      \u003ctd\u003eThis is the day of the reaping.\u003c/td\u003e\n","      \u003ctd\u003ethis be the day of the reap\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e6\u003c/th\u003e\n","      \u003ctd\u003eI prop myself up on one elbow.\u003c/td\u003e\n","      \u003ctd\u003ei prop myself up on one elbow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7\u003c/th\u003e\n","      \u003ctd\u003eTheres enough light in the bedroom to see them.\u003c/td\u003e\n","      \u003ctd\u003ethere be enough light in the bedroom to see them\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e8\u003c/th\u003e\n","      \u003ctd\u003eMy little sister, Prim, curled up on her side,...\u003c/td\u003e\n","      \u003ctd\u003emy little sister prim curl up on her side coco...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e9\u003c/th\u003e\n","      \u003ctd\u003eIn sleep, my mother looks younger, still worn ...\u003c/td\u003e\n","      \u003ctd\u003ein sleep my mother look young still wear but n...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e10\u003c/th\u003e\n","      \u003ctd\u003ePrims face is as fresh as a raindrop, as lovel...\u003c/td\u003e\n","      \u003ctd\u003eprims face be a fresh a a raindrop a lovely a ...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e11\u003c/th\u003e\n","      \u003ctd\u003eMy mother was very beautiful once, too.\u003c/td\u003e\n","      \u003ctd\u003emy mother be very beautiful once too\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e12\u003c/th\u003e\n","      \u003ctd\u003eOr so they tell me.\u003c/td\u003e\n","      \u003ctd\u003eor so they tell me\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e13\u003c/th\u003e\n","      \u003ctd\u003eSitting at Prims knees, guarding her, is the w...\u003c/td\u003e\n","      \u003ctd\u003esit at prims knee guard her be the world ugly cat\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e14\u003c/th\u003e\n","      \u003ctd\u003eMashed-in nose, half of one ear missing, eyes ...\u003c/td\u003e\n","      \u003ctd\u003emash in nose half of one ear miss eye the colo...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e15\u003c/th\u003e\n","      \u003ctd\u003ePrim named him Buttercup, insisting that his m...\u003c/td\u003e\n","      \u003ctd\u003eprim name him buttercup insist that his muddy ...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e16\u003c/th\u003e\n","      \u003ctd\u003eHe hates me.\u003c/td\u003e\n","      \u003ctd\u003ehe hate me\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e17\u003c/th\u003e\n","      \u003ctd\u003eOr at least distrusts me.\u003c/td\u003e\n","      \u003ctd\u003eor at little distrust me\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e18\u003c/th\u003e\n","      \u003ctd\u003eEven though it was years ago, I think he still...\u003c/td\u003e\n","      \u003ctd\u003eeven though it be year ago i think he still re...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e19\u003c/th\u003e\n","      \u003ctd\u003eScrawny kitten, belly swollen with worms, craw...\u003c/td\u003e\n","      \u003ctd\u003escrawny kitten belly swell with worm crawl wit...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc3c797f-8bf4-4fee-b0dc-473aad5fa68d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-fc3c797f-8bf4-4fee-b0dc-473aad5fa68d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fc3c797f-8bf4-4fee-b0dc-473aad5fa68d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                                             text_raw  \\\n","0                                      \"THE TRIBUTES\"   \n","1   When I wake up, the other side of the bed is c...   \n","2   My fingers stretch out, seeking Prims warmth b...   \n","3   She must have had bad dreams and climbed in wi...   \n","4                                 Of course, she did.   \n","5                     This is the day of the reaping.   \n","6                      I prop myself up on one elbow.   \n","7     Theres enough light in the bedroom to see them.   \n","8   My little sister, Prim, curled up on her side,...   \n","9   In sleep, my mother looks younger, still worn ...   \n","10  Prims face is as fresh as a raindrop, as lovel...   \n","11            My mother was very beautiful once, too.   \n","12                                Or so they tell me.   \n","13  Sitting at Prims knees, guarding her, is the w...   \n","14  Mashed-in nose, half of one ear missing, eyes ...   \n","15  Prim named him Buttercup, insisting that his m...   \n","16                                       He hates me.   \n","17                          Or at least distrusts me.   \n","18  Even though it was years ago, I think he still...   \n","19  Scrawny kitten, belly swollen with worms, craw...   \n","\n","                                           text_clean  \n","0                                         the tribute  \n","1    when i wake up the other side of the bed be cold  \n","2   my finger stretch out seek prims warmth but fi...  \n","3   she must have have bad dream and climb in with...  \n","4                                    of course she do  \n","5                         this be the day of the reap  \n","6                       i prop myself up on one elbow  \n","7    there be enough light in the bedroom to see them  \n","8   my little sister prim curl up on her side coco...  \n","9   in sleep my mother look young still wear but n...  \n","10  prims face be a fresh a a raindrop a lovely a ...  \n","11               my mother be very beautiful once too  \n","12                                 or so they tell me  \n","13  sit at prims knee guard her be the world ugly cat  \n","14  mash in nose half of one ear miss eye the colo...  \n","15  prim name him buttercup insist that his muddy ...  \n","16                                         he hate me  \n","17                           or at little distrust me  \n","18  even though it be year ago i think he still re...  \n","19  scrawny kitten belly swell with worm crawl wit...  "]},"execution_count":38,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","RangeIndex: 9021 entries, 0 to 9020\n","Data columns (total 2 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   text_raw    9021 non-null   object\n"," 1   text_clean  9021 non-null   string\n","dtypes: object(1), string(1)\n","memory usage: 141.1+ KB\n"]}],"source":["# Verify the first Text in Corpus is cleaned\n","\n","corpus_texts_dt[corpus_titles_ls[0]].head(20)\n","corpus_texts_dt[corpus_titles_ls[0]].info()"]},{"cell_type":"markdown","metadata":{"id":"WAjjOEFx7F5J"},"source":["## Save Cleaned Corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CrH24Dv7YwK","outputId":"0416f9f8-7d67-4cce-8594-4b185756b471"},"outputs":[{"name":"stdout","output_type":"stream","text":["Currently in SentimentArcs root directory:\n","/gdrive/MyDrive/cdh/sentiment_arcs\n","\n","Saving Clean Texts to Subdir: ./text_clean/novels_text_new_clean/\n","\n","Saving these Texts:\n","  dict_keys(['scollins_thehungergames1', 'cmieville_thecityandthecity'])\n"]}],"source":["# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","print('Currently in SentimentArcs root directory:')\n","!pwd\n","\n","# Verify Subdir to save Cleaned Texts and Texts into..\n","\n","print(f'\\nSaving Clean Texts to Subdir: {SUBDIR_TEXT_CLEAN}')\n","print(f'\\nSaving these Texts:\\n  {corpus_texts_dt.keys()}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgcmvfbvXqfy","outputId":"9b5d3e99-dd18-456d-9081-3321f1e7daa0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving Novel #0 to ./text_clean/novels_text_new_clean/scollins_thehungergames1.csv\n","Saving Novel #1 to ./text_clean/novels_text_new_clean/cmieville_thecityandthecity.csv\n"]}],"source":["# Save the cleaned Textfiles\n","\n","i = 0\n","for key_novel, anovel_df in corpus_texts_dt.items():\n","  anovel_fname = f'{key_novel}.csv'\n","\n","  anovel_fullpath = f'{SUBDIR_TEXT_CLEAN}{anovel_fname}'\n","  print(f'Saving Novel #{i} to {anovel_fullpath}')\n","  corpus_texts_dt[key_novel].to_csv(anovel_fullpath)\n","  i += 1"]},{"cell_type":"markdown","metadata":{"id":"_348z09gQKe3"},"source":["# **[END OF NOTEBOOK]**"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["CBoEHX9Z9imD","mGoFJmeFkTxk","EA1yTaY_9Qod","7dPPrZwyIIze"],"name":"sentiment_arcs_part1_text_preprocessing.ipynb","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}