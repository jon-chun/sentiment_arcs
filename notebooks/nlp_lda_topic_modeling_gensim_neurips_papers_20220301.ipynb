{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7MRHAaZgftt"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS_9h1COgftx"
      },
      "source": [
        "\n",
        "# LDA Model\n",
        "\n",
        "Introduces Gensim's LDA model and demonstrates its use on the NIPS corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPwId5Bpgft0"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt_T3h4Sgft0"
      },
      "source": [
        "The purpose of this tutorial is to demonstrate how to train and tune an LDA model.\n",
        "\n",
        "In this tutorial we will:\n",
        "\n",
        "* Load input data.\n",
        "* Pre-process that data.\n",
        "* Transform documents into bag-of-words vectors.\n",
        "* Train an LDA model.\n",
        "\n",
        "This tutorial will **not**:\n",
        "\n",
        "* Explain how Latent Dirichlet Allocation works\n",
        "* Explain how the LDA model performs inference\n",
        "* Teach you all the parameters and options for Gensim's LDA implementation\n",
        "\n",
        "If you are not familiar with the LDA model or how to use it in Gensim, I (Olavur Mortensen)\n",
        "suggest you read up on that before continuing with this tutorial. Basic\n",
        "understanding of the LDA model should suffice. Examples:\n",
        "\n",
        "* `Introduction to Latent Dirichlet Allocation <http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation>`_\n",
        "* Gensim tutorial: `sphx_glr_auto_examples_core_run_topics_and_transformations.py`\n",
        "* Gensim's LDA model API docs: :py:class:`gensim.models.LdaModel`\n",
        "\n",
        "I would also encourage you to consider each step when applying the model to\n",
        "your data, instead of just blindly applying my solution. The different steps\n",
        "will depend on your data and possibly your goal with the model.\n",
        "\n",
        "## Data\n",
        "\n",
        "I have used a corpus of NIPS papers in this tutorial, but if you're following\n",
        "this tutorial just to learn about LDA I encourage you to consider picking a\n",
        "corpus on a subject that you are familiar with. Qualitatively evaluating the\n",
        "output of an LDA model is challenging and can require you to understand the\n",
        "subject matter of your corpus (depending on your goal with the model).\n",
        "\n",
        "NIPS (Neural Information Processing Systems) is a machine learning conference\n",
        "so the subject matter should be well suited for most of the target audience\n",
        "of this tutorial.  You can download the original data from Sam Roweis'\n",
        "`website <http://www.cs.nyu.edu/~roweis/data.html>`_.  The code below will\n",
        "also do that for you.\n",
        "\n",
        ".. Important::\n",
        "    The corpus contains 1740 documents, and not particularly long ones.\n",
        "    So keep in mind that this tutorial is not geared towards efficiency, and be\n",
        "    careful before applying the code to a large dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkaZo7Avgft2"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os.path\n",
        "import re\n",
        "import tarfile\n",
        "\n",
        "import smart_open\n",
        "\n",
        "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
        "    with smart_open.open(url, \"rb\") as file:\n",
        "        with tarfile.open(fileobj=file) as tar:\n",
        "            for member in tar.getmembers():\n",
        "                if member.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', member.name):\n",
        "                    member_bytes = tar.extractfile(member).read()\n",
        "                    yield member_bytes.decode('utf-8', errors='replace')\n",
        "\n",
        "docs = list(extract_documents())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYw1r1PVgft2"
      },
      "source": [
        "So we have a list of 1740 documents, where each document is a Unicode string.\n",
        "If you're thinking about using your own corpus, then you need to make sure\n",
        "that it's in the same format (list of Unicode strings) before proceeding\n",
        "with the rest of this tutorial.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jA-R3YCgft3",
        "outputId": "380e20ea-5a24-49b3-8df3-cf58750fec39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1740\n",
            "387 \n",
            "Neural Net and Traditional Classifiers  \n",
            "William Y. Huang and Richard P. Lippmann \n",
            "MIT Lincoln Laboratory \n",
            "Lexington, MA 02173, USA \n",
            "Abstract\n",
            "Previous work on nets with continuous-valued inputs led to generative \n",
            "procedures to construct convex decision regions with two-layer percepttons (one hidden \n",
            "layer) and arbitrary decision regions with three-layer percepttons (two hidden layers). \n",
            "Here we demonstrate that two-layer perceptton classifiers trained with back propagation \n",
            "can form both c\n"
          ]
        }
      ],
      "source": [
        "print(len(docs))\n",
        "print(docs[0][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOzKW9vZgft4"
      },
      "source": [
        "## Pre-process and vectorize the documents\n",
        "\n",
        "As part of preprocessing, we will:\n",
        "\n",
        "* Tokenize (split the documents into tokens).\n",
        "* Lemmatize the tokens.\n",
        "* Compute bigrams.\n",
        "* Compute a bag-of-words representation of the data.\n",
        "\n",
        "First we tokenize the text using a regular expression tokenizer from NLTK. We\n",
        "remove numeric tokens and tokens that are only a single character, as they\n",
        "don't tend to be useful, and the dataset contains a lot of them.\n",
        "\n",
        ".. Important::\n",
        "\n",
        "   This tutorial uses the nltk library for preprocessing, although you can\n",
        "   replace it with something else if you want.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pplVNGTUgft4"
      },
      "outputs": [],
      "source": [
        "# Tokenize the documents.\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Split the documents into tokens.\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for idx in range(len(docs)):\n",
        "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
        "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
        "\n",
        "# Remove numbers, but not words that contain numbers.\n",
        "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
        "\n",
        "# Remove words that are only one character.\n",
        "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5F_B5V0gft5"
      },
      "source": [
        "We use the WordNet lemmatizer from NLTK. A lemmatizer is preferred over a\n",
        "stemmer in this case because it produces more readable words. Output that is\n",
        "easy to read is very desirable in topic modelling.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9XiwmYDg5Zh",
        "outputId": "2d07a4ac-8fd4-4318-8300-6b1b990c1dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF_I-F9Rgft5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be16504-66ba-447d-8c6a-2077e3d5ab0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 21.2 s, sys: 196 ms, total: 21.4 s\n",
            "Wall time: 21.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 0m24s @03:48 on 20220228 Colab Pro \n",
        "\n",
        "# Lemmatize the documents.\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeHP0Dougft6"
      },
      "source": [
        "We find bigrams in the documents. Bigrams are sets of two adjacent words.\n",
        "Using bigrams we can get phrases like \"machine_learning\" in our output\n",
        "(spaces are replaced with underscores); without bigrams we would only get\n",
        "\"machine\" and \"learning\".\n",
        "\n",
        "Note that in the code below, we find bigrams and then add them to the\n",
        "original data, because we would like to keep the words \"machine\" and\n",
        "\"learning\" as well as the bigram \"machine_learning\".\n",
        "\n",
        ".. Important::\n",
        "    Computing n-grams of large dataset can be very computationally\n",
        "    and memory intensive.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNuNBXQthLZO",
        "outputId": "e60a4e35-93e2-4210-8bda-30654e126906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Nzv3m0DhNs4",
        "outputId": "06985d02-e6e3-4c36-c040-8d2ba1866144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1740"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(docs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "IH1G1I0xhOzS",
        "outputId": "ab3772e4-c87e-4b61-f896-bacb527ccd7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'neural net and traditional classifier william huang and richard lippmann mit lincoln laboratory lexington ma usa abstract previous work on net with continuous valued input led to generative procedure to construct convex decision region with two layer percepttons one hidden layer and arbitrary decision region with three layer percepttons two hidden layer here we demonstrate that two layer perceptton classifier trained with back propagation can form both convex and disjoint decision region such classifier are robust train rapidly and provide good performance with simple decision region when complex decision region are required however convergence time can be excessively long and performance is often no better than that of nearest neighbor classifier three neural net classifier are presented that provide more rapid training under such situation two use fixed weight in the first one or two layer and are similar to classifier that estimate probability density function using histogram third feature map classifier us both unsupervised and supervised training it provides good performance with little supervised training in situation such a speech recognition where much unlabeled training data is available the architecture of this classifier can be used to implement neural net nearest neighbor classifier introduction neural net architecture can be used to construct many different type of classi tier in particular multi layer perceptron classifier with continuous valued in put trained with back propagation are robust often train rapidly and provide perfor mance similar to that provided by gaussian classifier when decision region are convex generative procedure demonstrate that such classifier can form convex deci sion region with two layer perceptrons one hidden layer and arbitrary decision region with three layer perceptrons two hidden layer more recent work ha demon strated that two layer perceptrons can form non convex and disjoint decision region example of hand crafted two layer network which generate such decision region are presented in this paper along with monte carlo simulation where complex decision region were generated using back propagation training these and previous simula tions demonstrate that convergence time with back propagation can be excessive when complex decision region are desired and performance is often no better than that obtained with nearest neighbor classifier these result led u to explore other neural net classifier that might provide faster convergence three classifier called fixed weight hypercube and feature map classifier were developed and eval uated all classifier were tested on illustrative problem with two continuous valued input and two class and more restricted set of classifier wa tested with vowel formant data capability of two layer perceptrons multi layer perceptron classifier with hard limiting nonlinearities node output of or and continuous valued input can form complex decision region simple constructive proof demonstrate that three layer perceptron two hidden layer can this work wa sponsored by the defense advanced research project agency and the department of the air force the view expressed are those of the author and do not reflect the policy or position of the government american institute of physic xl decision region for class bl b2 b6 xl fig two layer perceptton that form disjoint decision re9ions for class shaded area connec tion weight and node offset are shown in the left hyperplanes formed by all hidden node are drawn a dashed line with node label arrow on these line point to the half plane where the hidden node output is high form arbitrary decision region and two layer perceptron one hidden layer can form single convex decision region recently however it ha been demonstrated that two layer perceptrons can form decision region that are not simply convex fig for example show how disjoint decision region can be generated using two layer perceptron the two disjoint shaded area in this fig represent the decision region for class output node ha high output the remaining area represents the decision region for class output node ha low output node in this fig contain hard limiting nonlinearities connection weight and node offset are indicated in the left diagram ten other complex decision region formed using two layer perceptrons are presented in fig the above example suggest that two layer perceptrons can form decision region with arbitrary shape we however know of no general proof of this capability book by nilson discus this issue and contains proof that two layer net can divide finite number of point into two arbitrary set page this proof involves separating point using at most parallel hyperplanes formed by first layer node where no hyperplane intersects two or more point proving that given decision region can be formed in two layer net involves testing to determine whether the boolean representation at the output of the first layer for all point within the decision region for class are linearly separable from the boolean representation for class one test for linear separability wa presented in problem with forming complex decision region with two layer percepttons is that weight and offset must be adjusted carefully because they interact extensively to form decision region fig illustrates this sensitivity problem here it can be seen that weight to one hidden node form hyperplane which influence decision region in an entire hallplane for example small error in first layer weight that result in change in the slope of hyperplanes and b6 might only slightly extend the region but completely eliminate the a2 region this interdependence can be eliminated in three layer perceptrons it is possible to train two layer percepttons to form complex decision region using back propagation and sigmoidal nonlinearities despite weight interaction fig for example show disjoint decision region formed using back propagation for the problem of fig in this and all other simulation input were presented alternately from class and and selected from uniform distribution covering the desired decision region in addition the back propagation rate of descent term wa set equal to the momentum gain term and small value for and were necessary to guarantee convergence for the difficult problem in fig other simulation detail are fig ten complex decision region formed by two layer perceptrons the number assigned to each case are the acase number used in the rest of this paper a in also shown in fig are hyperplanes formed by those first layer node with the strongest connection weight to the output node these hyperplanes and weight are similar to those in the network created by hand except for sign inversion the occurrence of multiple similar hyperplanes formed by two node and the use of node offset with value near zero comparative result of two layer v three layer previous result a well a the weight interaction mentioned above suggest that three layer percepttons may be able to form complex decision region faster with back propagation than two layer percepttons this wa explored using monte carlo simulation for the first nine case of fig all network have node in the first hidden layer the number of node in the second hidden layer wa twice the number of convex region needed to form the decision region and for case through respectively ten run were typically averaged together to obtain smooth curve of percentage error v time number of training trial and enough trial were run to limit of until the curve appeared to fiatten out with little improvement over time the error curve wa then low pas filtered to determine the convergence time convergence time wa defined a the time when the curve crossed value percentage point above the final percentage error this definition provides framework for comparing the convergence time of the different classifier it however is not the time after which error rate do not improve fig summarizes result in term of convergence time and final percentage error in those case with disjoint decision region back propagation sometimes failed to form separate region after trial for example the two disjoint region required in case were never fully separated with fig decision region formed using back propagation for case of fig thick solid line represent decision boundary dashed line and arrow have the same meaning a in fig only hyperplanes for hidden node with large weight to the output node are shown over training trial were required to form separate region two layer perceptron but were separated with three layer perceptron this is noted by the use of filled symbol in fig fig show that there is no significant performance difference between two and three layer perceptrons when forming complex decision region using back propagation training both type of classifier take an excessively long time trial to form complex decision region minor difference is that in case and the two layer network failed to separate disjoint region after trial whereas the three layer network wa able to do so this however is not significant in term of convergence time and error rate problem that are difficult for the two layer network are also difficult for the three layer network and vice versa alternative classifier result presented above and previous result demonstrate that multi layer per ceptron classifier can take very long to converge for complex decision region three alternative classifier were studied to determine whether other type of neural net clas sifiers could provide faster convergence fixed weight classifier fixed weight classifier attempt to reduce training time by adapting only weight between upper layer of multi layer perceptrons weight to the first layer are fixed before training and remain unchanged these weight form fixed hyperplanes which can be used by upper layer to form decision region performance will be good if the fixed hyperplanes are near the decision region boundary that are required in specific problem weight between upper layer are trained using back propagation a described above two method were used to adjust weight to the first layer weight were adjusted to place hyperplanes randomly or in grid in the region x2 all decision region in fig fall within this region hyperplanes formed by first layer node for fixed random and fixed grid classifier for case of fig are shown a dashed line in fig also shown in this fig are decision region shaded area formed layer error kate layer convergence time 1ooooo case number see fig fro perce taae error top co oer9e ce time bottom for case throu9h of fi9 for ttvo and three layer perceptton classifier trained usin back propa9ation filled syrabois indicate that separate disjoint re9ions tvere not formed after trial using back propagation to train only the upper network layer these region illustrate how fixed hyperplanes are combined to form decision region it can be seen that decision boundary form along the available hyperplanes good solution is possible for the fixed grid classifier where desired decision region boundary are near hyperplanes the random grid classifier provides poor solution because hyperplanes are not near desired decision boundary the performance of fixed weight classifier depends both on the placement of hyperplanes and on the number of hyperplanes provided hypercube classifier many traditional cidsifters estimate probability density function of input variable for different class using histogram technique hypercube cidsifters use this tech nique by fixing weight in the first two layer to break the input space into hypercubes square in the case of two input hypercube classifier are similar to fixed weight classifier except weight to the first two layer are fixed and only weight to output node are trained hypercube classifier are also similar in structure to the cmac model described by albus the output of second layer node is high only if the input is in the hypercube corresponding to that node this is illustrated in fig for network with two input the top layer of hypercube classifier can be trained using back propagation maximum likelihood approach however suggests simpler training algorithm which consists of counting the output of second layer node hi is connected to the output node corresponding to that class with greatest frequency of occurrence of training input in hypercube hi that is if sample fall in hypercube hi then it is classified a class where ni ni for all in this equation ni is the number of training token in hypercube hi which belong to class this will be called maximum likelihood ml training it can be implemented by connection second layer node hi only to that output node corresponding to class in eq in all simulation hypercubes covered the area x2 random r_ it op grid fla decision region formed with xed random and xed grid classifier for case from fig using back propagation training line shown are iperplanes formed by the first layer node shaded area represent the decision region for class h2 trained layer fixed layer four bin created by fixed layer b3 jb6 be xl x2 input fig lpercube classifier left is three lager perceptton with fixed weight to the first two layer and trainable weight to output node weight are initialized such that output of node hi through h4 left are high oni when the input is in the corresponding ipercube right select class with majority in top select top exemplar calculate correlation to stored exemplar output only one high yl input supervised associative learning unsupervised kohonen feature map learning fig feature map classifier feature map classifier in many speech and image classification problem large quantity of unlabeled training data can be obtained but little labeled data is available in such situation unsupervised training with unlabeled training data can substantially reduce the amount of supervised training required the feature map classifier shown in fig us com bined supervised unsupervised training and is designed for such problem it is similar to histogram classifier used in discrete observation hidden markov model and the classifier used in the first layer of this classifier form feature map using self organizing clustering algorithm described by kohonen in all simulation reported in this paper trial of unsupervised training were used after unsupervised train ing first layer feature node sample the input space with node density proportional to the combined probability density of all class first layer feature map node perform function similar to that of second layer hypercube node except each node ha maximum output for input region that are more general than hypercubes and only the output of the node with maximum output is fed to the output node weight to output node are trained with supervision after the first layer ha been trained back propag tion or maximum likelihood training can be used maximum likelihood training reqmres ni eq to be the number of time first layer node ha maximum output for input from class in addition during classification the output of node with ni for all untrained node are not considered when the first layer node with the maximum output is selected the network architecture of feature map classifier can be used to implement nearest neighbor classifier in this case the feedback connection in fig large circular summing node and triangular integrator used to select those node with the maximum output must be slightly modified is for feature map classifier and must be adjusted to the desired value of for nearest neighbor classifier comparison between classifier the result of monte carlo simulation using all classifier for case are shown in fig error rate and convergence time were determined a in section all alter trial 25oo 2o00 5oo conventional gauss knn back prop lay percent correct flxed welght hypercube feature map lay knn gauss convergence time ndom 64k lay ra grid number of hidden node fig comparative performance of classifier for case trainin time of the feature map classifier doe not include the unsupervised trainin9 trial native classifier had shorter convergence time than multi layer perceptron classifier trained with back propagation the feature map classifier provided best performance with node it error rate wa similar to that of the nearest neighbor classifier but it required fewer than supervised training token the larger fixed weight and hypercube classifier performed well but required more supervised training than the feature map classifier these classifier will work well when the combined probability density function of all class varies smoothly and the domain where this function is non zero is known in this case weight and offset can be set such that hyperplanes and hypercubes cover the domain and provide good performance the feature map classifier automatically cover the domain fixed weight random classifier performed substan tially worse than fixed weight grid classifier back propagation training bp wa generally much slower than maximum likelihood training ml vowel classification multi layer perceptron feature map and traditional classifier were tested with vowel formant data from peterson and barney these data had been obtained by spectrographic analysis of vowel in hvd context spoken by men woman and child first and second formant data of ten vowel wa split into two set resulting in total of training token and testing token fig show the test data and the decision region formed by two layer perceptton classifier trained with back propagation the performance of classifier is presented in table all classifier had similar error rate the feature map classifier with only node required le than supervised training token sample per vowel class for convergence the perceptton classifier trained with back propagation required more than training token the first stage of the feature map classifier and the multi layer perceptton classifier were trained by ra ndomly selecting entry from the training token after label had been removed and using token repetitively 4ooo looc 5oo head hid hod had hawed heard heed hud who hood fig decision region formed by two layer network usin9 bp after trainin9 token from petersoh steady state vowel data peterson also shown are sample of the testin9 set le9end show example of the pronunciation of the vowel and the error within each vowel algorithm training token error knn gaussian ayer pe eptron feature map table performance of classifier on steady state vowel data conclusion neural net architecture form flexible framework that can be used to construct many different type of classifier these include gaussian nearest neighbor and multi layer perceptton classifier a well a classifier such a the feature map classifier which use unsupervised training here we first demonstrated that two layer percepttons one hidden layer can form non convex and disjoint decision region back propagation training however can be extremely slow when forming complex decision region with multi layer perceptrons alternative classifier were thus developed and tested all provided faster training and many provided improved performance two were similar to traditional classifier one hypercube classifier can be used to implement histogram classifier nd another feature map classifier can be used to implement nearest neighbor classifier the feature map classifier provided best overall performance it used combined supervised unsupervised training and attained the same error rate a nearest neighbor classifier but with fewer supervised training token furthermore it required fewer node then nearest neighbor classifier reference albus brain behavior and robotics mcgraw hill petersborough burr neural network digit recognizer in proceeding of the international conference on system man and cybernetics ieee cooper and freeman on the asymptotic improvement in the outcome of supervised learning provided by aaditional nonsupervised learning ieee transaction on computer vol pp november duda and hart pattern classification and scene analysis john wiley son new york huang and lippmann comparison between conventional and neural net classifier in 1st international conference on neural network ieee june kohonen makisara and saramaki phonotopic map insightful representation of phonological feature for speech recognition in proceeding of the 7th international confer ence on pattern recognition ieee august lippmann an introduction to computing with neural net ieee assp magazine vol pp april lippmann and gold neural classifier useful for speech recognition in 1st international conference on neural network ieee june longstaff and cross pattern recognition approach to understanding the multi layer perceptton mem royal signal and radar establishment july nilsson learning machine mcgraw hill paxsons voice and speech processing mcgraw hill new york rosenblurt percepttons and the theory of brain mechanism spartan book singleton test for linear separability a applied to self organizing machine in self organization system yovits jacobi and goldstein ed pp spartan book washington wieland and leighton geometric analysis of neural network capability in ist interna tional conference on neural network ieee june'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk5VQcqSgft6",
        "outputId": "3d61a77e-2bde-4063-e3e1-6f4606640f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-02-28 16:32:31,738 : INFO : 'pattern' package not found; tag filters are not available for English\n",
            "2022-02-28 16:32:31,756 : INFO : collecting all words and their counts\n",
            "2022-02-28 16:32:31,758 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
            "2022-02-28 16:32:41,347 : INFO : collected 1120198 word types from a corpus of 4629808 words (unigram + bigrams) and 1740 sentences\n",
            "2022-02-28 16:32:41,348 : INFO : using 1120198 counts as vocab in Phrases<0 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        }
      ],
      "source": [
        "# Compute bigrams.\n",
        "from gensim.models import Phrases\n",
        "\n",
        "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
        "bigram = Phrases(docs, min_count=20)\n",
        "for idx in range(len(docs)):\n",
        "    for token in bigram[docs[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            docs[idx].append(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5o8CBSMgft6"
      },
      "source": [
        "We remove rare words and common words based on their *document frequency*.\n",
        "Below we remove words that appear in less than 20 documents or in more than\n",
        "50% of the documents. Consider trying to remove words only based on their\n",
        "frequency, or maybe combining that with this approach.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Aa1mH88gft7",
        "outputId": "7ed1e85b-d61a-4eef-afe9-d3402100325b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-02-28 16:33:02,381 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
            "2022-02-28 16:33:05,995 : INFO : built Dictionary(79429 unique tokens: ['1ooooo', '1st', '25oo', '2o00', '4ooo']...) from 1740 documents (total 4953968 corpus positions)\n",
            "2022-02-28 16:33:06,165 : INFO : discarding 70785 tokens: [('1ooooo', 1), ('25oo', 2), ('2o00', 6), ('4ooo', 2), ('64k', 6), ('a', 1740), ('aaditional', 1), ('above', 1114), ('abstract', 1740), ('acase', 1)]...\n",
            "2022-02-28 16:33:06,167 : INFO : keeping 8644 tokens which were in no less than 20 and no more than 870 (=50.0%) documents\n",
            "2022-02-28 16:33:06,206 : INFO : resulting dictionary: Dictionary(8644 unique tokens: ['1st', '5oo', '7th', 'a2', 'a_well']...)\n"
          ]
        }
      ],
      "source": [
        "# Remove rare and common tokens.\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Create a dictionary representation of the documents.\n",
        "dictionary = Dictionary(docs)\n",
        "\n",
        "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
        "dictionary.filter_extremes(no_below=20, no_above=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0YX0ewEgft7"
      },
      "source": [
        "Finally, we transform the documents to a vectorized form. We simply compute\n",
        "the frequency of each word, including the bigrams.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7WBqZ5Mgft7"
      },
      "outputs": [],
      "source": [
        "# Bag-of-words representation of the documents.\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eAsWXotgft8"
      },
      "source": [
        "Let's see how many tokens and documents we have to train on.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqlkSJQLhco5",
        "outputId": "4d7dd830-87b7-4416-8504-3a1636d7d587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh9LzoeXhfip",
        "outputId": "8a662902-2649-4821-b10a-8fcdf862ab99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1740"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpRsFMgAhgzn",
        "outputId": "af352617-fb60-4cc4-8a07-f7de3e0fc5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42KjXcKPhtJF",
        "outputId": "448e12ed-6588-47e5-d28b-e957d7a1545a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 2),\n",
              " (1, 2),\n",
              " (2, 1),\n",
              " (3, 1),\n",
              " (4, 2),\n",
              " (5, 2),\n",
              " (6, 1),\n",
              " (7, 2),\n",
              " (8, 1),\n",
              " (9, 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oZGyeXzgft8",
        "outputId": "af756eee-66eb-4f61-e4cd-ac76d3ffe57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens: 8644\n",
            "Number of documents: 1740\n"
          ]
        }
      ],
      "source": [
        "print('Number of unique tokens: %d' % len(dictionary))\n",
        "print('Number of documents: %d' % len(corpus))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqaaUBeagft8"
      },
      "source": [
        "## Training\n",
        "\n",
        "We are ready to train the LDA model. We will first discuss how to set some of\n",
        "the training parameters.\n",
        "\n",
        "First of all, the elephant in the room: how many topics do I need? There is\n",
        "really no easy answer for this, it will depend on both your data and your\n",
        "application. I have used 10 topics here because I wanted to have a few topics\n",
        "that I could interpret and \"label\", and because that turned out to give me\n",
        "reasonably good results. You might not need to interpret all your topics, so\n",
        "you could use a large number of topics, for example 100.\n",
        "\n",
        "``chunksize`` controls how many documents are processed at a time in the\n",
        "training algorithm. Increasing chunksize will speed up training, at least as\n",
        "long as the chunk of documents easily fit into memory. I've set ``chunksize =\n",
        "2000``, which is more than the amount of documents, so I process all the\n",
        "data in one go. Chunksize can however influence the quality of the model, as\n",
        "discussed in Hoffman and co-authors [2], but the difference was not\n",
        "substantial in this case.\n",
        "\n",
        "``passes`` controls how often we train the model on the entire corpus.\n",
        "Another word for passes might be \"epochs\". ``iterations`` is somewhat\n",
        "technical, but essentially it controls how often we repeat a particular loop\n",
        "over each document. It is important to set the number of \"passes\" and\n",
        "\"iterations\" high enough.\n",
        "\n",
        "I suggest the following way to choose iterations and passes. First, enable\n",
        "logging (as described in many Gensim tutorials), and set ``eval_every = 1``\n",
        "in ``LdaModel``. When training the model look for a line in the log that\n",
        "looks something like this::\n",
        "\n",
        "   2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations\n",
        "\n",
        "If you set ``passes = 20`` you will see this line 20 times. Make sure that by\n",
        "the final passes, most of the documents have converged. So you want to choose\n",
        "both passes and iterations to be high enough for this to happen.\n",
        "\n",
        "We set ``alpha = 'auto'`` and ``eta = 'auto'``. Again this is somewhat\n",
        "technical, but essentially we are automatically learning two parameters in\n",
        "the model that we usually would have to specify explicitly.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0wOHHJCgft9",
        "outputId": "75f9673f-61c2-47e6-c5de-460808919293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-02-28 16:33:08,676 : INFO : using autotuned alpha, starting with [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "2022-02-28 16:33:08,681 : INFO : using serial LDA version on this node\n",
            "2022-02-28 16:33:08,702 : INFO : running online (multi-pass) LDA training, 10 topics, 20 passes over the supplied corpus of 1740 documents, updating model once every 1740 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
            "2022-02-28 16:33:08,705 : INFO : PROGRESS: pass 0, at document #1740/1740\n",
            "2022-02-28 16:33:23,555 : INFO : optimized alpha [0.08011236, 0.081912495, 0.054547794, 0.047545012, 0.055025768, 0.0811094, 0.10560397, 0.06562303, 0.055871338, 0.11568752]\n",
            "2022-02-28 16:33:23,570 : INFO : topic #3 (0.048): 0.005*\"hidden\" + 0.005*\"net\" + 0.004*\"noise\" + 0.004*\"layer\" + 0.003*\"image\" + 0.003*\"sequence\" + 0.003*\"component\" + 0.003*\"bound\" + 0.002*\"source\" + 0.002*\"signal\"\n",
            "2022-02-28 16:33:23,572 : INFO : topic #2 (0.055): 0.003*\"recognition\" + 0.003*\"gradient\" + 0.003*\"image\" + 0.003*\"layer\" + 0.003*\"cost\" + 0.002*\"class\" + 0.002*\"hidden\" + 0.002*\"control\" + 0.002*\"prior\" + 0.002*\"object\"\n",
            "2022-02-28 16:33:23,575 : INFO : topic #1 (0.082): 0.009*\"neuron\" + 0.004*\"tree\" + 0.003*\"layer\" + 0.003*\"node\" + 0.003*\"net\" + 0.003*\"noise\" + 0.003*\"image\" + 0.003*\"rule\" + 0.002*\"connection\" + 0.002*\"recognition\"\n",
            "2022-02-28 16:33:23,577 : INFO : topic #6 (0.106): 0.005*\"cell\" + 0.004*\"layer\" + 0.003*\"node\" + 0.003*\"image\" + 0.003*\"neuron\" + 0.003*\"rule\" + 0.003*\"object\" + 0.003*\"signal\" + 0.003*\"noise\" + 0.003*\"solution\"\n",
            "2022-02-28 16:33:23,581 : INFO : topic #9 (0.116): 0.007*\"image\" + 0.004*\"signal\" + 0.004*\"matrix\" + 0.003*\"field\" + 0.003*\"neuron\" + 0.003*\"stimulus\" + 0.003*\"component\" + 0.003*\"response\" + 0.003*\"class\" + 0.002*\"noise\"\n",
            "2022-02-28 16:33:23,585 : INFO : topic diff=1.201724, rho=1.000000\n",
            "2022-02-28 16:33:23,610 : INFO : PROGRESS: pass 1, at document #1740/1740\n",
            "2022-02-28 16:33:32,566 : INFO : optimized alpha [0.07382529, 0.06580792, 0.046510417, 0.039696377, 0.046790514, 0.06656594, 0.0732635, 0.05578793, 0.047396116, 0.09088285]\n",
            "2022-02-28 16:33:32,581 : INFO : topic #3 (0.040): 0.005*\"net\" + 0.005*\"hidden\" + 0.004*\"noise\" + 0.003*\"sequence\" + 0.003*\"policy\" + 0.003*\"component\" + 0.003*\"layer\" + 0.003*\"action\" + 0.003*\"bound\" + 0.003*\"image\"\n",
            "2022-02-28 16:33:32,583 : INFO : topic #2 (0.047): 0.004*\"gradient\" + 0.003*\"cost\" + 0.003*\"gaussian\" + 0.003*\"mixture\" + 0.003*\"prior\" + 0.003*\"convergence\" + 0.003*\"stochastic\" + 0.003*\"recognition\" + 0.003*\"optimal\" + 0.003*\"variance\"\n",
            "2022-02-28 16:33:32,587 : INFO : topic #6 (0.073): 0.004*\"cell\" + 0.004*\"object\" + 0.004*\"layer\" + 0.004*\"node\" + 0.004*\"image\" + 0.003*\"rule\" + 0.003*\"solution\" + 0.003*\"noise\" + 0.003*\"signal\" + 0.003*\"optimal\"\n",
            "2022-02-28 16:33:32,589 : INFO : topic #0 (0.074): 0.008*\"hidden\" + 0.005*\"layer\" + 0.005*\"class\" + 0.004*\"classifier\" + 0.004*\"net\" + 0.004*\"recognition\" + 0.004*\"node\" + 0.003*\"hidden_unit\" + 0.003*\"bound\" + 0.003*\"word\"\n",
            "2022-02-28 16:33:32,598 : INFO : topic #9 (0.091): 0.009*\"image\" + 0.004*\"signal\" + 0.003*\"field\" + 0.003*\"matrix\" + 0.003*\"visual\" + 0.003*\"component\" + 0.003*\"response\" + 0.003*\"motion\" + 0.003*\"stimulus\" + 0.003*\"map\"\n",
            "2022-02-28 16:33:32,602 : INFO : topic diff=0.270768, rho=0.577350\n",
            "2022-02-28 16:33:32,625 : INFO : PROGRESS: pass 2, at document #1740/1740\n",
            "2022-02-28 16:33:39,560 : INFO : optimized alpha [0.070773736, 0.05660159, 0.04349513, 0.035135195, 0.041992508, 0.05948427, 0.05941131, 0.051074475, 0.04240766, 0.07788764]\n",
            "2022-02-28 16:33:39,574 : INFO : topic #3 (0.035): 0.004*\"net\" + 0.004*\"hidden\" + 0.004*\"noise\" + 0.004*\"sequence\" + 0.003*\"component\" + 0.003*\"concept\" + 0.003*\"density\" + 0.003*\"source\" + 0.003*\"instance\" + 0.003*\"bound\"\n",
            "2022-02-28 16:33:39,576 : INFO : topic #4 (0.042): 0.006*\"noise\" + 0.005*\"kernel\" + 0.005*\"neuron\" + 0.004*\"regression\" + 0.003*\"generalization\" + 0.003*\"eye\" + 0.003*\"matrix\" + 0.003*\"validation\" + 0.003*\"support\" + 0.003*\"prediction\"\n",
            "2022-02-28 16:33:39,580 : INFO : topic #6 (0.059): 0.005*\"object\" + 0.004*\"node\" + 0.004*\"layer\" + 0.004*\"image\" + 0.003*\"cell\" + 0.003*\"rule\" + 0.003*\"solution\" + 0.003*\"view\" + 0.003*\"noise\" + 0.003*\"constraint\"\n",
            "2022-02-28 16:33:39,583 : INFO : topic #0 (0.071): 0.008*\"hidden\" + 0.006*\"class\" + 0.006*\"layer\" + 0.005*\"classifier\" + 0.004*\"recognition\" + 0.004*\"net\" + 0.004*\"node\" + 0.004*\"word\" + 0.004*\"hidden_unit\" + 0.004*\"bound\"\n",
            "2022-02-28 16:33:39,586 : INFO : topic #9 (0.078): 0.012*\"image\" + 0.005*\"signal\" + 0.004*\"field\" + 0.004*\"visual\" + 0.004*\"motion\" + 0.003*\"component\" + 0.003*\"matrix\" + 0.003*\"map\" + 0.003*\"object\" + 0.003*\"response\"\n",
            "2022-02-28 16:33:39,590 : INFO : topic diff=0.207932, rho=0.500000\n",
            "2022-02-28 16:33:39,614 : INFO : PROGRESS: pass 3, at document #1740/1740\n",
            "2022-02-28 16:33:45,649 : INFO : optimized alpha [0.068483934, 0.051370002, 0.041872807, 0.032193974, 0.03940115, 0.05466426, 0.05193356, 0.04842594, 0.039248038, 0.070104234]\n",
            "2022-02-28 16:33:45,665 : INFO : topic #3 (0.032): 0.004*\"concept\" + 0.004*\"net\" + 0.004*\"sequence\" + 0.004*\"density\" + 0.004*\"component\" + 0.003*\"hidden\" + 0.003*\"noise\" + 0.003*\"instance\" + 0.003*\"source\" + 0.003*\"joint\"\n",
            "2022-02-28 16:33:45,670 : INFO : topic #8 (0.039): 0.010*\"action\" + 0.008*\"control\" + 0.008*\"policy\" + 0.006*\"reinforcement\" + 0.005*\"optimal\" + 0.004*\"reinforcement_learning\" + 0.004*\"memory\" + 0.004*\"controller\" + 0.004*\"dynamic\" + 0.003*\"reward\"\n",
            "2022-02-28 16:33:45,673 : INFO : topic #5 (0.055): 0.014*\"cell\" + 0.012*\"neuron\" + 0.007*\"response\" + 0.005*\"stimulus\" + 0.005*\"activity\" + 0.005*\"synaptic\" + 0.004*\"connection\" + 0.004*\"visual\" + 0.004*\"frequency\" + 0.004*\"layer\"\n",
            "2022-02-28 16:33:45,677 : INFO : topic #0 (0.068): 0.008*\"hidden\" + 0.006*\"class\" + 0.006*\"layer\" + 0.006*\"classifier\" + 0.005*\"recognition\" + 0.004*\"node\" + 0.004*\"word\" + 0.004*\"net\" + 0.004*\"hidden_unit\" + 0.004*\"bound\"\n",
            "2022-02-28 16:33:45,681 : INFO : topic #9 (0.070): 0.014*\"image\" + 0.005*\"signal\" + 0.004*\"visual\" + 0.004*\"field\" + 0.004*\"motion\" + 0.004*\"object\" + 0.003*\"map\" + 0.003*\"component\" + 0.003*\"filter\" + 0.003*\"direction\"\n",
            "2022-02-28 16:33:45,684 : INFO : topic diff=0.165545, rho=0.447214\n",
            "2022-02-28 16:33:45,709 : INFO : PROGRESS: pass 4, at document #1740/1740\n",
            "2022-02-28 16:33:51,285 : INFO : optimized alpha [0.06689923, 0.04834745, 0.041037444, 0.030220004, 0.037950326, 0.0515257, 0.04731044, 0.046757996, 0.036911752, 0.065539345]\n",
            "2022-02-28 16:33:51,300 : INFO : topic #3 (0.030): 0.005*\"concept\" + 0.004*\"density\" + 0.004*\"component\" + 0.004*\"sequence\" + 0.004*\"instance\" + 0.004*\"query\" + 0.003*\"net\" + 0.003*\"wavelet\" + 0.003*\"source\" + 0.003*\"hypothesis\"\n",
            "2022-02-28 16:33:51,302 : INFO : topic #8 (0.037): 0.011*\"action\" + 0.009*\"control\" + 0.009*\"policy\" + 0.007*\"reinforcement\" + 0.006*\"optimal\" + 0.005*\"reinforcement_learning\" + 0.004*\"controller\" + 0.004*\"dynamic\" + 0.004*\"reward\" + 0.004*\"memory\"\n",
            "2022-02-28 16:33:51,305 : INFO : topic #5 (0.052): 0.015*\"cell\" + 0.013*\"neuron\" + 0.007*\"response\" + 0.006*\"stimulus\" + 0.005*\"activity\" + 0.005*\"synaptic\" + 0.005*\"visual\" + 0.004*\"connection\" + 0.004*\"frequency\" + 0.004*\"cortex\"\n",
            "2022-02-28 16:33:51,307 : INFO : topic #9 (0.066): 0.015*\"image\" + 0.005*\"signal\" + 0.005*\"visual\" + 0.004*\"motion\" + 0.004*\"field\" + 0.004*\"object\" + 0.004*\"map\" + 0.003*\"component\" + 0.003*\"filter\" + 0.003*\"position\"\n",
            "2022-02-28 16:33:51,313 : INFO : topic #0 (0.067): 0.008*\"hidden\" + 0.007*\"class\" + 0.006*\"layer\" + 0.006*\"classifier\" + 0.005*\"recognition\" + 0.004*\"word\" + 0.004*\"node\" + 0.004*\"net\" + 0.004*\"hidden_unit\" + 0.004*\"classification\"\n",
            "2022-02-28 16:33:51,317 : INFO : topic diff=0.137561, rho=0.408248\n",
            "2022-02-28 16:33:51,343 : INFO : PROGRESS: pass 5, at document #1740/1740\n",
            "2022-02-28 16:33:56,524 : INFO : optimized alpha [0.06524878, 0.04654462, 0.04058155, 0.02879228, 0.03719043, 0.0492692, 0.044253144, 0.04566972, 0.035057295, 0.062368028]\n",
            "2022-02-28 16:33:56,540 : INFO : topic #3 (0.029): 0.005*\"concept\" + 0.004*\"query\" + 0.004*\"instance\" + 0.004*\"density\" + 0.004*\"component\" + 0.004*\"hypothesis\" + 0.004*\"sequence\" + 0.003*\"class\" + 0.003*\"sample\" + 0.003*\"wavelet\"\n",
            "2022-02-28 16:33:56,543 : INFO : topic #8 (0.035): 0.012*\"action\" + 0.010*\"control\" + 0.009*\"policy\" + 0.007*\"reinforcement\" + 0.006*\"optimal\" + 0.005*\"reinforcement_learning\" + 0.005*\"controller\" + 0.004*\"dynamic\" + 0.004*\"reward\" + 0.004*\"environment\"\n",
            "2022-02-28 16:33:56,547 : INFO : topic #5 (0.049): 0.016*\"cell\" + 0.013*\"neuron\" + 0.008*\"response\" + 0.006*\"stimulus\" + 0.006*\"activity\" + 0.005*\"synaptic\" + 0.005*\"visual\" + 0.005*\"frequency\" + 0.004*\"connection\" + 0.004*\"cortex\"\n",
            "2022-02-28 16:33:56,552 : INFO : topic #9 (0.062): 0.016*\"image\" + 0.005*\"signal\" + 0.005*\"visual\" + 0.005*\"motion\" + 0.005*\"field\" + 0.004*\"object\" + 0.004*\"position\" + 0.004*\"map\" + 0.004*\"filter\" + 0.004*\"component\"\n",
            "2022-02-28 16:33:56,556 : INFO : topic #0 (0.065): 0.009*\"hidden\" + 0.008*\"class\" + 0.006*\"classifier\" + 0.006*\"layer\" + 0.005*\"recognition\" + 0.005*\"node\" + 0.005*\"word\" + 0.004*\"classification\" + 0.004*\"net\" + 0.004*\"hidden_unit\"\n",
            "2022-02-28 16:33:56,560 : INFO : topic diff=0.116709, rho=0.377964\n",
            "2022-02-28 16:33:56,582 : INFO : PROGRESS: pass 6, at document #1740/1740\n",
            "2022-02-28 16:34:01,593 : INFO : optimized alpha [0.063771784, 0.04534719, 0.040368725, 0.027722878, 0.036896348, 0.04766304, 0.042094722, 0.044904735, 0.033675026, 0.060185436]\n",
            "2022-02-28 16:34:01,608 : INFO : topic #3 (0.028): 0.005*\"concept\" + 0.005*\"query\" + 0.004*\"instance\" + 0.004*\"hypothesis\" + 0.004*\"component\" + 0.004*\"density\" + 0.004*\"class\" + 0.004*\"sample\" + 0.003*\"wavelet\" + 0.003*\"sequence\"\n",
            "2022-02-28 16:34:01,609 : INFO : topic #8 (0.034): 0.012*\"action\" + 0.011*\"control\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.005*\"controller\" + 0.005*\"reinforcement_learning\" + 0.004*\"dynamic\" + 0.004*\"reward\" + 0.004*\"environment\"\n",
            "2022-02-28 16:34:01,615 : INFO : topic #5 (0.048): 0.017*\"cell\" + 0.013*\"neuron\" + 0.008*\"response\" + 0.007*\"stimulus\" + 0.006*\"activity\" + 0.005*\"synaptic\" + 0.005*\"visual\" + 0.005*\"frequency\" + 0.004*\"cortex\" + 0.004*\"connection\"\n",
            "2022-02-28 16:34:01,620 : INFO : topic #9 (0.060): 0.017*\"image\" + 0.005*\"signal\" + 0.005*\"visual\" + 0.005*\"motion\" + 0.005*\"object\" + 0.005*\"field\" + 0.004*\"position\" + 0.004*\"filter\" + 0.004*\"map\" + 0.004*\"pixel\"\n",
            "2022-02-28 16:34:01,625 : INFO : topic #0 (0.064): 0.009*\"hidden\" + 0.008*\"class\" + 0.007*\"classifier\" + 0.007*\"layer\" + 0.005*\"recognition\" + 0.005*\"node\" + 0.005*\"classification\" + 0.005*\"word\" + 0.004*\"net\" + 0.004*\"hidden_unit\"\n",
            "2022-02-28 16:34:01,627 : INFO : topic diff=0.100774, rho=0.353553\n",
            "2022-02-28 16:34:01,651 : INFO : PROGRESS: pass 7, at document #1740/1740\n",
            "2022-02-28 16:34:06,545 : INFO : optimized alpha [0.06257766, 0.044450928, 0.04031896, 0.02690255, 0.03695739, 0.046434227, 0.040540837, 0.04447261, 0.03265083, 0.058556184]\n",
            "2022-02-28 16:34:06,559 : INFO : topic #3 (0.027): 0.005*\"query\" + 0.005*\"concept\" + 0.004*\"instance\" + 0.004*\"hypothesis\" + 0.004*\"class\" + 0.004*\"sample\" + 0.004*\"component\" + 0.004*\"density\" + 0.003*\"learner\" + 0.003*\"wavelet\"\n",
            "2022-02-28 16:34:06,561 : INFO : topic #8 (0.033): 0.012*\"action\" + 0.011*\"control\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.004*\"dynamic\" + 0.004*\"reward\" + 0.004*\"environment\"\n",
            "2022-02-28 16:34:06,564 : INFO : topic #5 (0.046): 0.017*\"cell\" + 0.013*\"neuron\" + 0.008*\"response\" + 0.007*\"stimulus\" + 0.006*\"activity\" + 0.005*\"synaptic\" + 0.005*\"frequency\" + 0.005*\"visual\" + 0.005*\"cortex\" + 0.004*\"spike\"\n",
            "2022-02-28 16:34:06,566 : INFO : topic #9 (0.059): 0.018*\"image\" + 0.006*\"signal\" + 0.005*\"visual\" + 0.005*\"motion\" + 0.005*\"object\" + 0.005*\"field\" + 0.004*\"position\" + 0.004*\"pixel\" + 0.004*\"filter\" + 0.004*\"map\"\n",
            "2022-02-28 16:34:06,569 : INFO : topic #0 (0.063): 0.009*\"hidden\" + 0.008*\"class\" + 0.007*\"classifier\" + 0.007*\"layer\" + 0.005*\"recognition\" + 0.005*\"classification\" + 0.005*\"node\" + 0.005*\"net\" + 0.004*\"word\" + 0.004*\"hidden_unit\"\n",
            "2022-02-28 16:34:06,572 : INFO : topic diff=0.088505, rho=0.333333\n",
            "2022-02-28 16:34:06,592 : INFO : PROGRESS: pass 8, at document #1740/1740\n",
            "2022-02-28 16:34:11,344 : INFO : optimized alpha [0.061550736, 0.04374203, 0.040318172, 0.026286881, 0.037214693, 0.045553915, 0.03942951, 0.044257853, 0.031877153, 0.057334103]\n",
            "2022-02-28 16:34:11,359 : INFO : topic #3 (0.026): 0.006*\"query\" + 0.005*\"concept\" + 0.005*\"hypothesis\" + 0.005*\"instance\" + 0.004*\"class\" + 0.004*\"sample\" + 0.003*\"component\" + 0.003*\"learner\" + 0.003*\"wavelet\" + 0.003*\"density\"\n",
            "2022-02-28 16:34:11,361 : INFO : topic #8 (0.032): 0.012*\"action\" + 0.012*\"control\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"dynamic\" + 0.004*\"reward\" + 0.004*\"environment\"\n",
            "2022-02-28 16:34:11,363 : INFO : topic #5 (0.046): 0.017*\"cell\" + 0.013*\"neuron\" + 0.008*\"response\" + 0.007*\"stimulus\" + 0.006*\"activity\" + 0.005*\"synaptic\" + 0.005*\"frequency\" + 0.005*\"visual\" + 0.005*\"spike\" + 0.005*\"cortex\"\n",
            "2022-02-28 16:34:11,366 : INFO : topic #9 (0.057): 0.018*\"image\" + 0.006*\"signal\" + 0.006*\"visual\" + 0.005*\"motion\" + 0.005*\"object\" + 0.005*\"field\" + 0.004*\"position\" + 0.004*\"pixel\" + 0.004*\"filter\" + 0.004*\"map\"\n",
            "2022-02-28 16:34:11,368 : INFO : topic #0 (0.062): 0.009*\"hidden\" + 0.009*\"class\" + 0.008*\"classifier\" + 0.007*\"layer\" + 0.006*\"classification\" + 0.005*\"node\" + 0.005*\"recognition\" + 0.005*\"net\" + 0.004*\"hidden_unit\" + 0.004*\"word\"\n",
            "2022-02-28 16:34:11,371 : INFO : topic diff=0.079374, rho=0.316228\n",
            "2022-02-28 16:34:11,392 : INFO : PROGRESS: pass 9, at document #1740/1740\n",
            "2022-02-28 16:34:16,110 : INFO : optimized alpha [0.060596053, 0.043169923, 0.040447358, 0.025839882, 0.037667427, 0.044902965, 0.038644984, 0.0441801, 0.031271208, 0.056351565]\n",
            "2022-02-28 16:34:16,127 : INFO : topic #3 (0.026): 0.006*\"query\" + 0.005*\"concept\" + 0.005*\"hypothesis\" + 0.005*\"instance\" + 0.004*\"class\" + 0.004*\"sample\" + 0.004*\"learner\" + 0.003*\"wavelet\" + 0.003*\"component\" + 0.003*\"bound\"\n",
            "2022-02-28 16:34:16,129 : INFO : topic #8 (0.031): 0.012*\"action\" + 0.012*\"control\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"dynamic\" + 0.004*\"reward\" + 0.004*\"environment\"\n",
            "2022-02-28 16:34:16,133 : INFO : topic #5 (0.045): 0.017*\"cell\" + 0.013*\"neuron\" + 0.008*\"response\" + 0.007*\"stimulus\" + 0.006*\"activity\" + 0.005*\"spike\" + 0.005*\"synaptic\" + 0.005*\"frequency\" + 0.005*\"visual\" + 0.005*\"cortex\"\n",
            "2022-02-28 16:34:16,135 : INFO : topic #9 (0.056): 0.019*\"image\" + 0.006*\"visual\" + 0.006*\"signal\" + 0.006*\"motion\" + 0.006*\"object\" + 0.005*\"field\" + 0.004*\"position\" + 0.004*\"pixel\" + 0.004*\"filter\" + 0.004*\"map\"\n",
            "2022-02-28 16:34:16,139 : INFO : topic #0 (0.061): 0.009*\"class\" + 0.009*\"hidden\" + 0.008*\"classifier\" + 0.007*\"layer\" + 0.006*\"classification\" + 0.006*\"node\" + 0.005*\"recognition\" + 0.005*\"net\" + 0.005*\"hidden_unit\" + 0.004*\"bound\"\n",
            "2022-02-28 16:34:16,143 : INFO : topic diff=0.072378, rho=0.301511\n",
            "2022-02-28 16:34:16,169 : INFO : PROGRESS: pass 10, at document #1740/1740\n",
            "2022-02-28 16:34:20,745 : INFO : optimized alpha [0.059717443, 0.04275733, 0.040598292, 0.025500322, 0.038210824, 0.044412415, 0.038103357, 0.0441368, 0.030814549, 0.055578325]\n",
            "2022-02-28 16:34:20,760 : INFO : topic #3 (0.026): 0.006*\"query\" + 0.005*\"concept\" + 0.005*\"hypothesis\" + 0.005*\"instance\" + 0.005*\"class\" + 0.004*\"sample\" + 0.004*\"learner\" + 0.003*\"bound\" + 0.003*\"wavelet\" + 0.003*\"xi\"\n",
            "2022-02-28 16:34:20,762 : INFO : topic #8 (0.031): 0.012*\"action\" + 0.012*\"control\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"dynamic\" + 0.004*\"environment\" + 0.004*\"reward\"\n",
            "2022-02-28 16:34:20,764 : INFO : topic #5 (0.044): 0.018*\"cell\" + 0.014*\"neuron\" + 0.008*\"response\" + 0.008*\"stimulus\" + 0.006*\"activity\" + 0.005*\"spike\" + 0.005*\"synaptic\" + 0.005*\"frequency\" + 0.005*\"visual\" + 0.005*\"cortex\"\n",
            "2022-02-28 16:34:20,767 : INFO : topic #9 (0.056): 0.019*\"image\" + 0.006*\"visual\" + 0.006*\"object\" + 0.006*\"motion\" + 0.006*\"signal\" + 0.005*\"field\" + 0.004*\"position\" + 0.004*\"pixel\" + 0.004*\"filter\" + 0.004*\"map\"\n",
            "2022-02-28 16:34:20,776 : INFO : topic #0 (0.060): 0.009*\"class\" + 0.009*\"hidden\" + 0.008*\"classifier\" + 0.007*\"layer\" + 0.006*\"classification\" + 0.006*\"node\" + 0.005*\"recognition\" + 0.005*\"net\" + 0.005*\"hidden_unit\" + 0.004*\"bound\"\n",
            "2022-02-28 16:34:20,778 : INFO : topic diff=0.067106, rho=0.288675\n",
            "2022-02-28 16:34:20,798 : INFO : PROGRESS: pass 11, at document #1740/1740\n",
            "2022-02-28 16:34:25,212 : INFO : optimized alpha [0.058973923, 0.0424655, 0.040761705, 0.025248969, 0.038874622, 0.04404253, 0.037729368, 0.044132963, 0.030442435, 0.054928795]\n",
            "2022-02-28 16:34:25,227 : INFO : topic #3 (0.025): 0.006*\"query\" + 0.005*\"concept\" + 0.005*\"hypothesis\" + 0.005*\"instance\" + 0.005*\"class\" + 0.004*\"sample\" + 0.004*\"learner\" + 0.003*\"bound\" + 0.003*\"xi\" + 0.003*\"wavelet\"\n",
            "2022-02-28 16:34:25,229 : INFO : topic #8 (0.030): 0.012*\"action\" + 0.012*\"control\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"dynamic\" + 0.004*\"environment\" + 0.004*\"reward\"\n",
            "2022-02-28 16:34:25,232 : INFO : topic #7 (0.044): 0.009*\"speech\" + 0.008*\"hidden\" + 0.006*\"sequence\" + 0.006*\"net\" + 0.006*\"word\" + 0.005*\"recurrent\" + 0.005*\"recognition\" + 0.005*\"layer\" + 0.004*\"context\" + 0.004*\"architecture\"\n",
            "2022-02-28 16:34:25,234 : INFO : topic #9 (0.055): 0.020*\"image\" + 0.006*\"visual\" + 0.006*\"object\" + 0.006*\"motion\" + 0.006*\"signal\" + 0.005*\"field\" + 0.005*\"position\" + 0.004*\"pixel\" + 0.004*\"filter\" + 0.004*\"map\"\n",
            "2022-02-28 16:34:25,237 : INFO : topic #0 (0.059): 0.009*\"class\" + 0.009*\"hidden\" + 0.008*\"classifier\" + 0.007*\"layer\" + 0.006*\"classification\" + 0.006*\"node\" + 0.005*\"recognition\" + 0.005*\"net\" + 0.005*\"hidden_unit\" + 0.004*\"bound\"\n",
            "2022-02-28 16:34:25,239 : INFO : topic diff=0.062921, rho=0.277350\n",
            "2022-02-28 16:34:25,260 : INFO : PROGRESS: pass 12, at document #1740/1740\n",
            "2022-02-28 16:34:29,586 : INFO : optimized alpha [0.05843323, 0.042229295, 0.040939204, 0.0250942, 0.039594635, 0.043774568, 0.037519228, 0.044196516, 0.030166943, 0.05441054]\n",
            "2022-02-28 16:34:29,601 : INFO : topic #3 (0.025): 0.006*\"query\" + 0.005*\"hypothesis\" + 0.005*\"concept\" + 0.005*\"class\" + 0.005*\"instance\" + 0.004*\"sample\" + 0.003*\"learner\" + 0.003*\"bound\" + 0.003*\"xi\" + 0.003*\"search\"\n",
            "2022-02-28 16:34:29,603 : INFO : topic #8 (0.030): 0.012*\"action\" + 0.012*\"control\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"dynamic\" + 0.004*\"environment\" + 0.004*\"robot\"\n",
            "2022-02-28 16:34:29,605 : INFO : topic #7 (0.044): 0.009*\"speech\" + 0.008*\"hidden\" + 0.006*\"sequence\" + 0.006*\"word\" + 0.006*\"net\" + 0.005*\"recurrent\" + 0.005*\"recognition\" + 0.005*\"layer\" + 0.005*\"context\" + 0.004*\"architecture\"\n",
            "2022-02-28 16:34:29,607 : INFO : topic #9 (0.054): 0.020*\"image\" + 0.006*\"visual\" + 0.006*\"object\" + 0.006*\"motion\" + 0.006*\"signal\" + 0.005*\"field\" + 0.005*\"position\" + 0.005*\"pixel\" + 0.004*\"filter\" + 0.004*\"face\"\n",
            "2022-02-28 16:34:29,611 : INFO : topic #0 (0.058): 0.010*\"class\" + 0.009*\"hidden\" + 0.009*\"classifier\" + 0.007*\"layer\" + 0.007*\"classification\" + 0.006*\"node\" + 0.005*\"net\" + 0.005*\"recognition\" + 0.005*\"hidden_unit\" + 0.004*\"tree\"\n",
            "2022-02-28 16:34:29,613 : INFO : topic diff=0.059688, rho=0.267261\n",
            "2022-02-28 16:34:29,640 : INFO : PROGRESS: pass 13, at document #1740/1740\n",
            "2022-02-28 16:34:33,946 : INFO : optimized alpha [0.057938434, 0.04203103, 0.041127086, 0.02498474, 0.040363785, 0.04355894, 0.03741904, 0.044285927, 0.02994738, 0.053995214]\n",
            "2022-02-28 16:34:33,963 : INFO : topic #3 (0.025): 0.006*\"query\" + 0.005*\"hypothesis\" + 0.005*\"concept\" + 0.005*\"class\" + 0.005*\"instance\" + 0.004*\"sample\" + 0.003*\"learner\" + 0.003*\"bound\" + 0.003*\"search\" + 0.003*\"xi\"\n",
            "2022-02-28 16:34:33,965 : INFO : topic #8 (0.030): 0.013*\"action\" + 0.012*\"control\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"dynamic\" + 0.004*\"environment\" + 0.004*\"robot\"\n",
            "2022-02-28 16:34:33,971 : INFO : topic #7 (0.044): 0.009*\"speech\" + 0.008*\"hidden\" + 0.007*\"word\" + 0.006*\"sequence\" + 0.006*\"net\" + 0.006*\"recognition\" + 0.005*\"recurrent\" + 0.005*\"layer\" + 0.005*\"context\" + 0.004*\"architecture\"\n",
            "2022-02-28 16:34:33,974 : INFO : topic #9 (0.054): 0.021*\"image\" + 0.006*\"object\" + 0.006*\"visual\" + 0.006*\"motion\" + 0.006*\"signal\" + 0.005*\"field\" + 0.005*\"position\" + 0.005*\"pixel\" + 0.004*\"filter\" + 0.004*\"face\"\n",
            "2022-02-28 16:34:33,981 : INFO : topic #0 (0.058): 0.010*\"class\" + 0.009*\"hidden\" + 0.009*\"classifier\" + 0.008*\"layer\" + 0.007*\"classification\" + 0.007*\"node\" + 0.005*\"net\" + 0.005*\"recognition\" + 0.005*\"hidden_unit\" + 0.005*\"tree\"\n",
            "2022-02-28 16:34:33,990 : INFO : topic diff=0.056974, rho=0.258199\n",
            "2022-02-28 16:34:34,011 : INFO : PROGRESS: pass 14, at document #1740/1740\n",
            "2022-02-28 16:34:38,284 : INFO : optimized alpha [0.057512887, 0.041879192, 0.04132465, 0.024930162, 0.041183423, 0.043388184, 0.037382845, 0.044384908, 0.029775886, 0.05363985]\n",
            "2022-02-28 16:34:38,302 : INFO : topic #3 (0.025): 0.006*\"query\" + 0.005*\"hypothesis\" + 0.005*\"class\" + 0.005*\"concept\" + 0.005*\"instance\" + 0.004*\"sample\" + 0.003*\"learner\" + 0.003*\"bound\" + 0.003*\"search\" + 0.003*\"selection\"\n",
            "2022-02-28 16:34:38,304 : INFO : topic #8 (0.030): 0.013*\"action\" + 0.012*\"control\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"dynamic\" + 0.005*\"robot\" + 0.005*\"environment\"\n",
            "2022-02-28 16:34:38,310 : INFO : topic #7 (0.044): 0.010*\"speech\" + 0.008*\"hidden\" + 0.007*\"word\" + 0.007*\"sequence\" + 0.006*\"net\" + 0.006*\"recognition\" + 0.005*\"layer\" + 0.005*\"recurrent\" + 0.005*\"context\" + 0.005*\"architecture\"\n",
            "2022-02-28 16:34:38,314 : INFO : topic #9 (0.054): 0.021*\"image\" + 0.007*\"object\" + 0.007*\"visual\" + 0.006*\"motion\" + 0.006*\"signal\" + 0.005*\"field\" + 0.005*\"position\" + 0.005*\"pixel\" + 0.004*\"filter\" + 0.004*\"face\"\n",
            "2022-02-28 16:34:38,318 : INFO : topic #0 (0.058): 0.010*\"class\" + 0.009*\"classifier\" + 0.009*\"hidden\" + 0.008*\"layer\" + 0.007*\"classification\" + 0.007*\"node\" + 0.005*\"net\" + 0.005*\"recognition\" + 0.005*\"hidden_unit\" + 0.005*\"tree\"\n",
            "2022-02-28 16:34:38,322 : INFO : topic diff=0.054570, rho=0.250000\n",
            "2022-02-28 16:34:38,346 : INFO : PROGRESS: pass 15, at document #1740/1740\n",
            "2022-02-28 16:34:42,522 : INFO : optimized alpha [0.057155322, 0.04177536, 0.041532468, 0.024905672, 0.041979726, 0.0432838, 0.037402414, 0.04453605, 0.029650953, 0.053369608]\n",
            "2022-02-28 16:34:42,537 : INFO : topic #3 (0.025): 0.006*\"query\" + 0.005*\"hypothesis\" + 0.005*\"class\" + 0.005*\"concept\" + 0.005*\"instance\" + 0.004*\"sample\" + 0.003*\"search\" + 0.003*\"learner\" + 0.003*\"bound\" + 0.003*\"selection\"\n",
            "2022-02-28 16:34:42,540 : INFO : topic #8 (0.030): 0.013*\"action\" + 0.013*\"control\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"dynamic\" + 0.005*\"robot\" + 0.005*\"environment\"\n",
            "2022-02-28 16:34:42,543 : INFO : topic #7 (0.045): 0.010*\"speech\" + 0.008*\"hidden\" + 0.007*\"word\" + 0.007*\"sequence\" + 0.006*\"net\" + 0.006*\"recognition\" + 0.006*\"layer\" + 0.005*\"recurrent\" + 0.005*\"context\" + 0.005*\"architecture\"\n",
            "2022-02-28 16:34:42,546 : INFO : topic #9 (0.053): 0.021*\"image\" + 0.007*\"object\" + 0.007*\"visual\" + 0.006*\"motion\" + 0.006*\"signal\" + 0.006*\"field\" + 0.005*\"position\" + 0.005*\"pixel\" + 0.004*\"filter\" + 0.004*\"face\"\n",
            "2022-02-28 16:34:42,553 : INFO : topic #0 (0.057): 0.010*\"class\" + 0.009*\"classifier\" + 0.009*\"hidden\" + 0.008*\"layer\" + 0.007*\"classification\" + 0.007*\"node\" + 0.005*\"net\" + 0.005*\"tree\" + 0.005*\"recognition\" + 0.005*\"hidden_unit\"\n",
            "2022-02-28 16:34:42,556 : INFO : topic diff=0.052453, rho=0.242536\n",
            "2022-02-28 16:34:42,582 : INFO : PROGRESS: pass 16, at document #1740/1740\n",
            "2022-02-28 16:34:46,889 : INFO : optimized alpha [0.056807715, 0.04168631, 0.041723415, 0.024914691, 0.0428043, 0.043221675, 0.03747136, 0.044731174, 0.029552968, 0.053129237]\n",
            "2022-02-28 16:34:46,912 : INFO : topic #3 (0.025): 0.006*\"query\" + 0.005*\"hypothesis\" + 0.005*\"class\" + 0.005*\"concept\" + 0.005*\"instance\" + 0.004*\"sample\" + 0.003*\"search\" + 0.003*\"learner\" + 0.003*\"bound\" + 0.003*\"selection\"\n",
            "2022-02-28 16:34:46,914 : INFO : topic #8 (0.030): 0.013*\"control\" + 0.013*\"action\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"dynamic\" + 0.005*\"robot\" + 0.005*\"environment\"\n",
            "2022-02-28 16:34:46,917 : INFO : topic #7 (0.045): 0.010*\"speech\" + 0.008*\"hidden\" + 0.008*\"word\" + 0.007*\"sequence\" + 0.006*\"net\" + 0.006*\"recognition\" + 0.006*\"layer\" + 0.005*\"recurrent\" + 0.005*\"context\" + 0.005*\"architecture\"\n",
            "2022-02-28 16:34:46,920 : INFO : topic #9 (0.053): 0.021*\"image\" + 0.007*\"object\" + 0.007*\"visual\" + 0.006*\"motion\" + 0.006*\"signal\" + 0.006*\"field\" + 0.005*\"position\" + 0.005*\"pixel\" + 0.004*\"filter\" + 0.004*\"face\"\n",
            "2022-02-28 16:34:46,922 : INFO : topic #0 (0.057): 0.010*\"class\" + 0.010*\"classifier\" + 0.009*\"hidden\" + 0.008*\"layer\" + 0.007*\"classification\" + 0.007*\"node\" + 0.006*\"net\" + 0.005*\"tree\" + 0.005*\"hidden_unit\" + 0.005*\"recognition\"\n",
            "2022-02-28 16:34:46,925 : INFO : topic diff=0.050586, rho=0.235702\n",
            "2022-02-28 16:34:46,956 : INFO : PROGRESS: pass 17, at document #1740/1740\n",
            "2022-02-28 16:34:51,159 : INFO : optimized alpha [0.05649157, 0.04162467, 0.041931264, 0.02494741, 0.04367301, 0.043192144, 0.03759055, 0.04495487, 0.029483322, 0.052949004]\n",
            "2022-02-28 16:34:51,177 : INFO : topic #3 (0.025): 0.006*\"query\" + 0.005*\"hypothesis\" + 0.005*\"class\" + 0.005*\"concept\" + 0.005*\"instance\" + 0.004*\"sample\" + 0.004*\"search\" + 0.003*\"bound\" + 0.003*\"learner\" + 0.003*\"loss\"\n",
            "2022-02-28 16:34:51,179 : INFO : topic #8 (0.029): 0.013*\"control\" + 0.013*\"action\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"dynamic\" + 0.005*\"robot\" + 0.005*\"environment\"\n",
            "2022-02-28 16:34:51,183 : INFO : topic #7 (0.045): 0.010*\"speech\" + 0.009*\"hidden\" + 0.008*\"word\" + 0.007*\"sequence\" + 0.006*\"recognition\" + 0.006*\"net\" + 0.006*\"layer\" + 0.005*\"recurrent\" + 0.005*\"context\" + 0.005*\"architecture\"\n",
            "2022-02-28 16:34:51,192 : INFO : topic #9 (0.053): 0.022*\"image\" + 0.007*\"object\" + 0.007*\"visual\" + 0.006*\"motion\" + 0.006*\"signal\" + 0.006*\"field\" + 0.005*\"position\" + 0.005*\"pixel\" + 0.004*\"filter\" + 0.004*\"face\"\n",
            "2022-02-28 16:34:51,194 : INFO : topic #0 (0.056): 0.010*\"class\" + 0.010*\"classifier\" + 0.009*\"hidden\" + 0.008*\"layer\" + 0.008*\"classification\" + 0.007*\"node\" + 0.006*\"net\" + 0.005*\"tree\" + 0.005*\"hidden_unit\" + 0.005*\"recognition\"\n",
            "2022-02-28 16:34:51,199 : INFO : topic diff=0.048837, rho=0.229416\n",
            "2022-02-28 16:34:51,225 : INFO : PROGRESS: pass 18, at document #1740/1740\n",
            "2022-02-28 16:34:55,412 : INFO : optimized alpha [0.056195334, 0.041617364, 0.0421446, 0.024998026, 0.044579208, 0.043229364, 0.037711646, 0.045217603, 0.029439304, 0.05281319]\n",
            "2022-02-28 16:34:55,427 : INFO : topic #3 (0.025): 0.006*\"query\" + 0.005*\"hypothesis\" + 0.005*\"class\" + 0.005*\"concept\" + 0.005*\"instance\" + 0.004*\"sample\" + 0.004*\"search\" + 0.003*\"loss\" + 0.003*\"bound\" + 0.003*\"learner\"\n",
            "2022-02-28 16:34:55,429 : INFO : topic #8 (0.029): 0.013*\"control\" + 0.013*\"action\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"dynamic\" + 0.005*\"robot\" + 0.005*\"environment\"\n",
            "2022-02-28 16:34:55,432 : INFO : topic #7 (0.045): 0.010*\"speech\" + 0.009*\"hidden\" + 0.008*\"word\" + 0.007*\"sequence\" + 0.006*\"recognition\" + 0.006*\"net\" + 0.006*\"layer\" + 0.005*\"recurrent\" + 0.005*\"context\" + 0.005*\"architecture\"\n",
            "2022-02-28 16:34:55,435 : INFO : topic #9 (0.053): 0.022*\"image\" + 0.007*\"object\" + 0.007*\"visual\" + 0.007*\"motion\" + 0.006*\"field\" + 0.006*\"signal\" + 0.005*\"position\" + 0.005*\"pixel\" + 0.005*\"filter\" + 0.004*\"face\"\n",
            "2022-02-28 16:34:55,440 : INFO : topic #0 (0.056): 0.010*\"class\" + 0.010*\"classifier\" + 0.009*\"hidden\" + 0.008*\"layer\" + 0.008*\"classification\" + 0.007*\"node\" + 0.006*\"net\" + 0.005*\"tree\" + 0.005*\"hidden_unit\" + 0.005*\"recognition\"\n",
            "2022-02-28 16:34:55,443 : INFO : topic diff=0.047274, rho=0.223607\n",
            "2022-02-28 16:34:55,468 : INFO : PROGRESS: pass 19, at document #1740/1740\n",
            "2022-02-28 16:34:59,618 : INFO : optimized alpha [0.055924546, 0.041647054, 0.04236863, 0.025065249, 0.045489606, 0.043276664, 0.037865996, 0.045509484, 0.029426556, 0.052716494]\n",
            "2022-02-28 16:34:59,632 : INFO : topic #3 (0.025): 0.006*\"query\" + 0.005*\"hypothesis\" + 0.005*\"class\" + 0.005*\"instance\" + 0.005*\"concept\" + 0.004*\"sample\" + 0.004*\"search\" + 0.003*\"loss\" + 0.003*\"bound\" + 0.003*\"selection\"\n",
            "2022-02-28 16:34:59,635 : INFO : topic #8 (0.029): 0.013*\"control\" + 0.013*\"action\" + 0.010*\"policy\" + 0.008*\"reinforcement\" + 0.006*\"optimal\" + 0.006*\"controller\" + 0.005*\"reinforcement_learning\" + 0.005*\"dynamic\" + 0.005*\"robot\" + 0.005*\"environment\"\n",
            "2022-02-28 16:34:59,638 : INFO : topic #7 (0.046): 0.010*\"speech\" + 0.009*\"hidden\" + 0.008*\"word\" + 0.007*\"sequence\" + 0.007*\"recognition\" + 0.006*\"net\" + 0.006*\"layer\" + 0.005*\"recurrent\" + 0.005*\"context\" + 0.005*\"architecture\"\n",
            "2022-02-28 16:34:59,641 : INFO : topic #9 (0.053): 0.022*\"image\" + 0.007*\"object\" + 0.007*\"visual\" + 0.007*\"motion\" + 0.006*\"field\" + 0.006*\"signal\" + 0.005*\"position\" + 0.005*\"pixel\" + 0.005*\"filter\" + 0.004*\"face\"\n",
            "2022-02-28 16:34:59,643 : INFO : topic #0 (0.056): 0.011*\"class\" + 0.010*\"classifier\" + 0.009*\"hidden\" + 0.008*\"layer\" + 0.008*\"classification\" + 0.007*\"node\" + 0.006*\"net\" + 0.005*\"tree\" + 0.005*\"hidden_unit\" + 0.005*\"recognition\"\n",
            "2022-02-28 16:34:59,649 : INFO : topic diff=0.045794, rho=0.218218\n"
          ]
        }
      ],
      "source": [
        "# Train LDA model.\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Set training parameters.\n",
        "num_topics = 10\n",
        "chunksize = 2000\n",
        "passes = 20\n",
        "iterations = 400\n",
        "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "# Make a index to word dictionary.\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        "    eval_every=eval_every\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4IDUbAygft9"
      },
      "source": [
        "We can compute the topic coherence of each topic. Below we display the\n",
        "average topic coherence and print the topics in order of topic coherence.\n",
        "\n",
        "Note that we use the \"Umass\" topic coherence measure here (see\n",
        ":py:func:`gensim.models.ldamodel.LdaModel.top_topics`), Gensim has recently\n",
        "obtained an implementation of the \"AKSW\" topic coherence measure (see\n",
        "accompanying blog post, http://rare-technologies.com/what-is-topic-coherence/).\n",
        "\n",
        "If you are familiar with the subject of the articles in this dataset, you can\n",
        "see that the topics below make a lot of sense. However, they are not without\n",
        "flaws. We can see that there is substantial overlap between some topics,\n",
        "others are hard to interpret, and most of them have at least some terms that\n",
        "seem out of place. If you were able to do better, feel free to share your\n",
        "methods on the blog at http://rare-technologies.com/lda-training-tips/ !\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa3_6XDbgft-",
        "outputId": "f76b0afb-507e-4da5-a75b-fd2f82fef1d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-02-28 16:34:59,809 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average topic coherence: -1.1091.\n",
            "[([(0.009576393, 'mixture'),\n",
            "   (0.009283968, 'gaussian'),\n",
            "   (0.007880073, 'likelihood'),\n",
            "   (0.007458933, 'density'),\n",
            "   (0.0064788484, 'prior'),\n",
            "   (0.005588813, 'bayesian'),\n",
            "   (0.0055426494, 'log'),\n",
            "   (0.0054305918, 'em'),\n",
            "   (0.005244151, 'estimate'),\n",
            "   (0.0052047973, 'posterior'),\n",
            "   (0.0049914788, 'matrix'),\n",
            "   (0.004475818, 'approximation'),\n",
            "   (0.00396063, 'component'),\n",
            "   (0.0038306671, 'estimation'),\n",
            "   (0.0037532405, 'variance'),\n",
            "   (0.0035808128, 'maximum'),\n",
            "   (0.003568433, 'hidden'),\n",
            "   (0.0035629566, 'sample'),\n",
            "   (0.0035503146, 'noise'),\n",
            "   (0.0035355003, 'stochastic')],\n",
            "  -0.7490241258086305),\n",
            " ([(0.018136544, 'cell'),\n",
            "   (0.014854553, 'neuron'),\n",
            "   (0.008870185, 'response'),\n",
            "   (0.008041935, 'stimulus'),\n",
            "   (0.0069554406, 'activity'),\n",
            "   (0.006901469, 'spike'),\n",
            "   (0.005323617, 'synaptic'),\n",
            "   (0.0051219277, 'frequency'),\n",
            "   (0.005050761, 'firing'),\n",
            "   (0.004970868, 'cortex'),\n",
            "   (0.0047788955, 'visual'),\n",
            "   (0.0045844163, 'signal'),\n",
            "   (0.0043326705, 'connection'),\n",
            "   (0.0040231864, 'cortical'),\n",
            "   (0.003928138, 'orientation'),\n",
            "   (0.0038037726, 'layer'),\n",
            "   (0.003579243, 'field'),\n",
            "   (0.003462137, 'fig'),\n",
            "   (0.0033394676, 'potential'),\n",
            "   (0.0031076898, 'simulation')],\n",
            "  -0.8282809229504998),\n",
            " ([(0.010510854, 'class'),\n",
            "   (0.010079343, 'classifier'),\n",
            "   (0.009237572, 'hidden'),\n",
            "   (0.008148294, 'layer'),\n",
            "   (0.007967092, 'classification'),\n",
            "   (0.007432267, 'node'),\n",
            "   (0.0058999853, 'net'),\n",
            "   (0.0053712283, 'tree'),\n",
            "   (0.004924952, 'hidden_unit'),\n",
            "   (0.004649373, 'recognition'),\n",
            "   (0.0043531745, 'character'),\n",
            "   (0.0042584767, 'training_set'),\n",
            "   (0.0041201506, 'bound'),\n",
            "   (0.0037715607, 'sample'),\n",
            "   (0.0037120252, 'decision'),\n",
            "   (0.0035328588, 'generalization'),\n",
            "   (0.00334189, 'threshold'),\n",
            "   (0.0032677662, 'dimension'),\n",
            "   (0.003147673, 'trained'),\n",
            "   (0.0026670329, 'complexity')],\n",
            "  -0.9800101915312888),\n",
            " ([(0.009917772, 'speech'),\n",
            "   (0.008788415, 'hidden'),\n",
            "   (0.008147262, 'word'),\n",
            "   (0.00684974, 'sequence'),\n",
            "   (0.0066294163, 'recognition'),\n",
            "   (0.0063475245, 'net'),\n",
            "   (0.00601249, 'layer'),\n",
            "   (0.0054562483, 'recurrent'),\n",
            "   (0.0049686353, 'context'),\n",
            "   (0.0048521548, 'architecture'),\n",
            "   (0.0044640396, 'trained'),\n",
            "   (0.0044534053, 'signal'),\n",
            "   (0.0039044772, 'hidden_unit'),\n",
            "   (0.0037619441, 'language'),\n",
            "   (0.003685568, 'back'),\n",
            "   (0.0035954206, 'speaker'),\n",
            "   (0.0035788298, 'propagation'),\n",
            "   (0.0028624795, 'rule'),\n",
            "   (0.0028584315, 'dynamic'),\n",
            "   (0.002802592, 'connectionist')],\n",
            "  -0.9885763669057317),\n",
            " ([(0.016142752, 'neuron'),\n",
            "   (0.0105571365, 'circuit'),\n",
            "   (0.008677442, 'memory'),\n",
            "   (0.0082217585, 'chip'),\n",
            "   (0.007971411, 'analog'),\n",
            "   (0.004991862, 'voltage'),\n",
            "   (0.0045660553, 'threshold'),\n",
            "   (0.0045206686, 'connection'),\n",
            "   (0.0043581147, 'bit'),\n",
            "   (0.0041242307, 'dynamic'),\n",
            "   (0.0036943166, 'vlsi'),\n",
            "   (0.0034139133, 'implementation'),\n",
            "   (0.003349626, 'gate'),\n",
            "   (0.0032879317, 'fig'),\n",
            "   (0.0031597125, 'signal'),\n",
            "   (0.0031342946, 'activation'),\n",
            "   (0.003042488, 'hopfield'),\n",
            "   (0.0030121247, 'noise'),\n",
            "   (0.0029522656, 'node'),\n",
            "   (0.002881006, 'element')],\n",
            "  -1.0709171945402338),\n",
            " ([(0.0076562627, 'rule'),\n",
            "   (0.0072873775, 'object'),\n",
            "   (0.0061436873, 'cluster'),\n",
            "   (0.005289508, 'constraint'),\n",
            "   (0.00523646, 'node'),\n",
            "   (0.005161035, 'image'),\n",
            "   (0.0050173094, 'distance'),\n",
            "   (0.004342308, 'clustering'),\n",
            "   (0.0040725996, 'graph'),\n",
            "   (0.003769382, 'optimization'),\n",
            "   (0.0036283254, 'map'),\n",
            "   (0.003360937, 'view'),\n",
            "   (0.0033317877, 'matching'),\n",
            "   (0.0033299248, 'rbf'),\n",
            "   (0.0033044727, 'layer'),\n",
            "   (0.00320919, 'basis'),\n",
            "   (0.003183303, 'solution'),\n",
            "   (0.0030320603, 'field'),\n",
            "   (0.0029286598, 'region'),\n",
            "   (0.002920093, 'dimensional')],\n",
            "  -1.1400272907348912),\n",
            " ([(0.022116862, 'image'),\n",
            "   (0.007474759, 'object'),\n",
            "   (0.0070877303, 'visual'),\n",
            "   (0.0066309334, 'motion'),\n",
            "   (0.0058289645, 'field'),\n",
            "   (0.0057420717, 'signal'),\n",
            "   (0.0051379655, 'position'),\n",
            "   (0.005052504, 'pixel'),\n",
            "   (0.004570099, 'filter'),\n",
            "   (0.004378661, 'face'),\n",
            "   (0.0041396995, 'direction'),\n",
            "   (0.004128795, 'human'),\n",
            "   (0.0040746587, 'recognition'),\n",
            "   (0.0040716664, 'component'),\n",
            "   (0.003942381, 'target'),\n",
            "   (0.0038089142, 'movement'),\n",
            "   (0.0037288114, 'map'),\n",
            "   (0.003685541, 'subject'),\n",
            "   (0.0036277864, 'location'),\n",
            "   (0.0033443496, 'motor')],\n",
            "  -1.166151930678906),\n",
            " ([(0.0065102815, 'noise'),\n",
            "   (0.0061021117, 'matrix'),\n",
            "   (0.00529259, 'generalization'),\n",
            "   (0.0051999735, 'kernel'),\n",
            "   (0.0050675734, 'optimal'),\n",
            "   (0.00472407, 'approximation'),\n",
            "   (0.004630228, 'let'),\n",
            "   (0.004406645, 'regression'),\n",
            "   (0.0043144617, 'solution'),\n",
            "   (0.0037832682, 'theorem'),\n",
            "   (0.003707441, 'gradient'),\n",
            "   (0.0034752034, 'xi'),\n",
            "   (0.0033223184, 'condition'),\n",
            "   (0.0031943037, 'eq'),\n",
            "   (0.0029492714, 'nonlinear'),\n",
            "   (0.002943019, 'estimate'),\n",
            "   (0.0027935214, 'convergence'),\n",
            "   (0.0026609094, 'generalization_error'),\n",
            "   (0.0026271974, 'support'),\n",
            "   (0.0025870148, 'prediction')],\n",
            "  -1.1809219347203728),\n",
            " ([(0.012846725, 'control'),\n",
            "   (0.012612047, 'action'),\n",
            "   (0.00957371, 'policy'),\n",
            "   (0.008248018, 'reinforcement'),\n",
            "   (0.006379507, 'optimal'),\n",
            "   (0.0063245804, 'controller'),\n",
            "   (0.005465683, 'reinforcement_learning'),\n",
            "   (0.004999186, 'dynamic'),\n",
            "   (0.0047998372, 'robot'),\n",
            "   (0.004665052, 'environment'),\n",
            "   (0.0043819044, 'reward'),\n",
            "   (0.00392937, 'goal'),\n",
            "   (0.003594727, 'decision'),\n",
            "   (0.0032668011, 'sutton'),\n",
            "   (0.0032579899, 'td'),\n",
            "   (0.0031328162, 'path'),\n",
            "   (0.003074601, 'trajectory'),\n",
            "   (0.0030555832, 'agent'),\n",
            "   (0.002971138, 'cost'),\n",
            "   (0.002908156, 'trial')],\n",
            "  -1.2404458823953157),\n",
            " ([(0.0058173067, 'query'),\n",
            "   (0.0053479504, 'hypothesis'),\n",
            "   (0.005296462, 'class'),\n",
            "   (0.00476374, 'instance'),\n",
            "   (0.004744872, 'concept'),\n",
            "   (0.0042447285, 'sample'),\n",
            "   (0.0036522162, 'search'),\n",
            "   (0.0034227734, 'loss'),\n",
            "   (0.0033423568, 'bound'),\n",
            "   (0.0033352766, 'selection'),\n",
            "   (0.0032899969, 'learner'),\n",
            "   (0.00314964, 'target'),\n",
            "   (0.0030923204, 'prediction'),\n",
            "   (0.0029792916, 'xi'),\n",
            "   (0.0029403884, 'label'),\n",
            "   (0.0027641358, 'domain'),\n",
            "   (0.0026531264, 'wavelet'),\n",
            "   (0.0026471713, 'positive'),\n",
            "   (0.0026084916, 'best'),\n",
            "   (0.0025599962, 'genetic')],\n",
            "  -1.746357376773224)]\n"
          ]
        }
      ],
      "source": [
        "top_topics = model.top_topics(corpus) #, num_words=20)\n",
        "\n",
        "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
        "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
        "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(top_topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aam57p7mgft-"
      },
      "source": [
        "## Things to experiment with\n",
        "\n",
        "* ``no_above`` and ``no_below`` parameters in ``filter_extremes`` method.\n",
        "* Adding trigrams or even higher order n-grams.\n",
        "* Consider whether using a hold-out set or cross-validation is the way to go for you.\n",
        "* Try other datasets.\n",
        "\n",
        "## Where to go from here\n",
        "\n",
        "* Check out a RaRe blog post on the AKSW topic coherence measure (http://rare-technologies.com/what-is-topic-coherence/).\n",
        "* pyLDAvis (https://pyldavis.readthedocs.io/en/latest/index.html).\n",
        "* Read some more Gensim tutorials (https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#tutorials).\n",
        "* If you haven't already, read [1] and [2] (see references).\n",
        "\n",
        "## References\n",
        "\n",
        "1. \"Latent Dirichlet Allocation\", Blei et al. 2003.\n",
        "2. \"Online Learning for Latent Dirichlet Allocation\", Hoffman et al. 2010.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "nlp_lda_topic_modeling_gensim_neurips_papers_20220228.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}